[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestatística Usando o R",
    "section": "",
    "text": "Prefácio\nEste livro foi escrito com o objetivo de consultoria educacional. Aqui você encontrará uma introdução à Bioestatística usando a linguagem R, salientando os assuntos que se constituem em uma instrumentação básica para a análise de dados na área da saúde.\nEste livro tem a ambição de ser um pouco mais tolerável e amistoso, a fim de estimular o estudo da Bioestatística e, assim, facilitar a crítica da literatura científica e , desta forma, ajudar a evitar a intoxicação causada pela pseudociência.\nPetrônio Fagundes de Oliveira Filho\nemail: petronioliveira@gmail.com\nWhatsApp: +55 54 9997 15499",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Importância da Bioestatística\nOs indivíduos variam em relação as suas características biológicas, psicológicas e sociais na saúde e na doença. Esta variabilidade gera uma grande quantidade de incertezas.\nA Bioestatística, estatística aplicada às ciências biológicas e da saúde, é a ferramenta utilizada pelos pesquisadores para trabalhar com essas incertezas advindas da variabilidade. Várias definições foram escritas para a estatística, uma delas é a seguinte (1):\nA Bioestatística lida com a variabilidade humana utilizando técnicas estatísticas quantitativas (2) que ajudam a diminuir a ignorância em relação a esta diversidade. A compreensão da variabilidade humana torna a medicina mais ciência, diminuindo as incertezas, na tentativa de verificar se os resultados encontrados de fato existem ou são apenas obra do acaso.\nNa década de 1990, houve um acesso maior aos computadores. Os profissionais da saúde não estatísticos passaram a ter mais interesse no campo da bioestatística. Isto gerou uma onda que facilitou o aparecimento de novas ferramentas estatísticas de ponta. Apesar disso, o conhecimento da Bioestatística permanece restrito aos especialistas na área.\nNos últimos anos, os pacotes de softwares foram aprimorados, tornando-se mais amigáveis e diminuindo significativamente o pânico ao se defrontar com uma série de números uma vez que a maioria deles exige apenas conhecimento básico de matemática.\nPara a tomada de decisão em saúde é fundamental o acúmulo de conhecimento adquirido através da prática clínica, geradora da experiência do profissional, do intercâmbio com os pares e da análise adequada das evidências científicas publicadas em periódicos de qualidade. Para atingir este objetivo, é fundamental o conhecimento de bioestatística, incluindo aqui que o pensamento que deve nortear os profissionais da saúde ao lidar com o ser humano é o pensamento probabilístico.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#importância-da-bioestatística",
    "href": "intro.html#importância-da-bioestatística",
    "title": "1  Introdução",
    "section": "",
    "text": "Estatística é a disciplina interessada com o tratamento dos dados numéricos obtidos a partir de grupos de indivíduos",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-historia",
    "href": "intro.html#sec-historia",
    "title": "1  Introdução",
    "section": "1.2 Pílulas históricas da Estatística",
    "text": "1.2 Pílulas históricas da Estatística\n\n A história deve começar em algum lugar, mas a história não tem começo (3) \n\nEntretanto, é natural, que se trace as raízes voltando ao passado, tanto quanto possível. Alguns referem-se à curiosidade em relação ao registro de dados à dinastia Shank, na China, possivelmente no século XIII a.c, com a realização de censos populacionais. Há relatos bíblicos de possíveis censos realizados por Moisés (1491 a.C.) e por Davi (1017 a.C.).\nOs romanos e os gregos já realizavam censos por volta do século VIII a IV a.C. Em 578-534 a.C., o imperador Servo Túlio mandou realizar um censo de população masculina adulta e suas propriedades que serviu para estabelecer o recrutamento para o exército, para o exercício dos direitos políticos e para o pagamento de impostos. Os romanos fizeram 72 censos entre 555 a.C. e 72 d.C. A punição para quem não respondia, geralmente era a morte! Na Idade Média, na Europa, existem registros de diversos censos: durante o domínio muçulmano, na Península Ibérica, nos séculos VII a XV; no reinado de Carlos Magno (712-814) e ainda o maior registro estatístico feito na época, o Domesday Book (Figura 1.1), realizado na Inglaterra, por Guilherme I (3) , o Conquistador, onde registravam nascimentos, mortes, batismos e casamentos. Houve, também, recenseamentos nas repúblicas italianas no século XII ao XIII (4).\n\n\n\n\n\n\n\n\nFigura 1.1: Domesday Book\n\n\n\n\n\nJohn Graunt (24/04/1620 - 18/04/1674) foi um cientista britânico a quem se deve vários estudos demográficos ingleses. Foi o precursor da construção de Tábuas de Mortalidade. Realizou estudos com William Petty (1623 - 1687), economista britânico que propôs a aritmética política.\nEm 1791, Sir John Sinclair (1754 - 1835) concebeu um plano de uma pesquisa empírica na Escócia para fornecer informações estatísticas. Foi a primeira vez que o termo estatística foi usado em inglês.\nGirolamo Cardano (24/09/1501 - 21/09/1576) foi um médico, matemático, físico e filósofo italiano. É tido como o primeiro a introduzir ideias gerais da teoria das equações algébricas e as primeiras regras da probabilidade, descritas no livro Liber de Ludo Aleae, publicado em 1663. Descreveu pela primeira vez a clínica da febre tifoide. Foi amigo de Leonardo da Vinci.\nPierre-Simon Laplace, Marquês Laplace (23/03/1749 - 05/03/1927) foi um matemático, astrônomo e físico francês. Embora conduzisse pesquisas substanciais sobre física, outro tema principal dos esforços de sua vida foi a teoria das probabilidades. Em seu Essai philosophique sur les probabilités, Laplace projetou um sistema matemático de raciocínio indutivo baseado em probabilidades, que hoje coincidem com as ideias bayesianas.\nAntoine Gombaud, conhecido como Chevalier de Méré (1607 - 1684) foi um nobre e jogador. Como não tinha mais sucesso nos jogos de azar, buscou ajuda de Blaise Pascal (19/06/1623 – 19/08/1662), matemático, físico francês, que se correspondeu com Pierre Fermat (matemático e cientista francês), nascendo desta colaboração a teoria matemática das probabilidades (1812). Blaise Pascal foi mais tarde chamado de o Pai da Teoria das Probabilidades.\nA moderna teoria das probabilidades foi atribuída a Abraham De Moivre (25/05/1667 – 27/11/1754), matemático francês, que adquiriu fama por seus estudos na trigonometria, teoria das probabilidades e pela equação da curva normal. Em 1742, Thomas Bayes (1701 – 07/04/1761, matemático e pastor presbiteriano, inglês, desenvolveu o Teorema de Bayes que descreve a probabilidade de um evento ocorrer, baseado em um conhecimento a priori.\nAdrien-Marie Legendre (18/09/1752 - 10/01/1833) foi um matemático francês. Em 1783, tornou-se membro adjunto da Academie des Sciences, instituição que esteve na vanguarda dos desenvolvimentos científicos dos séculos XVII e XVIII. Fez importantes contribuições à estatística, à teoria dos números e à álgebra abstrata.\nJohann Carl Friedrich Gauss (30/04/1777 - 23/02/1855) foi um matemático, astrônomo e físico alemão (Figura 1.2) que contribuiu em diversas áreas das ciências como teoria dos números, estatística, geometria diferencial, eletrostática, astronomia e ótica. Muitos referem-se a ele como o Príncipe da Matemática, o mais notável dos matemáticos. Descobriu o método dos mínimos quadrados e a lei de Gauss da distribuição normal de erros e sua curva em formato de sino, hoje tão familiar para todos que trabalham com estatística.\n\n\n\n\n\n\n\n\nFigura 1.2: Johann Carl Friedrich Gauss\n\n\n\n\n\nLambert Adolphe Jacques Quételet (22/02/1796 - 17/02/1874) foi um astrônomo, matemático, demógrafo e estatístico francês. Seu trabalho se concentrou em estatística social, criando regras de determinação de propensão ao crime\nFrancis Galton (16/02/1822 – 17/01/1911) foi um antropólogo, matemático e estatístico inglês. Entre muitos artigos e livros, criou o conceito estatístico de correlação e da regressão à média. Ele foi o primeiro a aplicar métodos estatísticos para o estudo das diferenças e herança humanas de inteligência. Criou o conceito de eugenia e afirmava que era possível a melhoria da espécie por seleção artificial. Acreditava que a raça humana poderia ser melhorada caso fossem evitados relacionamentos indesejáveis. Isto acompanhava o pensamento burguês europeu da época. Criou a psicometria, onde desenvolveu testes de inteligência para selecionar homens e mulheres brilhantes. Esta teoria teve papel importante na formação do fascismo e nazismo (5).\nWilliam Farr (30/11/1807 - 14/04/1883) foi um médico sanitarista e estatístico inglês, nascido na vila de Kenley, Shropshire. Foi o primeiro investigador a examinar séries temporais de morbimortalidade para longos períodos e, assim, considerado o criador da Estatística da Saúde Pública Moderna. Seus relatórios foram fundamentais para o desencadeamento das reformas sanitárias britânicas, em meados e final do século XIX (6).\nFlorence Nightingale (12/05/1820 – 13/08/1910) foi uma enfermeira (Figura 1.3) que ficou famosa por ser pioneira no tratamento de feridos, durante a Guerra da Criméia (7). Ficou conhecida na história pelo apelido de “A dama da lâmpada”, pelo fato de servir-se de uma lamparina para auxiliar no cuidado aos feridos durante a noite. Também contribuiu no campo da Estatística, sendo pioneira na utilização de métodos de representação visual de informações, como por exemplo gráfico de setores (habitualmente conhecido como gráfico do tipo “pizza”)\n\n\n\n\n\n\n\n\nFigura 1.3: Florence Nightingale\n\n\n\n\n\nJohn Snow (York, 15/03/1813 - Londres, 15/03/1858) foi um médico inglês (Figura 1.4)), considerado pai da Epidemiologia Moderna. Recebeu, em 1853, o título de Sir após ter anestesiado a rainha Vitória no parto sem dor de seu oitavo filho, Leopoldo de Albany. Este fato ajudou a divulgar a técnica entre os médicos da época. Demonstrou que a cólera era causada pelo consumo de águas contaminadas com matérias fecais, ao comprovar que os casos dessa doença se agrupavam em determinados locais da cidade de Londres, em 1854, onde havia fontes dessas águas (6).\n\n\n\n\n\n\n\n\nFigura 1.4: John Snow\n\n\n\n\n\nKarl Pearson (27/03/1857 - 27/04/1936) foi um importante estatístico inglês, fundador do Departamento de Estatística Aplicada da University College London em 1911. Juntamente com Weldon e Galton fundou, em 1901, a revista Biometrika com o objetivo era desenvolver as teorias estatísticas, editada até os dias de hoje. O trabalho de Pearson como estatístico fundamentou muitos métodos estatísticos de uso comum, nos dias atuais: regressão linear e o coeficiente de correlação, teste do qui-quadrado de Pearson, classificação das distribuições (8).\nCharles Edward Spearman (10/09/1863 - 17/09/1945) foi um psicólogo inglês conhecido pelo seu trabalho na área da estatística, como um pioneiro da análise fatorial e pelo coeficiente de correlação de postos de Spearman. Ele também fez bons trabalhos de modelos da inteligência humana.\nWilliam Sealy Gosset (13/07/1876 - 16/10/1937) foi um químico e estatístico inglês (Figura 1.5)). Em 1907, enquanto trabalhava químico da cervejaria experimental de Arthur Guinness & Son, criou a distribuição t que usou para identificar a melhor variedade de cevada, trabalhando com pequenas amostras. A cervejaria Guinness tinha uma política que proibia que seus empregados publicassem suas descobertas em seu próprio nome. Ele, então, usou o pseudônimo “Student” e o teste é chamado “t de Student” em sua homenagem (9).\n\n\n\n\n\n\n\n\nFigura 1.5: William Sealy Gosset\n\n\n\n\n\nRonald Aylmer Fisher (17/02/1890 - 29/07/1962) foi um estatístico, biólogo e geneticista inglês. Em 1919, Fisher se envolveu com pesquisa agrícola no centro de experimentos de Rothamsted Research, em Harpenden, Inglaterra, e desenvolveu novas metodologias e teoria no ramo de experimentos (10). Durante sua vida, Fisher (Figura 1.6) escreveu 7 livros e publicou cerca de 400 artigos acadêmicos em estatística e genética . Em um dos seus livros, The design of Experiments (1935), Fisher relata um experimento que surgiu de uma pergunta curiosa: o gosto do chá muda de acordo com a ordem em que as ervas e o leite são colocados? Essa simples questão resultou em um estudo pioneiro na área e serviu de sustentação para análise da aleatorização de dados experimentais (9). Ronald A. Fisher foi descrito (11) como “um gênio que criou praticamente sozinho os fundamentos para o moderno pensamento estatístico”. Era muito temperamental. Seus atritos com outros estatísticos ficaram famosos, entre eles encontra-se ninguém menos do que Karl Pearson, outro notável estatístico.\n\n\n\n\n\n\n\n\nFigura 1.6: Ronald A. Fisher\n\n\n\n\n\nAustin Bradford Hill (08/07/1897 - 18 /04/1991) foi um epidemiologista e estatístico inglês (Figura 1.7)), pioneiro no estudo do acaso nos ensaios clínicos e, juntamente com Richard Doll, foi o primeiro a demonstrar a ligação entre o uso do cigarro e o câncer de pulmão. Hill é amplamente conhecido pelos Critérios de Hill, conjunto de critérios para a determinação de uma associação causal (12).\n\n\n\n\n\n\n\n\nFigura 1.7: Bradford Hill\n\n\n\n\n\nJohn Wilder Tukey (16/06/1915 - 26/07/2000) foi um estatístico norte-americano. Desenvolveu uma filosofia para a análise de dados que mudou a maneira de pensar dos estatísticos, sugerindo que se faça uma visualização dos dados, interpretando o formato, centro, dispersão, presença de valores atípicos, sumarizar numericamente e por fim escolher um modelo matemático. Foi o criador do boxplot e introduziu a palavra “bit” como uma contração do termo binary digit.\nDouglas G. Altman (12 /07/1948 - 03/06/2018) foi um estatístico inglês (Figura 1.8)), conhecido por seu trabalho em melhorar a confiabilidade dos artigos de pesquisa médica (13) e por artigos altamente citados sobre metodologia estatística. Ele foi professor de estatística em medicina na Universidade de Oxford. Há praticamente 30 anos, Altman (14) escreveu um artigo sobre problema da qualidade da pesquisa em medicina que causou um grande impacto e permanece válido até hoje. Nesta publicação ele afirma:\n\n A má qualidade de muitas pesquisas médicas é amplamente reconhecida, mas, de forma perturbadora, os líderes da profissão médica parecem apenas minimamente preocupados com o problema e não fazem nenhum esforço aparente para encontrar uma solução.\n\n\n\n\n\n\n\n\n\nFigura 1.8: Douglas G. Altman",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#história-resumida-do-r",
    "href": "intro.html#história-resumida-do-r",
    "title": "1  Introdução",
    "section": "1.3 História resumida do R",
    "text": "1.3 História resumida do R\nO R é uma linguagem e um ambiente de desenvolvimento voltado fundamentalmente para a computação estatística. Foi inspirado em duas linguagens: S (John Chambers, do Bell Labs) que forneceu a sintaxe e Scheme (Hal Abelson e Gerald Sussman) implementou e forneceu a semântica.\nO nome R provém em parte das iniciais dos criadores, George Ross Ihaka e Robert Gentleman (Figura 1.9), e também de um jogo figurado com a linguagem S. Em 29 de Fevereiro de 2000, o software foi considerado com funcionalidades e estável o suficiente para a versão 1.0.\nO R é um projeto GNU 1. Software Livre significa que os usuários têm liberdade para executar, copiar, distribuir, estudar, alterar e melhorar o software. Foi desenvolvido em um esforço colaborativo de pessoas em vários locais do mundo (15).\nO projeto R fornece uma grande variedade de técnicas estatísticas e gráficas. É uma linguagem e um ambiente similar ao S. A linguagem do S que também é uma linguagem de computador voltada para cálculos estatísticos. Um dos pontos fortes de R é a facilidade com que produções gráficas de qualidade podem ser produzidas. O R é também altamente expansível com o uso dos pacotes, que são bibliotecas para sub-rotinas específicas ou áreas de estudo específicas. Um conjunto de pacotes é incluído com a instalação de R e muito outros estão disponíveis na rede de distribuição do R - Comprehensive R Archive Network (CRAN) (16).\n\n\n\n\n\n\n\n\nFigura 1.9: Robert Gentlemen (E) e George Ross (D)\n\n\n\n\n\nA linguagem R é largamente usada entre estatísticos e analistas de dados para desenvolver softwares de estatística e análise de dados. Pesquisas e levantamentos com profissionais da área da saúde mostram que a popularidade do R aumentou substancialmente nos últimos anos (17).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#sobre-o-autor",
    "href": "intro.html#sobre-o-autor",
    "title": "1  Introdução",
    "section": "1.4 Sobre o autor",
    "text": "1.4 Sobre o autor\nPetrônio Fagundes de Oliveira Filho nasceu em 04/10/1947, em Porto Alegre, Rio Grande do Sul, Brasil. Estudou no Ensino Médio do Colégio do Rosário, em Porto Alegre. Possui graduação em Medicina pela Universidade de Caxias do Sul (UCS), em 1973, residência em Pediatria no Hospital da Criança Conceição, Porto Alegre (1975) e mestrado em Saúde Pública Materno Infantil, Universidade de São Paulo (1998). Em 1980, obteve o Título de Especialista em Pediatria (TEP) e, em 2009, o título de especialista em Estatística Aplicada (UCS). Atuou como Pediatra no INAMPS até 2002 e em consultório privado até hoje. Aposentou-se como professor da Universidade de Caxias do Sul (UCS), em 2019, onde atuou desde 1975, nas áreas de Pediatria, Epidemiologia e Bioestatística, foi coordenador do Serviço e da Residência Médica em Pediatria, chefe de Departamento, coordenador do curso de Medicina e diretor de Ensino do Hospital Geral de Caxias do Sul (Hospital de Ensino da Universidade de Caxias do Sul) e membro de Conselho de Ética em Pesquisa da Universidade de Caxias do Sul, ligado ao CONEP (Conselho Nacional de pesquisa). Durante mais de 20 anos fez parte do Núcleo de Consultoria e Epidemiologia do Centro de Ciência da Saúde (UCS). É autor de dois livros: Epidemiologia e Bioestatística: Fundamentos para a leitura crítica (Editora Rubio, 2015/2018 e ,em 2ª edição, 2022) e SPSS - Análise de Dados Biomédicos, em coautoria com Valter Motta (MedBook, 2009). Além disso, participou de dezenas publicações e de capítulos de outros livros. Desde 1976, é casado com Lena Maria Cantergiani Fagundes de Oliveira. Tem duas filhas, Nathalia e Andressa, e dois lindos e inteligentes netos, Gabriel e Felix. Ah, teve um cão shitzu branco, marrom claro e com algumas manchas pretas, Floquinho, que ao acompanhar seus estudos e análises estatísticas, latia toda vez que ele mencionava o nome de Ronald Fisher. Infelizmente, faleceu em 2024 aos 17 anos, deixando um grande vazio…\n\n\n\n\n1. Armitage P, Berry G, Matthews JNS. Statistical methods in medical research. John Wiley & Sons; 2008. \n\n\n2. Massad E, Silveira PSP, Menezes RX de, Ortega NRS. Métodos quantitativos em medicina. Editora Manole Ltda; 2004. \n\n\n3. Kendall MG. Studies in the history of probability and statistics. Where shall the history of statistics begin? Biometrika. 1960;47(3/4):447–9. \n\n\n4. Breve História dos Censos [Internet]. Instituto Nacional de Estatistica. Statistics Portugal; 2014. Disponível em: https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria\n\n\n5. Salgado-Neto G, Salgado A. Sir Francis Galton e os extremos superiores da curva normal. Revista de Ciências Humanas. 2011;45(1):223–39. \n\n\n6. Stolley PD, Lasky T. The Beginnings of Epidemiology. Em: Investigating Disease Patterns. Scientific American Library; 2000. p. 23–49. \n\n\n7. Editors History com. Florence Nightingale. https://www.history.com/topics/womens-history/florence-nightingale-1; \n\n\n8. Moore DS. Topics in Inferency. Em: The basic practice of statistics. W.H. Freeman; 2000. p. 417. \n\n\n9. Salsburg D. Uma senhora toma chá... Em: Uma senhora toma chá. Zahar; 2009. p. 17–23. \n\n\n10. Hald A. Biography of Fisher. Em: A History of Parametric Statistics Inference from Bernoulli to Fisher,1713-1935. John Wiley & Sons; 2007. p. 159–63. \n\n\n11. Kruskal W. The Significance of Fisher: A Review of R.A. Fisher: The Life of a Scientist. Journal of the American Statistical Association [Internet]. 1980;75(372):1019–30. Disponível em: https://doi.org/10.1080/01621459.1980.10477590\n\n\n12. Stolley PD, Lasky T. Lung Cancer: New Methods of Studying Disease. Em: Investigating Disease Patterns. Scientific American Library; 2000. p. 51–79. \n\n\n13. Matthews R, Chalmers I, Rothwell P. Douglas G Altman: statistician, researcher, and driving force behind global initiatives to improve the reliability of health research. British Medical Journal Publishing Group; 2018. \n\n\n14. Altman DG. The scandal of poor medical research. Vol. 308, Bmj. British Medical Journal Publishing Group; 1994. p. 283–4. \n\n\n15. R Core Team. The R Project for Statistical Computing | What is R? Disponível em: https://www.r-project.org/about.html; 2022. \n\n\n16. R Core Team. The R Project for Statistical Computing | CRAN Mirrors. Disponível em: https://cran.r-project.org/mirrors.html; 2022. \n\n\n17. Whitney L et al. R programming language continues to grow in popularity [Internet]. TechRepublic. 2020. Disponível em: https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introdução",
    "section": "",
    "text": "Esta sigla está associada ao animal gnu africano, símbolo de software de distribuição livre, quer dizer is Not Unix, sigla recursiva muito comum entre nerds!↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "02-dados.html",
    "href": "02-dados.html",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "2.1 Variáveis e Dados\nAs pesquisas manuseiam dados referentes às variáveis que estão sendo estudadas. Variável é toda característica ou condição de interesse que pode de ser mensurada ou observada em cada elemento de uma amostra ou população. Como o próprio nome diz, seus valores são passíveis de variar de um indivíduo a outro ou no mesmo indivíduo. Em contraste com a variável, o valor de uma constante é fixo. As variáveis podem ter valores numéricos ou não numéricos. O resultado da mensuração ou observação de uma variável é denominado dado.\nA Tabela 2.1 mostra um conjunto de variáveis e suas medidas (dados) de um grupo de pacientes internados em uma determinada UTI. O termo medida deve ser entendido num sentido amplo, pois não é possível “medir” o sexo (observação) ou o estado geral (critérios) de alguém, ao contrário do peso e da pressão arterial que podem ser mensurados com instrumentos.\nTabela 2.1: Variáveis e dados\n\n\n\n\n\n\n\nId\nNome\nIdade\nSexo\nPAS\nPAD\nEstado Geral\n\n\n\n\n1\nJoão\n45\nmasculino\n140\n90\nbom\n\n\n2\nMaria\n32\nfeminino\n110\n70\nregular\n\n\n3\nPedro\n27\nmasculino\n120\n80\ngrave\n\n\n4\nTeresa\n18\nfeminino\n100\n60\nbom",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#população-e-amostra",
    "href": "02-dados.html#população-e-amostra",
    "title": "2  Natureza dos Dados",
    "section": "2.2 População e Amostra",
    "text": "2.2 População e Amostra\nNa pesquisa em saúde, a não ser quando se realiza um censo, coleta-se dados de um subconjunto de indivíduos denominado de amostra, pertencente a um grupo maior, conhecido como população. A população de interesse é, geralmente, chamada de população-alvo. A amostra, para ser representativa da população, deve ter as mesmas características desta. A partir da análise dos dados encontrados na amostra, deduz-se sobre a população. Este processo é denominado de inferência estatística. O interesse na amostra não está propriamente nela, mas na informação que ela fornece ao investigador sobre a população de onde ela provém. A amostra fornece estimativas (estatísticas) da população (Figura 2.1).\n\n População ou população-alvo consiste em todos os elementos (indivíduos, itens, objetos) cujas características estão sendo estudadas.\nAmostra é a parte, subconjunto, da população selecionada para estudo. \n\nEm decorrência do acaso, diferentes amostras de uma mesma população fornecem resultados diferentes. Este fato deve ser levado em consideração ao usar uma amostra para fazer inferência sobre uma população. Este fenômeno é denominado de variação amostral ou erro amostral (Consulte também o Capítulo 9) e é a essência da estatística. O grau de certeza na inferência estatística depende da representatividade da amostra.\nO processo de obtenção da amostra é chamado de amostragem. Mesmo que este processo seja adequado, a amostra nunca será uma cópia perfeita da população de onde ela foi extraída. Desta forma, em qualquer conclusão baseada em dados de uma amostra, sempre haverá o erro amostral. Este erro deve ser tratado estatisticamente tendo em mente a teoria da amostragem, baseada em probabilidades.\n\n\n\n\n\n\n\n\nFigura 2.1: População, amostra e inferência estatística",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#estatística-e-parâmetro",
    "href": "02-dados.html#estatística-e-parâmetro",
    "title": "2  Natureza dos Dados",
    "section": "2.3 Estatística e Parâmetro",
    "text": "2.3 Estatística e Parâmetro\nEstatística é uma característica que resume os dados de uma amostra e o parâmetro é uma característica estabelecida da população. Os valores dos parâmetros são normalmente desconhecidos, porque, na maioria das vezes, é inviável medir uma população inteira. A estatística é um valor aproximado, uma estimativa, do parâmetro. As estatísticas são representadas por letras romanas1 e os parâmetros por letras gregas. Por exemplo, a media da população é representada por \\(\\mu\\) e a média da amostra por \\(\\bar{x}\\); o desvio padrão da população é denotado \\(\\sigma\\) e o desvio padrão da amostra por s.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#escalas-de-medição",
    "href": "02-dados.html#escalas-de-medição",
    "title": "2  Natureza dos Dados",
    "section": "2.4 Escalas de medição",
    "text": "2.4 Escalas de medição\nEm um estudo científico, há necessidade de registrar os dados para que eles representem acuradamente as variáveis observadas. Este registro de valores necessita de escalas de medição. Mensuração ou medição é o processo de atribuir números ou rótulos a objetos, pessoas, estados ou eventos de acordo com regras específicas para representar quantidades ou qualidades dos dados. Para a mensuração das variáveis são usadas as escalas nominal, ordinal, intervalar e de razão (1).\n\n2.4.1 Escala Nominal\nAs escalas nominais são meramente classificativas, permitindo descrever as variáveis ou designar os sujeitos, sem recurso à quantificação. É o nível mais elementar de representação. São usados nomes, números ou outros símbolos para designar a variável. Os números, quando usados, representam códigos e como tal não permitem operações matemáticas. As variáveis nominais não podem ser ordenadas. Podem apenas ser comparadas utilizando as relações de igualdade ou de diferença, através de contagens. Os números atribuídos às variáveis servem como identificação, ou para associá-la a uma dada categoria. As categorias de uma escala nominal são exaustivas e mutuamente exclusivas. Quando existem duas categorias, a variável é dita dicotômica e com três ou mais categorias, politômicas.\nOs nomes e símbolos que designam as categorias podem ser intercambiáveis sem alterar a informação essencial.\nExemplos: Tipos sanguíneos: A, B, AB, O; variáveis dicotômicas: morto/vivo, homem/mulher, sim/não; cor dos olhos, etc.\n\n\n2.4.2 Escala Ordinal\nAs variáveis são medidas em uma escala ordinal quando ocorre uma ordem, crescente ou decrescente, inerente entre as categorias, estabelecida sob determinado critério. A diferença entre as categorias não é necessariamente igual e nem sempre mensuráveis. Geralmente, designam-se os valores de uma escala ordinal em termos de numerais ou postos (ranks), sendo estes apenas modos diferentes de expressar o mesmo tipo de dados. Também não faz sentido realizar operações matemática com variáveis ordinais. Pode-se continuar a usar contagem.\nExemplos: classe social (baixa, média, alta); estado geral do paciente: bom, regular, mau; estágios do câncer: 0, 1, 2, 3 e 4; escore de Apgar: 0, 1, 2… 10.\n\n\n2.4.3 Escala Intervalar\nUma escala intervalar contém todas as características das escalas ordinais com a diferença de que se conhece as distâncias entre quaisquer números. Em outras palavras, existe um espectro ordenado com intervalos quantificáveis. Este tipo de escala permite que se verifique a ordem e a diferença entre as variáveis, porém não tem um zero verdadeiro, o zero é arbitrário.\nO exemplo clássico é a mensuração da temperatura, usando as escalas de: Celsius ou Fahrenheit. Aqui é legítimo ordenar, fazer soma ou médias. No entanto, 0ºC não significa ausência de temperatura, portanto a operação divisão não é possível. Uma temperatura de 40ºC não é o dobro de 20ºC. Se 40ºC e 20ºC forem transformados para a escala Fahrenheit, passarão, respectivamente, para 104ºF e 68ºF e, sem dúvida, 104 não é o dobro de 68!\n\n\n2.4.4 Escala de Razão\nHá um espectro ordenado com intervalos quantificáveis como na escala intervalar. Entretanto, as medidas iniciam a partir de um zero verdadeiro e a escala tem intervalos iguais, permitindo as comparações de magnitude entre os valores. Refletem a quantidade real de uma variável, permitindo qualquer operação matemática.\nOs dados tanto na escala intervalar como na de razão, podem ser contínuos ou discretos. Dados contínuos necessitam de instrumentos para a sua mensuração e assumem qualquer valor em um certo intervalo. Por exemplo, o tempo para terminar qualquer tarefa pode assumir qualquer valor, 10 min, 20 min, 35 min, etc., de acordo com o tipo de tarefa. Outros exemplos: peso, dosagem de colesterol, glicemia.\nDados discretos possuem valores iguais a números inteiros, não existindo valores intermediários. A mensuração é feita através da contagem. Por exemplo: número de filhos, número de fraturas, número de pessoas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#tipos-de-variáveis",
    "href": "02-dados.html#tipos-de-variáveis",
    "title": "2  Natureza dos Dados",
    "section": "2.5 Tipos de Variáveis",
    "text": "2.5 Tipos de Variáveis\nA primeira etapa na descrição e análise dos dados é classificar as variáveis, pois a apresentação dos dados e os métodos estatísticos variam de acordo com os seus tipos. As variáveis, primariamente, podem ser divididas em dois tipos: numéricas ou quantitativas e categóricas ou qualitativas (2).\n\n2.5.1 Variáveis Numéricas\nAs variáveis numéricas são classificadas em dois tipos de acordo com a escala de mensuração: continuas e discretas.\nAs variáveis contínuas são aquelas cujos dados foram mensurados em uma escala intervalar ou de razão, podendo assumir, como visto, qualquer valor dentro de um intervalo de números reais, dependendo da precisão do instrumento de medição. O tratamento estatístico tanto para variável intervalar como de a razão é o mesmo. A diferença entre elas está na presença do zero absoluto. As variáveis numéricas contínuas têm unidade de medida. Por exemplo, um menino de 4 anos tem 104 cm.\nUma variável numérica é considerada discreta quando é apenas possível quantificar os resultados possíveis através do processo de contagem. Também têm unidade de medida – número de elementos. Por exemplo, o número de fraturas, o número de acidentes, etc.\n\n\n2.5.2 Variáveis Categóricas\nAs variáveis categóricas ou qualitativas são de dois tipos: nominal e ordinal, de acordo com a escala de mensuração. Um tipo particularmente comum é uma variável binária (ou variável dicotômica), que tem apenas dois valores possíveis. Por exemplo, o sexo é masculino ou feminino. Este tipo de variável é bastante utilizado na área da saúde, em Epidemiologia. As variáveis nominais não têm quaisquer unidades de medida e a nominação das categorias é completamente arbitrária e pertencer a uma categoria não significa ter maior importância do que pertencer à outra. Uma variável ordinal tem uma ordem inerente ou hierarquia entre as categorias. Do mesmo modo que as variáveis nominais, as variáveis ordinais não têm unidades de medida. Entretanto, a ordenação das categorias não é arbitrária. Assim, é possível ordená-las de modo lógico. Um exemplo comum de uma variável categórica ordinal é a classe social, que tem um ordenamento natural da maioria dos mais desfavorecidos para os mais ricos. As escalas, como a escore de Apgar e a escala de coma de Glasgow (3), também são variáveis ordinais. Mesmo que pareçam numéricas, elas apenas mostram uma ordem no estado dos pacientes. O escore de Apgar (4) é uma escala, desenvolvida para a avaliação clínica do recém-nascido imediatamente após o nascimento. Originalmente, a escala foi usada para avaliar a adaptação imediata do recém-nascido à vida extrauterina. A pontuação pode variar de zero a 10. Uma pontuação igual ou maior do que oito, indica um recém-nascido normal. Uma pontuação de sete ou menos pode significar depressão do sistema nervoso e abaixo de quatro, depressão grave.\nAs variáveis ordinais, da mesma forma que as nominais, não são números reais e não convém aplicar as regras da aritmética básica para estes tipos de dados. Este fato gera uma limitação na análise dos dados.\n\n\n2.5.3 Como identificar o tipo da variável?\nA maneira mais fácil de dizer se os dados são numéricos é verificar se eles têm unidades ligadas a eles, tais como: g, mm, ºC, ml, número de úlceras de pressão, número de mortes e assim por diante. Se não, podem ser ordinais ou nominais – ordinais se os valores podem ser colocados em ordem. A Figura 2.2 é uma ajuda para o reconhecimento do tipo de variável (5).\n\n\n\n\n\n\n\n\nFigura 2.2: Caminho para identificar o tipo de variável\n\n\n\n\n\n\n\n2.5.4 Variáveis Dependentes e Independentes\nDe um modo geral as pesquisas são realizadas para testar as hipóteses dos pesquisadores e, para isso, eles medem variáveis com a finalidade de compará-las. A maioria das hipóteses podem ser expressas por duas variáveis: uma variável explicativa ou preditora e uma variável desfecho (2).\nA variável preditora ou explanatória é a que se acredita ser a causa e também é conhecida como variável independente, porque o seu valor não depende de outras variáveis. Em Epidemiologia, é com frequência referida como exposição ou fator de risco.\nA variável desfecho é aquela que é o efeito, consequência ou resultado da ação de outra variável, por isso, também chamada de variável dependente. Em um estudo que tenta verificar se o tabagismo, durante a gestação, pode interferir no peso do recém-nascido, tem o fumo (variável categórica) como variável preditora (exposição ou fator de risco) e o peso do recém-nascido (variável numérica contínua) como variável desfecho\nNa maioria dos estudos, são utilizadas amostras que fornecem estimativas que, para serem representativas da população, devem ser probabilísticas. Ou seja, a amostra deve ser recrutada de forma aleatória, permitindo que cada um dos membros da população tenha a mesma probabilidade de ser incluído na amostra. Além disso, uma amostra deve ter um tamanho adequado para permitir inferências válidas.\n\n\n\n\n1. Oliveira Filho PF de. Natureza dos Dados. Em: Epidemiologia e Bioestatística–Fundamentos para a Leitura Crítica. 2ª edição. Editora Rubio; 2022. p. 3–6. \n\n\n2. Kirkwood BR, Sterne JA. Defining the Data. Em: Essential Medical Statistics. Second Edition. Blackwell Science Company; 2003. p. 9–14. \n\n\n3. Sternbach GL. The Glasgow coma scale. The Journal of emergency medicine. 2000;19(1):67–71. \n\n\n4. Pediatrics AA of, Obstetricians AC of. The apgar score. Pediatrics. 2006;117(4):1444–7. \n\n\n5. Bowers D. First things first-the nature of data. Em: Medical Statistics from Scratch. Second Edition. John Wiley; Sons; 2008. p. 3–13.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#footnotes",
    "href": "02-dados.html#footnotes",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "Também podem ser representadas pela letra grega correspondente ao respectivo parâmetro com um acento circunflexo, por exemplo, a média amostral é \\(\\hat{\\mu}\\), dita (mü chapéu).↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html",
    "href": "03-producaoDados.html",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "3.1 Processo de Pesquisa\nA pesquisa é um processo de construção do conhecimento. O objetivo deste processo é gerar um novo conhecimento e/ou confirmar ou refutar algum conhecimento prévio. A pesquisa é um processo de aprendizagem tanto do pesquisador quanto da sociedade que se beneficiará deste novo conhecimento. Para ser chamada de científica, a pesquisa deve obedecer aos princípios consagrados pela ciência (1).\nA pesquisa nasce de uma dúvida do pesquisador, de algum questionamento que ele considerou interessante sobre o mundo, ou seja, de algo que se costuma chamar de pergunta ou questão da pesquisa. Existem vários motivos que geram questões de pesquisa:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-pesquisa",
    "href": "03-producaoDados.html#processo-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "Avaliação crítica de pesquisas realizadas por outros pesquisadores.\nCondução de uma pesquisa primária com a finalidade de responder uma questão (ou questões), gerando um novo conhecimento ou ampliação do conhecimento existente.\nPara obter habilidades de pesquisa ou experiência, com frequência como parte de um programa educacional.\nTestar a viabilidade de um projeto ou técnica de pesquisa.\n\n\n3.1.1 Questão de Pesquisa\nA pesquisa visa estabelecer novos conhecimentos em torno de um tema específico. O tema de pesquisa pode surgir do próprio interesse ou experiência do pesquisador, ou partir da encomenda de alguma instituição financiadora. Algumas vezes, a pesquisa se origina de outros estudos realizados pelo próprio pesquisador ou outros pesquisadores.\nÀ medida que a ideia da pesquisa cresce, o pesquisador estabelece uma pergunta de pesquisa específica ou um conjunto de questões que ele deseja responder. Algumas vezes, o tema da pesquisa é tão amplo que o pesquisador tem que ter cuidado para não se perder do seu objetivo. Este objetivo é que vai guiá-lo no estabelecimento da pergunta ou perguntas a serem respondidas no estudo. Estes questionamentos são conhecidos como questão de pesquisa ou pergunta de partida.\nO foco da questão de pesquisa pode ser na descrição de um fenômeno clínico. Neste caso a pergunta é dita descritiva, por exemplo, pesquisa de prevalência de uma enfermidade, proporção de utilização de um serviço de saúde, características de um teste, etc. Quando a pergunta busca a explicação para um fenômeno, ela é dita analítica, por exemplo, comparação entre dois fenômenos. Em geral, perguntas analíticas são mais interessantes. Entretanto, as perguntas descritivas são fundamentais no início de um estudo analítico.\nUma boa pergunta de pesquisa deve ter as seguintes características (2):\n\nFactível: o pesquisador deve conhecer desde o início os limites e problemas práticos que podem interferir na pesquisa. A viabilidade está relacionada com o tamanho amostral, com o domínio técnico adequado, com o tempo e custos envolvidos e com um foco dirigido estritamente aos objetivos mais importantes.\nInteressante: a questão de pesquisa deve despertar o interesse não apenas do pesquisador, mas também de seus pares e agentes financiadores.\nNova: a pesquisa deve ser inovadora, original, em algum sentido, para que o estudo seja uma contribuição ao conhecimento ou amplie um conhecimento existente;\nÉtica: se o estudo impõe riscos físicos ou invasão de privacidade ou não traz nenhuma informação nova, o pesquisador deve suspendê-lo. É importante discutir previamente com pesquisadores mais experientes ou com algum representante do Comitê de Ética em Pesquisa da instituição.\nRelevante: nenhuma das características da questão de pesquisa é mais importante do que a sua relevância. Para isto basta pensar nos benefícios que os resultados da pesquisa trarão à Medicina atual.\n\nOu seja, antes de dedicar tempo e esforço para escrever um projeto de pesquisa deve-se avaliar se a questão de pesquisa é FINER (Factível, Interessante, Nova, Ética e Relevante).\n\n\n3.1.2 Hipótese de Pesquisa\nUma vez estabelecida a(s) pergunta(s) de pesquisa adequada(s), os pesquisadores formulam hipóteses para serem testadas. Enquanto a pergunta de pesquisa possa ser um pouco vaga em sua natureza como: “existe uma relação entre o tipo psicológico e a capacidade de parar de usar drogas?” Uma hipótese de pesquisa, necessita ser precisa. Há necessidade de especificar qual o tipo psicológico está relacionado à habilidade de parar de usar drogas.\nA precisão da hipótese é fundamental em um projeto de pesquisa, pois ela determinará o delineamento de pesquisa a ser seguido pelo pesquisador e as técnicas estatísticas apropriadas para a análise dos dados. A fonte e o tipo de dados são determinados pela característica do delineamento recomendado pela hipótese de pesquisa.\nO objetivo da pesquisa, usando o método científico, é refutar ou não as hipóteses de pesquisa. Se a hipótese do pesquisador não for rejeitada, houve a geração de um novo conhecimento.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-amostragem",
    "href": "03-producaoDados.html#processo-de-amostragem",
    "title": "3  Produção dos Dados",
    "section": "3.2 Processo de Amostragem",
    "text": "3.2 Processo de Amostragem\nApós o estabelecimento das hipóteses a serem testadas, há necessidade de coletar os dados. Uma vez que é praticamente impossível analisar toda a população que constitui a população-alvo, extrai-se uma amostra desta população. Este processo é denominado de amostragem (3).\nUma amostra deve ser representativa da população, ou seja, deve ter características semelhantes às da população e ser fidedigna. A fidedignidade está relacionada à precisão dos dados que sofrem influência dos instrumentos de aferição, questionários não validados e falhas humanas. Uma amostra inadequada ameaça a validade da pesquisa. Os dados coletados de maneira não aleatória são chamados de evidência anedótica. O nível de confiança nos resultados de uma pesquisa está diretamente relacionado à qualidade da amostra. A amostra deve ser representativa.\nUma amostra deve conter apenas dados úteis que permitam a resposta da pergunta de pesquisa, evitando desperdício e fuga dos objetivos traçados. A aleatoriedade provoca uma diferença entre o resultado da amostra e o verdadeiro valor da população que é denominada erro amostral. Não importa quão bem a amostra seja coletada, os erros amostrais irão sempre ocorrer. Entretanto, não existe técnica estatística que salve amostras coletadas incorretamente, tendenciosas!\n\n3.2.1 Amostras probabilística\nPara evitar vieses, erros sistemáticos, que favorecem determinados desfechos, o ideal é coletar uma amostra probabilística. A amostra probabilística adota o princípio da equiprobabilidade, isto é, “todos os sujeitos da população têm a mesma probabilidade de fazerem parte da amostra”. Esta probabilidade é conhecida e diferente de zero. As amostras probabilísticas têm o potencial de ser possível a generalização para a população; ser imparcial e com menor erro amostral.\nAmostra aleatória simples: é a mais utilizada pois garante representatividade da amostra junto à população. A amostra aleatória simples não emprega nenhum critério particular para a definição da amostra. O mecanismo mais comum de obter este tipo de amostra é por um simples sorteio, em geral, usando programas de computador.\nAmostra aleatória estratificada: quando a população é constituída por subpopulações ou estratos e é razoável supor que a variável de interesse apresenta comportamento diferente nos diferentes estratos, pode-se usar este tipo de amostragem. Neste caso, a amostra deve ter a mesma estratificação da população para ser representativa. Um exemplo comum de estratificação é o nível socioeconômico. A partir do momento que os estratos estão definidos se procede uma amostra aleatória simples de cada estrato.\nAmostra aleatória sistemática: as unidades amostrais são selecionadas a partir de um esquema rígido preestabelecido de sistematização que tem o propósito de abranger toda a população-alvo. Para isso, ordena-se os indivíduos da população (por exemplo, um grande arquivo com 20000 fichas) e calcula-se uma constante conveniente, \\(c = N/n\\), onde \\(N\\) é tamanho da população e \\(n\\) é o tamanho da amostra. Se \\(n = 500\\), a constante será \\(40\\), ou seja, será selecionado aleatoriamente o primeiro membro da amostra (\\(k\\)), de maneira que \\(k\\) seja menor do que a constante e maior do que \\(1\\). A partir daí os sucessivos membros serão: \\(k + c\\) ; \\(k + 2c\\) ; \\(k + 3c\\) ; … até atingir \\(n\\).\nAmostra aleatória por conglomerados (clusters): este tipo de amostra é utilizada quando dentro da população são identificados agrupamentos (clusters) naturais, por exemplo, espaços, vilas, etc. Neste tipo de amostragem o elemento focal não é o sujeito, mas o cluster. Identificados estes, sorteiam-se os conglomerados e se analisa todos os indivíduos dos conglomerados sorteados.\n\n\n3.2.2 Amostras não probabilísticas\nNa amostragem não aleatória ou intencionada há uma escolha deliberada da amostra, subordinada a objetivos específicos do pesquisador. Não há garantia de representatividade da população. É importante averiguar, neste tipo de amostragem, a presença de conflitos de interesse.\nAmostra de conveniência: é uma técnica comum onde é selecionada uma mostra que esteja acessível. Em outras palavras, os indivíduos são recrutados porque eles estão prontamente disponíveis. Neste tipo de amostra há incapacidade de fazer afirmações gerais com rigor estatístico sobre a população.\nAmostra por cotas: é uma versão não probabilística da amostra estratificada. Tem três etapas:\n\nSegmentação, onde se divide em grupos, por exemplo, sexo, classe social, região, etc.;\nDefinição do tamanho das cotas;\nSeleção por meio de amostras de conveniência.\n\nAmostra de resposta voluntária: o pesquisador solicita aos membros de uma população-alvo para que eles participem da amostra e as pessoas decidem se entram ou não. Esses tipos de amostras são enviesados porque as pessoas podem ter interesses particulares ou opiniões negativas e tendem a querer participar.\n\n\n3.2.3 Tamanho amostral\nA determinação do tamanho de uma amostra é de suma importância, pois amostras desnecessariamente grandes acarretam desperdício de tempo e de dinheiro e amostras muito pequenas podem levar a resultados não confiáveis, ameaçando a validade da pesquisa.\nNão existe um número estabelecido para o tamanho da amostra. Há uma solução para cada caso. O tamanho da amostra depende (4):\n\ndo tipo de problema;\ndo tipo de variável;\nda magnitude do erro estatístico aceito pelo pesquisador;\nda diferença minimamente importante entre os grupos;\nda probabilidade de que a amostra identifique uma diferença verdadeira: Poder estatístico;\ndo tempo, dinheiro e pessoal disponível, bem como da dificuldade em se obterem dados e da complexidade da pesquisa.\n\nO tamanho amostral mínimo é determinado por fórmulas estatísticas complexas. Os cálculos são muito pesados, mas agora, felizmente, existem programas de computador disponíveis que realizam este trabalho, por exemplo o G-Power3 (5). Além disso, é possível acessar um site que fornece informações e ferramentas para o cálculo amostral em pesquisas da área da saúde 1. Existem tabelas extensas para calcular o número de participantes (6) para um determinado nível de poder (e vice-versa).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "href": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "3.3 Principais Delineamentos de Pesquisa",
    "text": "3.3 Principais Delineamentos de Pesquisa\nEm geral, a pesquisa clínica, é dividida em dois tipos de investigação. O primeiro é aquele em que o observador apenas observa o doente, as características da sua doença e sua evolução, sem atuar de modo a modificar qualquer aspecto que esteja estudando. Trata-se de estudo observacional.\nO segundo corresponde aos estudos experimentais, onde o pesquisador não se limita a observar, mas promove uma intervenção com o objetivo de conhecer os efeitos dessa sobre os participantes da pesquisa. A intervenção pode ser a prescrição de um medicamento, uma dieta, atividade física ou repouso, ou simplesmente, o estabelecimento de um programa de atenção à saúde.\nOs estudos podem ser também classificados em primários ou secundários ou integrativos (7). Estudos primários correspondem a pesquisas originais que constituem a maioria das publicações encontradas nas revistas médicas. Estudos secundários são aqueles que procuram sumarizar e extrair conclusões de estudos primários\n\nEstudos Primários\n\nEstudos Observacionais\n\nRelato de Caso e Série de Casos\nEstudo Transversal\nEstudo Caso-controle\nEstudo de Coorte\n\nEstudos Experimentais\n\nExperimento laboratorial\nEnsaio Clínico\n\n\nEstudos Secundários\n\nRevisões não sistemáticas\nRevisões Sistemáticas\nDirerizes (Guidelines)\nAnálise de decisão\nAnálise Econômica\n\n\n\n3.3.1 Elementos básicos de um delineamento de pesquisa\nOs estudos contêm três elementos básicos:\n\nVariáveis componentes: Nas investigações das relações entre as variáveis identificam-se pelo menos duas variáveis nos estudos epidemiológicos.\n\nDesfecho: Aquilo que vai acontecer durante uma investigação na mensuração da condição de saúde-doença. Sinônimo: variável dependente.\nExposição: O fator que precede o desfecho. Sinônimos: fator em estudo, variável preditora, variável independente.\n\nTemporalidade: Quanto ao tempo os estudos podem ser contemporâneos, retrospectivos e prospectivos, de acordo como os dados são obtidos em relação ao momento atual.\nEnfoque: Um estudo pode ter vários enfoques. Na maioria deles, na área médica, eles relacionam-se à prevenção, ao diagnóstico, à terapêutica e ao prognóstico.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#estudos-observacionais",
    "href": "03-producaoDados.html#estudos-observacionais",
    "title": "3  Produção dos Dados",
    "section": "3.4 Estudos Observacionais",
    "text": "3.4 Estudos Observacionais\n\n3.4.1 Relato de Caso ou Série de casos\nNo relato de caso, descrevem-se casos raros, eventos não comuns ou inesperados, doenças desconhecidas ou raras. Um evento notável deve ser identificado. Um relato de caso tem a descrição de até dez casos. Acima deste número tem-se uma série de casos (8).\nMetodologicamente, faz-se um relato descritivo simples de características interessantes observadas em um paciente ou grupo de pacientes. Os indivíduos são acompanhados em um espaço de tempo curto e não possuem participantes-controles. A coleta dos dados é, na maioria das vezes, retrospectiva.\nUma série de casos não é planejada e não envolve quaisquer hipóteses investigativas. Pode ser empregada como precursor de outros estudos.\n\n\n3.4.2 Estudos Transversais ou Seccionais\nOs estudos transversais são também conhecidos como estudos seccionais. Este tipo de estudo fornece a informação sobre a prevalência, ou seja, a proporção dos indivíduos que tem a doença ou condição clínica em um determinado momento. Por este motivo são também conhecidos como estudos de prevalência (9).\nObservam dados coletados em um grupo de indivíduos em um único momento, sem um período de seguimento. O desfecho e exposição são avaliados no mesmo momento no tempo. Os dados são coletados apenas uma vez para cada indivíduo, podendo ser em dias diferentes em diferentes sujeitos. As informações são, em geral, obtidas em um curto espaço de tempo.\nÉ um estudo estático, representa a “fotografia” de um momento. Entretanto, se as variáveis preditora e de desfecho são definidas apenas com base nas hipóteses causa-efeito do investigador e não no delineamento do estudo, é possível também examinar associações.\nOs estudos de corte transversal, de um modo geral, são desenhados para determinar “O que está acontecendo?”. São usados para:\n\nDeterminar a prevalência de uma doença, como a prevalência de HIV em gestantes.\nPesquisar atitudes ou opiniões em relação a um determinado assunto (pesquisa de satisfação)\nVerificar interrelações entre variáveis, como observação das características de fumantes pesados em relação ao sexo, idade, etc.\nEnquetes\n\n\n3.4.2.1 Cuidados na interpretação de dados de estudos transversais\n\nEfeito temporal\n\nComo os dados (exposição e desfecho) são coletados no mesmo momento, fica difícil estabelecer qualquer relação temporal entre eles (dilema ovo/galinha). Por exemplo, não é possível estabelecer uma relação de causalidade entre hipertensão e doença cardíaca se os dados são coletados de forma a ficar impossível saber que surgiu em primeiro lugar.\n\nEstudos transversais repetidos\n\nOs estudos transversais, algumas vezes, são repetidos em outro momento ou em outros locais com a finalidade de verificar variabilidade nos achados. Por exemplo, medir a prevalência de uma doença em momentos diferentes ou em diferentes locais. Os indivíduos serão um pouco diferentes, devendo-se interpretar as diferenças destes resultados com cautela.\n\nEstudos transversais que parecem longitudinais\n\nUma armadilha comum é confundir um estudo seccional com um longitudinal porque os dados foram coletados através do tempo até completar o tamanho amostral previsto. O importante é que os dados (variável preditora e desfecho) foram coletados somente uma vez para cada indivíduo e no mesmo momento. Isto gera uma interpretação errônea se analisarmos como um estudo longitudinal.\n\n\n3.4.2.2 Análise dos Estudos Transversais\nQuando se compara a prevalência de doença em expostos e não expostos, a medida de associação usada é a Razão de Prevalência Pontual (RPP).\n\n\n\n3.4.3 Estudos Caso-Controle\nPara examinar a possível associação de uma exposição a uma determinada doença, identifica-se um grupo de doentes (casos) e, com a finalidade de comparação, um grupo de pessoas sem a doença (controles) e determina-se a chance (odds) de exposição e não exposição entre casos e entre controles.\nOs estudos caso-controle, portanto, partem da presença ou ausência de um desfecho e após olham para trás no tempo (retrospectivamente) para detectar possíveis fatores de risco (Figura 3.1)) (10). Analisam o que aconteceu e são usados para investigar fatores de risco de doenças raras onde um estudo prospectivo seria muito longo para identificar uma quantidade suficiente de casos.\nÉ útil também para investigar surtos agudos (infecção alimentar) para identificar se existe ou não associação entre a exposição e o desfecho investigado. Com frequência, os estudos caso-controle são o primeiro passo na busca de uma etiologia quando há suspeita de que alguma de várias exposições esteja associada a uma determinada doença.\n\n\n\n\n\n\n\n\nFigura 3.1: Desenho de um estudo caso controle.\n\n\n\n\n\n\n3.4.3.1 Seleção dos casos\nOs casos podem ser selecionados de várias fontes, incluindo indivíduos hospitalizados, de consultórios ou clínicas, principalmente quando registros adequados são mantidos.\nMuitos problemas podem ocorrer na seleção de casos, neste tipo de estudo. Se os casos forem selecionados de um único hospital, quaisquer fatores de risco identificados podem ser apenas daquele hospital, em decorrência do padrão de referência e nível de atendimento (um hospital terciário que apenas atende um determinado convênio, por exemplo, o Sistema Único de Saúde). Por isso, devem ser utilizados casos procedentes de vários hospitais da comunidade, pois aí os casos pertenceriam a diferentes grupos sociais e diferentes graus de gravidade da doença.\nCasos incidentes ou prevalentes\nOs casos usados nos estudos caso-controle podem ser casos incidentes (recém-diagnosticados) ou casos prevalentes da doença (pessoas que apresentaram a doença em algum período).\nO problema do uso de casos incidentes é que há necessidade de se esperar que novos casos sejam diagnosticados e isto pode requerer muito tempo. Enquanto os casos prevalentes já estão disponíveis havendo um maior número disponível para o estudo. Em ambos os modelos existem problemas, pois nos casos prevalentes algumas pessoas podem morrer logo após o diagnóstico e estarem pouco representadas no estudo. Por outro lado, nos casos incidentes, serão excluídos os pacientes que morreram antes do diagnóstico ser feito. Não existe uma solução fácil para este problema, mas é importante lembrar-se destas questões ao interpretar os resultados e tirar conclusões do estudo.\n\n\n3.4.3.2 Seleção dos controles\nDa mesma forma do que nos estudos experimentais, a escolha dos controles afeta a comparação com os casos (11). A escolha dos controles inclui:\n\nPacientes do mesmo hospital, mas com condições ou doenças não relacionadas;\nPacientes pareados um a um em relação a fatores prognósticos, tais como sexo e idade;\nUma amostra aleatória originária da mesma população de onde provêm os casos.\n\nSem dúvida, o melhor grupo controle é a terceira opção, mas esta é raramente possível. Por este motivo, alguns estudos caso-controle incluem mais de um grupo controle para tornar o estudo mais robusto\nControles pareados\nO emparelhamento é definido como processo de seleção dos controles para que sejam semelhantes aos casos em algumas características como, por exemplo, idade, gênero, raça, condição socioeconômica e ocupação.\nControles emparelhados são bastante comuns. O autor deve ter o cuidado de especificar cuidadosamente o modo como houve o pareamento. Por exemplo, “emparelhado por idade dentro de dois anos” mostra a amplitude do pareamento. É difícil realizar o emparelhamento para muitos fatores, pois um pareamento seguro não existe. Em um delineamento pareado, a análise estatística deve levar em conta o emparelhamento e os fatores usados por ele. Onde um indivíduo em um par tiver um dado perdido, ambos devem ser omitidos da análise estatística.\n\n\n3.4.3.3 Estudos caso-controle aninhados\nUm delineamento do tipo caso-controle aninhado é um estudo de caso-controle ’’aninhado” em um estudo de coorte (12). É um excelente desenho para variáveis preditoras que são caras para medir e que podem ser avaliadas no final do estudo em indivíduos que desenvolvem o resultado durante o estudo (casos) e em uma amostra daqueles que não o fazem (controles).\nO investigador começa com uma coorte adequada (Figura 3.2) (13) com casos suficientes ao final do acompanhamento para fornecer poder adequado para responder à pergunta de pesquisa. No final do estudo, aplica critérios que definem o resultado de interesse para identificar todos aqueles que desenvolveram o resultado (casos). Em seguida, seleciona uma amostra aleatória dos indivíduos que não desenvolveram o resultado (controles).\nA principal razão para usar delineamentos caso-controle aninhado é reduzir o trabalho e o custo na coleta de dados. A principal desvantagem desse projeto é que muitas questões e circunstâncias da pesquisa não são passíveis de armazenamento para posterior análise.\n\n\n\n\n\n\n\n\nFigura 3.2: Desenho de um estudo caso-controle aninhado.\n\n\n\n\n\n\n\n3.4.3.4 Estudo caso-controle de base populacional\nSão os estudos caso-controle onde os casos e controles são uma amostra completa ou probabilística de uma população definida.\n\n\n3.4.3.5 Limitações dos estudos caso-controle\nVárias limitações podem afetar os estudos caso-controle:\n\nA escolha do grupo controle afeta as comparações entre casos e controles;\nOs dados da exposição ao fator de risco são coletados retrospectivamente e dependem da memória dos participantes, registros médicos e, portanto, podem ser incompletos, sem acurácia ou enviesados (viés de memória);\nSe o processo que conduz à identificação dos casos está relacionado a um possível fator de risco, a interpretação dos resultados será difícil (viés averiguação).\n\nPor exemplo: suponha que os casos sejam mulheres jovens com hipertensão selecionadas de uma clínica de contracepção. Nesta situação, um possível fator de risco, o anticoncepcional oral (ACO), estará vinculado à seleção dos casos e, desta forma, o uso de ACO será mais comum entre os casos do que entre os controles populacionais.\n\n\n\n\n3.4.3.6 Análise dos Estudos Caso-controle\nA principal estratégia de análise é o cálculo da odds ratio (Razão de Chances), que pode ser interpretado como uma estimativa do Risco Relativo.\nO Risco Relativo somente pode ser calculado quando é possível o cálculo da incidência (ver ?sec-rr). Nos estudos caso-controle, isso não é possível, pois aqui o estudo começa com casos e controles em vez de indivíduos expostos e não expostos ao fator de risco. Desta maneira, se comparam as odds (chance) de uma exposição passada a um fator de risco suspeitado em indivíduos doentes e em controles não doentes. Esta relação é denominada de odds ratio (ver ?sec-or).\n\n\n\n3.4.4 Estudos de Coorte\nOs estudos de coorte são considerados o padrão-ouro dos estudos observacionais. Seu nome se originou das coortes dos soldados romanos, cada uma delas constituída por 480 a 600 legionários. As coortes romanas eram distintas entre si e tinham sua identidade determinada por, ao menos, uma característica comum entre os indivíduos de cada grupo. Podia ser por características estratégicas no campo de batalha, por uma cor presente na indumentária, ou outras. Em Epidemiologia, o termo coorte permaneceu com significado semelhante.\nEm um estudo de coorte, um grupo de pacientes sadios (coorte), expostos ou não a um suspeitado fator de risco, é seguido através do tempo para determinar a incidência da doença em questão em cada um dos grupos (14).\nNeste modelo de estudo, a característica comum aos dois grupos é a exposição. Tem-se uma coorte de expostos e uma coorte de não expostos que são acompanhadas por um período de tempo que permita o aparecimento do desfecho. No final do estudo, compara-se a incidência do desfecho (doença) entre os expostos com a incidência do desfecho entre os não expostos. Se existe uma associação positiva entre a exposição e o desfecho, se espera que a incidência do desfecho entre os expostos seja maior do que a incidência de desfecho entre não expostos.\nUm esquema simplificado de um estudo de coorte é mostrado na Figura 3.3 (15).\n\n\n\n\n\n\n\n\nFigura 3.3: Desenho de um estudo de coorte sobre risco.\n\n\n\n\n\nObservar que como se identifica novos casos (incidência) à medida que eles ocorrem, é possível determinar uma relação temporal entre a exposição e a doença, isto é, se a exposição precedeu o início da doença. Isto é fundamental para estabelecer uma relação causal entre a exposição e a doença.\nOs estudos de coorte têm semelhança com os ensaios clínicos randomizados. Ambos os estudos comparam grupos expostos a grupos não expostos. Não havendo possibilidade de realizar a randomização, por exemplo, por motivos éticos quando a exposição é sabidamente prejudicial, é indicado um estudo de coorte. A diferença fundamental, portanto, é a ausência de randomização nos estudos de coorte.\nExistem duas maneiras básicas para formar os grupos:\n\nSeleciona-se a população-alvo baseado no fato dos indivíduos estarem expostos ou não ao fator em estudo (Figura 3.3);\nOu seleciona-se a população-alvo antes que qualquer um dos seus membros se torne exposto, ou antes, que a exposição seja identificada (Figura 3.4). Um exemplo típico deste modelo é o clássico Estudo de Framingham (16).\n\n\n\n\n\n\n\n\n\nFigura 3.4: Desenho de uma coorte com grupos expostos e não expostos. (17).\n\n\n\n\n\n\n3.4.4.1 Tipos de estudo de coorte\nDe acordo com as características do seguimento, as coortes podem ser:\n\nEstudo de Coorte Prospectivo (Coorte Concorrente ou Longitudinal), onde os grupos são montados no presente, coletados os dados basais deles e continua-se a coletar dados com o passar do tempo até a doença se desenvolver ou não.\nEstudo de Coorte Retrospectivo ou Histórico (Coorte não concorrente), onde a exposição é avaliada em dados passados e o desfecho (doença ou não) é verificado no momento do início do estudo. O problema aqui é que a averiguação da exposição depende dos registros pregressos.\nEstudo de Coorte Misto (Prospectivo e Retrospectivo), onde a exposição é verificada em registros objetivos no passado (como em uma coorte histórica) e o seguimento e a medida do desfecho se fazem no futuro.\n\n\n\n3.4.4.2 Vieses em estudos de coorte\nOs potenciais vieses nos estudos de coorte são os seguintes:\n\nViés de confusão – é a grande ameaça dos estudos observacionais. O confundimento causa um erro sistemático na inferência, podendo aumentar ou diminuir uma associação observada entre exposição e doença. Uma variável funciona como fator de confusão quando ela está associada com a exposição e ao mesmo tempo com a doença. Ela não deve fazer parte da cadeia causal da exposição à doença. Por exemplo, num estudo sobre fatores de risco, uma associação entre o hábito de beber café e a doença coronária é detectada. Porém, se não for considerado o fato de que os fumantes bebem mais café do que os não-fumantes, pode-se chegar à errônea conclusão de que o café é um fator de risco independente para doença coronária, o que não corresponde à realidade. Neste caso, o café é um fator de confusão e não um fator causal independente para a doença coronária (18).\nViés na avaliação dos desfechos – este viés pode ocorrer quando o pesquisador que avalia o desfecho também sabe sobre o status de exposição dos sujeitos da pesquisa. Evita-se este problema “cegando” a pessoa que faz a avaliação da doença.\nViés de informação – ocorrem principalmente em estudos históricos onde as informações dependem de registros passados e podem ser diferentes entre as pessoas expostas e não expostas.\nViés de não resposta e perdas de acompanhamento – a não participação e as perdas podem introduzir um grande viés, alterando o cálculo da incidência nos expostos e entre os não expostos.\nViés de análise – se os estatísticos tiverem alguma hipótese em relação aos dados que estão analisando, eles podem introduzir vieses em suas análises.\n\n\n\n3.4.4.3 Análise dos estudos de coorte\nPara verificar se existe associação entre certo desfecho (doença) e uma determinada exposição calcula-se o Risco Relativo (RR). Este é definido como a razão entre a incidência (risco) em expostos e a incidência (risco) em não expostos (ver ?sec-rr).\n\n\n3.4.4.4 Vantagens e desvantagens dos estudos de coorte\n\nVantagens\n\nAdequado para exposições raras\nBom poder para testar hipóteses\nImportante em estudos etiológicos e prognósticos\nSalienta os múltiplos desfechos de uma exposição\n\nDesvantagens\n\nInadequado em desfechos raros\nPerdas no seguimento levam a viés de seleção\nDemorado/elevado custo",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#ensaios-clínicos",
    "href": "03-producaoDados.html#ensaios-clínicos",
    "title": "3  Produção dos Dados",
    "section": "3.5 Ensaios Clínicos",
    "text": "3.5 Ensaios Clínicos\nExperimentos são estudos nos quais o pesquisador manipula a variável preditora (intervenção) e observa o efeito no desfecho que está sendo avaliado ao longo do tempo. A abordagem experimental, especificamente, o ensaio clínico randomizado controlado é a ferramenta de escolha para comparar terapêuticas ou intervenções.\nOs estudos experimentais podem também comparar os cuidados prestados por serviços de saúde, programas de educação em saúde e estratégias administrativas. Os estudos experimentais realizados com seres humanos são denominados de ensaios clínicos.\nNos ensaios clínicos não controlados os indivíduos servem como seus próprios controles (antes-e-depois). Os resultados destes estudos estão sujeitos vários problemas:\n\nMelhora previsível. Paciente melhora espontaneamente e não pelo tratamento.\nFlutuação na gravidade da doença.\nEfeito Hawthorne: o indivíduo melhora pela atenção e não pela terapêutica (19).\nRegressão à média: uma limitação importante surge quando se quer avaliar a evolução de um grupo que tenha sido selecionado por estar no extremo de uma distribuição sem que haja um grupo controle. Empiricamente, observa-se que indivíduos que se encontrem num determinado momento, em um dos extremos de uma distribuição, tendem a estarem menos distantes da média em um momento posterior, sem que qualquer intervenção tenha sido desenvolvida. Este fenômeno é conhecido como efeito de regressão à média. Por exemplo: uma pessoa com uma doença crônica tem dias piores e outros melhores. Se ela é medicada com gotas homeopáticas ou faz uso de florais nos dias em que se sente excepcionalmente mal vai notar que é frequente uma melhora, seguindo estes “tratamentos”. Não que eles funcionem, mas pela regressão à média (20).\n\n\n3.5.1 Características de um ensaio clínico\nUm ensaio clínico deve ter algumas características fundamentais (Figura 3.5) (21):\n\nOs indivíduos devem ser designados por randomização para os grupos de comparação.\n\nA randomização é a melhor abordagem no delineamento de um ensaio clínico (22).\nRandomizar significa sortear (por meio de computadores, tábua de números aleatórios) os indivíduos para decidir a alocação dos mesmos em um dos grupos de estudo. O elemento decisivo da randomização é a imprevisibilidade da próxima alocação.\n\nO pesquisador compara o grupo de estudo com um grupo controle apropriado.\nO investigador manipula a variável independente (preditora).\n\n\n\n\n\n\n\n\n\nFigura 3.5: Estrutura de um ensaio clínico randomizado.\n\n\n\n\n\n\n\n3.5.2 Elementos básicos de um ensaio clínico\n\nSeleção dos participantes\n\nOs pesquisadores devem determinar e explicar detalhadamente os critérios de inclusão e de exclusão:\n\nObjetivos dos critérios de inclusão e exclusão\n\nRestringir a heterogeneidade da amostra\nDiminuir o número de variáveis independentes\nFazer com que exista uma chance maior de que as diferenças nos desfechos estejam relacionadas aos tratamentos\nMelhorar a validade interna, ou seja, o grau em que os resultados do estudo são consistentes para aquela amostra particular de indivíduos. Esta validade depende basicamente do rigor metodológico usado para delinear o ensaio clínico, podendo ser ameaçada por dois tipos de erros: sistemático ou aleatório.\n\nTornar a generalização (validade externa) mais precisa. Entretanto deve-se ter cuidado com critérios de inclusão e exclusão muito rígidos, pois podem diminuir esta capacidade de generalização\n\n\nO grau de detalhamento deve ser suficientemente preciso para permitir que outros reproduzam o estudo. O tamanho da amostra deve ser claramente determinado pelo poder do teste estatístico. Poder é a habilidade de o teste estatístico detectar diferenças entre os grupos, dado que tais diferenças existam na população em estudo. Lembrar que resultados não significativos podem ser apenas uma evidência para um inadequado tamanho amostral.\nO grupo controle deve ser selecionado utilizando-se os mesmos critérios do grupo experimental. Prestar atenção em possíveis armadilhas que podem gerar vieses:\n\nUso de grupo controle histórico (não concorrente);\nGrupo controle selecionado de outros locais (outras clínicas, outros hospitais).\n\nO grupo controle adequado é um grupo controle concorrente, tratado no mesmo momento e no mesmo local do grupo experimental. O característico é o grupo controle não receber tratamento. Mais comumente recebem um placebo, indistinguível do tratamento experimental, mas sem componente ativo. Mesmo assim, pode haver melhora dos participantes do grupo controle (Efeito Placebo ) (23). Quando não for ético suspender o tratamento e administrar placebo, o grupo controle pode ser constituído por indivíduos que recebem o tratamento padrão.\n\nAlocação\n\nA alocação deve ser aleatória. A randomização é a principal técnica para reduzir o viés, criando grupos homogêneos. Como foi visto, é uma das características fundamentais dos ensaios clínicos. O poder da randomização depende da ocultação da sequência de alocação.\nA randomização pode ser:\n\nCompleta: os indivíduos que obedecem ao critério de inclusão e exclusão são randomizados de modo que todos têm a mesma probabilidade de pertencer a cada um dos grupos. Isto maximiza o poder. Pode ser feita por blocos para assegurar a igualdade numérica dos grupos (estudos multicêntricos).\nEstratificada: os participantes são estratificados de acordo com possíveis variáveis de confusão (gravidade da doença, idade, sexo, etc.) e a randomização é realizada dentro de cada estrato.\nRandomização e alocação desigual: os sujeitos têm uma maior probabilidade de ser randomizados em um grupo (em geral, grupo experimental) do que o outro (comparação). Este tipo de estudo tem menor poder.\n\n\nCondução/Seguimento/Avaliação\n\nEm um ensaio clínico deve estar assegurado de que o estudo tenha um tempo de seguimento adequado, pois nem todos os indivíduos participam conforme o plano original. Podem ocorrer perdas de alguns pacientes durante o acompanhamento, seja porque com o tempo se constata que eles não têm a doença em estudo ou porque não aderiram ao tratamento ou intervenção e abandonaram o estudo. Quanto maior o número de pacientes perdidos e menos informações sobre eles, menos confiança pode ser colocada nos resultados do estudo. De um modo geral, não se deve tolerar perdas que sejam maiores que a incidência do desfecho no estudo. Uma regra simples é que perdas menores que 5% produzem pouco viés e perdas maiores que 20% são uma ameaça importante à validade do estudo. As perdas entre 5 e 20% devem ser avaliadas com cuidado, se possível utilizando-se uma análise de sensibilidade (pior cenário), principalmente se as perdas forem diferentes nos grupos pelo maior risco de viés.\nNeste tipo de análise, nos estudos com resultado positivo, todos os pacientes perdidos no grupo experimental, inicialmente, são considerados como tendo o desfecho. Posteriormente, analisa-se como se nenhum dos indivíduos perdidos no grupo controle atingiu o desfecho. Se o resultado permanecer positivo, as perdas não afetaram a validade do estudo. Estudos sem relato adequado ou nenhum relato de perdas ou exclusões devem ser avaliados com muito cuidado.\nOutro aspecto importante, no seguimento dos sujeitos da pesquisa, é o tratamento igual de todos os grupos. Para garantir este princípio, utiliza-se da técnica de cegamento ou mascaramento (24). Esta técnica impede que os participantes da pesquisa (pesquisadores, avaliadores e participantes) tomem conhecimento de qual grupo de tratamento o participante se encontra. Este conhecimento antecipado pode influenciar as expectativas, as opiniões e as crenças em relação aos resultados do estudo. O cegamento tem como principal finalidade a eliminação do viés de aferição, além de melhorar a adesão ao tratamento, reduzir as perdas de seguimento e diminuir o viés causado por co-intervenções (assistência suplementar maior para um dos grupos).\nQuando o cegamento ocorre nos pacientes e nos pesquisadores, diz-se que o estudo é duplo-cego. Se ele também incluir os avaliadores do estudo, ele é triplo cego. Um ensaio clínico em que não há cegamento é dito aberto (open label, no caso de estudos com fármacos).\nA avaliação dos desfechos também pode afetar os resultados. É importante garantir-se que aqueles que registram os desfechos estejam cegados em relação a que grupo o sujeito da pesquisa pertence. Os autores devem estabelecer regras cuidadosas para decidir se um desfecho ocorreu ou não e despender esforços iguais para identificar desfechos para todos os pacientes no estudo.\n\nIntenção de tratar\n\nOs pesquisadores violam a randomização se omitirem da análise os pacientes que não receberam a intervenção designada ou, pior ainda, contarem eventos que ocorreram nos sujeitos não aderentes que foram designados para a intervenção contra o grupo controle. Os sujeitos de uma pesquisa, para evitar tal viés, devem ser analisados dentro do grupo para o qual eles foram alocados pela randomização (25). Este princípio é denominado intenção de tratar.\n\nAnálise da magnitude do efeito\n\nCalcula-se uma série de estimativas quantitativas para analisar a magnitude do efeito da intervenção em um ensaio clínico. Entre elas, destacam-se o Risco Relativo, Redução Relativa do Risco, Número Necessário para Tratar que serão estudados no capítulo @ref(sec-cap18).\nOutro método para avaliar resultados de um ensaio clínico para dados de tempo até o evento é a análise de sobrevida. Esta fornece informação sobre a rapidez com que os eventos ocorrem. A curva de sobrevida pode utilizar dados de pacientes acompanhados por diferentes períodos de tempo.\n\n\n3.5.3 Ensaios clínicos de equivalência e não inferioridade\nEnsaios clínicos controlados com placebo são ideais para avaliar a eficácia de um tratamento. Eles permitem o controle do efeito placebo e são mais eficientes, exigindo um menor número de pacientes para detectar um efeito do tratamento. Um ensaio clínico placebo controlado é eticamente justificado se não existe tratamento padrão, se o tratamento padrão não se mostrou eficaz, não há riscos associados com o retardo no tratamento e se a possiblidade de se retirar do estudo está incluída no protocolo. Sempre que possível e justificado, os ensaios clínicos placebo controlados devem ser a primeira escolha para avaliação de um tratamento.\nDado que um grande número de tratamentos eficazes comprovados está disponível, ensaios clínicos controlados por placebo são, muitas vezes, antiéticos. Nestas situações, ensaios clínicos com controle ativo são geralmente apropriados.\nSe o objetivo do ensaio clínico é testar se um novo tratamento é similar em eficácia a um tratamento já existente, ele é denominado de Estudo de Equivalência. O Ensaio Clínico é delineado de maneira que possa demonstrar que, dentro limites aceitáveis, os dois tratamentos são igualmente eficazes. Existe equivalência quando a diferença observada entre os dois tratamentos for menor que a máxima diferença aceitável, determinada previamente. Estes limites devem ser clinicamente apropriados. Se condição em investigação for muito grave, os limites para a equivalência devem ser estreitados. Quanto menor forem os limites de equivalência, maior o tamanho amostral. Este delineamento é útil se o novo tratamento trouxer benefícios, tais como menores efeitos colaterais, facilidade no uso e ser mais barato.\nEm muitos estudos com controle ativo, os pesquisadores desejam comprovar que o tratamento em estudo, no mínimo, não é substancialmente pior que o tratamento controle. Estes estudos são chamados de Estudos de Não Inferioridade. Um aspecto importante do delineamento e da interpretação desses estudos é a determinação da margem de não inferioridade. Os estudos de não inferioridade devem demonstrar, pelo menos, que o tratamento em estudo tem alguma eficácia, não inferior ao tratamento padrão. A análise dos estudos de não inferioridade é, por natureza, unidirecional.\nQuando um ensaio clínico busca evidenciar que um tratamento é melhor do que outro ele é denominado Estudos de Superioridade. Quando o ensaio clínico é delineado, ele deve ter uma hipótese bilateral e o tamanho da amostra definido de maneira que haja alto poder estatístico para detectar uma diferença clinicamente significativa entre os dois tratamentos. Os ensaios clínicos clássicos têm esta característica. Entretanto, nos dias atuais, este desenho de estudo pode não ser eticamente possível, uma vez que é pouco provável que não exista um tratamento com algum benefício comprovado. A comparação, portanto, deverá ser feita com o tratamento já existente, provando que o tratamento em estudo é similar ou, pelo menos, não seja inferior (26).\n\n\n3.5.4 Outros tipos de ensaios clínicos\n\n3.5.4.1 Ensaio clínico com delineamento cruzado\nNo delineamento cruzado (crossover design), os sujeitos da pesquisa são randomizados para um grupo e depois mudados para o outro grupo (Figura 3.6). Cada sujeito serve como seu próprio controle, diminuindo a variabilidade intragrupo, aumentando o poder e consequentemente, reduzindo o erro \\(\\beta\\) (erro que ocorre quando a análise estatística dos dados não consegue rejeitar uma hipótese, no caso desta hipótese ser falsa). É um tipo de delineamento bastante atrativo e útil (27).\nA maior desvantagem é o efeito residual (carryover), por isso os estudos cruzados devem ter um período de washout, período sem nenhum tratamento. Este período de tempo deve ser suficiente para a eliminação da droga para se ter certeza de que nenhum efeito da terapia permaneceu. Também pode haver um viés de acordo com a ordem de administração das terapias, pois os pacientes podem reagir de modo diferente como resultado do entusiasmo no início do tratamento que pode diminuir com o tempo.\n\n\n\n\n\n\n\n\nFigura 3.6: Ensaio clínico randomizado com delineamento cruzado.\n\n\n\n\n\n\n\n3.5.4.2 Delineamento Fatorial\nUma variação interessante de ensaio clínico é o delineamento fatorial. Este tipo de estudo permite que sejam testadas duas drogas em apenas um estudo, assumindo que os desfechos antecipados para as duas são diferentes e que seus modos de ação são independentes. Este desenho de estudo gera economia.\nUm exemplo de delineamento fatorial é observado no Physician’s Health Study onde usando um delineamento fatorial 2 x 2 foi testada a aspirina para a prevenção primária de doença cardiovascular (28), e betacaroteno para a prevenção primária de câncer.\nNo estudo da prevenção primária do câncer, os autores concluíram, após 12 anos de suplementação de betacaroteno, que o mesmo não produziu nem benefícios e nem prejuízos em termos de incidência de câncer (29).\n\n\n\n3.5.5 Fases de um ensaio clínico\nPara a realização de um ensaio clínico, a intervenção deve passar por várias fases (30).\n\n3.5.5.1 Fase Não Clínica\nAntes de começar a testar novos tratamentos em seres humanos, os cientistas testam as substâncias em laboratórios (in vitro) e em animais de experimentação. O objetivo principal desta fase é verificar como esta substância se comporta em um organismo. Assim, após esta fase se pode verificar se o medicamento é seguro para ser testado em seres humanos. Todo este processo é regido por leis da bioética em pesquisa em animais.\n\n\n3.5.5.2 Fase Clínica\nA fase clínica é a fase de testes em seres humanos. Esta etapa é constituída por quatro fases consecutivas e somente depois de finalizadas todas as fases, a droga poderá ser autorizada para comercialização e disponibilizada para uso em seres humanos. As sucessivas fases dentro da fase clínica são:\n\nFase I - Um estudo de fase I testa a droga pela primeira vez. O objetivo principal é avaliar a segurança do produto investigado. Nesta fase, o medicamento é testado em pequenos grupos (10 – 30 pessoas), geralmente, de voluntários sadios. Podemos ter exceções se estivermos avaliando medicamentos para câncer ou portadores de HIV-AIDS. Se a droga se mostrar segura, é possível ir para a Fase II.\nFase II - Nesta fase, o número de pacientes é maior (70 - 100). O objetivo é avaliar a eficácia da medicação, isto é, se ela funciona para tratar determinada doença, e também conseguir informações mais detalhadas sobre a segurança (toxicidade). Somente se os resultados forem bons é que o medicamento será estudado como um estudo clínico fase III.\nFase III - Nesta fase, o novo tratamento é comparado com o tratamento padrão existente. São os ensaios clínicos. O número de pacientes aumenta e depende da hipótese (em geral, 100 a 1.000). Devem de preferência utilizar desfechos clínicos, grupo controle, além de serem randomizados e duplo-cegos.\nFase IV - Estes estudos são realizados para se confirmar que os resultados obtidos na fase III são aplicáveis a grande parte dos doentes. Nesta fase, o medicamento já foi aprovado para ser comercializado. A vantagem dos estudos fase IV é que eles permitem acompanhar os efeitos dos medicamentos em longo prazo. É uma fase de vigilância pós-comercialização.\n\n\n\n\n\n1. Ribeiro Mendes F. O que é um trabalho científico. Em: Iniciacão Cientifica. Autonomia Editora; 2012. p. 17–26. \n\n\n2. Hulley SB, Cummings SR, Browner WS, Grady DG, Newman TB. Elaborando a questão de pesquisa e desenvolvendo o plano de estudo. Em: Delineando a pesquisa clinica. Quarta Edição. Artmed Editora; 2015. p. 15–24. \n\n\n3. McCombes S. Sampling Methods [Internet]. https://www.scribbr.com/methodology/sampling-methods/. scribbr.com Team; 2019. Disponível em: https://www.scribbr.com/\n\n\n4. Callegari-Jacques SM. Amostras. Em: Bioestatistica: principios e aplicações. Artmed Editora; 2003. p. 146–7. \n\n\n5. Faul F, Erdfelder E, Lang A-G, Buchner A. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods. 2007;39(2):175–91. \n\n\n6. Cohen J. Statistical power analysis for the behavioral sciences. Lawrence Erlbaum Associates; 1988. \n\n\n7. Grimes DA, Schulz KF. An overview of clinical research: the lay of the land. The lancet. 2002;359(9300):57–61. \n\n\n8. Fletcher RH, Fletcher SW, Fletcher GS. Prognóstico. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 108–9. \n\n\n9. Grimes DA, Schulz KF. Descriptive studies: what they can and cannot do. The Lancet. 2002;359(9301):145–9. \n\n\n10. Fletcher RH, Fletcher SW, Fletcher GS. Risco: da doença à exposição. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 88. \n\n\n11. Grimes DA, Schulz KF. Compared to what? Finding controls for case-control studies. The Lancet. 2005;365(9468):1429–33. \n\n\n12. Ernster VL. Nested case-control studies. Preventive Medicine. 1994;23(5):587–90. \n\n\n13. Newman TB, Browner WS, Cummings SR, Hulley SB. Delineando estudos de caso-controle. Em: Delineando a pesquisa clinica. Quarta Edição. Artmed Editora; 2015. p. 111. \n\n\n14. Grimes DA, Schulz KF. Cohort studies: marching towards outcomes. The Lancet. 2002;359(9303):341–5. \n\n\n15. Fletcher RH, Fletcher SW, Fletcher GS. Risco: da doença à exposição. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 68. \n\n\n16. Kannel WB, McGee DL. Diabetes and cardiovascular risk factors: the Framingham study. Circulation. 1979;59(1):8–13. \n\n\n17. Celentano DD, Szklo M. Cohort Studies. Em: Gordis Epidemiology. 6th Edition. Elsevier; 2019. p. 179. \n\n\n18. Coutinho M. Principios de epidemiologia clínica aplicada a cardiologia. Arquivos Brasileiros de Cardiologia. 1998;71:109–16. \n\n\n19. McCambridge J, Witton J, Elbourne DR. Systematic review of the Hawthorne effect: new concepts are needed to study research participation effects. Journal of Clinical Epidemiology. 2014;67(3):267–77. \n\n\n20. Bland JM, Altman DG. Statistic Notes: Regression towards the mean. BMJ. 1994;308(6942):1499. \n\n\n21. Fletcher RH, Fletcher SW, Fletcher GS. Tratamento. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 143. \n\n\n22. Kabisch M, Ruckes C, Seibert-Grafe M, Blettner M. Randomized controlled trials: part 17 of a series on evaluation of scientific publications. Deutsches Ärzteblatt International. 2011;108(39):663. \n\n\n23. Elander G, Hermerén G. Placebo effect and randomized clinical trials. Theoretical Medicine. 1995;16(2):171–82. \n\n\n24. Schulz KF, Grimes DA. Blinding in randomised trials: hiding who got what. The Lancet. 2002;359(9307):696–700. \n\n\n25. Montori VM, Guyatt GH. Intention-to-treat principle. CMAJ. 2001;165(10):1339–41. \n\n\n26. Christensen E. Methodology of superiority vs. equivalence trials and non-inferiority trials. Journal of hepatology. 2007;46(5):947–54. \n\n\n27. Health Improvement O for, Disparities. Crossover randomised controlled trial: comparative studies [Internet]. Office for Health Improvement and Disparities. UK Health improvement; 2020. Disponível em: https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies\n\n\n28. Physicians’ Health Study Research Group* SC of the. Final report on the aspirin component of the ongoing Physicians’ Health Study. New England Journal of Medicine. 1989;321(3):129–35. \n\n\n29. Hennekens CH, Buring JE, et al. Lack of effect of long-term supplementation with beta carotene on the incidence of malignant neoplasms and cardiovascular disease. New England Journal of Medicine. 1996;334(18):1145–9. \n\n\n30. Stanley K. Design of randomized controlled trials. Circulation. 2007;115(9):1164–9.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#footnotes",
    "href": "03-producaoDados.html#footnotes",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "http://calculoamostral.bauru.usp.br/calculoamostral/index.php↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html",
    "href": "04-ambienteR.html",
    "title": "4  Ambiente do R",
    "section": "",
    "text": "4.1 Instalação do R básico\nPara usar o R, há necessidade de carregar o programa básico que contém a sua linguagem de programação. O sistema é formado por um programa básico, Graphical User Interface (R-Gui) e muitos pacotes com procedimentos adicionais.\nO site oficial do R fornece as versões atualizadas do software e informações sobre este sofisticado projeto de computação estatística.\nPara baixar o R, usa-se um “CRAN Mirror”, clicando em CRAN (Comprehensive R Archive Network) na margem esquerda, abaixo de Download. O CRAN é central no uso do R: é o local de onde se carrega o software e todos os pacotes necessários para instalar e para expandir o R.\nEm vez de ter um único local, o CRAN é “espelhado” em diferentes locais do mundo. “Espelhado” significa simplesmente que existem versões idênticas do CRAN distribuídas por todo o mundo. É possível baixar o R diretamente da nuvem ou escolher uma origem mais próxima do seu local de atuação. No Brasil, encontram-se várias opções, como a Universidade Federal do Paraná, Fundação Oswaldo Cruz, RJ, Universidade de São Paulo, São Paulo e Universidade de São Paulo, Piracicaba\nApós escolher uma das alternativas acima (pode ser qualquer uma delas) surgirá a página The Comprehensive R Archive Network com as opções para escolher o sistema operacional. Escolha o sitema de acordo com o seu computador (Windows, macOS ou Linux). Ao clicar em uma dessas opções, se o sistema operacional escolhido é o Windows, aparecerá a página R for Windows. Nesta, deve-se clicar em base. No caso de outros sistemas operacionais, seguir as orientações mostradas no site do R.\nClicando em base, haverá um redirecionamento para a a página onde aparece a versão do R para o Windows mais atual. Clique no link que diz Download R-…for Window para baixar o instalador em um diretório do computador, em geral Downloads.\nPara instalar o programa básico, basta executar o instalador R-…-win.exe baixado no diretório. Ao fazer isso, aparece na tela do computador,no canto esquerdo, em baixo, o arquivo salvo. Execute este arquivo com um clique sobre ele. Aparecerá u,a janela perguntando “Deseja permitir que este aplicativo faça alterações no seu dispositivo?”. Clique em Sim. A seguir o instalador pedirá para escolher o Idioma. Selecione Português Brasileiro.\nEm sequência aparecerão informações sobre o diretório no qual o R será instalado em seu computador. Recomenda-se aceitar a configuração padrão sugerida pelo instalador do software.\nA próxima janela pedirá para personalizar os componentes que serão instalados. Recomenda-se usar as configurações sugeridas pelo instalador que irá reconhecer automaticamente a arquitetura do seu sistema Windows (32 e/ou 64 bits).\nA partir daqui, siga as recomendações padrão propostas pelo instalador até completar a instalação, clicando em Concluir.\nO R não precisa ser iniciado, pois o software que será usado, neste livro, é o RStudio. Este, para ser executado, necessita ter o R instalado no computador. Ou seja, o R é o programa “cérebro” necessário para as análises de dados que serão realizadas. Ele precisa estar instalado para permitir o funcionamento do RStudio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#rstudio",
    "href": "04-ambienteR.html#rstudio",
    "title": "4  Ambiente do R",
    "section": "4.2 RStudio",
    "text": "4.2 RStudio\nO RStudio é um ambiente de desenvolvimento integrado (IDE1). Ele serve para facilitar a escrita, execução e depuração de código R, bem como para gerenciar projetos, visualizar dados e criar gráficos.\nO RStudio é um membro ativo da comunidade R. Foi fundado em 2009 por Joseph J. Allaire, engenheiro de software americano. O RStudio, inspirado pelas inovações dos usuários de R em ciência, educação e indústria, desenvolveu ferramentas gratuitas e abertas para facilitar o uso do R.\nO RStudio é escrito em linguagem C++ e foi inicialmente focado apenas na linguagem R. Com o tempo o desenvolveu suporte para Python e VSCode. Em 2022, para acompanhar essa mudança, foi anunciada a mudança do nome da empresa que o desenvolve para Posit e, recentemente, introduzido um novo IDE, denominado Positron, um projeto inicial, com um ambiente semelhante ao RStudio e que continua em desenvolvimento. Talvez, no futuro, possa substituir o RStudio. Por enquanto, isso será difícil , pois o Positron não tem todas as funcionalidades do RStudio.\n\n4.2.1 Instalação do R Studio\nPara instalar o RStudio , acessar o site e clicar em Download para obter a versão desejada. Recomenda-se a versão RStudio Desktop – Open Source License que é gratuita. Esta versão entrega as ferramentas integradas para o R.\nA seguir, aparecerão os instaladores disponíveis, conforme a plataforma suportada pelo seu computador. As mais utilizadas são Windows e Mac OS X. Neste livro, como base, serão mostrados os passos para a plataforma Windows 2.\nEm sequência, executar o instalador baixado RStudio-2025.05.1-513.exe 3 e seguir as suas instruções.\n\n\n4.2.2 Iniciando o RStudio\nPara iniciar o RStudio basta clicar no ícone indicativo (Figura 4.1) que se encontra no menu Iniciar do Windows.\n\n\n\n\n\n\n\n\nFigura 4.1: Ícone do RStudio\n\n\n\n\n\nO RStudio abre como mostrado na Figura 4.2. O RStudio é uma interface mais funcional e amigável para o R. Contém um conjunto de ferramentas integradas projetadas para ajudá-lo a ser mais produtivo com o R.\n\n\n\n\n\n\n\n\nFigura 4.2: Tela inicial do RStudio\n\n\n\n\n\nInclui o Console , editor que suporta execução direta de códigos e uma variedade de ferramentas robustas para plotagem, exibição de histórico, depuração e gerenciamento de seu espaço de trabalho incluídos em uma interface que está, inicialmente, dividida em 3 paineis:\n\nConsole\nEnvironment, History, Connections, Tutorial\nFiles, Plots, Packages, Help\n\nConsole e R Script\nDo lado esquerdo fica o Console (Figura 4.2), em vermelho), onde os comandos podem ser digitados e aparecem os resultados da execução dos comandos. Ao abrir o RStudio , vê-se no Console uma série de informações sobre o R, como versão em uso e, por último, o diretório onde está armazenado o espaço de trabalho (workspace). Estas informações podem ser facilmente apagadas, clicando na barra de ferramentas, no menu Edit, e após em Clear Console ou, usando as teclas Ctrl+L.\nO Console é a principal parte do R. Aqui é onde o R realmente executa o comando. No início do Console, existe um caractere (&gt;). Este é um prompt que informa que o R está pronto para receber um novo código. Pode-se digitar o código diretamente no Console após o prompt e obter uma resposta imediata. Por exemplo, se for digitado 1 + 1 e pressionado Enter, o R imediatamente gera uma saída de 2 (Figura 4.3).\n\n\n\n\n\n\n\n\nFigura 4.3: Console\n\n\n\n\n\nRecomenda-se que a maior parte dos comandos sejam digitados no bloco de notas do RStudio , o R Script. Reservar o Console apenas para depurar ou fazer análises e cálculos rápidos. A razão para isso é simples: se o comando for digitado diretamente no Console, ele não será salvo e se for cometido um erro na digitação, haverá necessidade de digitar tudo novamente. Portanto, é melhor escrever os comandos no R Script e, quando estiver pronto para executar, enviar para o Console.\nO R Script é o quarto painel do RStudio e seu bloco de notas. Ele é criado através do menu File &gt; New File &gt; R Script ou clicando no botão verde com o sinal (+), na barra de ferramentas de acesso rápido, na parte superior à esquerda. Ao criar um novo R Script será aberto o painel do bloco de notas (Figura 4.4), em verde).\n\n\n\n\n\n\n\n\nFigura 4.4: R Script\n\n\n\n\n\nUm diferencial do RStudio é que os comandos são autocompletáveis. Basta começar a escrever o comando, inserindo 3 ou mais caracteres, por exemplo, summ referente a função summary (), usada para sumarizar um conjunto de dados, e surge um menu de opções, facilitando a digitação (Figura 4.5).\n\n\n\n\n\n\n\n\nFigura 4.5: Menu autocompletável\n\n\n\n\n\nApós digitar no Console, para que seja executado o comando há necessidade de clicar na tecla Enter; no RScript, clicar em Run, acima, na barra, no lado direito, ou usar o atalho Ctrl + Enter. Textos podem ser copiados e colados no script e linhas em branco podem ser inseridas. Além disso, no final da sua sessão, é possível salvar o arquivo, que poderá ser recarregado no futuro, se precisar refazer a análise.\nOs scripts do R são apenas arquivos de texto com a extensão (.R). Quando se cria um R Script, aparece como Sem título (Untitled). Antes de começar a digitar um novo script no R Sem título, recomenda-se salvar o arquivo com um novo nome de arquivo. Dessa forma, se algo no computador falhar durante o trabalho, o R terá o código protegido.\nAo digitar o código em um script, o R não executa o código enquanto se digita. Para que o R realmente avalie o código digitado, há necessidade de primeiro enviar o código para o Console, clicando no botão Run ou usando a tecla de atalho Crtl + Enter. Cada linha é marcada no início por um número em sequência.\nAlém da digitação de comandos, o R Script permite fazer comentários onde tudo que for escrito, após o símbolo \\(\\#\\), não é considerado, é apenas uma explicação, um esclarecimento. Os comentários são literais, escritos diretamente para explicar o comando executado. São repetidos na saída do Console sem aparecer nos resultados.\nAmbiente, História, Conexão e Tutorial\nNo lado superior direito há um painel com quatro abas (Figura 4.2), em azul):\n\nAmbiente (Environment) - onde ficam armazenados os objetos criados, as bases de dados importadas, etc., na sessão ativa. É possível visualizar informações como o número de observações e linhas dos bancos de dados ativos. A guia também tem algumas ações clicáveis, como Import Dataset, que permite importar arquivos csv, Excel, SPSS, etc.\nHistória (History) - onde fica o histórico dos comandos executados no Console. Estes comandos podem ser pesquisados nesta guia. Os comandos são exibidos em ordem (mais recentes na parte inferior) e agrupados por bloco de tempo.\nConexões (Connections) - mostra todas as conexões feitas com fontes de dados suportadas e permite saber quais conexões estão ativas no momento. O RStudio suporta múltiplas conexões de banco de dados simultâneas.\nTutorial - a partir da versão 1.3, o R Script ganhou um painel Tutorial dedicado, usado para executar tutoriais que ajudarão você a aprender e dominar a linguagem de programação R. Na primeira vez que se abre o programa, clicando nesta aba, o RStudio solicita que seja instalado o pacote learnr (Figura 4.6)). Isto permite acesso a vários tutoriais úteis que merecem ser explorados\n\n\n\n\n\n\n\n\n\nFigura 4.6: Tutoriais do RStudio\n\n\n\n\n\nArquivos, Gráficos, Pacotes, Ajuda e Apresentação\nNo lado direito, abaixo, existem outras abas muito úteis (Figura 4.2), em amarelo):\n\nArquivos (Files) - esta guia dá acesso ao diretório onde se encontram os seus arquivos. Um bom recurso do painel Files é que se pode usá-lo para definir seu diretório de trabalho. Para isso, clique em More e depois em Set As Working Directory.\nGráficos (Plots) - local onde ficam os gráficos gerados. Existem botões para abrir o gráfico em uma janela separada e exportar o gráfico como um .pdf ou .jpeg.\nPacotes (Packages) - mostra uma lista de todos os pacotes R instalados no seu computador e indica se eles estão atualmente carregados ou não. Pacotes que estão sendo executados na sessão atual, estão marcados, enquanto aqueles que estão instalados, mas ainda inativos, estão desmarcados.\nAjuda (Help) - menu de ajuda para as funções R. Você pode digitar o nome de uma função na janela de pesquisa (por exemplo, histogram ou usar o ?hist), no Console ou no R Script, para procurar ajuda sobre uma função (Figura 4.7)). A Ajuda no R Studio pode também ser acessada no menu Help da barra de ferramentas onde existem várias opções. Para complementar, alguns livros são muito uteis, como o R Cookbook (1) ou Using R* for introductory statistics* (2). No entanto, na maioria das vezes a forma mais prática de conseguir ajuda com uma dúvida específica é a busca em fóruns na internet, como o Stack Overflow: https://stackoverflow.com/.\nApresentação (Presentation) – é visualizador de apresentações. Nas últimas versões do Rstudio, é possível com o Quarto, editar um código em R Markdown para construir uma apresentação. Não faz parte do objetivo deste livro desenvolver este assunto. É possível encontrar um tutorial em https://quarto.org/docs/get-started/hello/rstudio.html.\n\n\n\n\n\n\n\n\n\nFigura 4.7: Ajuda do RStudio",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#pacotes",
    "href": "04-ambienteR.html#pacotes",
    "title": "4  Ambiente do R",
    "section": "4.3 Pacotes",
    "text": "4.3 Pacotes\nPara que o R desempenhe sua função de interagir com o usuário, realizar análises estatísticas e gerar gráficos, a instalação de pacotes é essencial.\nAo instalar o R básico, ele já vem acompanhado de diversos pacotes que possibilitam uma ampla gama de análises. No entanto, à medida que o uso do R se intensifica, torna-se necessário instalar novos pacotes desenvolvidos pela comunidade, que oferecem funcionalidades adicionais por meio de novas funções e comandos.\nUm pacote é um conjunto de funções, dados e documentação que expande os recursos do R base. O uso de pacotes é fundamental para aproveitar todo o potencial da ferramenta, sendo instalados conforme as demandas do trabalho realizado no R.\n\n4.3.1 Repositório de pacotes\nQuando se identifica a necessidade de um novo pacote, é fundamental saber onde ele se encontra. O principal repositório de pacotes é o CRAN (Comprehensible R Archive Network), já comentado anteriormente. Para acessar este repositório, use o link e escolha um espelho (0-Cloud ou o mais próximo geograficamente). Depois que o pacote for instalado, ele será mantido em sua biblioteca (library) R associada à sua versão principal atual do R. Haverá necessidade de atualizar e reinstalar os pacotes sempre que atualizar uma versão principal do R.\nEstando na página do CRAN, no menu, à esquerda, clique em Packages . Isto o colocará na página dos Contributed Packages, onde a maioria dos pacotes podem ser encontrados em Table of available packages, sorted by name . Também é possível clicar em CRAN Task Views , onde estão os pacotes separados por tópicos.\n\n\n4.3.2 Instalação de um pacote novo\nInstalar um pacote significa simplesmente baixar o código do pacote em um computador pessoal. Existem duas maneiras principais de instalar novos pacotes. O método mais comum é baixá-los do CRAN, usando a função install.packages(). Dentro dos parênteses, como argumento, coloca-se entre aspas (duplas ou simples) o nome do pacote. Como visto, deve-se, de preferência, digitar o comando no R Script. Por exemplo, será instalado o pacote ggplot2 que contém múltiplas funções gráficas como abaixo:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nPara carregar o pacote, isto é, para fazer com que suas funções se tornem ativas para uso na na sessão, deve-se usar a função library(), como mostrado no comando acima. Se o RStudio for fechado e reaberto, o o pacote deverá ser novamente ativado. Observe que a função library() não requer que o nome do pacote seja digitado entre aspas. Isto acontece porque antes de o pacote ser instalado o R não o reconhece , portanto, há necessidade de indicar o nome (caracteres), para que o R procure na internet, por exemplo, o que ele deve baixar. Já, depois de instalado, o pacote é um objeto conhecido pelo R, logo as aspas não são mais necessárias.\nUma outra maneira de instalar pacotes no R, é usar o botão Install, localizado na aba Packages, no painel inferior, à direita. Clicando em Install, abre-se a caixa de diálogo da Figura 4.8. Digitar em Packages o nome do pacote (ggplot2) e o RStudio completará com opções para achar o pacote. Clicar em ggplot2 e verifique se Install dependencies foi selecionado. A seguir clicar em Install e aguardar aparecer no Console a mensagem que o pacote foi instalado com sucesso.\n\n\n\n\n\n\n\n\nFigura 4.8: Instalação do pacote ‘ggplot2’ usando a caixa de diálogo ‘Install Packages’\n\n\n\n\n\n\n\n4.3.3 Atualização dos pacotes\nPeriodicamente, há necessidade de atualizar os pacotes instalados. Essa necessidade advém do fato que, com o tempo, os autores de pacotes lançarão novas versões com correções de defeitos e novos recursos e, geralmente, é uma boa ideia manter-se atualizado. Para realizar a atualização proceda da seguinte maneira:\n\n# atualiza todos os pacotes disponíveis, solicitando permissão\nupdate.packages()\n# atualiza, sem solicitações de permissão/esclarecimento\nupdate.packages(ask = FALSE)\n# atualiza um pacote específico\nupdate.packages(\"ggplot2\")\n\n\n\n4.3.4 Instalando e carregando mais de um pacote\nPara carregar mais de um pacote simultaneamente, pode-se usar uma das funções: libraries() ou packages() do pacote easypackages. Em primeiro lugar, instalar e carregar o pacote:\n\ninstall.packages(\"easypackages\")\nlibrary(easypackages)\n\nPosteriormente, basta usar uma das funções do easypackages:\n\nlibraries(\"readxl\", \"dplyr\", \"ggplot2\", \"car\")\n\nOutro pacote que gerencia pacotes do R é o pacman (3). Este pacote tem a função p_load() que instala e carrega um ou mais pacotes. Usar esta função, escrevendo o nome dos pacotes sem necessidade de aspas:\n\ninstall.packages(\"pacman\")\nlibrary(pacman)\np_load(readxl, dplyr, ggplot2, car)\n\nOu, escrever diretamente:\n\npacman::p_load(readxl, dplyr, ggplot2, car)\n\nO pacote pacman tem outas funções, entre elas a função p_update() que atualiza o pacote e , se usada sem especificar o pacote , atualiza todos. Para saber mais sobre o pacote pacman, use a ajuda.\n\npacman::p_update(readxl, dplyr, ggplot2, car)\n\n\n\n4.3.5 Citação de pacotes em publicações\nNo R existe um comando que mostra como citar o R ou um de seus pacotes. Basta digitar a função citation() no Console ou no R Script e observar a saída. Para um pacote específico, basta colocar o nome do pacote entre aspas, na função.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation (\"ggplot2\")\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#diretório-de-trabalho",
    "href": "04-ambienteR.html#diretório-de-trabalho",
    "title": "4  Ambiente do R",
    "section": "4.4 Diretório de trabalho",
    "text": "4.4 Diretório de trabalho\nO diretório de trabalho (Working Directory) é uma pasta onde o R lê e salva arquivos. Deve-se criar um diretório de trabalho para a sessão . Para isso, no RStudio siga o caminho: Session &gt; Set Working Directory &gt; Choose Directory ou use o atalho Ctrl + Shift + H e escolha o diretório desejado ou crie um novo.\nAo finalizar, aparecerá no Console (Figura 4.9):\n\n\n\n\n\n\n\n\nFigura 4.9: Diretório de trabalho\n\n\n\n\n\nNote que o R usou a função setwd() que significa “definir diretório de trabalho”. Também é possível usar esta função diretamente no R Script ou no Console, digitando conforme o caminho do diretório.\nPara saber qual é o diretório de trabalho que está sendo usado pelo R pode-se executar a função getwd(). A saída no Console mostrará o diretório de trabalho usado, portanto é recomendado que se faça isso no início da sessão para verificar se há ou não necessidade de modificar o diretório.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#projeto",
    "href": "04-ambienteR.html#projeto",
    "title": "4  Ambiente do R",
    "section": "4.5 Projeto",
    "text": "4.5 Projeto\nUma funcionalidade importante do RStudio é a possibilidade de se criar projetos. Um projeto nada mais é do que uma pasta no seu computador. Nessa pasta, estarão todos os arquivos que serão usados ou criados na sua análise.\nA principal razão de se utilizar projetos é simplesmente organização. Com eles, fica muito mais fácil importar conjunto de dados para dentro do R, criar análises reprodutíveis e compartilhar o trabalho realizado.\nAo se começar uma nova análise, é interessante criar um Novo Projeto. Para isso, clicar File &gt; New Project ou clicar no menu que está na parte superior, à direita, Project (none) &gt; New Project…. Abrirá a janela da Figura 4.10).\n\n\n\n\n\n\n\n\nFigura 4.10: Assistente de novo projeto.\n\n\n\n\n\nClique em New Directory para criar um novo diretório. Por exemplo, para as aulas de Bioestatística, pode-se criar um diretório com o nome de bioestatistica (evite usar acentos, maiúsculas ou caracteres especiais) ou qualquer outro nome.\nQuaisquer documentos Excel ou arquivos de texto associados podem ser salvos nesta nova pasta e facilmente acessados, indo ao menu Project (none) &gt; Open Project…. A partir daí, é possível realizar análises de dados ou produzir visualizações com seus dados importados.\nQuando um projeto estiver aberto no RStudio, o seu nome aparecerá no canto superior direito da tela. Na aba Files, aparecerão todos os arquivos contidos no projeto. Quando se clica no nome do projeto, abre um menu que torna muito fácil a navegação pelos projetos existentes. Basta clicar em qualquer um deles para trocar de projeto, isto é, deixar de trabalhar em uma análise e começar a trabalhar em outra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#o-r-como-calculadora",
    "href": "04-ambienteR.html#o-r-como-calculadora",
    "title": "4  Ambiente do R",
    "section": "4.6 O R como calculadora",
    "text": "4.6 O R como calculadora\nO R pode ser utilizado para uma série de operações matemáticas desde as mais simples às mais complexas. Para isso, basta digitar no Console ou no R Script, usando os operadores.\n\n4.6.1 Operadores\nOperadores são usados para realizar operações com variáveis e valores.\nOperadores aritméticos\nNo R, você pode usar operadores aritméticos para realizar operações matemáticas comuns.\n\n 10 + 5        # Adição\n\n[1] 15\n\n 10 - 5        # Subtração\n\n[1] 5\n\n 10 * 5        # Multiplicação\n\n[1] 50\n\n 10 / 5        # Divisão\n\n[1] 2\n\n 10 ^ 5        # Potência\n\n[1] 1e+05\n\n 10 %% 3       # Divisão modular (divisão com resto)\n\n[1] 1\n\n 10 %/% 3      # Divisão inteiro\n\n[1] 3\n\n\nObserve que o R repete a operação e coloca em baixo o resultado precedido por [1]. O resultado da operação de exponenciação é exibido como notação científica, onde \\(e+05\\) significa \\(10^5\\).\nOperadores de atribuição\nOperadores de atribuição são usados para atribuir valores a variáveis, como será visto na Seção 4.7, adiante.\nOperadores de comparação\nSão usados para comparar dois valores.\n\n# Igualdade\n3 == 3\n\n[1] TRUE\n\n3 == 4\n\n[1] FALSE\n\n# Não igual (diferente)\n3 != 4\n\n[1] TRUE\n\n# Maior\n6 &gt; 3\n\n[1] TRUE\n\n# Menor\n3 &lt; 4\n\n[1] TRUE\n\n# Maior ou igual\n5 &gt;= 3\n\n[1] TRUE\n\n# Menor ou igual  \n3 &lt;= 4\n\n[1] TRUE\n\n\nATENÇÃO, na linguagem R, o sinal de igualdade é escrito com duplo \\(=\\).\nOperadores lógicos\nOperadores lógicos são usados para combinar declarações condicionais:\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros \n6 == 6 & 7 == 8\n\n[1] FALSE\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros\n2 * 3 && 1 * 6\n\n[1] TRUE\n\n# Conjunção lógica OU, retorna TRUE se um dos elementos é verdadeiro\n(2 * 2) | sqrt(16)\n\n[1] TRUE\n\n6 == 6 | 7 == 8 \n\n[1] TRUE\n\n# Conjunção lógica NÃO, retorna FALSE se o  elemento é verdadeiro\n!6==6\n\n[1] FALSE\n\n!2==4\n\n[1] TRUE\n\n# Operador lógico que verifica se um elemento pertence a um conjunto (%in%)\n\n pares &lt;- c(0, 2, 4, 6, 8, 10)\n 5 %in% pares\n\n[1] FALSE\n\n\nOutros operadores\n\n# Logarítmo natural (base e)\nlog (10) \n\n[1] 2.302585\n\n# Logarítmo base 10\nlog10 (10)       \n\n[1] 1\n\n# Raiz quadrada\nsqrt (81)\n\n[1] 9\n\n# Resultado absoluto\nabs (3 - 6)\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-objetos",
    "href": "04-ambienteR.html#sec-objetos",
    "title": "4  Ambiente do R",
    "section": "4.7 Objetos",
    "text": "4.7 Objetos\nO R permite salvar valores dentro de um objeto. Os objetos são criados utilizando o operador de atribuição (&lt;-). Para digitar este operador, basta teclar o sinal menor que (&lt;), seguido de hífen (-) , sem espaços. Existe um atalho que é pressionar (Alt) \\(+\\) (-). O símbolo \\(=\\) pode ser usado no lugar de &lt;-.\nObjeto é um pequeno espaço na memória do computador onde o R armazenará um valor ou o resultado de um comando, utilizando um nome arbitrariamente definido. Tudo criado pelo R pode se constituir em um objeto, por exemplo: uma variável, uma operação aritmética, um gráfico, uma matriz ou um modelo estatístico. Através de um objeto torna-se simples acessar os dados armazenados na memória. Ao criar um objeto, se faz uma declaração. Isto significa que se está afirmando que uma determinada operação aritmética irá, agora, tornar-se um objeto que irá armazenar um determinado valor. As declarações são feitas uma em cada linha do R Script.\nOs objetos devem receber um nome e é obrigatório que ele comece por uma letra (ou um ponto) e não é permitido o uso do hífen. Pode-se usar o ponto ou underlines para separar palavras. Deve ser evitado o uso de nomes que sejam de objetos do sistema, ou outros objetos já criados, funções ou constantes. Por exemplo, não deve ser utilizado: c, q, r, s, t, C, D, F, I, T, diff, exp, log, mean, pi, range, rank, var, NA, NaN, NULL, FALSE, TRUE, break, else, if, break, function, in, while que devem ser reservados, pois têm significados especiais.\nQuando se usa um objeto com o nome pi, ele assumirá outro valor diferente de 3,141593. Preservando este nome, toda vez que usarmos a palavra pi, o R assume o valor pré-estabelecido. Além disso, o R faz a diferença entre letras maiúsculas e minúsculas. Ou seja, soma é um objeto diferente de Soma e ambos são diferentes de SOMA.\nPara exibir o conteúdo de um objeto, basta digitar seu nome no R Script ou no Console e executar. Em análises mais extensas, verificar se já há um objeto com o mesmo nome, pois seus valores serão substituídos ao executar o novo objeto. Para saber se já existe um objeto com o nome definido, digite as primeiras letras do objeto criado e o R Studio listará, usando a sua função de autocompletar, tudo que começar com essas letras no arquivo. Assim ficará fácil verificar se já existe um objeto com o nome desejado.\nNo comando abaixo, é criado um objeto que receberá a soma de dez números, utilizando a função sum(). O objeto foi denominado de soma. Para exibir o valor contido no objeto soma, é necessário digitar soma no R Script ou Console e executar:\n\nsoma &lt;- sum (2, 3, 12, 15, 21, 4, 8, 7, 13, 21)\nsoma\n\n[1] 106",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-funcoes",
    "href": "04-ambienteR.html#sec-funcoes",
    "title": "4  Ambiente do R",
    "section": "4.8 Funções",
    "text": "4.8 Funções\nA função é uma orientação ao R para que ele execute uma ação que é algum procedimento específico. Em decorrência, em geral, uma função ttem um nome sugestivo da ação que ela realiza. Por exemplo, a função mean () realiza a média aritmética de uma série de números. O resultado, como regra geral, deve ser colocado em um objeto que será armazenado na memória do computador.\nEsta série de números, concatenados na função c(), é armazenada por um objeto, nomeado dadose, posteriormente, se usa a função mean()com este objeto dados. O resultado da função mean, exibido no Console, será recebido por outro objeto media_dados e colocado na memória do computador.\n\ndados &lt;- c(3, 5, 7, 9, 6, 7)\nmedia_dados &lt;- mean(dados)\nmedia_dados\n\n[1] 6.166667\n\n\nAs funções podem ser criadas pelo pesquisador, de acordo com as suas necessidades. Entretanto, na maioria das vezes, elas são encontradas prontas, fazendo parte de um pacote. Pacotes contêm muitas funções que para serem executadas necessitam que estes estejam instalados e carregados. As funções para exercerem a sua ação devem receber dentro delas (entre parênteses) os argumentos que elas exigem. Os argumentos de uma função são sempre separados por vírgulas.\nPara se saber quais argumentos necessários para uma determinada função basta consultar a ajuda, onde se encontrará a documentação da mesma. Para isso basta digitar no Console, no caso da função mean(), help(mean) ou ?mean:\n\nhelp(mean)\n\nO resultado deste comando aparecerá na aba Help, na parte inferior, à direita (Figura 4.11):\n\n\n\n\n\n\n\n\nFigura 4.11: Ajuda para Média Aritmética.\n\n\n\n\n\nOs principais argumentos da função mean() são:\n\nx \\(\\to\\) vetor numérico\ntrim \\(\\to\\) fração das observações (varia de 0 a 0,5) extraída de cada extremidade de x para calcular a média aparada\nna.rm \\(\\to\\) valor lógico (TRUE ou FALSE) que indicam se os valores ausentes (NA) devem ser removidos antes que o cálculo continue\n\nEste último argumento é muito importante quando, na sequência de valores existe algum não informado ou inexistente. No R, eles são denominados de valores ausentes (missing values) e denotados por NA (Not Available).\nPor exemplo, em uma coleta de uma série de valores, correspondentes ao peso de 15 recém-nascidos, havendo a “falta” de um dos registros, ao calcular a média com a função mean(), ela retornará NA.\n\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,NA,3970,3060,3180,  \n             2865,2815,3245,2051,2630)\nmean (pesoRN)\n\n[1] NA\n\n\nColocando o argumento na.rm = TRUE, para remover os valores faltantes, a função retornará a média aritmética sem este valor:\n\nmean (pesoRN, na.rm = TRUE)\n\n[1] 3227.929\n\n\n\n4.8.1 Criando funções\nNo R, é possível criar funções pessoais que podem simplificar um código e, eventualmente, diminuir o tempo de execução das análises.\nFórmula geral\nAs funções têm uma fórmula geral:\n\nnome_da_funcao &lt;- function (x){transformar x}\n\nPor exemplo, a área de um circulo é igual a \\(\\pi\\times raio^2\\). Para calcular a área do círculo, pode-se criar uma função que faça este trabalho:\n\narea.circ &lt;- function(r){\n  area &lt;- pi*r^2\n  return(area)                \n}\n\nOu seja, foi usada a função function(), com o raio do círculo como argumento. A seguir, entre chaves {}, coloca-se a ação que a função realizará, no caso o cálculo da área do círculo. O resultado deste cálculo (pi*r^2) é recebido por um objeto denominado area.4 A seguir, usou-se a função return () para retornar o resultado do cálculo realizado.\nAo executar essa função, é possível usá-la para calcular a área de um círculo, cujo raio é igual a 5 cm:\n\narea.circ(5)\n\n[1] 78.53982\n\n\nOutros exemplos\nO Indice de Massa Corporal é igual ao peso (kg) dividido pela \\(altura^2\\), em metros. Uma função para fazer este cálculo é:\n\nimc &lt;- function(peso, altura){\n  res &lt;- peso/altura^2\n  return(res)\n}\n\nLogo, o IMC de um indivíduo que tenha 67 kg e 1,7 m é:\n\npeso &lt;-  67\naltura &lt;-  1.70\nimc(67, 1.70)\n\n[1] 23.18339\n\n\nOs exemplos mostrados são muito simples. Quase não haveria necessidade de construir uma função. Entretanto, quando se tem uma ação mais complexa, a função mostra a sua utilidade. Por exemplo, se for necessário realizar a comparação entre duas médias, usando um teste t e apresentar o resultado junto com boxplots, a função fica mais complexa. Sempre que for necessário cálculo semelhante, a função automatiza a ação, sem necessidade de repetir os códigos:\n\nplotBpT &lt;- function(df, var.x, var.y){\n  library(ggplot2)\n  library(ggpubr)\n  ggplot(df, aes(x = {{var.x}}, y = {{var.y}}, fill = {{var.x}})) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1) +\n    geom_boxplot() +\n    theme_classic() +\n    theme(legend.position = \"none\") +\n    stat_compare_means(method = \"t.test\", label.x = 0.5)\n}\n\nNeste momento, não serão discutidos os códigos da função. Ela será utilizada como uma função qualquer com os dados do arquivo dadosPop.xlsx^[Para maiores detalhes, consulte a o Capítulo 12. Os argumentos da função são o dataframe (df = dados), a variável x (var.x = pop) e a variável y (var.y = altura). Dessa forma, está se comparando a altura de mulheres de duas populações de duas regiões diferentes :\n\ndados &lt;- readxl::read_excel(\"dados/dadosPop.xlsx\")\ndados$pop &lt;- as.factor(dados$pop)\nplotBpT (df = dados, var.x = pop, var.y = altura)\n\n\n\n\n\n\n\nFigura 4.12: Comparação da altura de mulheres em duas populações\n\n\n\n\n\nObserve na Figura 4.12 onde aparece o resultado do teste t (\\(P = 2,2 \\times 10^-16\\)) e os boxplots em posições bem diferentes (veja Seção 6.5.5).\nAtivação de uma função criada\nPara ativar uma função previamente criada, usa-se a função nativa source (). O argumento desta função é o caminho (no exemplo, é o diretório do autor) onde se encontra a função buscada, por exemplo, a função imc() criada acima:\n\nsource('C:/Users/petro/Dropbox/Estatistica/Bioestatistica_usando_R/Funcoes/imc.R')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#classes",
    "href": "04-ambienteR.html#classes",
    "title": "4  Ambiente do R",
    "section": "4.9 Classes",
    "text": "4.9 Classes\nSão os atributos de um objeto e o seu conhecimento é de suma importância. É a partir do conhecimento do tipo de classe que as funções sabem o que extamente fazer com um objeto. Por exemplo, não é possivel somar duas letras e se for feita a tentativa de somar “a” e “b”, o Rretorna um erro:  Error in “a” + “b”: non-numeric argument to binary operator .\nNo R, os textos são escritos entre aspas simples ou duplas. As aspas servem para diferenciar nomes (objetos, funções, pacotes) de textos (letras e palavras). Os textos são muito comuns em variáveis categóricas e são popularmente chamados de strings ou character. Além desta classe, o R tem outras classes básicas que são a numeric e a logical. Um objeto de qualquer uma dessas classes é chamado de objeto atômico. Esse nome se deve ao fato de essas classes não se misturarem (4).\nPara saber qual o tipo de classe que um objeto pertence, basta usar a função class().\n\nidade &lt;- c(3, 5, 7, 9, 6, 7)\nclass (idade)\n\n[1] \"numeric\"\n\nnome &lt;- c(\"Pedro\", \"Maria\", \"Margarida\", \"Alice\", \"João\", \"Luís\")\nclass(nome)\n\n[1] \"character\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-vetores",
    "href": "04-ambienteR.html#sec-vetores",
    "title": "4  Ambiente do R",
    "section": "4.10 Vetores",
    "text": "4.10 Vetores\nUm vetor é uma variável com um ou mais valores do mesmo tipo. Por exemplo, o número de filhos em 10 famílias foi 4, 5, 3, 2, 2, 1, 2, 1, 3 e 2. O vetor nomeado de n.filhos é um objeto numérico de comprimento = 10. A maneira mais fácil de criar um vetor em R é concatenar (ligar) os 10 valores, usando a função concatenar c(), vista acima:\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\nn.filhos\n\n [1] 4 5 3 2 2 1 2 1 3 2\n\n\nComo os vetores são conjuntos indexados, pode-se dizer que cada valor dentro de um vetor tem uma posição. Essa posição é dada pela ordem em que os elementos foram colocados no momento em que o vetor foi criado. Isso nos permite acessar individualmente cada valor de um vetor (4).\nPara acessar um determinado valor, basta colocar a posição do mesmo entre colchetes [ ]. Se há interesse em conhecer o número de filhos da quinta família, procede-se da seguinte forma:\n\nn.filhos[5]\n\n[1] 2\n\n\nSe houver tentativa de acessar um valor inexixtente, o R retorna NA.\n\nn.filhos[11]\n\n[1] NA\n\n\nSe houver necessidade de excluir um dos elementos, basta colocar entre colchetes a posição do mesmo com sinal negativo. Por exemplo, para excluir o valor correspondente a sexta família, usa-se:\n\nn.filhos[-6]\n\n[1] 4 5 3 2 2 2 1 3 2\n\n\nObserva-se que o valor 1 foi excluído da série de elementos.\nQuando são colocados elementos em um vetor que pertençam a classes diferentes, o R promove o que se denomina de coerção, pois o vetor pode ter apenas uma classe de objeto. Dessa forma, as classes mais fortes reprimem as mais fracas. Por exemplo, sempre que for misturado números e texto em um vetor, os números serão considerados como texto:\n\nvetor &lt;- c(12, 15, 4, 6, \"A\", \"D\")\nvetor\n\n[1] \"12\" \"15\" \"4\"  \"6\"  \"A\"  \"D\" \n\n\nVeja que, agora, todos os elementos do vetor passaram a ser textos e, porisso, estão entre aspas.\n\n4.10.1 Tipos de vetores\nDado um vetor, pode-se determinar seu tipo com typeof(), ou verificar se é um tipo específico com uma das funções: is.character(), ’is.double(),is.integer(),is.logical( )`.\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\ntypeof(n.filhos)\n\n[1] \"double\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\n\nAs expressões do tipo character devem aparecer entre aspas duplas ou simples. Os números no R são geralmente tratados como objetos numéricos (números reais de dupla precisão). Mesmo números inteiros são tratados como numéricos. Para fazer um número inteiro ser tratado como objeto inteiro, deve-se utilizar a letra L após o número.\nOs valores lógicos (ou booleanos) são TRUE ou FALSE. T ou F também são aceitos.\n\nn.filhos &lt;- c(4L, 5L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L)\ntypeof(n.filhos)\n\n[1] \"integer\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\nis.double(n.filhos)\n\n[1] FALSE\n\n\n\nnomes &lt;- c('Maria', 'João', 'Manuel', 'Petronio', 'José')\ntypeof(nomes)\n\n[1] \"character\"\n\nis.numeric(nomes)\n\n[1] FALSE\n\nis.double(nomes)\n\n[1] FALSE\n\n\n\naltura &lt;- c(1.60, 1.78, 1.55, 1.67, 1.69)\ntypeof(altura)\n\n[1] \"double\"\n\nis.numeric(altura)\n\n[1] TRUE\n\nis.double(altura)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-dataframes",
    "href": "04-ambienteR.html#sec-dataframes",
    "title": "4  Ambiente do R",
    "section": "4.11 Dataframes",
    "text": "4.11 Dataframes\nDataframes são objetos de dados genéricos do R em formato tabular, onde os dados são organizados de maneira lógica em linha-e-coluna semelhante ao de uma planilha do Excel. O dataframe é uma estrutura bidimensional. Estas dimensões podem ser encontradas com a função dim(). Os dataframes podem ser formados com objetos criados previamente, desde que tenham o mesmo comprimento (5). Abaixo serão criadas algumas variáveis, todas relacionadas ao nascimento de 15 bebês:\n\nid &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,3380,3970,3060,3180,  \n             2865,2815,3245,2051,2630)  \ncompRN &lt;- c (50,48,52,48,50,51,50,51,47,47,47,49,51,50,44)\nsexo &lt;- c (2,2,2,1,1,1,2,1,1,1,2,2,1,1,2)\ntipoParto &lt;- c (1,1,2,1,2,2,1,2,1,1,1,2,1,1,1)\nidadeMae &lt;- c (40,19,26,19,32,24,27,20,21,19,23,36,21,23,23) \n\nTem-se um grupo de variáveis isoladas. Seria útil reuni-las em um só objeto. Pode-se fazer isso, usando a função data.frame(). Este novo objeto receberá o nome de dadosNeonatos.\n\ndadosNeonatos &lt;- data.frame (id,\n                             pesoRN, \n                             compRN, \n                             sexo, \n                             tipoParto, \n                             idadeMae)\n\nAo ser executado o comando retornará um novo objeto da classe data.frame:\n\nclass (dadosNeonatos)\n\n[1] \"data.frame\"\n\n\nHavendo necessidade de acrescentar outra variável no dataframe dadosNeonatos, por exemplo, os dados da ida ou não dos recém-nascidos para a UTI. Para isso, será atribuido a um vetor, contendo a situação dos 15 recém-nascidos, o nome de utiNeo e para relacioná-lo a uma coluna do dataframe dadosNeonatos, será usado o símbolo $, como mostrado abaixo 5:\n\ndadosNeonatos$utiNeo &lt;- c (2,2,2,2,1,2,1,2,2,2,2,1,2,2,2)\n\nPara observar a modificação realizada, pode-se usar a função str() do R base 6, digitando no R Script:\n\nstr (dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : num  2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: num  1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : num  2 2 2 2 1 2 1 2 2 2 ...\n\n\nNa saida da função, verifica-se que o dataframe contém 15 linhas e 6 colunas e que todas as variáveis estão como variáveis numéricas , mas as variáveis sexo, tipoParto são variáveis categóricas, bem como a variável utiNeo, acrescentada depois. Há necessidade de fazer uma transformação dessas variáveis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-fatores",
    "href": "04-ambienteR.html#sec-fatores",
    "title": "4  Ambiente do R",
    "section": "4.12 Fatores",
    "text": "4.12 Fatores\nOs fatores são usados para trabalhar com variáveis categóricas. São variáveis usadas para categorizar e armazenar os dados, tendo um número limitado de valores diferentes. Um fator armazena os dados como um vetor de valores inteiros. O fator em R também é conhecido como uma variável categórica que armazena valores de dados de string e inteiros como níveis. O fator é usado principalmente em modelagem estatística e análise exploratória de dados com R (6).\n\n4.12.1 Criando fatores\nNo data frame dadosNeonatos, criado anteriormente, contém três variáveis (sexo, tipoParto e utiNeo) que estão como variáveis numéricas. É possível, desta forma, realizar operações aritméticas com elas. Isto, obviamente, seria um absurdo. Assim, é necessário transformá-las em fatores. Para isso, pode ser usada a função factor(), nativa do R. Os principais argumentos desta função são:\n\nx \\(\\to\\) vetor numérico\nlevels \\(\\to\\) vetor opcional dos valores que x pode assumir\nlabels \\(\\to\\) vetor de caracteres dos rótulos para os níveis, na mesma ordem\nordered \\(\\to\\) vetor lógico (TRUE ou FALSE). Se TRUE, os níveis dos fatores são assumidos como ordenados\n\nNo exemplo, as variáveis não têm uma ordem lógica, então, o argumento ordered não é necessário.\n\ndadosNeonatos$utiNeo &lt;- factor (dadosNeonatos$utiNeo,\n                                levels = c(1,2), \n                                labels = c('sim','não'))\ndadosNeonatos$tipoParto &lt;- factor(dadosNeonatos$tipoParto, \n                                   levels = c(1,2),\n                                   labels = c(\"normal\",\"cesareo\"))\ndadosNeonatos$sexo &lt;- factor (dadosNeonatos$sexo, \n                               levels = c(1,2), \n                               labels = c(\"M\",\"F\")) \n\nApós a transformação, executa-se novamente a função str() para ver como ficou a estrutura do dataframe:\n\nstr(dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : Factor w/ 2 levels \"M\",\"F\": 2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 1 2 2 2 ...\n\n\nAgora, as três varáveis passaram a ser fatores e as outras mantiveram-se numéricas.\nDesta forma, é possível trabalhar com ela fazendo, por exemplo, uma contagem da frequência do tipo de parto, usando a função table():\n\ntable(dadosNeonatos$tipoParto)\n\n\n normal cesareo \n     10       5 \n\n\nOu seja, aproximadamente 70% dos partos desta amostra são normais.\n\n\n4.12.2 Salvando o dataframe criado\nO dataframe, criado e modificado anteriormente, pode ser salvo para uso posterior no diretório de trabalho.\nA função save() realiza esta ação, usando como argumentos o dataframe a ser salvo e o nome do arquivo (file =) entre aspas. Por convenção, esta função salva com a extensão .RData que deve ser digitada, pois o R não a adiciona automaticamente.\n\nsave(dadosNeonatos, file = \"dadosNeonatos.RData\")\n\nEste comando colocará o arquivo no diretório de trabalho em uso. Portanto, se o objetivo é salvar em outro local, deve ser informado qual o novo diretório.\nPara carregar o objeto salvo anteriormente com o comando save(), usa-se a função load(). Se o arquivo a ser lido não estiver no diretório de trabalho da sessão, há necessidade de especificar o caminho até o arquivo:\n\nload(\"dadosNeonatos.RData\")\n\nOu, indicando o diretório onde está o arquivo:\n\nload(\"C:/Users/petro/Dropbox/Estatistica/Meus_Livros/Bioestatistica_R/Book/dadosNeonatos.RData\")\n\nÉ possível salvar em outro tipo de extensão como Excel (.xlsx), Valores Separados por Vírgula (.csv), etc. O procedimento é o mesmo, mudando a função. Para salvar em uma extensão .xlsx,utiliza-se a função write_xlsx () do pacote writexl (7):\n\nwritexl::write_xlsx(dadosNeonatos, \"dadosNeonatos.xlsx\")\n\nPara salvar com a extensão .csv, usar a função write.csv() ou write.csv2() que faz parte do pacote utils, incluido no R base. A primeira função, usa \".\" para a separação dos decimais e \",\" para separar as variáveis; a segunda função usa \",\" para os decimais e \";\" para separar as variáveis, convenção do Excel para algumas localidades, como o Brasil (8). Portanto, uma maneira de salvar o arquivo é:\n\nwrite.csv2 (dadosNeonatos, \"dadosNeonatos.csv\")\n\n\n\n\n\n1. Chang W. Cookbook for R. Cookbook for R. http://www.cookbook-r.com; 2021. \n\n\n2. Verzani J. Using R for introductory statistics. Chapman; Hall/CRC; 2004. \n\n\n3. Rinker TW, Kurkiewicz D. pacman: Package Management for R [Internet]. Buffalo, New York; 2018. Disponível em: http://github.com/trinker/pacman\n\n\n4. Damiani A, Milz B, Lente C, al et. Ciência de Dados em R [Internet]. R6 Consultoria; 2015. Disponível em: https://livro.curso-r.com/index.html\n\n\n5. Zuur AF, Ieno EN, Meesters EH. Getting Data into R. Em: A Beginner’s Guide to R. Springer; 2009. p. 29–56. \n\n\n6. Wickham H, Grolemund G. 15 Factors|R for data science [Internet]. Welcome | R for Data Science. O’Reilly; 2017. Disponível em: https://r4ds.had.co.nz/factors.html\n\n\n7. Ooms J. writexl: Export Data Frames to Excel ’xlsx’ Format [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=writexl\n\n\n8. Team RC. write.table: Data Output/CSV files [Internet]. DataCamp; 2022. Disponível em: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#footnotes",
    "href": "04-ambienteR.html#footnotes",
    "title": "4  Ambiente do R",
    "section": "",
    "text": "Do inglês: Integrated Development Environment↩︎\nA instalação para Mac OS X pode ser facilmente obtida em busca do Google. Depois de instalado, o uso do RStudio não difere do Windows↩︎\nVersão disponível em 10/06/2025↩︎\nFoi usado o nome area sem acentuação, mas poderia ser qualquer nome.↩︎\nA variável criada, utiNeo, possui dois níveis: 1 = sim; 2 = não, referente se o bebê foi ou não para a UTI.↩︎\na função mostra a estrutura do dataframe↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html",
    "href": "05-manipulandoDados.html",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "5.1 Importando dados de outros softwares\nÉ possível inserir dados diretamente no R Script, como mostrado na Seção 4.11. Entretanto, se o conjunto de dados for muito extenso, torna-se complicado. Desta forma, é melhor importar os dados de outro software, como o Excel, SPSS, etc. A recomendação é que se construa o banco de dados, por exemplo, no Excel, e, depois, exporte o arquivo em um formato que o R reconheça – .xlsx, .csv, .sav.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "href": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "5.1.1 Importando dados de um arquivo CSV\nO formato CSV significa Comma Separated Values, ou seja, é um arquivo de valores separados por vírgula. Esse formato de armazenamento é simples e agrupa informações de arquivos de texto em planilhas. É possível gerar um arquivo .csv, a partir de uma planilha do Excel, usando o menu salvar como e escolher CSV.\nAs funções read.csv() e read.csv2(), incluídas no R base, podem ser utilizadas para importar arquivos CSV. Existe uma pequena diferença entre elas. Dois argumentos dessas funções têm padrão diferentes em cada uma. São eles: sep (separador de colunas) e dec (separador de decimais). Na read.csv(), o padrão é sep = ”,” e dec = ”.” e em read.csv2() o padrão é sep = “;” e dec = ”,”. Portanto, quando se importa um arquivo .csv, é importante saber qual a sua estrutura. Verificar se os decimais estão separados por ponto ou por vírgula e se as colunas (variáveis), por vírgula ou ponto e vírgula. Para ver isso, basta abrir o arquivo em um bloco de notas (por exemplo, Bloco de Notas do Windows, Notepad ++).\nQuando se usa o read.csv() há necessidade de informar o separador e o decimal, pois senão ele usará o padrão inglês e o arquivo não será lido. Já com read.csv2(), que usa o padrão brasileiro, não há necessidade de informar ao R qual o separador de colunas e nem o separador dos decimais.\nAlém disso, é necessário saber em que diretório do computador está o arquivo para informar ao comando. Recomenda-se colocar o arquivo na pasta do diretório de trabalho, pois assim basta apenas colocar o nome do arquivo na função de leitura dos dados. Caso contrário, tem-se que se usar todo o caminho (path).\nComo exemplo, será importado o arquivo dadosNeonatos.csv que se encontra no diretório de trabalho do autor, salvo anteriormente. Para obter o arquivo, clique aqui e salve em seu diretório de trabalho.\nA estrutura deste arquivo mostra que as colunas estão separadas por ponto-e-virgula e, portanto, a leitura dos dados será feita com a função read.csv2() e, como o arquivo está no diretório de trabalho, não há necessidade de informar o diretório completo. Os dados serão colocados em um objeto de nome neonatos 1:\n\nneonatos &lt;- read.csv2(\"./dados/dadosNeonatos.csv\")\n\nUse a função str() para visualizar o conjunto de dados:2\n\nstr(neonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : int  3340 3345 3750 3650 3220 4070 3380 3970 3060 3180 ...\n $ compRN   : int  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr  \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr  \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : int  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr  \"não\" \"não\" \"não\" \"não\" ...\n\n\nRecentemente, foi desenvolvido o pacote readr, incluído no conjunto de pacotes tidyverse (1), para lidar rapidamente com a leitura de grandes arquivos. O pacote fornece substituições para funções como read.csv(). As funções read_csv() e read_csv2() oferecidas pelo readr são análogas às do R base. Entretanto, são muito mais rápidas e fornecem mais recursos, como um método compacto para especificar tipos de coluna. Além disso, produzem tibbles (ver adiante, Seção 5.2) que são mais reproduzíveis, pois as funções básicas do R herdam alguns comportamentos do sistema operacional e das variáveis de ambiente, portanto, o código de importação que funciona no seu computador pode não funcionar no de outra pessoa. Para usar a função é necessário instalar e ativar o pacote readr. A função read_csv2() será utilizada para criar um outro objeto de nome recemNascidos, mas o conjunto de dados a ser ativado é o mesmo (dadosNeonatos):\n\n library(readr)\n recemNascidos &lt;- read_csv2(\"dados/dadosNeonatos.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 15 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): sexo, tipoParto, utiNeo\ndbl (4): id, pesoRN, compRN, idadeMae\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQuando você executa read_csv2(), ele imprime uma especificação de coluna que fornece o nome e o tipo de cada coluna.\nNovamente, a função str() mostrará a estrutura do arquivo 3:\n\nstr(recemNascidos)\n\nspc_tbl_ [15 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   pesoRN = col_double(),\n  ..   compRN = col_double(),\n  ..   sexo = col_character(),\n  ..   tipoParto = col_character(),\n  ..   idadeMae = col_double(),\n  ..   utiNeo = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n5.1.2 Importando um arquivo do Excel\nO pacote readxl, pertencente ao conjunto de pacotes do tidyverse, facilita a obtenção de dados do Excel para o R, através da função read_excel(). esta função tem o argumento sheet = , que deve ser usado indicando o número ou o nome da planilha, colocado entre aspas. Este argumento é importante se houver mais de uma planilha, caso contrário, ele é opcional. Para saber os outros argumentos da função, colque o cursor dentro da função e aperte a tecla Tab (Figura 5.1). Isto abrirá um menu com os argumentos:\n\n\n\n\n\n\n\n\nFigura 5.1: Argumentos da função para importar arquivos xlsx\n\n\n\n\n\nSerá feita a leitura dos mesmos dados, usados na leitura de dados csv, apenas o arquivo agora está no formato .xlsx. Para obter o arquivo, siga os mesmos passos, usados anteriormente. Clique aqui e salve em seu diretório de trabalho.\nOs dados serão atribuídos a um objeto com outro nome (recemNatos):\n\nrecemNatos &lt;- readxl::read_excel(\"dados/dadosNeonatos.xlsx\")\nstr(recemNatos)\n\ntibble [15 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n\n\nNa Figura 5.1, o duplo dois pontos (::) precedido do nome do pacote, no caso readxl, especifica a procedência da função usada. Nesta situação, não há necessidade de usar a função library() para carregar o pacote já instalado em um diretório (biblioteca) previamente.\n\n\n5.1.3 Importando arquivos com o RStudio\nO RStudio permite importar arquivos sem a necessidade de digitar comandos, que, para alguns podem ser tediosos.\nNa tela inicial do RStudio, à direita, na parte superior, clique na aba Environment e em Import Dataset. Esta ação abre um menu que permite importar arquivos .csv, Excel, SPSS, etc.\nPor exemplo, para importar o arquivo dadosNeonatos.xlsx, clicar em From Excel... Abre uma janela com uma caixa de diálogo. Clicar no botão Browse..., localizado em cima à direita, para buscar o arquivo dadosNeonatos.xlsx. Assim que o arquivo for aberto, ele mostra uma preview do arquivo e, em baixo, à direita mostra uma preview do código (Figura 5.2)), igual ao digitado anteriormente, que cria um objeto denominado dadosNeonatos, nome do objeto escolhido pelo R, mas pode ser modificado na janela, à esquerda, Import Option em Name, onde pode-se digitar qualquer nome. Após encerrar as escolhas, clicar em Import. É um caminho diferente para fazer o mesmo. Este é um dos fascínios do R!\n\n\n\n\n\n\n\n\nFigura 5.2: Importando arquivos do excel com o RStudio.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-tibble",
    "href": "05-manipulandoDados.html#sec-tibble",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.2 Tibble",
    "text": "5.2 Tibble\nA maneira mais comum de armazenar dados no R é usar data.frames ou tibble.\nTibble é um novo tipo de dataframe. É como se fosse um dataframe mais moderno. Ele mantém muitos recursos importantes do data frame original, mas remove muitos dos recursos desatualizados.\nOs tibbles são outro recurso incrível adicionado ao R por Hadley Wickham, através do tidyverse, conjunto de pacotes que formam um conjunto básico de funções que facilitam a manipulação e representação gráfica dos dados (1). Para saber mais sobre tibble, veja vignette(‘tibbles’), em `help´.\nA maioria dos pacotes do R usa dataframes tradicionais, entretanto é possível transformá-los para tibble, usando a função as_tibble(), incluída no pacote tidyr (2). O único propósito deste pacote é simplificar o processo de criação de tidy data(dados organizados).\nO conceito de tidy data, introduzido por Wickman (3), se refere à estrutura dos dados organizados de maneira que cada linha é uma observação, cada coluna representa variáveis e cada entrada nas células do dataframe são os valores. A transformação de um dataframe tradicional em um tibble, é um procedimento rescomendável, em função da maior flexibilidade destes.\nComo exemplo deste procedimento, será usado o famoso conjunto de dados da flor iris (4) que fornece as medidas em centímetros das variáveis comprimento e largura da sepala e comprimento e largura da pétala, repectivamente, para 50 flores de cada uma das 3 espécies de íris (Iris setosa, versicolor e virginica). Este conjunto de dados encontra-se no pacote datasets no R base. Para visualizar os dados, será usado a função str(), também do R base, que mostra a estrutura interna de um objeto:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nObserva-se que é um conjunto de dados da classe data.frame, contendo 150 observações de 5 variáveis (colunas). Fazendo a coerção para um tibble, tem-se:\n\nlibrary(tidyr)\nas_tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nVerifica-se que não houve grandes mudanças, apenas o conjunto de dados está estruturalmente mais organizado, mais flexível.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-mater",
    "href": "05-manipulandoDados.html#sec-mater",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.3 Pacote dplyr",
    "text": "5.3 Pacote dplyr\nO pacote dpylr é comumente usado para limpar e trabalhar com dados (5). No nível mais básico, as funções do pacote referem-se a “verbos” de manipulação de dados, como select, filter, mutate, arrange, summarize, entre outros, que permitem encadear várias etapas em algumas linhas de código, como será visto adiante.\nO pacote dplyr é adequado para trabalhar com um único conjunto de dados, bem como para obter resultados complexos em grandes conjuntos de dados. As funções dplyr são processadas mais rápido do que as funções R base.\nPara trabalhar na manipulação dos dados serão usados alguns pacotes, já mencionados anteriormente, readxl e dplyr, e o conjunto de dados dadosMater.xlsx. Para obter estes dados, clique aqui e faça o download para o seu diretório de trabalho, como orientado anteriormente.\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAnexando pacote: 'dplyr'\n\n\nOs seguintes objetos são mascarados por 'package:stats':\n\n    filter, lag\n\n\nOs seguintes objetos são mascarados por 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nmater &lt;- read_excel(\"dados/dadosMater.xlsx\")\n\nA função read_excel() carrega o arquivo e o atribui a um objeto , arbitrariamente, denominado de mater4.\n\nas_tibble(mater)\n\n# A tibble: 1,368 × 30\n      id idadeMae altura  peso ganhoPeso anosEst   cor eCivil renda  fumo\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1       42   1.65  69.9       3.9       3     2      1  1.45     2\n 2     2       29   1.66  78        16.5      11     1      2  2.41     2\n 3     3       19   1.72  81         5         9     2      1  1.93     2\n 4     4       31   1.55  74        43         5     2      2  1.45     2\n 5     5       34   1.6   60        15         7     2      2  0.48     2\n 6     6       29   1.5   60        11.4       8     2      2  0.96     1\n 7     7       30   1.54  75.5      10.5       4     1      2  1.2      1\n 8     8       34   1.63  61         9         6     1      2  2.41     2\n 9     9       17   1.68  57        15        10     1      2  2.17     2\n10    10       32   1.5   70        11.4       1     2      2  0.72     2\n# ℹ 1,358 more rows\n# ℹ 20 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;dbl&gt;, para &lt;dbl&gt;,\n#   droga &lt;dbl&gt;, ig &lt;dbl&gt;, tipoParto &lt;dbl&gt;, pesoPla &lt;dbl&gt;, sexo &lt;dbl&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;dbl&gt;, obito &lt;dbl&gt;, hiv &lt;dbl&gt;, sifilis &lt;dbl&gt;, rubeola &lt;dbl&gt;,\n#   toxo &lt;dbl&gt;, infCong &lt;dbl&gt;\n\n\nPor padrão, a função retorna as dez primeiras linhas. Além disso, colunas que não couberem na largura da tela serão omitidas. Também são apresentadas a dimensão da tabela e as classes de cada coluna. Observa-se que ele tem 1368 linhas (observações) e 30 colunas (variáveis). Além disso, verifica-se que todas as variáveis estão como numéricas (dbl) e, certamente, algumas, dependendo do objetivo na análise, precisarão ser transformadas.\nO significado de cada uma das variáveis do arquivo dadosMater.xlsx 5 é mostrado abaixo.\n\nid \\(\\to\\) identificação do participante\n\nidadeMae \\(\\to\\) idade da parturiente em anos\n\naltura \\(\\to\\) altura da parturiente em metros\n\npeso \\(\\to\\) peso da parturiente em kg\n\nganhoPeso \\(\\to\\) aumento de peso durante a gestação\n\nanosEst \\(\\to\\) anos de estudo completos\n\ncor \\(\\to\\) cor declarada pela parturiente: 1 = branca; 2 = não branca\n\neCivil \\(\\to\\) estado civil: 1 = solteira; 2 = casada ou companheira\n\nrenda \\(\\to\\) renda familiar em salários minimos\n\nfumo \\(\\to\\) tabagismo: 1 = sim; 2 = não\n\nquantFumo \\(\\to\\) quantidade de cigarros fumados diariamente\n\nprenatal \\(\\to\\) realizou pelo menos 6 consultas no pré-natal? 1 = sim; 2 = não\npara \\(\\to\\) número de filhos paridos\n\ndroga \\(\\to\\) drogadição? 1 = sim; 2 = não\n\nig \\(\\to\\) idade gestacional em semanas\n\ntipoParto \\(\\to\\) tipo de parto: 1 = normal; 2 = cesareana\n\npesoPla \\(\\to\\) peso da placenta em gramas\nsexo \\(\\to\\) sexo do recém-nascido (RN): 1 = masc; 2 = fem\n\npesoRN \\(\\to\\) peso do RN em gramas\n\ncompRN \\(\\to\\) comprimento do RN em cm\n\npcRN \\(\\to\\) perímetro cefálico dorecém-nascido em cm\n\napgar1 \\(\\to\\) escore de Apgar no primeiro minuto\n\napgar5 \\(\\to\\) escore de Apgar no quinto minuto\n\nutiNeo \\(\\to\\) RN necessitou de terapia intesiva? 1 = sim; 2 = não\n\nobito \\(\\to\\) obito no período neonatal? 1 = sim; 2 = não\n\nhiv \\(\\to\\) parturiente portadora de HIV? 1 = sim; 2 = não\n\nsifilis \\(\\to\\) parturiente portadora de sífilis? 1 = sim; 2 = não\n\nrubeola \\(\\to\\) parturiente portadora de rubéola? 1 = sim; 2 = não\n\ntoxo \\(\\longrightarrow\\) parturiente portadora de toxoplasmose? 1 = sim; 2 = não\n\ninfCong \\(\\to\\) parturiente portadora de alguma infecção congênita? 1 = sim; 2 = não\n\n\n5.3.1 Função select()\nA função select () é usada para escolher quais colunas (variáveis) entrarão na análise. Ela recebe os nomes das colunas como argumentos e cria um novo banco de dados usando as colunas selecionadas. A função select () pode ser combinada com outras funções, como filter ().\nPor exemplo, um novo banco de dados será criado (mater1), contendo as mesmas 1368 linhas, mas apenas com as variáveis idadeMae, altura, peso, anosEst, renda, ig, fumo, pesoRN, sexo. Consulte a ajuda (?select()) para obter maiores informações em relação aos argumentos da função:\n\nmater1 &lt;- select(mater, idadeMae, altura, peso, anosEst, renda, ig, tipoParto, fumo, pesoRN, sexo)\n\nPara visualizar este novo banco de dados, pode-se usar a função str():\n\nstr(mater1)\n\ntibble [1,368 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: num [1:1368] 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n\n\nComo mostrado anteriormente, muitas variáveis numéricas do mater, na realidade, são fatores e necessitam de modificação. Entretanto, das selecionadas, para constituir o novo banco de dados, apenas tipoParto, fumo e sexo necessitam serem transformadas para fator:\n\nmater1$tipoParto &lt;- factor(mater1$tipoParto, \n                           levels = c(1,2),\n                           labels = c(\"normal\",\"cesareo\"))\n\nmater1$fumo &lt;- factor (mater1$fumo,\n                       levels = c(1,2), \n                       labels = c('sim','não'))\n\nmater1$sexo &lt;- factor (mater1$sexo, \n                       levels = c(1,2), \n                       labels = c(\"masc\",\"fem\"))\n\nUsando, de novo, a função str(), é possível observar a transformação:\n\nstr(mater1)\n\ntibble [1,368 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nSe houver necessidade de se excluir alguma variável (coluna), basta colocar o sinal de subtração (-) antes do nome da variável:\n\nmater2 &lt;- select(mater1, -altura)\n\n\nstr(mater2)\n\ntibble [1,368 × 9] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n5.3.2 Função filter()\nA função filter() é usada para criar um subconjunto de dados que obedeçam determinadas condições lógicas: & (e), | (ou) e ! (não). Por exemplo:\n\ny & !x \\(\\to\\) seleciona y e não x\nx & !y \\(\\to\\) seleciona x e não y\nx | !x \\(\\to\\) seleciona x ou y\nx & !x \\(\\to\\) seleciona x e y\n\nUm recém-nascido é dito a termo quando a duração da gestação é igual a 37 a 42 semanas incompletas. Se quisermos extrair do banco de dados mater1 os recém-nascidos a termo (mater3), pode-se usar a função filter():\n\nmater3 &lt;- filter (mater1, ig&gt;=37 & ig&lt;42)\n\nPara exibir o resultado, execute a função str():\n\nstr(mater3)\n\ntibble [1,085 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1085] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:1085] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:1085] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:1085] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:1085] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:1085] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:1085] 3285 3100 3100 2800 3270 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nObserve que, agora, o conjunto de dados mater3 tem 1085 linhas, número de recém-nascidos a termo do banco de dados original mater (1368). Logo, os recém nascidos a termo correspondem a 79.3% dos nascimentos, nesta maternidade.\nOutro exemplo\nPara selecionar apenas os meninos, nascidos a termo, codificados como \"masc\", procede-se da seguinte maneira6:\n\nmeninos &lt;- filter (mater3, sexo == 'masc')\n\n\nstr(meninos)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:592] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:592] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:592] 3285 3100 3100 2800 3270 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nO banco de dados meninos é constituídos por 592 meninos. Isto representa 43.3% dos nascimentos.\nUma outra maneira de se fazer a mesma coisa, é usar a função grepl(), dentro da função filter (). Ela é usada para pesquisar a correspondência de padrões. No código a seguir, pesquisa-se os registros em que a variável sexo contém “fem”, correspondentes às meninas.\n\nmeninas &lt;- filter (mater3, grepl(\"fem\", sexo))\n\n\nstr(meninas)\n\ntibble [493 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:493] 17 30 27 28 17 21 28 19 24 43 ...\n $ altura   : num [1:493] 1.65 1.6 1.53 1.4 1.55 1.52 1.58 1.55 1.72 1.6 ...\n $ peso     : num [1:493] 60 54 43.5 60 78 52 50 60.5 60 53 ...\n $ anosEst  : num [1:493] 7 5 11 8 10 11 11 8 8 4 ...\n $ renda    : num [1:493] 1.92 1.92 1.93 2.17 4.82 0.96 4.82 1.69 0.96 1.92 ...\n $ ig       : num [1:493] 38 39 39 38 40 38 37 38 40 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 1 1 2 1 1 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 1 1 1 2 ...\n $ pesoRN   : num [1:493] 3165 3150 2980 3095 3020 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n5.3.3 Função mutate()\nEsta função tem a finalidade de computar ou anexar uma ou mais colunas (variáveis) novas.\nO Índice de Massa Corporal (IMC) é igual a\n\\[IMC = \\frac {peso}{altura ^2}\\].\nO peso deve ser expreso em kg e a altura em metros. Para acrescentar este indicador no banco de dados mater1 , se fará uso da função mutate(), nomeando a nova variável de imc:\nSerá acrescentado a variável imc, no banco de dados mater1, usando a função mutate(), nomenando essa variável de imc:\n\nmater1 &lt;- mutate(mater1, imc = peso/altura^2)\nstr (mater1)\n\ntibble [1,368 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n $ imc      : num [1:1368] 25.7 28.3 27.4 30.8 23.4 ...\n\n\n\n\n5.3.4 Função sample_n()\nFunção usada para selecionar de forma aleatória linhas de um dataframe. Acostume-se a usar a ajuda (?sample_n) para obter informações das funções. Os seus argumentos básicos são:\n\ntbl \\(\\to\\) dataframe\nsize \\(\\to\\) número de linhas para selecionar\nreplace \\(\\to\\) amostra com ou sem reposição?. Padrão = FALSE\n\nUma mostra de 20 neonatos do banco de dados meninos, pode ser selecionada, usando a função sample_n:\n\nmeninos1 &lt;- sample_n(meninos, 20)\n\nAssim, selecionou-se uma amostra de 20 meninos do banco de dados de recém-nascidos a termo. Pode-se, fazer um resumo da variável peso do recém-nascido (meninos1$pesoRN) para ver como ela se comporta, usando a função summary():\n\nsummary(meninos1$pesoRN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1425    3162    3432    3367    3695    4660 \n\n\nÉ importante mencionar que toda vez que a função sample_n() for executada, ela irá gerar uma amostra aleatória diferente. Em consequência não se deve esperar, por exemplo, que a média dos pesos dos recém-nascidos de amostras diferentes sejam iguais.\n\nmeninos2 &lt;- sample_n(meninos, 20)\nsummary(meninos2$pesoRN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2270    2995    3210    3171    3435    3920 \n\n\nNa Seção 9.2.2 (Distribuições Amostrais), este assunto voltará à cena.\nA função sample_n() está sendo eliminada, pois será substituída por slice_sample() do conjunto de funções que acompanham a função slice().\n\n\n5.3.5 Função slice()\nEsta função é usada para selecionar um subconjunto linhas com base em seus locais inteiros. Permite selecionar, remover e duplicar linhas. Para os exemplos, será usado o conjunto de dados meninos, criado acima. Os argumentos básicos da função slice() são\n\n.data \\(\\to\\) dataframe\n.by \\(\\to\\) seleciona por grupo\nn, prop \\(\\to\\) fornecer n, o número de linhas, ou prop, a proporção de linhas a serem selecionadas.\n\nExemplpos:\n\n# Selecionando a linha 10\nslice(.data = meninos, 10)\n\n# A tibble: 1 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       14   1.51  56.7       6  1.92    39 normal    não     3200 masc \n\n# Selecionando varias linhas, por exemplo, de 1 a 5\nslice(.data = meninos, 1:5)\n\n# A tibble: 5 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       28   1.5   48.5       6  3.13    37 normal    não     3285 masc \n2       31   1.55  65         5  0.72    37 cesareo   não     3100 masc \n3       27   1.6   60         8  2.41    37 cesareo   sim     3100 masc \n4       28   1.58  47         8  1.69    38 normal    não     2800 masc \n5       18   1.76  65.5       7  1.93    39 normal    sim     3270 masc \n\n\nÉ possível também selecionar linhas de acordo com determinado grupo. Como grupo, será usada a variável fumo que tem dois níveis (sim e não). Será selecionada uma linha de cada grupo:\n\nslice(.data = meninos, .by = fumo, 1)\n\n# A tibble: 2 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       28    1.5  48.5       6  3.13    37 normal    não     3285 masc \n2       27    1.6  60         8  2.41    37 cesareo   sim     3100 masc \n\n\nPara separar por grupo , é possível usar a função group_by(), incluída no pacote dplyr. A melhor solução, neste caso, para aplicar diversas funções de manipulação em um dataframe é aplicar o operador pipe: %&gt;%. No final desta seção, será discutido com mais detalhes este operador.\n\n meninos %&gt;% \n   group_by(fumo) %&gt;% \n   slice (1)\n\n# A tibble: 2 × 10\n# Groups:   fumo [2]\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       27    1.6  60         8  2.41    37 cesareo   sim     3100 masc \n2       28    1.5  48.5       6  3.13    37 normal    não     3285 masc \n\n\nA saída desse código é a mesma vista anteriormente. A vantagem é termos escrito o código na ordem em que as funções são aplicadas. Portanto, é um código mais legível.\n\n5.3.5.1 Funções auxiliares da função slice()\nA função slice() é acompanhada por várias funções auxiliares para casos de uso comuns:\n\nslice_head() e slice_tail() selecionam a primeira ou a última linha;\nslice_sample() seleciona linhas aleatoriamente, substitui a sample_n();\nslice_min() e slice_max() selecionam linhas com valores mais altos ou mais baixos de uma variável.\n\nPor exemplo, para selecionar uma amostra aleatória de 20 meninos do conjunto de dados meninos, pode-se usar a função slice_sample():\n\n meninos3 &lt;- slice_sample(.data = meninos, n = 20)\n str (meninos3)\n\ntibble [20 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:20] 28 23 14 35 21 27 27 39 32 25 ...\n $ altura   : num [1:20] 1.56 1.73 1.51 1.54 1.7 1.52 1.65 1.59 1.49 1.66 ...\n $ peso     : num [1:20] 57 80 56.7 68 68 71.5 60 61 52 96.5 ...\n $ anosEst  : num [1:20] 11 10 6 11 11 6 7 11 1 5 ...\n $ renda    : num [1:20] 3.61 2.41 1.92 3.61 4.82 4.34 1.45 4.82 0.89 1.93 ...\n $ ig       : num [1:20] 40 38 39 39 37 41 39 38 38 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 1 2 1 2 1 1 2 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 1 2 2 2 2 1 2 2 2 ...\n $ pesoRN   : num [1:20] 2920 2770 3200 3480 2900 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nComo se observa no código, o argumento n da função deve ser nomeado explicitamente (n = 20). É possível também usar o argumento prop, colocando a proporção de linhas que se deseja selecionar. Se o objetivo é selecionar 10% da amostra, coloca-se o argumento como prop = 0.10. Como o dataframe meninos contém 592 casos, serão selecionados 59. Além disso, a função permite selecionar por grupos com o argumento by =.\n\nmeninos4 &lt;- slice_sample(.data = meninos, prop = 0.10)\nstr (meninos4)\n\ntibble [59 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:59] 32 25 30 25 28 23 26 26 25 20 ...\n $ altura   : num [1:59] 1.61 1.54 1.75 1.56 1.55 1.65 1.59 1.65 1.5 1.6 ...\n $ peso     : num [1:59] 76.4 54 64 56 53 70 79 46 49 49 ...\n $ anosEst  : num [1:59] 3 3 5 8 5 11 4 8 8 8 ...\n $ renda    : num [1:59] 2.41 1.45 0.72 4.82 1.2 0.98 1.92 1.2 1.92 2.41 ...\n $ ig       : num [1:59] 39 37 39 41 39 40 40 38 38 41 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 1 2 1 1 1 1 2 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 2 2 2 2 2 2 ...\n $ pesoRN   : num [1:59] 3485 2805 3540 2710 3255 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nPara maiores informações em relação a estas funções consulte a ajuda (?slice()).\n\n\n\n5.3.6 Função arrange()\nOrdena as linhas pelos valores de uma coluna de forma ascendente ou descentente.\nVoltando a amostra meninos1, será colocado em ordem crescente a variável pesoRN:\n\narrange(meninos1, pesoRN)\n\n# A tibble: 20 × 10\n   idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1       28   1.62  68         4  2.41    40 normal    não     1425 masc \n 2       19   1.65  60         5  1.92    37 normal    sim     2350 masc \n 3       40   1.65  60         5  1.2     41 normal    sim     2860 masc \n 4       42   1.6   77.5       4  1.45    37 normal    não     3065 masc \n 5       25   1.55  70        10  3.13    40 cesareo   sim     3140 masc \n 6       27   1.65  60         7  1.45    39 normal    sim     3170 masc \n 7       17   1.65  76         7  1.2     40 normal    não     3205 masc \n 8       29   1.65  61        11  3.61    38 normal    não     3350 masc \n 9       27   1.52  80         6  1.69    39 normal    sim     3400 masc \n10       27   1.55  67         2  2.89    40 normal    não     3410 masc \n11       25   1.76  98        11  1.45    39 cesareo   sim     3455 masc \n12       22   1.58  62         6  1.45    40 normal    não     3460 masc \n13       28   1.62 100        13  9.64    41 cesareo   não     3510 masc \n14       25   1.65  49         8  2.29    39 cesareo   não     3585 masc \n15       24   1.68  77         2  1.45    38 cesareo   não     3690 masc \n16       32   1.47  57        11  4.34    41 cesareo   sim     3710 masc \n17       24   1.61  72         8  1.25    40 normal    não     3730 masc \n18       26   1.59  79         4  1.92    40 normal    não     3855 masc \n19       32   1.68  72         6  1.92    39 cesareo   não     4315 masc \n20       29   1.71  87        10  1.92    41 normal    não     4660 masc \n\n\nPara a ordem decrescente, colocar a função desc(), dentro da função arrange()\n\narrange(meninos1, desc(pesoRN))\n\n# A tibble: 20 × 10\n   idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1       29   1.71  87        10  1.92    41 normal    não     4660 masc \n 2       32   1.68  72         6  1.92    39 cesareo   não     4315 masc \n 3       26   1.59  79         4  1.92    40 normal    não     3855 masc \n 4       24   1.61  72         8  1.25    40 normal    não     3730 masc \n 5       32   1.47  57        11  4.34    41 cesareo   sim     3710 masc \n 6       24   1.68  77         2  1.45    38 cesareo   não     3690 masc \n 7       25   1.65  49         8  2.29    39 cesareo   não     3585 masc \n 8       28   1.62 100        13  9.64    41 cesareo   não     3510 masc \n 9       22   1.58  62         6  1.45    40 normal    não     3460 masc \n10       25   1.76  98        11  1.45    39 cesareo   sim     3455 masc \n11       27   1.55  67         2  2.89    40 normal    não     3410 masc \n12       27   1.52  80         6  1.69    39 normal    sim     3400 masc \n13       29   1.65  61        11  3.61    38 normal    não     3350 masc \n14       17   1.65  76         7  1.2     40 normal    não     3205 masc \n15       27   1.65  60         7  1.45    39 normal    sim     3170 masc \n16       25   1.55  70        10  3.13    40 cesareo   sim     3140 masc \n17       42   1.6   77.5       4  1.45    37 normal    não     3065 masc \n18       40   1.65  60         5  1.2     41 normal    sim     2860 masc \n19       19   1.65  60         5  1.92    37 normal    sim     2350 masc \n20       28   1.62  68         4  2.41    40 normal    não     1425 masc \n\n\n\n\n5.3.7 Função count()\nPermite contar rapidamente os valores únicos de uma ou mais variáveis. Esta função tem os seguintes argumentos:\n\nx \\(\\to\\) dataframe\nwt \\(\\to\\) pode ser NULL (padrão) ou uma variável\nsort \\(\\to\\) padrão = FALSE; se TRUE, mostrará os maiores grupos no topo\nname \\(\\to\\) O nome da nova coluna na saída; padrão = NULL\n\nQuando o argumento name é omitido, a função retorna n como nome padrão.\nUsando o dataframe mater1, a função count() irá contar o número de parturientes fumantes, variável dicotômica fumo:\n\ncount(mater1, fumo)\n\n# A tibble: 2 × 2\n  fumo      n\n  &lt;fct&gt; &lt;int&gt;\n1 sim     301\n2 não    1067\n\n\n\n\n5.3.8 Operador pipe %&gt;%\nO operador pipe %&gt;% pode ser usado para inserir um valor ou um objeto no primeiro argumento de uma função. Ele pode ser acionado digitando %&gt;% ou usando o atalho ctrl + shift + M. Em vez de passar o argumento para a função separadamente, é possível escrever o valor ou objeto e, em seguida, usar o pipe para convertê-lo como o argumento da função na mesma linha. Funciona como se o pipe jogasse o objeto dentro da função seguinte.\nVários comando foram utilizados, manipulando o banco de dados mater. Alguns procedimentos, serão mostrados, usando, agora, o operador pipe.\nEm primeiro lugar, serão selecionadas algumas colunas do dataframe mater; adicionada a variável imc; selecionado os recém-nascidos a termo do sexo masculino, que no banco de dados mater está codificado como 1. Tudo em um só comando!\n\nmeusDados &lt;- mater %&gt;% \n  select(idadeMae, altura, peso, anosEst, renda, \n         ig, tipoParto, fumo, pesoRN, sexo) %&gt;% \n  mutate(imc = peso/altura^2) %&gt;% \n  filter (ig&gt;=37 & ig&lt;42, sexo == 1)\n\n\nstr(meusDados)\n\ntibble [592 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:592] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:592] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: num [1:592] 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : num [1:592] 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:592] 3285 3100 3100 2800 3270 ...\n $ sexo     : num [1:592] 1 1 1 1 1 1 1 1 1 1 ...\n $ imc      : num [1:592] 21.6 27.1 23.4 18.8 21.1 ...\n\n\nObserve que o dataframe mater aparece apenas no início e, como ele é um argumento das outras funções, ele é transferido, automaticamente, não havendo necessidade de escrever dentro na função.\nNo final, retornará um novo dataframe que foi colocado em um objeto, denominado meuDados, o qual contém informações de todos os 592 meninos, nascidos a termo e de suas mães.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#manipulação-de-datas",
    "href": "05-manipulandoDados.html#manipulação-de-datas",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.4 Manipulação de datas",
    "text": "5.4 Manipulação de datas\nOriginalmente, todos os que trabalham com o R queixavam-se de como era frustrante trabalhar com datas. Era um processo que causava grande perda de tempo nas análises. O pacote lubridate (6) foi criado para simplificar ao máximo a leitura de datas e extração de informações das mesmas.\nQuando o lubridate é carregado aparece uma mensagem, avisando que alguns nomes de funções também estão contidas no pacote base do R.\n\nlibrary(lubridate)\n\n\nAnexando pacote: 'lubridate'\n\n\nOs seguintes objetos são mascarados por 'package:base':\n\n    date, intersect, setdiff, union\n\n\nPara evitar confusões e verificar que as funções corretas estão sendo usadas, usa-se o duplo dois pontos (::) antes do nome da função, precedido do nome do pacote, por exemplo: lubridate::date().\nPara obter a data atual ou a data-hora, você pode usar as funções today()7 ou now()8:\n\ntoday() \n\n[1] \"2025-06-17\"\n\nnow()\n\n[1] \"2025-06-17 18:00:55 -03\"\n\n\n\n5.4.1 Convertendo strings ou caractere para data\nPara converter string ou caracteres em datas, basta executar funções específicas adequadas aos dados. Elas determinam automaticamente o formato quando você especifica a ordem do componente. Para usá-los, identifique a ordem em que o ano, o mês e o dia aparecem em suas datas e, em seguida, organize “y”, “m” e “d” na mesma ordem. Isso lhe dá o nome da função do lubridate que analisará a data. Por exemplo, suponhamos a data de 25/12/2022:\n\nnatal &lt;- \"25/12/2022\"\nnatal\n\n[1] \"25/12/2022\"\n\n\nAparentemente, o R aceitou a informação como uma data. Entretanto, se for verificada a classe do objeto, tem-se:\n\nclass(natal)\n\n[1] \"character\"\n\n\nEstando como caractere, esta data não poderá ser usada em operações com datas, pois necessitaria estar como uma classe date. Para converte-la em data, usa-se a função dmy():\n\nnatal &lt;- dmy(natal)\nnatal\n\n[1] \"2022-12-25\"\n\nclass(natal)\n\n[1] \"Date\"\n\n\nDessa forma, a data, agora está sendo reconhecida pelo R como date. É sempre importante verificar a classe da data.\nÀs vezes, as datas escritas estão com o mês abreviado, como 25/dez/2022. O procedimento é o mesmo\n\nminha.data &lt;- \"25/dez/2022\"\nclass (minha.data)\n\n[1] \"character\"\n\n\n\nminha.data &lt;- dmy(minha.data)\nclass (minha.data)\n\n[1] \"Date\"\n\n\nSe além da data, houver necessidade de especificar o horário, basta usar dmy_h(), dmy_hm() e dmy_hms(). No padrão americano, pode ser usado ymd().\nO lubridate traz diversas funções para extrair os componentes de um objeto da classe date.\n\nsecond() \\(\\rightarrow\\) extrai os segundos.\nminute() \\(\\rightarrow\\) extrai os minutos.\nhour() \\(\\rightarrow\\) extrai a hora.\nwday() \\(\\rightarrow\\) extrai o dia da semana.\nmday() \\(\\rightarrow\\) extrai o dia do mês.\nmonth() \\(\\rightarrow\\) extrai o mês.\nyear() \\(\\rightarrow\\) extrai o ano.\n\nPor exemplo, usando a data de nascimento (dn) de um dos netos do autor:\n\ndn &lt;- dmy(\"06/06/2018\")\nyear(dn)\n\n[1] 2018\n\n\nPara acrescentar um horário ao objeto data de nascimento (dn)^[UTC = Coordinated Universal Time}:\n\nhour(dn) &lt;- 18\ndn\n\n[1] \"2018-06-06 18:00:00 UTC\"\n\n\n\n\n5.4.2 Juntando componentes de datas\nPara juntar componentes de datas e horas, pode-se utilizar as funções make_date() e make_datetime(). Em muitos arquivos, os componentes da data estão em colunas diferentes e há necessidade de juntá-los em uma única coluna para compor a data:\n\nfelix &lt;- make_date(year = 2018, month = 06, day = 06)\nfelix\n\n[1] \"2018-06-06\"\n\n\nPara juntar ano, mês, dia, hora e minuto:\n\nminha.data &lt;- make_datetime(year = 2018, \n                            month = 06, \n                            day = 06, \n                            hour = 18 ,\n                            min = 00, \n                            sec = 15)\nminha.data\n\n[1] \"2018-06-06 18:00:15 UTC\"\n\n\n\n\n5.4.3 Extraindo componentes de datas\nQuando temos objetos do tipo POSIXt9 podemos extrair componentes ou elementos deles. Para isso são usadas algumas funções específicas do pacote lubridate como mostrado a seguir.\n\ndata &lt;- now()\n\nyear(data)            # Extrai o ano\n\n[1] 2025\n\nmonth(data)           # Extrai o mês\n\n[1] 6\n\nweek(data)            # Extrai a semana\n\n[1] 24\n\nday(data)             # Extrai o dia\n\n[1] 17\n\nminute(data)          # Extrai o minuto\n\n[1] 0\n\nsecond(data)          # Extrai o segundo\n\n[1] 56.26519\n\n\nPara verificar o número de dias tem em um determinado mês, usa-se a função days_in_month():\n\n data1 &lt;- dmy(\"25/02/2000\")\n days_in_month(data1)          \n\nFeb \n 29 \n\n\n\n\n5.4.4 Operações com datas\nO pacote lubridate possui funções de duração e de período para manipular as datas. As funções de duração calculam o número de segundos em um determinado num determinado número de dias. As funções de duração não levam em consideração anos bissextos e horário de verão, enquanto as funções de período consideram esses fatores.\n\nddays (1)           # Número de segundos em 1 dia\n\n[1] \"86400s (~1 days)\"\n\ndhours (1)          # Número de segundos em 1 hora\n\n[1] \"3600s (~1 hours)\"\n\ndminutes (1)        # Número de segundos em 1 minuto\n\n[1] \"60s (~1 minutes)\"\n\ndays (5)            # Cria um período de 5 dias\n\n[1] \"5d 0H 0M 0S\"\n\nweeks (5)           # Cria um período de 5 semanas\n\n[1] \"35d 0H 0M 0S\"\n\n\nSuponha-se que haja necessidade de saber em qual dia cairá após acrescentarmos 5 semanas à data1 (25/02/2000), criada acima:\n\ndata1 + weeks (5)           \n\n[1] \"2000-03-31\"\n\n\nAdicionando 1 ano à data1 (25/02/2000) com uma função de duração, tem-se:\n\ndata1 + dyears (1)           \n\n[1] \"2001-02-24 06:00:00 UTC\"\n\n\nSe for adicionado um ano à mesma data, mas agora com uma função de período, tem-se:\n\ndata1 + years (1)           \n\n[1] \"2001-02-25\"\n\n\nUm intervalo de tempo pode ser obtido a partir de uma data inicial e uma data final. Suponha que uma gestante tenha como data da sua última menstruação 04/10/2022 e o bebê tenha nascido em 30/06/2023. Qual a idade gestacional em dias? A sintaxe para calcular um intervalo é dada pela subtração das duas datas:\n\ndata.inicial &lt;- dmy(\"04/10/2022\")\ndata.final &lt;- dmy(\"30/06/2023\")\nidade_gesta &lt;- data.final - data.inicial\nidade_gesta\n\nTime difference of 269 days\n\n\nOu seja a gestação durou 269 dias, constituindo-se em um parto a termo, entre 37 (259 dias) e 42 semanas (294 dias).\nPara mais informações sobre o lubridate, consulte a ajuda do pacote ou o capítulo 16 do livro R for Data Science, Hadley Wickman e Garrett Grolemund, 2017 [https://r4ds.had.co.nz/index.html] .\n\n\n\n\n1. Wickham H, Averick M, Bryan J, Chang W, et al. Welcome to the Tidyverse. Journal of Open Source Software. 2019;4(43):1686. \n\n\n2. Wickham H, Girlich M. tidyr: Tidy Messy Data [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=tidyr\n\n\n3. Wickham H. Tidy Data. Journal of Statistical Software. 2014;59(10):11–23. \n\n\n4. Fisher RA. The use of multiple measurements in taxonomic problems. Annals of eugenics. 1936;7(2):179–88. \n\n\n5. Wickham H, François R, Henry L, Müller K, et al. dplyr: A grammar of data manipulation. R package version 04. 2015;3:156. \n\n\n6. Grolemund G, Wickham H. Dates and Times Made Easy with lubridate. Journal of Statistical Software [Internet]. 2011;40(3):1–25. Disponível em: https://www.jstatsoft.org/v40/i03/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#footnotes",
    "href": "05-manipulandoDados.html#footnotes",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "A mudança do nome do dataframe de dadosNeonatos para neonatos é desnecessária. Foi realizada apenas por questões didáticas.↩︎\nObserve, na saída, que a variável utiNeo aparece palavras com acentuação (“não”). Às vezes, ao abrir o arquivo com a função read.csv2(), pode acontecer de esta palavra aparecer, por exemplo, como: “n3o”. Louco, não é? Se ocorrer isso, use, após o nome do arquivo e separado por vírgula, o argumento fileEncoding = “latin1”. Dessa forma, o erro será corrigido.↩︎\nDa mesma maneira, como acontece com a função read.csv2(), a função equivalente do readr pode retornar erro na leitura de palavras com acento. Para corrigir isso, usa-se o argumento locale (encoding = \"latin1\")↩︎\nATENÇÃO: Volta-se a insistir, o comando para carregar o conjunto de dados somente funciona, sem colocar o caminho (path) completo, se tudo está sendo realizado no diretório de trabalho.↩︎\nConjunto de dados coletados na maternidade-escola do Hospital Geral de Caxias do Sul↩︎\nLembrar que o sinal de igualdade, no R, é duplo \\(=\\)↩︎\nEquivalente ao Sys.Date()que acessa a data do sistema operacional.↩︎\nEquivalente ao Sys.time() que acessa a data e hora do sistema operacional.↩︎\nPOSIXt é uma classe de objetos do R que representa datas e horas. POSIXt significa Portable Operating System Interface for Unix Time, que é um padrão para medir o tempo em segundos desde 1 de janeiro de 1970. Existem duas formas internas de implementar POSIXt: POSIXct e POSIXlt. POSIXct armazena os segundos desde a época UNIX e POSIXlt armazena uma lista de dia, mês, ano, hora, minuto, segundo, etc.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html",
    "href": "06-descrevendoDados.html",
    "title": "6  Descrevendo os dados",
    "section": "",
    "text": "6.1 Pacotes necessários neste capítulo\nNos relatórios ou artigos científicos, a comunicação dos resultados é feita através da combinação de medidas resumidoras e visualização dos dados por meio de tabelas e gráficos.\nPara trabalhar neste capítulo, serão necessários os seguintes pacotes.\npacman::p_load(dplyr,\n               DescTools,\n               ggplot2, \n               ggpubr, \n               ggsci, \n               grDevices, \n               Hmisc, \n               kableExtra, \n               knitr, \n               plotrix, \n               readxl, \n               scales)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#dados-brutos",
    "href": "06-descrevendoDados.html#dados-brutos",
    "title": "6  Descrevendo os dados",
    "section": "6.2 Dados brutos",
    "text": "6.2 Dados brutos\nHabitualmente, costuma-se armazenar os dados em bancos de dados (dataframes ou tibbles). Entretanto, eles estão registrados de forma aleatória e não classificada. Ao se visualizar um dataframe, é difícil responder perguntas em relação a qualquer variável, principalmente, em grandes banco de dados. Eles se constituem uma lista, um rol de valores colocados na ordem em que foram obtidos. Parecem um jogo de quebra cabeça antes de serem organizados e resumidos (Figura 6.1)! São denominados de dados brutos ou, também, de dados não agrupados.\n\n\n\n\n\n\n\n\nFigura 6.1: Quebra-cabeça da Torre de Babel (pintura de Pieter Bruegel, 1563)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#medidas-resumidoras",
    "href": "06-descrevendoDados.html#medidas-resumidoras",
    "title": "6  Descrevendo os dados",
    "section": "6.3 Medidas resumidoras",
    "text": "6.3 Medidas resumidoras\n\n6.3.1 Dados usados nesta seção\nPara a demonstração prática será usado um conjunto de dados que é uma amostra aleatória de 15 recém-nascidos do banco de dados dadosMater.xlsx (ver Seção 5.3), extraída com a função slice_sample() do pacote dplyr. Cada vez que este comando for reproduzido, retornará uma nova série de 15 valores diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada. Após extrair a amostra, serão selecionadas as variáveis usadas nesta seção. A amostra será atribuída a um objeto denominado, dados:\n\nset.seed(1234) \ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;= 37 & ig&lt;42) %&gt;% \n  select(idadeMae, anosEst, pesoRN, apgar1) %&gt;% \n  slice_sample(n=15)\nstr(dados) \n\ntibble [15 × 4] (S3: tbl_df/tbl/data.frame)\n $ idadeMae: num [1:15] 23 26 18 25 35 19 20 23 26 35 ...\n $ anosEst : num [1:15] 5 9 7 10 11 9 10 11 4 5 ...\n $ pesoRN  : num [1:15] 3190 3715 2555 3795 2970 ...\n $ apgar1  : num [1:15] 9 7 9 6 8 8 8 8 9 9 ...\n\n\nApós a manipulação dos dados, tem-se um tibble de 15 linhas e quatro colunas.\n\n\n6.3.2 Introdução\nSempre que se está diante de um novo conjunto de dados para analisar, uma das primeiras tarefas é encontrar maneiras de resumir os dados de forma compacta e fácil de entender. Este processo se constitui na estatística descritiva que compreende métodos de tabulação, gráficos e resumo dos dados. Nesta seção, serão verificadas as medidas de resumo dos dados de duas maneiras:\n\nPrimeiro, um valor em torno do qual os dados têm uma tendência para se reunir ou se agrupar, denominado de medida sumária de localização ou medida de tendência central.\nEm segundo lugar, um valor que mede o grau em que os dados se dispersam, denominado de medida de dispersão ou variabilidade\n\n\n\n6.3.3 Medidas de tendência central\n\n6.3.3.1 Média\nA média ( \\(\\overline{x}\\) ) é a mais usada medida de tendência central para representar um valor típico dentro de um conjunto de números. O conceito mais comum é a média aritmética, que se calcula somando todos os valores do conjunto e dividindo pelo número total de elementos. A média é mais adequada para medidas numéricas simétricas, pois ela é sensível aos valores extremos (outliers).\n\\[\n\\overline{x}= \\frac{\\sum(x_1 + x_2 + x_3 + ... + x_n)}{n}\n\\]\nO R base possui uma função para o cálculo da média, mean(), apresentada na Seção 4.8, onde foi mostrado os seus argumentos. Se a variável analisada contiver algum valor ausente (missing), deve-se usar o argumento na.rm = TRUE, para removê-los, pois, caso contrário, a função retorna um resultado como NA (Not Available). Para evitar transtornos, recomenta-se usar sempre o argumento.\n\nmean (dados$pesoRN, na.rm = TRUE)\n\n[1] 3307.667\n\n\nPara reduzir o número de dígitos decimais, na saída do resultado, pode-se colocar a função mean(), dentro da função round()1, atribuindo o resultado da função a um objeto, por exemplo media.\n\nmedia &lt;- round(mean (dados$pesoRN, na.rm = TRUE), 1)\nprint(media)\n\n[1] 3307.7\n\n\nOu, usar a função round(), separadamente:\n\nmedia &lt;- mean (dados$pesoRN, na.rm = TRUE)\nround(media, 1)\n\n[1] 3307.7\n\n\n\n\n6.3.3.2 Mediana\nA mediana (Md) representa o valor central em uma série ordenada de valores. Assim, metade dos valores será igual ou menor que o valor mediano e a outra metade igual ou maior do que ele. Para encontrar a média:\n\nOrdene o conjunto de dados, por exemplo, a variável dados$pesoRN:\n\n\nvalores_ordenados &lt;- sort(dados$pesoRN)\nprint(valores_ordenados)\n\n [1] 2285 2555 2965 2970 3185 3190 3220 3500 3570 3585 3645 3710 3715 3725 3795\n\n\n\nSe o número de valores no conjunto de dados for ímpar, a mediana é o valor do meio. No exemplo, tem-se 15 valores, onde o valor do meio é o 7º valor e, portanto, a mediana é igual a 3220 g.\nSe o número de valores no conjunto de dados for par, a mediana é a média dos dois valores do meio. Apenas como exemplo, para se obter um número par de valores, será eliminado o 15º valor da variável dados$pesoRN:\n\n\nvalores_ordenados_pares &lt;- valores_ordenados [-15]\nprint(valores_ordenados_pares)\n\n [1] 2285 2555 2965 2970 3185 3190 3220 3500 3570 3585 3645 3710 3715 3725\n\n\nAssim, a mediana será igual a média do 7º e 8º e igual a 3360 g.\nÉ bem fácil de se calcular a mediana quando o número de observações é pequeno. Entretanto, quando se tem um número grande de valores seria extremamente tedioso esse calculo. O R facilita esse trabalho, fornecendo a função median().\nAgora, será usada a variável apgar1 do arquivo dadosMater.xlsx. Como o Apgar é um escore, a medida resumidora mais adequada, realmente, é a mediana.\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;= 37 & ig&lt;42) %&gt;% \n  select(idadeMae, anosEst, pesoRN, apgar1)\nmedian (mater$apgar1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\n6.3.3.3 Moda\nModa (Mo) é o valor que ocorre com maior frequência em um conjunto de dados. O R não possui uma função nativa e direta para calcular a moda como tem para a média (mean()) e a mediana (median()). Isso acontece porque a moda pode não ser única em um conjunto de dados (podem existir múltiplos valores com a mesma frequência máxima) ou pode nem existir (se todos os valores ocorrerem apenas uma vez).Tem o menor nível de sofisticação. No entanto, pode-se facilmente criar uma função própria para calcular a moda ou usar pacotes que oferecem essa funcionalidade, como o DescTools que oferece uma função chamada Mode(). Aqui estão algumas maneiras de calcular a moda em R:\nFunção personalizada\n\nmoda &lt;- function(v) {\n  freq_tab &lt;- table(v)\n  max_freq &lt;- max(freq_tab)\n  moda &lt;- names(freq_tab[freq_tab == max_freq])\n  return(moda)\n}\n\nEsta função moda()é constituída por:\n\ntable(v): Cria uma tabela de frequência dos valores v.\nmax(freq_tab): Encontra a frequência máxima.\nfreq_table[freq_table == max_freq]: Seleciona as entradas da tabela de frequência que são iguais à frequência máxima.\nnames(…): Obtém os nomes (os valores originais) dessas entradas, que são as modas.\n\nUsando a função criada, a moda da variável dados$apgar1 é igual a:\n\nmoda (dados$apgar1) \n\n[1] \"8\"\n\n\nA função moda() pode ser salva em seu diretório de trabalho, na pasta das suas funções próprias. Quando necessário ela pode ser acessada, como foi visto na Seção 4.8.1).\nFunção Mode() do pacote DescTools\n\nmoda &lt;- Mode(dados$apgar1)\nprint(moda)\n\n[1] 8\nattr(,\"freq\")\n[1] 7\n\n\nA saída mostra que a média é igual a 8 e que frequência deste valor entre os 15 valores é 7.\n\n\n6.3.3.4 Quantil\nUma medida de localização bastante utilizada são os quantis que são pontos estabelecidos em intervalos regulares que dividem a amostra em subconjuntos iguais. Se estes subconjuntos são em número de 100, são denominados de percentis; se são em número de 10, são os decis e em número de 4, são os quartis. A função nativa no R para obter o quantil é quantile().\nPara determinar os três quartis do peso dos recém-nascidos (dados$pexoRN), usa-se:\n\nquantile (dados$pesoRN, c (0.25, 0.50, 0.75))\n\n   25%    50%    75% \n3077.5 3500.0 3677.5 \n\n\nObserve que o percentil 50º é igual a mediana. O percentil 75º é o ponto do conjunto de dados onde 75% dos recém-nascidos têm um peso inferior a 3677.5g e 25% está acima deste valor.\n\n\n6.3.3.5 Média aparada\nAs médias aparadas são estimadores robustos da tendência central. Para calcular uma média aparada, é removida uma quantidade predeterminada de observações em cada lado de uma distribuição e realizada a média das observações restantes. Um exemplo de média aparada é a própria mediana.\nA base R tem como calcular a média aparada acrescentando o argumento trim =, proporção a ser aparada. Se for aparado 20%, usa-se trim = 0.2. isto significa que serão removidos 20% dos dados dos dois extremos. No caso da amostra de 15 recém-nascidos, serão removidos três valores mais baixos e três valores mais altos, passando a mostra a ter 9 valores, e a média aparada será a média destes 9 valores.\nO comando para obter a média aparada é:\n\nround(mean (dados$pesoRN, na.rm = TRUE, trim = 0.20), 1)\n\n[1] 3397.2\n\n\n\n\n\n6.3.4 Medidas de Dispersão\n\n6.3.4.1 Amplitude\nA amplitude de um grupo de medições é definida como a diferença entre a maior observação e a menor.\nNo conjunto de dados dos pesos dos recém-nascidos, a amplitude pode ser obtida, no R, com a função range(), que retorna o valor mínimo e o máximo.\n\nrange (dados$pesoRN, na.rm = TRUE)\n\n[1] 2285 3795\n\n\n\n\n6.3.4.2 Intervalo Interquartil\nA intervalo interquartil (IIQ), também conhecido como amplitude interquartil (AIQ) é uma forma de média aparada. É simplesmente a diferença entre o terceiro e o primeiro quartil, ou seja, a diferença entre o percentil 75 e o percentil 25. Considere a variável escolaridade (dados$anosEst), anos de estudos completos.\nOs percentis 25 e 75 são obtidos, usando a função quantile(), vista acima, ou com a função summary() , que retorna os valores mínimo, primeiro quartil, mediana, média, terceiro quartil e máximo.\n\nquantile (dados$anosEst, c(0.25,0.75))\n\n 25%  75% \n 6.5 11.0 \n\n\n\nsummary(dados$anosEst)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   6.500   9.000   8.467  11.000  11.000 \n\n\nPortanto, o IIQ está entre 6,5 a 11 anos de estudo ou, 11 – 6,5 = 4,5 anos de estudos completos. Em outras palavras, 50% das mulheres desta amostra têm de 6 a 8 anos de estudo.\nO R possui uma função específica para calcular o intervalo interquartil, denominada IQR() e incluída no R base. Ela possui os seguintes argumentos:\nx \\(\\to\\) Representa o vetor numérico;\nna.rm \\(\\to\\) Este assume um valor lógico, TRUE ou FALSE, indicando se os valores ausentes devem ser removidos ou não;\ntype \\(\\to\\) Representa um número inteiro selecionando um dos muitos algoritmos de quantil. Este é um parâmetro opcional.\n\nIQR(dados$anosEst, na.rm = TRUE)\n\n[1] 4.5\n\n\n\n\n6.3.4.3 Variância e Desvio Padrão\nA variância e o desvio padrão fornecem uma indicação de quão aglomerados em torno da média os dados de uma amostra estão. Estes tipos de medidas representam desvios (erros)da média. Quando se verifica o desvio de cada valor (x) em relação à média \\(\\overline{x}\\), os desvios positivos se anulam com os negativos, resultando em uma soma igual a zero.\nA consequência deste fato é que não é possível resumir os desvios numa única medida de variabilidade. Para se chegar a uma medida de variabilidade há necessidade de se eliminar os sinais, antes de somar todos os desvios em relação à média.\nUma maneira de se fazer isso é elevar todas as diferenças ao quadrado. Assim, se obtém o desvio em relação à média elevado ao quadrado. A soma destes valores é denominada de Soma dos Quadrados (SQ) dos Desvios ou Soma dos Erros ao Quadrado. Se o interesse é apenas saber o erro ou desvio médio, divide-se por n (tamanho da amostra). No entanto, em geral o interesse se concentra em usar o desvio ou erro na amostra para estimar o erro na população. Dessa maneira, divide-se a Soma dos Quadrados por \\(n-1\\). Essa medida é conhecida como variância (\\(s^2\\)). O divisor, \\(n – 1\\), é denominado de graus de liberdade (gl) associados à variância.\nOs graus de liberdade representam o número de desvios que estão livres para variar. É um conceito de difícil explicação, mas é possível compreendê-lo, usando a seguinte explicação:\n\n Suponha uma maternidade há 50 anos atrás, quando não havia alojamento conjunto. Nessa época era comum os recém-nascidos normais ficarem em um berçário. A cada horário de amamentação eles eram levados para os quartos de suas mães para mamar. Posteriormente, eram trazidos para o berçário e colocados nos berços até a próxima mamada. Suponha que, em um determinado momento, havia 15 bebês e que, no berçário, existiam 15 berços (postos) para colocá-los durante o intervalo das mamadas. Quando o primeiro recém-nascido chega, a enfermeira poderá escolher qualquer um dos berços para o colocar. Depois, quando o próximo recém-nascido chegar, ela terá 14 opções de escolha, pois um dos berços está ocupado. Ainda existe uma boa liberdade de escolha. No entanto, à medida que os recém-nascidos forem sendo trazidos para o berçário, chegará a um ponto em que 14 berços estarão ocupados. Agora, a enfermeira não terá liberdade de escolha, pois só resta um berço. Nesse exemplo existem 14 graus de liberdade. Para o último recém-nascido não houve liberdade de escolha (1). Portanto, os graus de liberdade são iguais ao tamanho da amostra menos um (\\(n-1\\)).\n\nA variância é a razão entre a soma dos quadrados e os graus de liberdade (observações realizadas menos um).\n\\[\ns^2= \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\n\\]\nNo R existem embutidas as funções sd() e var()que facilmente calculam essas medidas de dispersão.\nUsando a variável dados$pesoRN, tem-se:\n\nvar(dados$pesoRN, na.rm =TRUE)\n\n[1] 208310.2\n\n\nO desvio padrão é a raiz quadrada da variância: \\(s = \\sqrt var\\)\n\nsqrt (var(dados$pesoRN))\n\n[1] 456.4102\n\n\nOu, usando a função sd() e arredondando para 1 dígito decimal:\n\nround(sd (dados$pesoRN, na.rm = TRUE), 1)\n\n[1] 456.4\n\n\nA variância e desvio padrão são medidas de variabilidade e revelam quão bem a média representa os dados. Informa se ela está funcionando bem como modelo. Pequenos desvios padrão mostram que existe pouca variabilidade nos dados, que eles se aproximam da média. Quando existe um grande desvio padrão, a média não é muito precisa para representar os dados.\nO desvio padrão, além de medir a precisão com que a média representa os dados, também informa sobre o formato dos dados e por isso é uma medida de dispersão. Em uma amostra onde desvio padrão é pequeno, os dados se agrupam próximo a média e o formato da distribuição fica mais pontiagudo (curva em azul, Figura 6.2). Nesse caso a média representa bem os dados. Em outra amostra, com a mesma média anterior, mas com os dados mais dispersos entorno da média, o desvio padrão é maior e o formato da distribuição fica achatado (curva verde, na Figura Figura 6.2). Nesse caso a média não é uma boa representação dos dados.\n\n\n\n\n\n\n\n\nFigura 6.2: Dispersão dos dados em torno da média.\n\n\n\n\n\n\n\n6.3.4.4 Coeficiente de Variação\nO desvio padrão por si só tem limitações. Um desvio padrão igual a 2 pode ser considerado pequeno para um conjunto de valores cuja média é 100. Entretanto, se a média for 5, ele se torna muito grande. Além disso, o desvio padrão por ser expresso na mesma unidade dos dados, não permite aplicá-lo na comparação de dois ou mais conjunto de dados que têm unidades diferentes. Para eliminar essas limitações, é possível caracterizar a dispersão ou variabilidade dos dados em termos relativos, usando uma medida denominada Coeficiente de Variação (CV), também conhecido como como Desvio Padrão Relativo ou Coeficiente de Variação de Pearson. É expresso, em geral como uma porcentagem, sendo definido como a razão do desvio padrão pela média:\n\\[\nCV = \\frac{s}{\\overline{x}}\n\\]\nMultiplicando o valor da equação por 100 tem-se o CV percentual. O R não possui uma função específica para calcular o CV.\nFoi criada uma função específica para isso,já multiplicada por 100.\n\ncoef_var &lt;- function (valores) {\n  (sd(valores, na.rm=TRUE) / mean(valores, na.rm=TRUE))*100}\n\nPortanto, o CV da variável dados$pesoRN é igual a:\n\nround (coef_var (dados$pesoRN),1)\n\n[1] 13.8\n\n\nUsdando outra variável do banco de dados, por exemplo, dados$idadeMae, o CV será igual a:\n\nround(coef_var (dados$idadeMae), 1)\n\n[1] 21.4\n\n\nO peso do recem-nascido tem um CV = 13.8 % e a idade materna um CV = 21.4 %, mostrando que esta tem uma maior variabilidade. Quanto menor o desvio padrão, menor o CV e, consequentemente, menor a variabilidade. Um CV \\(\\ge\\) 50%, sugere que a variável tem uma distribuição assimétrica.\n\n\n\n6.3.5 Escolha da medida resumidora\nA seleção da medida de tendência central mais adequada depende de vários fatores, incluindo a natureza dos dados e do propósito da sumarização.\nO tipo da variável tem substancial influência na escolha da medida de tendência central a ser usada. A moda é mais apropriada para dados nominais e seu uso com variáveis ordinais resulta em uma perda no poder em termos de informação que se poderia obter dos dados.\nA mediana é mais adequada para variáveis ordinais, embora possa ser usada para variáveis contínuas, especialmente quando a distribuição dos dados é assimétrica. A mediana não deveria ser usada com dados nominais porque os postos assumidos não podem ser obtidos com dados de nível nominal.\nFinalmente, a média somente deve ser usada com dados contínuos simétricos, se houver assimetria a mediana deve ser preferida.\nAs medidas de dispersão devem estar associadas a uma medida de tendência central. Elas caracterizam a variabilidade dos dados na amostra. Com dados ordinais usar a amplitude ou o intervalo interquartil. O desvio padrão não é apropriado em dados ordinais devido à natureza não numérica destes.\nCom os dados numéricos deve-se usar o desvio padrão, que utiliza toda a informação nos dados, ou o intervalo interquartil (IIQ). Quando os dados forem simétricos, usar a média acompanhada do desvio padrão, caso contrário, usar a mediana e o IIQ. Não misturar e combinar medidas (2).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#sec-tabelas",
    "href": "06-descrevendoDados.html#sec-tabelas",
    "title": "6  Descrevendo os dados",
    "section": "6.4 Tabelas",
    "text": "6.4 Tabelas\nA apresentação tabular dos dados é a apresentação das informações por meio de tabelas. Uma tabela é uma forma eficiente de mostrar os dados levantados, facilitando a sua compreensão e interpretação. No R existem muitas maneira de criar tabelas.\n\n6.4.1 Dados usados nesta seção\nPara mostrar como construir uma tabela, será feita novamente a leitura do conjunto de dados dadosMater.xlsx. Como visto, este conjunto de dados contém uma grande quantidade colunas e, para tornar mais fácil a análise, serão selecionadas aquelas utilizadas nesta seção.\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n   select(idadeMae, altura, peso, anosEst, fumo, \n           para, ig, sexo, pesoRN, compRN, utiNeo)\n\nstr(mater)\n\ntibble [1,368 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae: num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura  : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso    : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo    : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para    : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig      : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo    : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN  : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN  : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo  : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n\n\n\n\n6.4.2 Tabelas de Frequência\n\n6.4.2.1 Tabela de frequência para dados categóricos\nUma maneira concisa que permite observar a variável e extrair informação sobre o seu comportamento, é a utilização de uma tabela de frequência. A tabela de frequência deve ser simples, clara e objetiva, ou seja, não deve ter um volume muito grande de informações. Deve ser autoexplicativa, não deve haver necessidade de ler o texto para entendê-la.\nA tabela de frequência agrupa os dados por categorias ou classes, contabilizando o número ocorrências em cada categoria. O número de observações em uma determinada classe recebe o nome de frequência absoluta (f). Além da frequência absoluta, costuma aparecer a frequência relativa (fr) que representa a proporção da classe em relação ao número total de observações (n), calculada por \\(fr = \\frac{f}{n}\\), a frequência percentual (fp), obtida pela multiplicação da frequência relativa por 100 e a frequência acumulada, que é a soma de todas as classes até a classe atual, podendo ser frequência acumulada absoluta (F), frequência acumulada relativa (Fr) ou frequência acumulada percentual (Fp).\nEm uma tabela, os dados são apresentados em colunas verticais indicadoras e linhas horizontais. Nas linhas aparecem as categorias e nas colunas as frequências, constituindo o corpo da tabela. O cabeçalho indica a natureza do conteúdo de cada coluna. No cruzamento das colunas e das linhas, tem-se as caselas ou casas.\nExistem algumas recomendações na construção de uma tabela de frequência (3):\n\nDeve ter um título na parte superior que responda as perguntas: “o que? quando? onde?” relativas ao fato estudado;\n\nDeve ter um rodapé, na parte inferior da tabela, onde se coloca notas necessárias e a fonte dos dados;\n\nAs colunas externas da tabela devem ser abertas, o emprego de linhas verticais para a separação das colunas no corpo da tabela é opcional;\n\nNa parte superior e inferior, as tabelas devem, ser fechadas por linhas horizontais;\n\nNenhuma casela deve ficar vazia, apresentando um número ou um símbolo. Se não se dispuser do dado, colocar reticências … e a presença de um X representa que o dado foi omitido para evitar a identificação.\n\nSe os dados forem nominais, a ordenação das categorias é arbitrária, costuma-se colocar em primeiro lugar a maior frequência (Tabela 6.1) , colocando-os em categorias ordenadas (4).\n\n\n\n\nTabela 6.1: Distribuição de frequência de drogadição em parturientes, Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\nDroga\nf\nfr\nfp (%)\nFp (%)\n\n\n\n\nNenhuma\n904\n0.955\n95.5\n95.5\n\n\nMedicamentos\n23\n0.024\n2.4\n97.9\n\n\nÁlcool\n17\n0.018\n1.8\n99.7\n\n\nCrack\n2\n0.002\n0.2\n99.9\n\n\nCocaína\n1\n0.001\n0.1\n100\n\n\nTotal\n947\n1.000\n100.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2.2 Construção da tabela de frequência\nPara demonstrar como construir uma tabela de frequência, será usada uma variável categórica que não existe no conjunto de dados mater. Esta variável vai ser criada, categorizando a variável numérica idadeMae (idade da parturiente) em três categoria, classicamente, usadas: menores de 20 anos (adolescentes), 20 a 35 anos e maiores de 35 anos. No conjunto mater, a variável idadeMae tem como idade mínima 13 anos e idade máxima 46 anos. A nova variável receberá o nome de categIdade. Para realizar este trabalho de transformação da variável numérica em categórica, será usada a função mutate() do pacote dplyr (veja Seção 5.3.3). Esta função tem vários argumentos:\n\nmater &lt;- mater %&gt;%\n  mutate(categIdade = case_when(\n    idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n    idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n    idadeMae &gt; 35 ~ \"&gt; 35 anos\")) %&gt;%\n  mutate(categIdade = factor(categIdade, levels = c(\"&lt; 20 anos\", \"20 a 35 anos\", \"&gt; 35 anos\")))\n\nPara visualizar se a variável foi criada de maneira adequada, pode-se usar novamente a função str():\n\nstr(mater)\n\ntibble [1,368 × 12] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura    : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso      : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst   : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo      : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para      : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig        : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo      : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN    : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN    : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo    : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 3 2 1 2 2 2 2 2 1 2 ...\n\n\nPara se verificar como ficou a distribuição de frequência absoluta, constrói-se uma tabela, inicialmente com a função table():\n\nf_abs &lt;- table (mater$categIdade)\nf_abs\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n         219          992          157 \n\n\nAs frequências relativas podem ser obtidas com a função prop.table(), Esta função será usada dentro da função round() para arredondar os valores para 3 dígitos.\n\nf_rel &lt;- round(prop.table(f_abs), 3)\nf_rel\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n       0.160        0.725        0.115 \n\n\nMultiplicando por 100 a f_rel, tem-se a frequência percentual f_perc. De novo, a operação será colocada dentro da função round(), arredondando o resultado para dois dígitos.\n\nf_perc &lt;- round(f_rel*100, 2)\nf_perc\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n        16.0         72.5         11.5 \n\n\nPara construir uma tabela simples no R, pode-se proceder da seguinte maneira:\n\n# Criando as colunas das tabelas com o total de cada uma delas\nf_abs &lt;- c (f_abs, sum(f_abs))\nf_rel &lt;- c (f_rel, sum (f_rel))\nf_perc &lt;- c (f_perc, sum (f_perc))\n\n# Criando a tabela inicial com a concatenação das coluna, usando a função cbind()\ntab1 &lt;- cbind(f_abs,\n              f_rel ,\n              f_perc)\ntab1\n\n             f_abs f_rel f_perc\n&lt; 20 anos      219 0.160   16.0\n20 a 35 anos   992 0.725   72.5\n&gt; 35 anos      157 0.115   11.5\n              1368 1.000  100.0\n\n\nTransformando a tab1 em um dataframe, nomeando a linha 4 e renomeando as colunas para que tenham os nomes mencionados no início da seção:\n\ntab1 &lt;- as.data.frame(tab1)\nrow.names(tab1)[4] &lt;-  \"Total\"\ncolnames(tab1) &lt;- c(\"f\", \"fr (%)\", \"fp (%)\")\ntab1\n\n                f fr (%) fp (%)\n&lt; 20 anos     219  0.160   16.0\n20 a 35 anos  992  0.725   72.5\n&gt; 35 anos     157  0.115   11.5\nTotal        1368  1.000  100.0\n\n\nEsta uma tabela simples que serve para visualizar as informações. Não serve para publicações. Para obter uma tabela simples, mas muito mais profissional (Tabela 6.2) pode ser utilizada a função kable() do pacote knitr (5). Esta função possui um grande número de argumentos para personalizar a aparência das tabelas2:\n\nknitr::kable(tab1,\n             format = \"html\",\n             col.names = c(\"Faixa Etária\", \"f\", \"fr\", \"fp (%)\"),\n             align = c('l', 'c', 'c', 'c')) %&gt;%\n kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE, \n    position = \"left\",\n    font_size = 12) %&gt;%\n  kableExtra::column_spec(1, width = \"4cm\") %&gt;%\n  kableExtra::column_spec(2:4, width = \"1.5cm\")\n\n\n\nTabela 6.2: Distribuição das puérperas por faixa etária, Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\nFaixa Etária\nf\nfr\nfp (%)\n\n\n\n\n&lt; 20 anos\n219\n0.160\n16.0\n\n\n20 a 35 anos\n992\n0.725\n72.5\n\n\n&gt; 35 anos\n157\n0.115\n11.5\n\n\nTotal\n1368\n1.000\n100.0\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.2.3 Tabela de frequência para dados numéricos\nComo fazer a distribuição de frequência de uma variável contínua sem um critério pré-determinado para as classes?\nComo exemplo, será usado, agora, o IMC pré-gestacional das parturientes do banco de dados dadosMater.xlsx). Esta variável ainda não existe no banco de dados, tem-se apenas o peso e a altura e, portanto, com estes dados, ela pode ser criada:\n\nmater$imc &lt;- round(mater$peso/mater$altura^2, 1)\nstr(mater)\n\ntibble [1,368 × 13] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura    : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso      : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst   : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo      : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para      : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig        : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo      : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN    : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN    : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo    : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 3 2 1 2 2 2 2 2 1 2 ...\n $ imc       : num [1:1368] 25.7 28.3 27.4 30.8 23.4 26.7 31.8 23 20.2 31.1 ...\n\n\nA variável imc foi criada de forma adequada. Após, isso, para verificar a sua distribuição, segue-se os seguintes passos:\n\nEstabelecimento do número de classes (k):\n\nAntes, as classes foram estabelecidas de acordo com algum critério. Em geral, quando não há um padrão pré-determinado, o número de classes é estabelecido de acordo com o tamanho da amostra. Este número pode ser escolhido lembrando-se das oscilações que ocorrem nos dados e do interesse do pesquisador em mostrar seus dados. Não existe uma regra totalmente eficiente para determinar o número de classes. É importante ter bom senso, de maneira que seja possível ver como os valores se distribuem.\nPara a maioria dos dados, é recomendado e 8 a 20 classes, isto é, 8 \\(\\le\\) k \\(\\le\\) 20. Com poucas classes, perde-se precisão e, com muitas classes, a tabela torna-se muito extensa. Baseado na regra de Sturges , é sugerido usar a recomendação da Figura 6.3 (7).\n\n\n\n\n\n\n\n\nFigura 6.3: Número de classes baseado em Sturges\n\n\n\n\n\nPara a variável imc, como existem 1368 observações, deve-se usar ao redor de 10 classes. Executando a função nclass.Sturges (), abaixo, o número de classes é igual a:\n\nk &lt;- nclass.Sturges (mater$imc)\nk\n\n[1] 12\n\n\n\nAmplitude e limites das classes:\n\nA classe possui um limite inferior e um limite superior. O importante é que os limites dos intervalos sejam mutuamente exclusivos, isto é, cada valor deve ser representado em um único intervalo. Além disso, os intervalos devem ser exaustivos, isto é, devem conter todos os valores possíveis entre o valor mínimo e o máximo. O recomendado é que as classes sejam homogêneas, ou seja, tenham a mesma amplitude. A amplitude dos valores pode ser obtida com a função range():\n\namplitude &lt;- range(mater$imc) \namplitude \n\n[1] 11.8 48.7\n\n\nUsando esta amplitude dos dados, é possível ter a largura (amplitude) das classes (h), usando a diferença entre o mínimo e máximo e divdindo pelo número de classes (k):\n\nh &lt;- round(diff(amplitude)/k, 0)\nh\n\n[1] 3\n\n\nA fórmula é apenas a diferença absoluta dos limites inferior e superior dividida pelo número de classes, arredondado com o a função round ().\nA partir desses dados, é possível construir as classes. A primeira classe será o valor mínimo de 11,8 e máximo 14,8 (11,8 + 3) exclusive; a segunda classe será 14,8 até 17,8 (14,8 + 3) e assim por diante.\n\nConstrução da tabela:\n\nPode-se construir a tabela, usando a função mutate() e dentro desta a função cut()e dentro dela a função seq(limite inferior, limite superior, length.out = k + 1). A função cut() tem vários argumentos:\n\nx \\(\\to\\) vetor numérico\nbreaks \\(\\to\\) vetor numérico de dois ou mais pontos de corte exclusivos ou um único número (maior ou igual a 2) dando o número de intervalos nos quais x deve ser subdividido\nlabels \\(\\to\\) rótulos para os níveis das categorias resultante. Por padrão, os rótulos são construídos usando a notação de intervalo (a,b] - aberto à esquerda e fechado à direita.\ninclude.lowest \\(\\to\\) valor lógico, se o menor valor (ou o maior, se right = TRUE) será incluido. Padrão = include.lowest=TRUE.\nright \\(\\to\\) valor lógico indicando se o intervalo deve ser fechado à direita e aberto a esquerda. Padrão = right = TRUE.\nordered_result \\(\\to\\) valor lógico indicando se o resultado deve ser um fator ordenado.\n\n\nmater &lt;- mater %&gt;%\n  mutate(categImc = cut(\n    imc,\n    breaks = seq(11.8, 48.7, length.out = 13), # 12 intervalos definidos\n    include.lowest = TRUE,\n    ordered_result = TRUE))\n\ntable(mater$categImc)\n\n\n[11.8,14.9]   (14.9,18]     (18,21]   (21,24.1] (24.1,27.2] (27.2,30.3] \n          2          46         258         480         237         176 \n(30.3,33.3] (33.3,36.4] (36.4,39.5] (39.5,42.6] (42.6,45.6] (45.6,48.7] \n         87          39          22          12           5           4 \n\n\nPreste atenção! Estes comandos que vão gerar a tabela têm o argumento right = TRUE (padrão). Neste caso, os símbolos aparecem como (] (na tabela) e significa que o limite inferior da classe foi excluído (aberto à esquerda) e o superior foi incluído (fechado à direita). Aqui, também foi introduzido o argumento include.lowest  = TRUE para incluir o valor mínimo dos dados (11,8), e a representação gráfica fica [].\nOlhando a saída do objeto categImc, ela parece pouco esclarecedora e, no caso do IMC, talvez fosse melhor usar outro critério. Como por exemplo o que define o estado nutricional no 1° trimestre de gestação e classifica as gestantes em:\n\nbaixo peso (IMC \\(&lt;\\) 18,5 kg/\\(m^2\\)),\n\npeso adequado (18,5 \\(\\le\\) IMC \\(\\le\\) 24,9 kg/\\(m^2\\)),\n\nsobrepeso (25,0 \\(\\le\\) IMC \\(\\le\\) 29,9 kg/\\(m^2\\)) e\n\nobesidade (IMC \\(\\ge\\) 30 kg/\\(m^2\\)).\n\nAssim, é recomendado um ganho de peso total adequado de 12,5 kg a 18 kg para as gestantes classificadas como baixo peso; de 11,5 kg a 16,0 kg para as classificadas como peso adequado; de 7,0 a 11,5 kg nas classificadas com sobrepeso; e de 5,0 a 9,0 kg nas obesas (8).\n\nmater &lt;- mater %&gt;% \n  mutate (estNutri = case_when(\n    imc &lt; 18.5 ~ \"Baixo Peso\",\n    imc &gt;= 18.5 & imc &lt; 25 ~ \"Peso adequado\", \n    imc &gt;= 25 & imc &lt; 30 ~ \"Sobrepeso\",\n    imc &gt;= 30 ~ \"Obesidade\")) %&gt;%\n  mutate(estNutri = factor(estNutri, \n                           levels = c(\"Baixo Peso\", \"Peso adequado\", \n                                      \"Sobrepeso\", \"Obesidade\")))\n\nIsto cria uma nova variável estNutri (estado nutricional), no conjunto de dados mater, com 4 níveis (Baixo Peso, Peso Adequado, Sobrepeso e Obesidade). Desta forma, pode-se construir uma tabela que melhor define este grupo de mulheres quanto ao estado nutricional.\n\nf.abs &lt;- table (mater$estNutri)\nf.rel &lt;- round(prop.table(f.abs), 3)\nf.perc &lt;- round(f.rel*100, 2)\n\nf.abs &lt;- c (f.abs, sum(f.abs))\nf.rel &lt;- c (f.rel, sum (f.rel))\nf.perc &lt;- c (f.perc, sum (f.perc))\n\ntab2 &lt;- cbind(f.abs,\n              f.rel ,\n              f.perc)\n\ntab2 &lt;- as.data.frame(tab2)\nrow.names(tab2)[5] &lt;-  \"Total\"\ncolnames(tab2) &lt;- c(\"f\", \"fr\", \"fp\")\ntab2\n\n                 f    fr    fp\nBaixo Peso      67 0.049   4.9\nPeso adequado  791 0.578  57.8\nSobrepeso      335 0.245  24.5\nObesidade      175 0.128  12.8\nTotal         1368 1.000 100.0\n\n\nColocando em um formato mais científico, tem-se uma tabela ( Tabela 6.3)) bem mais elegante sobre o estado nutricional pré-gestacional:\n\nknitr::kable(tab2,\n             format = \"html\",\n             col.names = c(\"Estado Nutricional\", \"f\", \"fr\", \"fp (%)\"),\n             align = c('l', 'c', 'c', 'c')) %&gt;%\n kableExtra::kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE, \n    position = \"left\",\n    font_size = 12) %&gt;%\n  kableExtra::column_spec(1, width = \"4cm\") %&gt;%\n  kableExtra::column_spec(2:4, width = \"1.5cm\")\n\n\n\nTabela 6.3: Estado nutricional pré-gestacional das parturientesa, Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\nEstado Nutricional\nf\nfr\nfp (%)\n\n\n\n\nBaixo Peso\n67\n0.049\n4.9\n\n\nPeso adequado\n791\n0.578\n57.8\n\n\nSobrepeso\n335\n0.245\n24.5\n\n\nObesidade\n175\n0.128\n12.8\n\n\nTotal\n1368\n1.000\n100.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.3 Tabelas de contingência\nAs tabelas de contingência, também chamadas tabelas cruzadas, são bastante usadas em estatísticas epidemiológicas para resumir a relação entre duas ou mais variáveis categóricas.\nUma tabela de contingência é um tipo especial de tabela de distribuição de frequência, onde duas variáveis são mostradas simultaneamente. Por exemplo, um pesquisador pode estar interessado em saber se o hábito de fumar na gestação aumenta o risco de o recém-nascido precisar de cuidados intensivos.\nExistem duas variáveis fumo (fumo na gestação) e utiNeo (necessidade de cuidados intensivos neonatais) no banco de dados dadosMater.xlsx. Cada uma dessas variáveis tem duas alternativas, sim e não, por isso a tabela de cruzamento é denominada tabela de contingência 2 x 2. No arquivo, estão registradas como variáveis numéricas , 1 e 2, e devem ser transformadas para fatores (1 = sim e 2 = não)3, usando a função factor().\n\nmater$fumo &lt;- factor (mater$fumo, \n                      ordered = TRUE, \n                      levels = c (1,2), \n                      labels = c (\"sim\", \"não\"))\nmater$utiNeo &lt;- factor (mater$utiNeo, \n                        ordered = TRUE, \n                        levels = c (1,2), \n                        labels = c (\"sim\", \"não\"))\n\nBasta agora, usar a função with() junto com a função table(variável da linha, variável das colunas). Por convenção, costuma-se colocar a variável explicativa ou explanatória nas linhas (fumo) e o desfecho nas colunas (utiNeo):\n\ntabFumo &lt;- with(data = mater, table(fumo, utiNeo))\ntabFumo\n\n     utiNeo\nfumo  sim não\n  sim  71 230\n  não 204 863\n\n\nPara ter a soma das margens, usar a função addmargins (tabela, margin = c (1,2), FUN = sum) do pacote stats, incluído na instalação básica do R. A função adiciona a soma das linhas (1) e das colunas (2) às margens da tabela (tabFumo).\n\naddmargins (tabFumo, margin = c(1,2), FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: fumo\n2: utiNeo\n\n\n     utiNeo\nfumo   sim  não  sum\n  sim   71  230  301\n  não  204  863 1067\n  sum  275 1093 1368\n\n\nObservando a tabela de contingência, verifica-se que a proporção entre as gestantes fumantes de internação na UTI neonatal foi 71/301 = 0,236 e entre as não fumantes foi de 204/1067 = 0,191. Para verificar se esta diferença ocorreu por acaso ou ela é significativa, há necessidade de se realizar um teste de hipótese, o qui-quadrado, que será visto na ?sec-qui.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#gráficos",
    "href": "06-descrevendoDados.html#gráficos",
    "title": "6  Descrevendo os dados",
    "section": "6.5 Gráficos",
    "text": "6.5 Gráficos\nPara descrever os dados e visualizar o que está acontecendo, recomenda-se utilizar um gráfico adequado. O que é adequado depende principalmente do tipo de dados, bem como das características particulares do que se quer explorar. Além disso, um gráfico em um relatório sempre é um fator de “impacto”. Ou seja, pode ter um efeito positivo no leitor ou fazê-lo abandonar a leitura. Finalmente, um gráfico de frequência pode ser utilizado para ilustrar, explicar uma situação complexa onde palavras ou uma tabela podem ser confusos, extensos ou de outro modo insuficiente. Por outro lado, deve-se evitar usar gráficos onde poucas palavras expressam claramente o que se quer mostrar. Aconselha-se que, ao analisar os dados, é importante inspecioná-los como se fossem uma imagem, uma fotografia, ver como eles se parecem, qual o seu aspecto, e só então pensar em interpretar os aspectos vitais da estatística (9).\nO R básico fornece uma grande variedade de funções para visualizar dados, elas de uma maneira relativamente simples permitem a construção de gráficos que facilitam a interpretação tanto de variáveis categórica como contínuas. Para gráficos mais sofisticados existe um pacote denominado ggplot2 (10). Este pacote é uma ferramenta extremamente versátil. É um pouco mais complexo e exige mais tempo para dominá-lo, mas, uma vez que se aprenda a sua lógica, oferece uma estrutura extremamente flexível para exibir os dados . Inicialmente, serão usadas as funções do R básico e,posteriormente, será feita uma introdução ao ggplot2 (Seção 6.6).\n\n6.5.1 Gráfico de setores\nTambém conhecido como gráfico de pizza. Cada segmento (fatia) do gráfico de pizza deve ser proporcional à frequência da categoria que representa. A desvantagem do gráfico de pizza é que ele só pode representar uma variável, portanto, há necessidade de um gráfico separado para cada variável que se deseja representar. Além disso, um gráfico de pizza pode perder clareza se ele é usado para representar mais do que quatro ou cinco categorias. Na maioria das vezes, em um artigo ou relatório científico não há necessidade de se usar este tipo de gráfico. As tabelas são muito melhores. Segundo Edward Tufte, professor emérito de estatística, design gráfico e economia política na Universidade de Yale, o único gráfico pior do que um gráfico de pizza são vários deles (11)! Ele é usado mais no mundo dos negócios. Como regra, evite usar gráfico de pizza!\nEm uma consulta, entre estudantes de Medicina, foi perguntado a sua opinião em relação a este tipo de gráfico. A pergunta feita foi: “O que você sente ao ver um gráfico de pizza em um artigo científico?” As alternativas para a resposta eram quatro (ódio, irritação, indiferença, amor). O resultado do inquérito está na Tabela 6.4.\n\n\n\n\nTabela 6.4: Sentimento dos alunos de Medicina em relação ao gráfico de pizza, UCS, 2012.\n\n\n\n\n\n\nSentimento\nf\nfr\nfp (%)\nFp (%)\n\n\n\n\nOdeiam\n6\n0.15\n15\n15\n\n\nNão gostam\n12\n0.30\n30\n45\n\n\nIndiferentes\n14\n0.35\n35\n80\n\n\nAmam\n8\n0.20\n20\n100\n\n\nTotal\n40\n1.00\n100\nNA\n\n\n\n\n\n\n\n\n\n\nNo R base, pacote graphics, existe a função pie()para obter um gráfico de setores simples. Esta função usa os seguintes argumentos basicos, consulte a ajuda do R para outras informações:\n\nx \\(\\to\\) vetor numérico não negativo\n\nlabels \\(\\to\\) caracteres que fornecem nomes para as fatias. Para rótulos vazios ou NA (após coerção para caractere), nenhum rótulo ou linha indicadora é desenhada\n\nradius \\(\\to\\) A pizza é desenhada centralizada em um quadrado cujos lados variam de -1 a +1. Se os caracteres que rotulam as fatias forem longos, pode ser necessário usar um raio menor. O padrão é 0,8.\n\ndensity \\(\\to\\) Densidade das linhas de sombreamento, em linhas por polegada. O padrão é NULL significa que nenhuma linha de sombreamento é desenhada. Valores não positivos de densidade também inibem o desenho de linhas sombreadas\ncol \\(\\to\\) Vetor de cores a ser usado no preenchimento ou sombreamento das fatias. Se estiver faltando, um conjunto de 6 cores pastel é usado\n\nOs valores da coluna de frequência absoluta (f) da Tabela 6.4 serão usados como o argumento x. Ele informa a área (proporção de cada fatia). Os rótulos das fatias são escritos com a função concatenar c().\n\npie(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"))\n\n\n\n\n\n\n\nFigura 6.4: Gráfico de Pizza: Opinião dos estudantes de Medicina.\n\n\n\n\n\nAs cores que aparecem na Figura 6.4 foram escolhidas pelo R, usando o seu padrão. Entretanto, elas podem ser customizadas, especificando-as pelo nome colocado entre parênteses. Por exemplo, col = \"red\" e se for mais de uma cor usar a função concatenar, col = c(“gray58“, “yellow4”, “cyan”, “tomato”). As cores também podem se denotadas pelo sistemas RGB ou hexadecimal. A sigla RGB representa as cores primárias em inglês (Red, Green, Blue). O código hexadecimal da cor branca é #FFFFFFF, da gray58 é #949494, da yellow4 é #999900, da cyan é #00FFFFe da tomato é #FF6347 (Figura 6.5).\n\npie(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"),\n    col = c(\"gray58\", \"yellow4\", \"cyan\", \"tomato\"))\n\n\n\n\n\n\n\nFigura 6.5: Figura anterior com cores personalizadas.\n\n\n\n\n\nAs cores parecem “espetaculosas”, mas o objetivo foi de criticar os gráficos tipo pizza. Se o leitor quiser insistir no seu uso e com um gráfico em 3D (Figura 6.6), pode-se instalar o pacote plotrix (12) e carregar a função pie3D(). Os argumentos são praticamente os mesmos do gráfico simples. Acrescenta-se radius = 0.9 que muda o raio da pizza e explode = 0.1 que determina o afastamento das fatias (0, as mantém juntas). Além disso, como o gráfico exibe rótulos com textos muito grandes, usa-se o argumento labelcex = 1. Como qualquer função nova, basta clicar na tecla Tab, dentro da mesma, que aparece um menu com as alternativas de argumentos.\n\nlibrary (plotrix)\n\npie3D(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"),\n    radius = 0.9,\n    explode = 0.1,\n    col = c(\"gray58\", \"yellow4\", \"cyan\", \"tomato\"),\n    labelcex = 1)\n\n\n\n\n\n\n\nFigura 6.6: Gráfico de Pizza: Opinião dos estudantes de Medicina.\n\n\n\n\n\n\n\n6.5.2 Gráfico de barras\nOs gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas (13).\nAssim como o gráfico de setores, o gráfico de barras é utilizado para representar a frequência absoluta ou percentual de diferentes categorias. As barras são proporcionais as frequências. A forma mais simples de solicitar um gráfico de barra no R é digitar a função barplot() do pacote básico. Esta função é específica para desenhar gráficos de barras horizontais e verticais e usa os seguintes argumentos:\n\nheight \\(\\to\\) um vetor ou matriz de valores que descreve as barras que constituem o gráfico;\n\nwidth \\(\\to\\) especifica largura das barras, com padrão de 1, opcional;\n\nspace \\(\\to\\) a quantidade de espaço (como uma fração da largura média da barra) restante antes de cada barra. Pode ser fornecido como um único número ou um número por barra;\n\nbeside \\(\\to\\) argumento lógico para especificar se colunas devem ser mostradas lado a lado;\n\ncol \\(\\to\\) cores das barras componentes das barras, por padrão é usado grey (cinza);\n\nborder \\(\\to\\) cor das bordas das barras;\n\n… \\(\\to\\) outros argumentos. Consulte a ajuda do R.\n\nPara a construção do gráfico de barras simples da Figura 6.7, será utilizada a variável categIdade, anteriormente criada (Seção 6.4.2.2), a partir do conjunto de dados dadosMater.xlsx.\n\nbarplot(table(mater$categIdade))\n\n\n\n\n\n\n\nFigura 6.7: Gráfico de barra simples.\n\n\n\n\n\nObservando a Figura 6.7, verifica-se que não existem rótulos nos eixos x e y e o eixo y tem um tamanho inferior a barra mais alta. Estes e outros problemas podem ser resolvidos modificando-se ou acrescentando outros argumentos na função barplot(). Existem vários argumentos e para conhece-los melhor pesquise no Help do RStudio. Em um gráfico de barra simples são suficientes as seguintes modificações que irão resultar na Figura 6.8:\n\nPara corrigir a amplitude do eixo y, existe o argumento ylim = c(lim inf, lim sup). Na Tabela 6.2, observa-se que a frequência máxima é de 992, assim estende-se até 1000, bem próximo da frequência da categoria, acrescentando ylim = c (0,1000), separado por vírgulas de outros argumentos.\nPara os rótulos se utiliza os argumentos ylab = (“Frequência”) e xlab = (“Faixa Etária”). Também, pode ser incluído um título no gráfico com o argumento main = “Título”. Observe que os títulos estão entre aspas.\nPara modificar o tamanho das letras dos eixos x e y, que estão pouco visíveis, existe o argumento cex.lab = 1, que é o padrão. Para aumentar em 30%, por exemplo, usar cex.lab = 1.3. Os nomes tem padrão cex.names = 1, para modificar pode-se usar 1.3, 1.5, etc. Se nada for modificado, o R imprime o padrão.\nPara a cor das barras, use o argumento col = (“cor”). Escolha a cor entre as 657 opções, ou deixe o padrão cinza (grey). O argumento col.axis = “cor” controla a cor dos valores dos eixos (veja também Seção 6.6.3).\nPara modificar a borda das barras que por padrão é preta, usar o argumento border = “cor”. Se não desejar a borda, basta colocar 0 (zero), no lugar da cor.\nPara colocar as barras na posição horizontal, pode ser utilizado o argumento horiz = TRUE. Lembrar de inverter as barras. Ou seja, a variável x passa a ser y e vice-versa.\nO argumento las = 1 faz o o texto do eixo y ficar horizontal\nA função box(bty = \"L\"), colocada após é opcional e faz os eixos se encontraren em 0.\n\n\nbarplot(table(mater$categIdade), \n        ylim = c (0,1000), \n        col= \"tomato\", \n        border = \"black\", \n        ylab= \"Frequência absoluta\", \n        xlab = \"Faixa etária\", \n        cex.lab = 1.2,\n        las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.8: Gráfico de barra simples modificado.\n\n\n\n\n\nPara que as barras fiquem horizontais como na Figura 6.9, usa-se o argumento horiz=TRUE:\n\nbarplot(table(mater$categIdade), \n        xlim = c (0,1000), \n        col= \"tomato\", \n        border = \"black\", \n        ylab= \"Faixa Etária\", \n        xlab = \"Frequência absoluta\", \n        cex.lab = 1.2, \n        horiz=TRUE)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.9: Gráfico com barras horizontais.\n\n\n\n\n\nAlém das modificações realizadas, pode-se fazer outras para tornar o gráfico mais informativo . Por exemplo, colocar as frequência de cada barra no topo das mesmas (Figura 6.10):\n\n1º Passo: Criar um gráfico de barras , colocando-o em um objeto qualquer, por exemplo, x 4, que conterá o eixo X do centro de cada uma das barras. Para verificar isso, basta executar o objeto x;\n2º Passo: colocar a tabela table(mater$idadeCateg) como um objeto y 5, onde estarão as frequências absolutas);\n3º Passo: usar a função text() para colocar os valores.Consulte o Help para maiores detalhes desta função.\n\n\ny &lt;- table(mater$categIdade)\n\nx &lt;- barplot(y, \n             ylim = c (0,1000), \n             col= \"springgreen\", \n             border = \"black\", \n             ylab = \"Frequência absoluta\", \n             xlab = \"Faixa etária\", \n             cex.lab = 1.2,\n             las = 1)\nbox(bty = \"L\")\n\ntext (x, y, labels = as.character(y), adj = c(0.5, 2), col = \"black\")\n\n\n\n\n\n\n\nFigura 6.10: Gráfico de barra simples com frequências no topo.\n\n\n\n\n\n\n6.5.2.1 Gráfico de barras empilhadas\nPara este tipo de apresentação são utilizados, praticamente, os mesmos argumentos vistos para gerar um gráfico de barra simples. Como existem duas variáveis, há necessidade de avisar ao R como elas devem aparecer. Para isso, entra o argumento beside = FALSE, que informa que as barras não estarão uma ao lado da outra e sim empilhadas (Figura 6.11). O padrão é as barras ficarem uma ao lado da outra.\nAcrescenta-se uma legenda com a função legend() na parte superior esquerda (topleft). O argumento bty = \"n\" informa que será removido o quadro ao redor da legenda e fill = c(\"dimgrey\", \"salmon\") são as cores das barras.\nAs duas variáveis a serem visualizadas são o hábito tabagista entre as puérperas de acordo com a idade. No conjunto de dados dadosMater.xlsx, o hábito tabagista está registrado na variável fumo, vista quando se estudou tabelas de contingência. Aqui se construirá uma tabela 3 x 2, tabFumo2:\n\ntabFumo2 &lt;- table(mater$fumo, mater$categIdade)\n\nbarplot(tabFumo2,\n        beside = FALSE,\n        ylim = c(0, 1000),\n        xlab=\"Faixa Etária\", \n        ylab = \"Frequência\", \n        col = c (\"dimgrey\", \"cadetblue1\"),  \n        cex.lab = 1, \n        cex.axis = 1, \n        cex.names = 1,\n        las = 1)\nbox(bty = \"L\")\nlegend (\"topleft\",\n        legend = c(\"Fumantes\", \"Não Fumantes\"), \n        fill = c(\"dimgrey\", \"cadetblue1\"), \n        bty=\"n\", \n        cex = 1)\n\n\n\n\n\n\n\nFigura 6.11: Gráfico de barras empilhadas.\n\n\n\n\n\n\n\n6.5.2.2 Gráfico de barras lado a lado\nÉ igual a anterior, apenas com o argumento beside = TRUE (Figura 6.12).\n\nbarplot(tabFumo2,\n        beside = TRUE,\n        ylim = c(0, 1000),\n        xlab=\"Faixa Etária\", \n        ylab = \"Frequência\", \n        col = c (\"dimgrey\", \"cadetblue1\"),  \n        cex.lab = 1, \n        cex.axis = 1, \n        cex.names = 1,\n        las = 1)\nbox(bty = \"L\")\nlegend (\"topleft\",\n        legend = c(\"Fumantes\", \"Não Fumantes\"), \n        fill = c(\"dimgrey\", \"cadetblue1\"), \n        bty=\"n\", \n        cex = 1)\n\n\n\n\n\n\n\nFigura 6.12: Gráfico de barras lado a lado\n\n\n\n\n\n\n\n6.5.2.3 Gráfico de barras para uma variável discreta\nA variável mater$para, número de filhos anteriores ao atual, é uma variável numérica discreta e, para representá-la, o mais adequado é usar um gráfico de barras simples (Figura 6.13).\n\ntab_filhos&lt;- table (mater$para) \n\nbarplot (tab_filhos, \n         col = \"tomato\", \n         xlab=\"Número de filhos anteriores ao atual\", \n         ylab = \"Frequência\",\n         ylim = c(0, 500),\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.13: Gráfico de barras para uma variável discreta\n\n\n\n\n\n\n\n\n6.5.3 Gráfico de barra de erro\nO gráfico de barra de erro é um tipo de gráfico barra acrescido de uma medida de dispersão: desvio padrão, intervalos de confiança ou erro padrão. As barras de erro dão uma ideia geral de quão precisa é uma medição ou, inversamente, quão longe o valor observado está do valor verdadeiro.\nContinuando a usar o arquivo dadosMater.xlsx, será selecionada uma amostra de recém-nascidos a termo, definido pela OMS como o nascido de 37 semanas completas a 42 semanas incompletas (259 a 293 dias). A partir destes dados, será construido um gráfico de barra de erro dos recém-nascidos do sexo masculino e feminino.\nInicialmente, deve ser instalado e carregado o pacote Hmisc (14), necessário para fornecer a função errbar() que irá construir o gráfico de de barra de erro.\nA seguir, serão filtrados do conjunto de dados em uso, mater, os recém-nascidos a termo. O conjunto resultante será atribuído a um objeto denominado rnt e , usando o operador pipe %&gt;% será usada a função summarise() e group_by() provenientes do pacote dplyr, para calcular as medidas resumidoras, de acordo com o sexo. Como a variável sexo encontra-se como numérica, será transformada em fator:\n\nmater$sexo &lt;- factor(mater$sexo,\n                      labels = c('masc', 'fem'))\n \n rnt &lt;- mater %&gt;% \n   filter(ig &gt;= 37 & ig &lt; 42) %&gt;% \n   group_by(sexo) %&gt;% \n   summarise(n = n(),\n             media = mean(pesoRN, na.rm = T),\n             dp = sd(pesoRN, na.rm = T),\n             l_inf = media - 1.96*dp,\n             l_sup = media + 1.96*dp)\n\n rnt\n\n# A tibble: 2 × 6\n  sexo      n media    dp l_inf l_sup\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 masc    592 3274.  458. 2376. 4172.\n2 fem     493 3147.  458. 2250. 4044.\n\n\nO próximo passo é criar um objeto, denominado barras, que irá receber as médias dos pesos dos recém-nascidos masculinos e femininos, que representam a altura das barras. Este objeto servirá de base para a construção de um gráfico de barras que será recebido por outro objeto, bp. Finalmente, coloca-se os limites inferiores e superiores para cada sexo, usando os valores calculados pela função summarise() que junto com o objeto bp constituem-se de argumentos da função errbar() (Figura 6.14). Veja maiores detalhes na ajuda do R (?errbar).\n\nbarras &lt;- c(rnt$media[1], rnt$media[2])\n\n bp &lt;- barplot(barras,\n               ylim=c(0,4200), \n               ylab = \"Peso do Recém-nascido (g)\",\n               cex.lab = 1.2,\n               cex.axis = 0.8,\n               cex.names = 1,\n               space = c(0,0.5),\n               names.arg=c(\"Meninos\", \"Meninas\"), \n               col = c(\"lightblue\", \" pink2\"),\n               las = 1)\n box(bty = \"L\")\n\n lim_inf &lt;- c(rnt$l_inf[1], rnt$l_inf[2])\n lim_sup &lt;- c(rnt$l_sup[1], rnt$l_sup[2])\n\n errbar(bp, barras, lim_inf, lim_sup, add = TRUE, xlab = NULL)\n\n\n\n\n\n\n\nFigura 6.14: Gráfico de barras de erro\n\n\n\n\n\n\n\n6.5.4 Histograma\nO histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos.\nNo histograma, as frequências observadas são representadas por intervalos de classes de ocorrência que estão no eixo x e a altura das barras, representando a frequência de cada intervalo, no eixo y. A área de cada barra é proporcional à porcentagem de observações de cada intervalo.\nO R base possui uma função, denominada de hist() que constroi o histograma e possui vários argumentos:\n\nx \\(\\to\\) um vetor numérico usado na construção do histograma\n\nbreaks \\(\\to\\) especifica o número de barras\nfreq \\(\\to\\) lógico; se TRUE (padrão), o histograma é uma representação de frequências; se FALSE, densidades de probabilidade, densidade de componentes, são plotados\ncol \\(\\to\\) cor a ser usada para preencher as barras. O padrão de NULL produz barras não preenchidas\nborder \\(\\to\\) cor da borda ao redor das barras. O padrão é usar a cor de primeiro plano padrão\nmain, xlab, ylab \\(\\to\\) rótulo do título, do eixo x e do eixo y. Para remover o rótulousar NULL.\nxlim, ylim \\(\\to\\) limites do eixo x e do eixo y.\n\n\n6.5.4.1 Histograma Simples\nSerá usada a variável altura, incluída da arquivo mater (veja início da Seção 6.4), para a construção do histograma, executando:.\n\nhist(mater$altura)\n\n\n\n\n\n\n\nFigura 6.15: Histograma básico\n\n\n\n\n\nNo histograma da Figura 6.15, observam-se alguns problemas que devem ser melhorados para tornar a sua aparência mais elegante.\n\nO rótulo dos eixo x está com o nome da variável e do eixo y está em inglês;\nO título do histograma está em inglês e repete o eixo x. Pode ser removido.\nO eixo y tem um limite superior menor do que a barra mais alta;\nO gráfico está na cor cinza, que conforme o interesse pode ser modificada;\nO número de barras pode ser modificado com o argumento breaks. Existe uma função no R que permite calcular o número de intervalos, usando a regra de Sturges (nclass.Sturges()). Entretanto, na maioria das vezes, é o objetivo do estudo quem determina o número de barras e, também, porque nem sempre o R obedece ao argumento.\n\nÉ importante saber o limite inferior e superior da variável, para construir o eixo x. Pode-se fazer isso, com as funções min() e max():\n\nmin(mater$altura, na.rm = TRUE)\n\n[1] 1.4\n\nmax(mater$altura, na.rm = TRUE)\n\n[1] 1.85\n\n\nO número de classes é igual a:\n\nnclass.Sturges(mater$altura)\n\n[1] 12\n\n\nAcrescentado argumentos, modifica-se o aspecto do histograma (Figura 6.16):\n\nhist(mater$altura,\n     breaks = 12,\n     ylim = c (0, 450),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Frequência\", \n     xlab = \"Altura da gestante (metros)\",\n     col = \"tomato\",\n     las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.16: Histograma modificado\n\n\n\n\n\nObserve que o formato do histograma é igual ao anterior, mudando a cor das barras, o limite do eixo y e os rótulos dos eixos. O R não modificou o número de barras. Ou seja, não obedeceu à modificação do argumento breaks = 12. A função escolheu o que achou mais adequado!\n\n\n6.5.4.2 Histograma com curva normal sobreposta\nEventualmente, para melhor comparar a distribuição dos dados, é interessante incluir uma curva normal sobreposta que servirá de indicador (Figura 6.17). A distribuição normal será discutida mais adiante (Seção 7.7).\nOs passos para colocar a curva normal sobreposta são:\n\nConstruir um histograma de densidade, que é a proporção de todas as observações que se enquadram dentro do intervalo. Na função hist(), modificar o argumento para freq = FALSE.\nAdicionar uma curva normal ao histograma, usando a função curve(). Calcular antes a média e o desvio padrão da variável mater$altura.\n\n\nmu &lt;- mean(mater$altura, na.rm =TRUE)\ndp &lt;- sd(mater$altura, na.rm = TRUE)\n\nhist(mater$altura,\n     ylim = c (0, 6),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Densidade\", \n     xlab = \"Altura da gestante (metros)\",\n     col =\"steelblue\",\n     freq = FALSE,            \n     border = \"white\")\nbox (bty = \"L\")\n\ncurve (dnorm (x, \n              mean=mu, \n              sd=dp), \n       col=\"red\", \n       lty=1,\n       lwd=2,\n       add=TRUE)\n\n\n\n\n\n\n\nFigura 6.17: Histograma com curva normal sobreposta\n\n\n\n\n\n\n\n6.5.4.3 Componentes do Histograma\nPara verificar a lista de componentes de um histograma , há necessidade de colocar o histograma da ?fig-histh em um objeto, no exemplo, denominado de h:\n\nh &lt;- hist(mater$altura,\n          breaks = 8,\n          ylim = c (0, 450),\n          xlim = c (1.4, 1.9),\n          main= NULL, \n          ylab = \"Frequência\", \n          xlab = \"Altura da gestante (metros)\",\n          col =\"tomato\",\n          freq = TRUE,           \n          border = \"white\")\n      box (bty = \"L\")\n\n\n\n\nHistograma da altura da gestante\n\n\n\nh\n\n$breaks\n [1] 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85\n\n$counts\n[1]  18  87 304 406 334 151  50  16   2\n\n$density\n[1] 0.26315789 1.27192982 4.44444444 5.93567251 4.88304094 2.20760234 0.73099415\n[8] 0.23391813 0.02923977\n\n$mids\n[1] 1.425 1.475 1.525 1.575 1.625 1.675 1.725 1.775 1.825\n\n$xname\n[1] \"mater$altura\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nUma das utilidades dos componentes, é construir um histograma com os valores correspondentes as barras sobrepostos ao gráfico (Figura 6.18).\n\nhist(mater$altura,\n     breaks = 8,\n     ylim = c (0, 450),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Frequência\", \n     xlab = \"Altura da gestante (metros)\",\n     col = \"salmon\")\nbox (bty = \"L\")\n\ntext (h$mids, h$counts, labels = h$counts, adj= c(0.5, -0.5))\n\n\n\n\n\n\n\nFigura 6.18: Histograma com frequência sobreposta\n\n\n\n\n\nNote que as informações deste gráfico são as mesmas de uma tabela de frequência construída com os mesmos dados. Maneiras diferentes de informar uma distribuição de frequência (veja a Seção 6.4.2).\n\n\n\n6.5.5 Boxplot\nO boxplot descreve a distribuição de uma variável contínua exibindo o resumo de cinco números: mínimo, 1º quartil (percentil 25), mediana (percentil 50), 3ª quartil (percentil 75) e máximo (Figura 6.19).\n\n\n\n\n\n\n\n\nFigura 6.19: Boxplot\n\n\n\n\n\n\n6.5.5.1 Boxplot a partir de um vetor\nPara construir um boxplot, serão usados os mesmos dados dos recém-nascidos a termo, filtrados do conjunto de dados dadosMater.xlsx, como realizado na seção da construção de um gráfico de barra de erro (Seção 6.5.3). Os dados obtidos, novamente serão atribuídos a um objeto de nome rnt. A variável usada para construir o boxplot será rnt$pesoRN.\nO R possui uma função no pacote básico denominada boxplot() que foi usada para construir o gráfico da Figura Figura 6.20. A função solicita vários argumentos que podem alterar a sua aparência e devem ser utilizados de acordo com necessidade:\n\nformula \\(\\to\\) este parâmetro é definido como um vetor ou uma fórmula (y ~ grupo);\ndata \\(\\to\\)este parâmetro define o conjunto de dados;\nnotch \\(\\to\\) parâmetro lógico. Se TRUE um entalhe será desenhado em cada lado da caixa, representando o intervalo de confiança para a mediana. Se os entalhes de dois boxplots não se sobrepuserem, indica uma “forte evidência” de que as duas medianas diferem;\nvarwidth \\(\\to\\) parâmetro lógico. Se for TRUE, as caixas serão desenhadas com larguras proporcionais às raízes quadradas do número de observações nos grupos;\nborder \\(\\to\\)um vetor opcional de cores para os contornos dos boxplots:\nmain \\(\\to\\) este parâmetro é o título do gráfico;\nxlab, ylab \\(\\to\\) rótulos dos eixos x e y ;\ncex \\(\\to\\) ver https://www.statology.org/r-plot-cex/;\nlas \\(\\to\\) altera orientação do rótulos do eixo. Valores aceitos 0 (paralelo ao eixo), 1 (horizontal), 2 (perpendicular) e 3 (vertical);\nnames \\(\\to\\) Este parâmetro são os rótulos dos grupos que serão mostrados em cada boxplot;\n… \\(\\to\\) Outros parâmetros (ver ajuda do R, digitando ?boxplot no Console)\n\n\nrnt &lt;- mater %&gt;% filter(ig &gt;= 37 & ig &lt; 42)\n\nrnt$sexo &lt;- factor(rnt$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\nboxplot (rnt$pesoRN)\n\n\n\n\n\n\n\nFigura 6.20: Boxplot simples\n\n\n\n\n\nEsse boxplot pode ser modificado (Figura 6.21), alterando alguns argumentos como colocação de um título no gráfico, e rótulos nos eixos e mudança na cor. Os argumento cex.lab, cex.axis e cex.names estabelecem o tamanho fontes. Por exemplo, para aumentar em 20%, usamos 1.2.\n\nboxplot (rnt$pesoRN, \n         col = \"lightblue2\", \n         main = \"RN a termo\", \n         ylab = \"Peso do Recém-nascido (g)\",\n         border = \"black\",\n         cex.lab = 1.2, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.21: Boxplot modificado\n\n\n\n\n\nInterpretação do boxplot\nO boxplot nos fornece uma análise visual da posição, dispersão, simetria, caudas e valores discrepantes (outliers) do conjunto de dados (Figura 6.19).\n\nPosição – Em relação à posição dos dados, observa-se a linha central do retângulo (a mediana ou segundo quartil).\nDispersão – A dispersão dos dados pode ser representada pelo intervalo interquartil (IIQ), tamanho da caixa, que é a diferença entre o terceiro quartil (3ºQ) e o primeiro quartil (1ºQ), ou ainda pela amplitude que é calculada da seguinte maneira: valor máximo – valor mínimo. Embora a amplitude seja de fácil entendimento, o intervalo interquartil é uma estatística mais robusta para medir variabilidade uma vez que não sofre influência de outliers.\nSimetria – Um conjunto de dados que tem uma distribuição simétrica, terá a linha da mediana no centro do retângulo. Quando a linha da mediana está próxima ao primeiro quartil, os dados são assimétricos positivos e quando a posição da linha da mediana é próxima ao terceiro quartil, os dados são assimétricos negativos. Vale lembrar que a mediana é a medida de tendência central mais indicada quando os dados possuem distribuição assimétrica, uma vez que a média aritmética é influenciada pelos valores extremos.\nCaudas – As linhas que vão do retângulo até aos outliers podem fornecer o comprimento das caudas da distribuição.\nValores atípicos (Outliers) – Os outliers indicam possíveis valores discrepantes. No boxplot, as observações são consideradas atípicas quando estão abaixo ou acima dos limites superior e inferior. O limite de detecção de outliers é construído utilizando o intervalo interquartil, dado pela distância entre o primeiro e o terceiro quartil. Sendo assim, os limites inferior e superior de detecção de outlier são dados por:\n\no Limite Inferior: 1ºQ – (1,5 * IIQ);\no Limite Superior: 3ºQ + (1,5 * IIQ). Tanto o limite superior como o inferior são representados por (º).\nos Valores extremos: são valores que estão acima ou abaixo de 3 vezes o IIQ e são representados por (*).\n\n\n\n\n6.5.5.2 Adicionando pontos ao boxplot\nQuando se observa um boxplot, verifica-se que os mesmos ocultam a distribuição subjacente dos dados. Para resolver este “problema”, pode-se adicionar pontos ao gráfico, usando a função stripchart(). Esta função permite criar um gráfico de dispersão unidimensional sobreposto ao boxplot (Figura 6.22). Os comandos para esta ação são:\n\nboxplot (rnt$pesoRN, \n          col = \"lightblue2\", \n          ylab = \"Peso do Recém-nascido (g)\", \n          border = \"black\",\n          cex.lab = 1.2, \n          cex.axis = 1, \n          cex.names = 1,\n          las = 1)\n\n stripchart(x= rnt$pesoRN, \n            method = \"jitter\", \n            col = \"tomato\",\n            cex = 0.5,\n            pch = 16,\n            vertical = TRUE, \n            add = TRUE)\n\n\n\n\n\n\n\nFigura 6.22: Boxplot com pontos de dispersão\n\n\n\n\n\nNeste exemplo, há uma grande sobreposição de pontos, pois a amostra é muito grande (n = 1085). Isto dificulta um pouco a visualização, mas ajuda a ver como a dispersão se comporta. Você também pode personalizar o símbolo (pontos) para criar o gráfico, a largura da linha e sua cor com os argumentos pch, lwd e col, respectivamente. Alguns símbolos, como pch = 21 a 25 permitem que você modifique a cor de fundo do símbolo com o argumento bg. O argumento vertical = TRUE, coloca os pontos na vertical sobreposto ao boxplot, quando o argumento add = TRUE. O argumento cex = 0.5 é o tamanho dos pontos e method = \"jitter\", espalha os pontos para diminuir a sobreposição entre eles.\n\n\n6.5.5.3 Boxplot com intervalos de confiança para a mediana\nÉ possível representar os intervalos de confiança de 95% para a mediana em um boxplot (Figura 6.23), definindo o argumento notch como TRUE.\n\nboxplot (rnt$pesoRN, \n          col = \"lightblue2\", \n          ylab = \"Peso do Recém-nascido (g)\", \n          border = \"black\",\n          cex.lab = 1.2, \n          cex.axis = 1, \n          cex.names = 1,\n          las = 1,\n          notch = TRUE)\n\n\n\n\n\n\n\nFigura 6.23: Boxplot modificado\n\n\n\n\n\n\n\n6.5.5.4 Estatísticas do boxplot\nA função boxplot.stats() do pacote grDevices fornece as estatísticas do boxplot, facilitando a interpretação do mesmo, de modo semelhante ao visto para o histograma.\n\nboxplot.stats (rnt$pesoRN)\n\n$stats\n[1] 2051 2920 3215 3505 4380\n\n$n\n[1] 1085\n\n$conf\n[1] 3186.939 3243.061\n\n$out\n [1] 1440 1980 1795 1810 4400 4950 4535 4670 1425 4410 4660 1715 1895 4485 4390\n[16] 4445 4620 1785\n\n\nInterpretação das estatísticas\n* $stats = é o resumo dos 5 números: mínimo, percentil 25, mediana, percentil 75 e máximo;\n* $n = nº de obs;\n* $conf = limite inf/sup do entalhe se houver;\n* $out = são os outliers.\n\n\n6.5.5.5 Múltiplos boxplots\nOs boxplots são muito usados na comparação de grupos. A necessidade mais comum é ordenar as categorias de acordo com o aumento da mediana, mas isto é opcional. Permite identificar rapidamente qual grupo tem o maior valor e como as categorias são classificadas (Figura 6.24).\nSerá realizada uma comparação visual, usando boxplots, dos pesos dos recém-nascidos por sexo. As variáveis são rnt$pesoRN e rnt$sexo. Esta última está codificada como numérica 1 e 2, portanto há necessidade de ser transformada em fator:\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(idadeMae, altura, peso, anosEst, fumo, \n         para, ig, sexo, pesoRN, compRN, utiNeo) %&gt;% \n  mutate(categIdade = case_when(\n    idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n    idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n    idadeMae &gt; 35 ~ \"&gt; 35 anos\")) %&gt;%\n  mutate(categIdade = factor(categIdade, \n                             levels = c(\"&lt; 20 anos\", \"20 a 35 anos\",\n                                                    \"&gt; 35 anos\")))\n\nrnt &lt;- mater %&gt;% filter(ig &gt;= 37 & ig &lt; 42)\n\nrnt$sexo &lt;- factor(rnt$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink\"), \n         ylab = \"Peso do Recém-nascido (g)\", \n         xlab = \"Sexo\",\n         border = \"black\",\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.24: Múltiplos boxplots\n\n\n\n\n\nObserve que foi utilizado o argumento rnt$pesoRN ~ rnt$sexo (y ~ grupo) para obter os dois boxplots. Existe uma pequena diferença entre eles, as caixas são quase coincidentes. Foi suprimido o argumento (xlab = sexo) relativo ao rótulo do eixo x, pois seria redundante.\nPode-se fazer um entalhe (notch) que podem ser interpretados como um intervalo de confiança em torno dos valores medianos Figura 6.25). É calculado pela fórmula :\\(mediana \\pm 1.57\\times IIQ/\\sqrt{n}\\). No nosso exemplo, observe que o entalhe nos meninos está um pouco acima do das meninas..\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink\"), \n         ylab = \"Peso do Recém-nascido (g)\", \n         xlab = \"Sexo\",\n         border = \"black\",\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1,\n         notch = TRUE)\n\n\n\n\n\n\n\nFigura 6.25: Boxplots com entalhes\n\n\n\n\n\n\n\n6.5.5.6 Boxplots horizontais\nPara criar um boxplot horizontal Figura 6.26), usamos o argumento horizontal = TRUE e invertemos os rotulos dos eixos x e y.\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink2\"), \n         xlab = \"Peso do Recém-nascido (g)\", \n         ylab = NULL,\n         horizontal = TRUE,\n         border = \"black\",\n         cex.lab = 1.2, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.26: Boxplots horizontais\n\n\n\n\n\n\n\n\n6.5.6 Gráfico de Dispersão\nUm gráfico de dispersão (Scatterplot) exibe a relação entre duas variáveis numéricas (Figura 6.27). Cada ponto representa uma observação. Suas posições nos eixos x (horizontal) e y (vertical) representam os valores das duas variáveis.\nO R Base é uma boa opção para construir um gráfico de dispersão, usando a função plot(). Ambas as variáveis numéricas do banco de dados devem ser especificadas nos argumentos x e y.\nA função plot() é uma função genérica que pode ser facilmente editada com múltiplos argumentos envolvendo os eixos e caracteres plotados da mesma maneira que foi feita com os gráficos anteriores. Aqui, novamente, serão usados os dados incluídos no conjunto de dados rnt:\n\nplot (x = rnt$compRN,\n      y = rnt$pesoRN,\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      cex.axis = 0.8,\n      las = 1)\n\n\n\n\n\n\n\nFigura 6.27: Gráfico de dispersão\n\n\n\n\n\nEste mesmo gráfico pode ser obtido, usando uma fórmula y~x e acrescentando o argumento bty = \"L\" (Figura 6.28). Este argumento permite personalizar a caixa ao redor do gráfico.\n\no: caixa completa (parâmetro padrão),\nn: sem caixa\n7: superior + direita\nL: inferior + esquerda\nC: superior + esquerda + inferior\nU: esquerda + inferior + direita\n\n\nplot (pesoRN ~ compRN,\n      data = rnt,\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      cex.axis = 0.8,\n      las = 1,\n      bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.28: Gráfico de dispersão\n\n\n\n\n\nComo em qualquer outro gráfico, este também pode ser melhorado em seu aspecto, tornando os pontos sólidos e coloridos. O argumento pch estabelece o tipo de pontos (Figura 6.29).\n\n\n\n\n\n\n\n\nFigura 6.29: Símbolo dos formatos\n\n\n\n\n\nNa Figura 6.27, como os pontos estão aglomerados, devido a quantidade, é possível tentar espalhá-los, usando a função jitter() na variável compRN (Figura 6.30). O argumento 10 é variável e significa o grau de espalhamento:\n\nplot (jitter(rnt$compRN,10),\n      rnt$pesoRN,\n      col = \"steelblue\",\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      las = 1,\n      bty = \"L\",\n      pch = 16,\n      cex = 1,\n      cex.lab = 1.1,\n      cex.axis = 0.8)\n\n\n\n\n\n\n\nFigura 6.30: Gráfico de dispersão com jitter\n\n\n\n\n\n\n6.5.6.1 Mapeamento dos pontos de acordo com uma variável categórica\nInicialmente, será criado um vetor para representar as cores, de acordo com o sexo (meninos = azul; meninas = vermelho). Usa-se a função unclass() para discriminar os sexos (Figura 6.31). Acrescenta-se uma legenda para ilustrar a separação.\n\ncores &lt;- c(\"dodgerblue3\", \"tomato\")\n\nplot(x = jitter(rnt$compRN, 10), \n  y = rnt$pesoRN,\n  bg = cores[unclass(rnt$sexo)],\n  ylab = \"Peso de Recém-nascido (g)\",\n  xlab = \"Comprimento do Recém-nascido (cm)\",\n  las = 1,\n  bty = \"L\",\n  cex = 1.5,\n  pch=21,\n  cex.lab = 1,\n  cex.axis = 0.8)\n\nlegend (legend = c(\"Meninos\", \"Meninas\"), \n        fill = cores, \n        bty=\"n\", \n        cex = 1,\n        \"topleft\")\n\n\n\n\n\n\n\nFigura 6.31: Mapeamento dos pontos de acordo com uma variável categórica\n\n\n\n\n\n\n\n6.5.6.2 Adição da reta de ajuste\nUma linha reta de ajuste dos dados (Figura 6.32) pode ser acrescentada usando a função abline (), associada a função lm (). Um modelo típico lm (linear model) tem o formato resposta (y) ~ preditor (x). Mais detalhes sobre o modelo de ajuste linear na ?sec-rls (regressão linear).\n\n# Construção do gráfico de dispersão\nplot (jitter(rnt$compRN,10),\n      rnt$pesoRN,\n      col = \"gray40\",\n      bg = \"darkturquoise\",\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      las = 1,\n      bty = \"L\",\n      pch = 21,\n      cex = 1.3,\n      cex.lab = 1,\n      cex.axis = 0.8)\n\n# Criação do modelo de ajuste\nmodelo &lt;- lm (rnt$pesoRN ~ rnt$compRN)\n\n# Adição da reta, usando o modelo\nabline (modelo, \n        col=\"red\", \n        lwd=2, \n        lty = 2)\n\n\n\n\n\n\n\nFigura 6.32: Gráfico de dispersão com reta de ajuste\n\n\n\n\n\nAo executar o modelo, se obtém os parâmetros para a construção da equação da regressão linear:\n\nsummary(modelo)\n\n\nCall:\nlm(formula = rnt$pesoRN ~ rnt$compRN)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1434.56  -218.40   -19.56   177.76  2097.87 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3416.451    215.821  -15.83   &lt;2e-16 ***\nrnt$compRN    137.674      4.475   30.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 337.7 on 1083 degrees of freedom\nMultiple R-squared:  0.4664,    Adjusted R-squared:  0.4659 \nF-statistic: 946.6 on 1 and 1083 DF,  p-value: &lt; 2.2e-16\n\n\nA equação de predição da regressão linear permite que, conhecendo o valor do comprimento, é possível prever o peso do recem-nascido:\n\\[\n\\hat{y} = b_{0}+ b_{1}\\times x\n\\]\nDesta forma, substituindo pelos valores contidos nas estimativas da tabela dos coeficientes do sumário do modelo, um bebê com 50 cm terá um peso de aproximadamente:\n\\[\n\\hat{y} = -3416.45 + 137.67\\times 50 = 3467.05\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#sec-ggplot2",
    "href": "06-descrevendoDados.html#sec-ggplot2",
    "title": "6  Descrevendo os dados",
    "section": "6.6 Introdução ao ggplot2",
    "text": "6.6 Introdução ao ggplot2\nO R tem vários sistemas para fazer gráficos e, na ,maioria das vezes, eles são suficientes. Entretanto, o surgimento do ggplot2 (15) trouxe a possibilidade de serem construídos gráficos mais elegantes e versáteis. Além disso, torna o processo mais rápido, baseado em uma sofisticada gramática (16).\nOs três componentes principais de cada gráfico do ggplot2são: dados, estética e geometria.\n\nDados (data),\nMapeamentos estéticos (aesthetic mappings) entre variáveis e propriedades visuais (como posição, cor, tamanho, forma e transparência), e\ngeom são as funções que criam camadas, pelo menos uma, que descrevem como renderizar cada observação.\n\n\n6.6.1 Principais gráficos usando ggplot2\n\n6.6.1.1 Gráfico de dispersão\nSerão usados os dados coletados em um ambulatório pediátrico relacionados à idade e ao comprimento de 40 crianças entre 18 e 36 meses (20 meninos e 20 meninas), incluídos no arquivo dadosReg.xlsx. Eles podem ser baixados aqui. Salve o arquivo no seu diretório de trabalho e carrege-o com a função read_excel() do pacote readxl:\n\ndados &lt;- readxl::read_excel(\"dados/dadosReg.xlsx\")\nstr(dados)\n\ntibble [40 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ...\n $ comp  : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ...\n $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ...\n $ sexo  : chr [1:40] \"masc\" \"fem\" \"masc\" \"fem\" ...\n\n\nPara introduzir a lógica do ggplot2, será construído um gráfico de dispersão. Os mapeamentos de dados e estéticos são fornecidos na função ggplot() e aes(). Em seguida, as camadas são adicionadas com sinal +. Esse é um padrão importante e, à medida que se aprende mais sobre o ggplot2, se construirá gráficos cada vez mais sofisticados adicionando mais tipos de componentes. A primeira camada do gráfico, a camada base (Figura 6.33)), é dada pela função ggplot(). Essa função recebe um dataframe ou tibble, no exemplo dados, onde serão acrescentadas outras camadas.\nExecutando apenas a função ggplot(), aparece o seguinte painel:\n\nggplot(data = dados) \n\n\n\n\n\n\n\nFigura 6.33: Camada base do ggplot2\n\n\n\n\n\nObserva-se um painel de cor cinza, vazio, apesar de todos os dados terem sido passados para função. Para que o gráfico seja esboçado, é necessário que as observações sejam mapeadas (estética) e as formas geométricas especificadas. Como será construído um gráfico de dispersão, que mostra a correlação entre duas variáveis (idade e comprimento), o código inicial é o seguinte:\n\nggplot(data = dados, \n       mapping = aes(x = idade, y = comp)) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.34: Gráfico de dispersão\n\n\n\n\n\nA estética, com a função aes(), foi adicionada na função ggplot() (Figura 6.34) e isto significa que ela será usada em todas as outras camadas que forem acrescentadas. Pode-se colocar a a estética dentro da função do geom específico e ,dessa forma, funcionárá apenas para ele.\nO mesmo resultado da Figura 6.34, pode ser obtido da seguinte maneira:\n\nggplot(data = dados) +\n  geom_point(mapping = aes(x = idade, y = comp))\n\nEm gráficos de dispersão, é útil acrescentar uma terceira camada, representando uma suavização dos dados com função geom_smooth(). Se você não estiver interessado no intervalo de confiança, desative-o com geom_smooth(se = FALSE). Um argumento importante para geom_smooth() é o method, que permite que você escolha qual tipo de modelo é usado para ajustar a curva. Como o gráfico de dispersão indica uma correlação linear, o melhor ajuste é dado pelo method = \"lm\" que se ajusta a um modelo linear, fornecendo a reta de melhor ajuste 6.\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 6.35: Gráfico de dispersão com reta de regressão\n\n\n\n\n\nO gráfico da Figura 6.35, mostra um ajuste dos pontos a uma reta, representando a correlação perfeita entre x ~ y. A distância dos pontos à reta é o erro (resíduo). A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada (veja também ?sec-rls. O gráfico mostra uma forte correlação: à medida que o idade aumenta, aumenta o comprimento da criança\nOutros atributos estéticos\nO estabelecimento das cores, formato e tamanho dos pontos pode ser realizado adicionando o argumento color = dentro da função aes(). Por exemplo:\n\nggplot(dados, aes(x = idade, y = comp, color = sexo)) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.36: Gráfico de dispersão com separação dos pontos por uma variável categórica (sexo)\n\n\n\n\n\nNa Figura 6.36, as cores, escolhidas pelo padrão do R, estão separando os sexos (masculino e feminino). A legenda, à direita identifica, as cores dos sexos.\nO que acontece se colocarmos a cor fora da função aes()?\n\nggplot(dados, aes(x = idade, y = comp), color = sexo) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.37: Gráfico de dispersão com a cor sendo especificada fora da aes().\n\n\n\n\n\nNão acontece nada! O R ignora o código e retorna um gráfico com as sua cor padrão, a preta (Figura 6.37).\nPara mudar a cor dos pontos de acordo com a nossa escolha (Figura 6.38), é necessário informar a cor no geom (color = \"cor\").\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(color = \"tomato\")\n\n\n\n\n\n\n\nFigura 6.38: Gráfico de dispersão com a cor dos pontos por escolha pessoal\n\n\n\n\n\nPara aumentar o tamanho dos pontos usa-se o argumento size =7 no geom; o formato é modificado, da mesma maneira, com o argumento shape =8. O tamanho do ponto pode também ser modificado de acordo com uma variável, como feito com o argumento color = sexo (Figura 6.37), colocando o argumento dentro da função aes(), no ggplot().\nO código a seguir, modifica o tamanho do ponto e o seu formato de acordo com o sexo.\n\nggplot(dados, \n       aes(x = idade, y = comp, shape = sexo)) + \n  geom_point(color = \"tomato\", size = 3)\n\n\n\n\n\n\n\nFigura 6.39: Gráfico de dispersão com pontos aumentados e com formatos diferentes de acordo com o sexo.\n\n\n\n\n\nA saída (Figura 6.39) mostra um triangulo e um ponto, correspondendo, respectivamente, ao sexo masculino e feminino com tamanhos duas vezes maior do que o padrão.\nSerá criado outro exemplo (Figura 6.40) com o argumento fill = sexo para permitir a visualização dos sexos com preenchimento em cores diferentes, conforme o padrão do ggplot2. Observe que foi utilizado o argumento dentro da estética do ggplot(). O argumento color=“black” será colocado no geom_point(), para colocar uma borda preta ao redor de todos os pontos. O argumento shape = 21 corresponde a um formato de pontos vazios.\n\nggplot(dados, aes(x = idade, y = comp, fill = sexo)) +\n  geom_point(color = \"black\", size = 5, shape = 21) \n\n\n\n\n\n\n\nFigura 6.40: Gráfico de dispersão semelhante ao da Figura 5, apenas com pontos maiores.\n\n\n\n\n\nFacetamento\nOutra técnica para exibir variáveis categóricas adicionais em um gráfico é o facetamento. O facetamento cria gráficos dividindo os dados em subconjuntos e exibindo o mesmo gráfico para cada subconjunto (Figura 6.41) . Para facetar um gráfico, basta adicionar uma especificação de facetamento com a função facet_wrap(), que recebe o nome de uma variável precedido pelo sinal ~.\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(fill = \"tomato\", color = \"black\", size = 5, shape = 21) +\n  facet_wrap(~sexo)\n\n\n\n\n\n\n\nFigura 6.41: Facetamento: gráfico de dispersão por categorias.\n\n\n\n\n\nPara observar um padrão dominante, como feito acima (Figura 6.35), pode-se adicionar ao gráfico a função geom_smooth(method = \"lm\") para ajustar um modelo linear, para comparação 9:\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(fill = \"tomato\", color = \"black\", size = 5, shape = 21) +\n  facet_wrap(~sexo) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 6.42: Reta de regressão com intervalo de confiança por categorias\n\n\n\n\n\nNeste caso (Figura 6.42), observa-se que as inclinações das retas são praticamente iguais, indicando que o sexo não modifica a correlação entre variáveis.\n\n\n6.6.1.2 Boxplot\nNa Seção 6.5.5 foi mostrado o objetivo de um boxplot, agora, será construído um boxplot, usando o geom_boxplot() para comparar o comprimento das crianças de acordo com o sexo (Figura 6.43):\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFigura 6.43: FIGURA 14: Boxplot padrão do ggplot2\n\n\n\n\n\nEste é um boxplot simples, mas facilmente, pode-se modificá-lo, tornando-o mais atraente 10. Por exemplo, controlando a cor das caixas (Figura 6.44) com o argumento color (ou colour) para modificar as bordas e fill para modificar o preenchimento das caixas. A adição do argumento alpha modifica a transparência de um geom. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes:\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot(colour = \"skyblue4\", \n               fill = \"skyblue\", \n               alpha = 0.3)\n\n\n\n\n\n\n\nFigura 6.44: Boxplots com as cores das caixas alteradas.\n\n\n\n\n\nA escolha da cor pode ser realizada entre as 657 opções que o R oferta, consultando Dealing with colors in ggplot2 ou Colors in R 11.\nO boxplot oculta a distribuição subjacente dos dados. Para resolver isto, pode-se adicionar pontos ao gráfico (Figura 6.45) usando o geom_jitter(). Para evitar que os pontos fiquem muito espalhados, dificultando a visualização das categorias, usa-se o argumento width 12. Um espalhamento de 10% (width = 0.10)para poucos pontos, como no exemplo, parece bom, para observar a distribuição dos mesmos.\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot(colour = \"skyblue4\", \n               fill = \"skyblue\", \n               alpha = 0.3) +\n  geom_jitter(width = 0.10)\n\n\n\n\n\n\n\nFigura 6.45: Boxplot com jitter.\n\n\n\n\n\nContinuando a alteração da aparência do boxplot, é posível modificar o seu formato para o formato clássico com “bigodes” terminando em “T” (Figura Figura 6.46) e não um traço simples. Para isso, cria-se uma camada de barra de erro, usando a função geom_errorbar(), antes de geom_boxblot(). Assim, como o boxplot passa ser a camada mais superficial, ele impede que se visualize a barra de erro na caixa, desde que ele seja opaco (remover ou zerar o argumento alpha). A função geom_errorbar() normalmente é usada para barras de erro. No entanto, aqui ela está sendo utilizada com stat = \"boxplot\" 13, o que significa que os cálculos de estatística do boxplot serão aplicados à barra de erro. O argumento width = 0.1 ajusta a largura das barras de erro, tornando-as mais estreitas.\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(colour = \"black\", \n               fill = \"chartreuse\") \n\n\n\n\n\n\n\nFigura 6.46: Boxplot no ggplot2 com bigodes finalizando em T.\n\n\n\n\n\n\n\n6.6.1.3 Gráfico de Violino (Violin plot)\nOs gráficos de violino permitem visualizar a distribuição de uma variável numérica para um ou vários grupos. No ggplot2, são construidos com o geom_violin()e, com frequência, substituem os boxplots.\nCada “violino” representa uma variável de agrupamento. A forma representa a estimativa de densidade da variável: quanto mais pontos de dados em um intervalo específico, mais largo será o violino para esse intervalo. É muito parecido com um boxplot, mas permite uma compreensão mais profunda da distribuição.\nO gráfico de violino é uma técnica poderosa de visualização de dados, pois permite comparar a classificação de vários grupos e sua distribuição. São particularmente adequados quando a quantidade de dados é grande e é impossível mostrar observações individuais. Para conjuntos de dados pequenos, um boxplot com jitter é provavelmente uma opção melhor, pois realmente mostra todas as informações.\nAqui, será usado o conjunto de dados dadosFumo.xlsx que mostra a distribuição dos pesos dos recém-nascidos a termo de acordo com o tabagismo materno (não fumante, fumante leve. fumante moderada, fumante pesada). Ele pode ser obtido aqui, para ser salvo em seu diretório de trabalho.\n\ndados_fumo &lt;- readxl::read_excel(\"dados/dadosFumo3.xlsx\")\nstr(dados_fumo)\n\ntibble [310 × 3] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:310] 3055 3190 4350 2740 2270 ...\n $ quantFumo: chr [1:310] \"fumante_leve\" \"fumante_leve\" \"fumante_leve\" \"fumante_leve\" ...\n\n\nPara construir o gráfico de violino, serão usados os argumentos trim = FALSE, para não aparar caudas, e draw_quantiles = c(0.25, 0.5, 0.75), para traçar os quartis (Figura 6.47). No final, a função theme() será colocada para evitar que a legenda das categorias apareça, uma vez que ela é explicita no gráfico.\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN,         \n                       fill=quantFumo)) + \n  geom_violin(trim = FALSE,\n              draw_quantiles = c(0.25, 0.5, 0.75)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.47: Gráfico de violino com os quartis.\n\n\n\n\n\nUma alteração interessante que pode ser feita no gráfico de violino é colocar um boxplot (Figura 6.48), dentro do mesmo, faz o efeito do argumento draw_quantiles, usado na Figura 6.47. Facilita a interpretação e, na opinião do autor, é mais elegante. O argumento width = 0.5, na função geom_boxplot(), estabelece a largura do boxplot.\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN, fill=quantFumo)) + \n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.48: Gráfico de violino com boxplot.\n\n\n\n\n\nPara obter uma versão horizontal da Figura 6.48, chama-se a função coord_flip() 14 que permite inverter os eixos X e Y e, assim, tornar a interpretação mais intuitiva, mais amigável (Figura 6.49).\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN, fill=quantFumo)) + \n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.5) +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.49: Gráfico de violino horizontal.\n\n\n\n\n\nPara interpretar um gráfico de violino, observar o seguinte:\n\nForma do violino, observando a largura em diferentes pontos para entender onde os dados se concentram.\nA linha mediana e a caixa do boxplot associado indicam a mediana e o intervalo interquartil, respectivamente.\nSe o violino é simétrico em torno da mediana, a distribuição dos dados é aproximadamente simétrica.\nSe a parte superior do violino é mais larga, os dados podem ser assimétricos, inclinados para valores maiores.\nEm múltiplas categorias, pode-se comparar rapidamente as distribuições. Diferentes formas e larguras entre as categorias fornecem uma visão clara das variações entre elas.\n\n\n\n6.6.1.4 Histograma\nComo visto na Seção 6.5.4, o histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e da dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos.\nO geom_histogram() é a geometria para a construção de um histograma. Aqui, há necessidade apenas do eixo x, pois existe uma única variável. A execução do comando retorna a distribuição dessa variável.\nPara construir um histograma, será usada a variável pesoRN do banco de dados usado nos gráficos de violino (dadosFumo.xlsx). O histograma (Figura 6.50) pode ser obtido por qualquer um dos códigos abaixo:\n\nggplot(dados_fumo) + \n  geom_histogram(aes(x = pesoRN))\n\nOu\n\nggplot(dados_fumo, aes(x = pesoRN)) + \n  geom_histogram()\n\n\n\n\n\n\n\nFigura 6.50: Histograma padrão.\n\n\n\n\n\nA aparência deste histograma não está boa. A sua mensagem depende dessa aparência. O histograma recebe uma variável numérica e a divide em vários “compartimentos”, os intervalos, representados pelas barras. A escolha do tamanho (amplitude) do intervalo é de extrema importância para a aparência do histograma.\nO geom_histogram() tem um argumento, denominado binwidth que permite alterar a amplitude do intervalo. O binwidth é um intervalo e sua unidade é igual a da variável que se está histogramando. No exemplo, foi usado o peso do recém-nascido (g). Se quisermos um intervalo de 100 em 100 gramas, o binwidth = 100. Uma outra maneira, é usar bins que agrupa em intervalos de mesmo tamanho. Se estabelecermos bins = 15, o geom_histogram() dividirá em 15 intervalos iguais.\nJunto com a alteração dos intervalos, vamos modificar a cor de preenchimento (fill) e bordas (colour) das barras (Figura 6.51).\n\nggplot(dados_fumo, aes(x = pesoRN)) + \n  geom_histogram(binwidth = 100,\n                 fill = \"chartreuse\",\n                 colour = \"darkgreen\")\n\n\n\n\n\n\n\nFigura 6.51: Histograma modificado com o argumento binwidth = 100 e com preenchimento e bordas das barras customizadas.\n\n\n\n\n\nCom frequência se observa um histograma com curva normal sobreposta (Figura 6.52) para facilitar a comparação dos dados com a distribuição normal. Isso pode ser conseguido com um código que usa função stat_function() para a construção da curva normal, baseada nos dados (média e desvio padrão da variável pesoRN) e a função after_stat(density), colocada na estética do histograma no eixo Y, para substituir a frequência pela densidade de probabilidade. O restante do código somente estabelece que a linha da curva será tracejada (linetype = “dashed”), de cor vermelha (color = “red”) e com tamanho 1 (linewidth = 1).\n\nggplot(dados_fumo) + \n  geom_histogram(aes(x = pesoRN, \n                     y = after_stat(density)),\n                 binwidth = 100,\n                 fill = \"chartreuse\",\n                 colour = \"darkgreen\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dados_fumo$pesoRN),\n                            sd = sd(dados_fumo$pesoRN)),\n                linetype = \"dashed\",\n                linewidth = 1,\n                color = \"red\")\n\n\n\n\n\n\n\nFigura 6.52: Histograma com curva normal sobreposta.\n\n\n\n\n\nO gráfico da Figura 6.52, mostra que os pesos dos recém-nascidos se ajustam bem à curva normal.\n\n\n6.6.1.5 Gráfico de barras\nO gráfico de barras é uma análogo do histograma, onde as barras , ao contrário deste, são separadas. Os gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas.\nA função geom_bar() permite delinear um gráfico de barras (Figura 6.53). O exemplo será construído com a variável quantFumo do arquivo dadosFumo.xlsx.\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count))))\n\n\n\n\n\n\n\nFigura 6.53: Gráfico de barras padrão do ggplot2.\n\n\n\n\n\nAs cores de preenchimento das barras podem ser alteradas, de acordo com a variável categórica (Figura 6.54). As cores serão estabelecidas de acordo com o padrão do ggplot2:\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo))\n\n\n\n\n\n\n\nFigura 6.54: Gráfico de barras com as cores das barras estabelecidas pelo ggplot2.\n\n\n\n\n\nO gráfico retorna uma legenda, mostrando o que representa cada cor. Ela é desnecessária porque já está explicito, no eixo X, o que cada barra representa. Portanto, vamos remover a legenda (Figura 6.55)com a função theme(legend.position = \"none\"):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.55: Gráfico de barras anterior sem legenda.\n\n\n\n\n\nEssas cores do padrão do ggplot2 podem ser modificadas com a função scale_fill_manual()que permite a customização das cores (Figura 6.56):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.56: Gráfico de barras com cores customizadas.\n\n\n\n\n\n\n\n\n6.6.2 Modificação dos eixos do gráfico\n\n6.6.2.1 Rótulos nos eixos\nObserve que no gráfico da Figura 6.56, o eixo y contém um rótulo que é a fórmula para o cálculo da proporção e o eixo x, o nome da variável quantFumo que também é pouco esclarecedor. Os rótulos dos eixos são muito importantes em um gráfico, pois a sua leitura deve explicar o que o gráfico está apresentando. Nessa figura, o gráfico está mostrando a proporção da intensidade de tabagismo em um grupo de gestantes. Isto deve estar claro no gráfico. Para modificar o rótulo dos eixos, utiliza-se, com frequência, as funções xlab() e ylab() (Figura 6.57).\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.57: Gráfico de barras com os rótulos dos eixos alterados.\n\n\n\n\n\nO mesmo trabalho de alteração dos rótulos pode ser feito com a função labs() que faz , retornando um gráfico igual ao da Figura 6.57:\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  labs (y = \"Proporção por categoria\",\n        x = \"Tabagismo Materno\") +\n  theme(legend.position = \"none\")\n\n\n\n6.6.2.2 Mudando o nome e a ordem dos rótulos no eixo x\nA Figura 6.57 ficou melhor, mas ainda tem problemas. O rótulo de cada barra está como cada nível está escrito no banco de dados, por exemplo, a palavra não está sem acentuação. Isto pode ser corrigido, usando a função scale_x_discrete() com os argumentos limits = (coloca os níveis na ordem desejada) e labels = (coloca os novos nomes15 na ordem estabelecida pelo argumento limits =):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.58: Gráfico de barras com nome e ordem dos rótulos alterados.\n\n\n\n\n\nObserve que a barra azul das não fumantes foi trocada de posição, passando a ser a primeira, para colocar a intensidade do tabagismo em ordem crescente (Figura 6.58).\n\n\n6.6.2.3 Título e subtítulo do gráfico\nNem sempre necessários, o título, o subtítulo ou uma nota de rodapé podem ser adicionados ao gráfico através da função labs(), usada anteriormente para colocar rótulos nos eixos X e Y, mas que também tem argumentos para colocar título, subtítulo e nota de rodapé (caption).\nAgora, será desenhado um gráfico com boxplots que ilustrem a influência do tabagismo materno sobre o peso do recém-nascido. Será usado o mesmo banco de dados dadosFumo.xlsx. Além de usar todos os argumento da função labs(), se repetirá o que foi feito na construção do gráfico da Figura 6.58, alterando os nomes e a posição das não fumantes. O gráfico (Figura 6.59) vai ser atribuído a um objeto denominado bxp:\n\nbxp &lt;- ggplot(dados_fumo, aes(x = quantFumo, \n                              y = pesoRN,\n                              fill = quantFumo)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.1) +\n  geom_boxplot() + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  stat_summary(fun = \"mean\", \n               colour = \"red\", \n               size = 3, \n               geom = \"point\") +\n  labs(title = \"Tabagismo Materno e Peso do Recém Nascido\", \n       subtitle = \"Maternidade do Hospital Geral de Caxias do Sul, 2010\",\n       x = \"Tabagismo Materno\",\n       y = \"Peso dos Recém-Nascidos (g)\",\n       caption = \"O ponto vermelho é a média de cada grupo\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                    labels = c(\"Não fumante\", \"Leve\", \n                               \"Moderado\", \"Pesado\")) +\n  theme(legend.position = \"none\")\n\nprint(bxp)\n\n\n\n\n\n\n\nFigura 6.59: Boxplots mostrando o impacto do tabagismo materno no peso do recém-nascido\n\n\n\n\n\nO gráfico da Figura 6.59 está com bom aspecto, mas alguém poderia dizer que gostaria de alterar o tamanho da fonte do título e do subtítulo, mudando inclusive a cor. É exagero? Poder ser, mas é possível! Para isso, existe, no ggplot2, uma função denominada theme() com múltiplos argumentos que exercem uma grande quantidade ações. Sugere-se consultar a ajuda (?theme())para maiores informações. Para realizar essas alterações, vamos começar com o objeto bxp 16 e adicionar os códigos com a função theme() para alterar a cor do título e subtítulo (Figura 6.60):\n\nbxp +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\"))\n\n\n\n\n\n\n\nFigura 6.60: Gráfico anterior com pequenas alterações na fonte e cores do título e subtítulo\n\n\n\n\n\n\n\n6.6.2.4 Modificação dos limites dos eixos\nO pacote ggplot2 possui uma família de funções scale_ para modificar as propriedades referentes às escalas do gráfico. Como é possível ter escalas de números, categorias, cores, datas, entre outras, é disponibilizada uma função específica para cada tipo de escala. Cada tipo fundamental é manipulado por uma das três funções construtoras de escala: continuous_scale(), discrete_scale() e binned_scale().\nNo gráfico da Figura 6.60, os pesos dos RNs estão dispostos em uma escala que varia a cada 1000 g. Para modificar esses limites, pode-se usar a função scale_y_continuous() para ter intervalos de 500 g:\n\nbxp +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\")) +\n  scale_y_continuous(breaks = seq(1500, 5000, 500)) \n\n\n\n\n\n\n\nFigura 6.61: Gráfico anterior com modificação da escala do eixo Y\n\n\n\n\n\nAgora, na Figura 6.61, o eixo Y contém intervalos de 500 g. Este mesmo grafico já é um exemplo de modificação do eixo X com a função scale_x_discrete() para mudar a ordem dos níveis originais, realizado na construção do gráfico da Figura 6.59.\n\n\n6.6.2.5 Modificação da expansão\nVoltando ao gráfico da Figura 6.58, observe que abaixo do valor 0 (zero) existe uma expansão. Isto, visualmente, é desagradável (pelo menos para o autor). Para que as barras tenham início exatamente no 0 (zero), pode-se empregar a função scale_y_continuous() com o argumento expand = expansion (add = c(0,50)), significando que não se expande nada abaixo do 0 e se adiciona 50 unidades para cima, criando uma margem superior (Figura 6.62).\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.62: Gráfico de barra sem a expansão abaixo de 0 (zero)\n\n\n\n\n\n\n\n6.6.2.6 Proporção ou percentagem nos eixos\nNa Figura 6.62, a unidade do eixo Y encontra-se como um proporção y = after_stat(count/sum(count). É possível modificar para porcentagem (Figura 6.63), empregando a função percent_format() do pacote scales. O código é praticamente igual, apenas acrescentar o argumento labels = percent_format(accuracy = 0.1, decimal.mark = “,”) dentro da função scale_y_continuous().\n\nlibrary(scales)\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05)),\n                      labels = percent_format (accuracy = 0.1,\n                                               decimal.mark = \",\")) +\n  scale_x_discrete(limits = c(\"fumante_leve\", \"fumante_moderada\", \n                              \"fumante_pesada\", \"nao_fumante\"),\n                   labels = c(\"Leve\", \"Moderado\",\n                              \"Pesado\", \"Não fumante\")) +\n  labs(x = \"Tabagismo Materno na Gestação\",\n       y = \"Proporção em cada categoria\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.63: Gráfico de barra com eixo Y em porcentagem\n\n\n\n\n\n\n\n\n6.6.3 Modificação das cores\nNa Seção 6.6.1.2, foi introduzido o uso de cores no R. Agora, apesar deste tema praticamente não ter limites, serão mostrados alguns princípios do manuseio das cores no ggplot2.\nNo gráfico de barras da Figura 6.63, as cores foram selecionadas com a função scale_fill_manual(). A escolha das cores pode ser feita especificando o seu nome em inglês. Essa escolha é pessoal. O R possui 657 cores integradas que permitem uma gama ampla de opções. Uma outra maneira de especificar as cores, é usar o sistema RGB ou hexadecimal. O código hexadecimal da cor branca é #FFFFFFF, da “gray58” é #949494, da “yellow4” é #999900, etc. Opcionalmente, a cor pode ser transparente, usando o formato “#RRGGBBAA”. Alpha refere-se à transparência de um geom. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes. Alpha também pode ser modificada por meio da estética de colour ou fill se qualquer uma das estéticas fornecer valores de cor usando uma especificação RGB.\nUsando este mesmo gráfico, iremos mudar a cor, usando a cor cinza-claro para o preenchimento das barras (fill = “gray70”) e vermelho-escuro para bordas das barras (color = “darkred”), colocados fora da aes(), pois se forem colocadas dentro, as cores ficarão diferentes dentro do padrão do ggplot2 (Figura 6.64).\n\nlibrary(scales)\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count))),\n               fill = \"gray70\",\n               color = \"darkred\") + \n  labs(x = \"Tabagismo Materno na Gestação\",\n       y = \"Percentagem em cada categoria\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05)),\n                      labels = percent_format(accuracy = 0.1,  \n                                              decimal.mark = \",\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.64: Gráfico de barras com cores modificadas fora da estética do geom.\n\n\n\n\n\n\n6.6.3.1 Cores de acordo com uma determinada paleta\nO ggsci é um pacote que oferece uma coleção de paletas de alta qualidade inspiradas em cores usadas em revistas científicas, bibliotecas de visualização de dados, filmes de ficção científica e programas de TV. As paletas de cores no ggsci estão disponíveis como escalas ggplot2. Para todas usa-se as seguintes funções: scale_color_palname() e scale_fill_palname(). Por exemplo, para a paleta do Lancet, usa-se para o preenchimento: scale_fill_lancet() . O pacote ggsci deve ser instalado e carregado para usar estas paletas.\nComo base, será usado o código que gerou o gráfico da Figura 6.61 com alterações, usando a paleta do periódico Lancet (Figura 6.65).\n\nlibrary(ggsci)\n\nbxp1 &lt;- ggplot(dados_fumo, \n               aes(x = quantFumo, y = pesoRN, fill = quantFumo)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.1) +\ngeom_boxplot() + \n  scale_fill_lancet() +\n  stat_summary(fun = \"mean\", colour = \"white\", size = 2, geom = \"point\") +\n  labs(title = \"Tabagismo Materno e Peso do Recém Nascido\", \n       subtitle = \"Maternidade do Hospital Geral de Caxias do Sul, 2010\",\n       x = \"Tabagismo Materno\",\n       y = \"Peso dos Recém-Nascidos (g)\",\n       caption = \"O ponto branco é a média de cada grupo\") +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\")) +\n  scale_x_discrete(limits = c(\"nao_fumante\", \"fumante_leve\", \n                              \"fumante_moderada\", \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous(breaks = seq(1500, 5000, 500)) +\n  theme(legend.position = \"none\")\nprint(bxp1)\n\n\n\n\n\n\n\nFigura 6.65: Gráfico com paleta de cores do Lancet.\n\n\n\n\n\n\n\n\n6.6.4 Mudança dos temas\nO tema padrão do ggplot2 tem uma aparência acinzentada que pode ser modificada pela definição de outro tema integrado, como o theme_bw(), que é uma variação de theme_grey(), que usa um fundo branco e linhas finas de grade cinza . Outro tema interessante é o theme_classic() que é um tema de aparência clássica, com linhas dos eixos X e Y e sem linhas de grade . Para ver outras possibilidades acesse Completes themes - ggplt2.\nSerá repetido o boxplot da Figura 6.65, adicionando o theme_bw() (Figura 6.66) e, após, o theme_classic()(Figura 6.67).\n\nbxp1 +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.66: Gráfico da Figura 36 com o theme_bw().\n\n\n\n\n\n\nbxp1 +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.67: Gráfico da Figura 36 com o theme_classic().\n\n\n\n\n\n\n\n6.6.5 Estudo de caso: Gráfico de barra de erro\nUm gráfico de barra de erro (Figura 6.68) é uma ferramenta visual que mostra a variabilidade de dados em um ponto específico (veja também Seção 6.5.3). Ele consiste em pontos ou barras que representam as médias (ou outras estatísticas) de um conjunto de dados, com linhas verticais (ou horizontais) que indicam o intervalo de confiança, o desvio padrão ou o erro padrão da média. Essas linhas verticais são conhecidas como “barras de erro”. Usado para comparar as médias de diferentes grupos, mostrando a variabilidade dentro de cada grupo. É visto com frequência em pesquisas científicas e publicações para apresentar os resultados experimentais com suas respectivas variabilidades.\n\n6.6.5.1 Caso\nPara visualizar a influência do sexo e do tabagismo materno no peso do recém-nascido, será usado um gráfico de barras de erro, onde a representação das colunas (barras) e as barras de erro com intervalo de confiança de 95%, calculado usando média ± margem de erro, onde a margem de erro = 1.96 × erro padrão.\n\n\n6.6.5.2 Dados\n\nset.seed(12)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;%   filter(ig &gt; 37 & ig &lt;= 42) %&gt;% \n  select(id, fumo, sexo, pesoRN) %&gt;% \n  slice_sample(n = 100)\n\nExploração e transformação dos dados\n\ndados$sexo &lt;- factor(dados$sexo, \n                     levels = c(1, 2),\n                     labels = c(\"Masculino\", \"Feminino\"))\n\ndados$fumo &lt;- factor(dados$fumo,\n                     levels = c(1, 2),\n                     labels = c(\"fumante\", \"nao_fumante\"))\n\nresumo &lt;- dados %&gt;% \n  group_by(sexo, fumo) %&gt;% \n  summarise(n = n(),\n            media = mean(pesoRN, na.rm = TRUE),\n            dp = sd(pesoRN, na.rm = TRUE),\n            me = 1.96 * dp/sqrt(n),\n            .groups = 'drop')\nprint(resumo)\n\n# A tibble: 4 × 6\n  sexo      fumo            n media    dp    me\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino fumante        11 3258.  544.  322.\n2 Masculino nao_fumante    47 3217.  409.  117.\n3 Feminino  fumante         6 2920   384.  307.\n4 Feminino  nao_fumante    36 3200.  413.  135.\n\n\n\n\n6.6.5.3 Gráfico de barra de erro\n\nggplot(resumo, \n       aes(x=sexo, y=media, fill=fumo)) + \n  geom_bar(stat=\"identity\", color=\"black\", \n           position=position_dodge(0.9)) +\n  geom_point(position=position_dodge(0.9)) +\n  geom_errorbar(aes(ymin=media, ymax=media+me), width=0.2,\n                position=position_dodge(.9)) +\n  labs(x=\"Sexo\", \n       y = \"Peso do Neonato(g)\",\n       fill = \"Tabagismo\") +\n  coord_cartesian(ylim = c(0, 3600),\n                  expand = TRUE) +\n  scale_fill_manual(values = c(\"nao_fumante\" = \"darkslategray1\", \n                               \"fumante\" = \"gray80\"),\n                    labels = c(\"nao_fumante\" = \"Não fumante\", \n                               \"fumante\" = \"Fumante\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  theme_classic()\n\n\n\n\n\n\n\nFigura 6.68: Gráfico de barra de erro.\n\n\n\n\n\n\n\n\n\n1. Field A, Miles J, Field Z. Everithing you ever wanted to know about statistics (well, sort of). Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 38. \n\n\n2. Bowers D. First things first-the nature of data. Em: Medical Statistics from Scratch. Second Edition. John Wiley; Sons; 2008. p. 3–13. \n\n\n3. Arango HG. Organização dos dados em tabelas. Em: Bioestatística: teórica e computacional. 3ª edição. Guanabara Koogan; 2009. p. 32–57. \n\n\n4. Oliveira Filho PF de. Tabelas. Em: Epidemiologia e Bioestatística-Fundamentos para a Leitura Crítica. 2ª edição. Editora Rubio; 2022. p. 9–12. \n\n\n5. Xie Y. knitr: a comprehensive tool for reproducible research in R. Em: Implementing reproducible research. Chapman; Hall/CRC; 2018. p. 3–31. \n\n\n6. Zhu H et al. Construct complex table with «kable» and Pipe syntax [Internet]. 2021. Disponível em: https://CRAN.R-project.org/package=kableExtra\n\n\n7. Arango HG. Números de classes e Intervalo de Classes. Em: Bioestatística teórica e computacional. Terceira edição. Guanabara Koogan, RJ; 2009. p. 35–40. \n\n\n8. Rasmussen KM, Yaktine AL, et al. Weight gain during pregnancy: reexamining the guidelines. 2009; \n\n\n9. Field A, Miles J, Field Z. Exploring data with graphs. Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 117. \n\n\n10. Wickham H. Getting Started with ggplot2. Em: ggplot2. Second edition. Springer; 2016. p. 11–31. \n\n\n11. Tufte ER. Aesthetics and Technique in Data Graphical Design. Em: The Visual Display of Quantitative Information. Second edition. Graphics Press; 2001. p. 178. \n\n\n12. Lemon J, Bolker B, Oom S, et al. Package «plotrix». Vienna: R Development Core Team. 2015; \n\n\n13. Kabacoff RI. Basic graphs. Em: R in Action: Data analysis and graphics with R. Manning Publications Co.; 2011. p. 120–4. \n\n\n14. Harrell FE, Dupont C. Hmisc: Harrell Miscellaneous [Internet]. R package version. 2022. Disponível em: https://cran.r-project.org/web/packages/Hmisc/index.html\n\n\n15. Wickham H. ggplot2: Elegant Graphics for Data Analysis [Internet]. Springer-Verlag New York; 2016. Disponível em: https://ggplot2.tidyverse.org\n\n\n16. Wickham H. A layered grammar of graphics. Journal of Computational and Graphical Statistics. 2010;19(1):3–28.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#footnotes",
    "href": "06-descrevendoDados.html#footnotes",
    "title": "6  Descrevendo os dados",
    "section": "",
    "text": "Usa-se esta sintaxe: round(x, digits = 0), onde x é o numero que se quer arredondar e digits é número de casas decimais. O padrão é 0, ou seja, arredonda para o inteiro mais próximo.↩︎\nPara maiores detalhes: https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf ou consulte a ajuda digitando no Console ?kable() ou help(kable). Um pacote adicional, kableExtra (6), permite opções de formatação simples, melhorando o aspecto da tabela, utilizando-se o operador pipe %&gt;%. O pacote kableExtra foi projetado para estender a funcionalidade básica das tabelas produzidas usando knitr::kable(). Podem ser acrescentadas várias das suas funções como kable_styling() ou kable_classic() para especificar estilos à tabela, como extensão da tabela, alinhamento, tipo e tamanho da fonte↩︎\nPoderiam ser transformados em fatores sem trocar os rótulos e manter os números 1 e 2, como se fossem palavras. O autor prefere usar nomes.↩︎\nFoi escolhido o nome x, porque lembra o eixo X, onde estão as barras↩︎\npara lembrar o eixo Y↩︎\nOutros métodos: “loess”, “gam”, “rlm”. Para mais informações, consulte a ajuda ?loess, ?gam ou ?rlm↩︎\npadrão = 1.5↩︎\nhttp://www.sthda.com/english/wiki/ggplot2-point-shapes↩︎\nObserve que não foi usado o argumento se = FALSE. Por isso, aparece o intervalo de confiança de 95%.↩︎\nÉ claro, que este aspecto tem uma conotação pessoal. Em publicações, consulte as normas do periódico↩︎\nVeja mais detalhes na Seção 6.6.3↩︎\nO padrão é width = 0.40. Usando width = 0, teremos uma distribuição dos pontos em uma mesma posição, como o gráfico inicial do Boxplot↩︎\nO padrão é stat = \"identity\", o que significa que os valores das barras de erro devem ser fornecidos diretamente no conjunto de dados, sem cálculos adicionais↩︎\nEsta função pode ser utilizada para outros gráficos, consulte a ajuda↩︎\nPodemos aproveitar aqui para trocar os nomes ou , simplesmente, corrigir acentuação que, às vezes, não foi colocada nos níveis↩︎\nEste procedimento de atribuir a um objeto os comandos que constroem um gráfico, facilita a digitação, pois não é necessário repetir os códigos da Figura @ref(fig:ggbxp4)↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html",
    "href": "07-probabilidades.html",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "7.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr, readxl)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#introdução",
    "href": "07-probabilidades.html#introdução",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.2 Introdução",
    "text": "7.2 Introdução\nA teoria das probabilidades é a base sobre a qual a estatística é desenvolvida. Os jogos de azar deram um grande impulso ao conhecimento da moderna teoria das probabilidades, principalmente, pelo trabalho de Blaise Pascal (1623-1662), em parceria com Pierre de Fermat (1601-1665). Eles foram estimulados por um escritor francês e matemático amador, Antoine Gombaud (1607-1684), conhecido como Chevalier de Méré, que era muito interessado em jogos de azar (1).\nA Teoria das probabilidades permite que seja possível modelar populações, experimentos ou qualquer situação que possa ser considerada aleatória. Estes modelos possibilitam fazer inferência sobre populações a partir da observação de uma amostra dessa população. Ao usar apenas uma parte da população, inevitavelmente, é cometido um erro, o erro amostral. Este erro amostral pode ser dimensionado pela teoria das probabilidades.\nExistem duas interpretações alternativas de probabilidades: a frequentista e a bayesiana (2). Neste livro, será discutida, basicamente, a definição de probabilidade frequentista. O processo bayesiano de formulação de um modelo probabilístico faz uso do conhecimento subjetivo, estabelecendo uma especificação a priori, combinado com a informação objetiva ou empírica. A teoria bayesiana é a estrutura integradora dessas duas fontes de informação, derivando como resultado a distribuição a posteriori dos parâmetros de interesse. Na ?sec-diagbayes, sobre análise de testes diagnósticos, serão abordados alguns aspectos relacionados à teoria bayesiana em medicina.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#processo-aleatório",
    "href": "07-probabilidades.html#processo-aleatório",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.3 Processo aleatório",
    "text": "7.3 Processo aleatório\nUm processo ou experimento é dito aleatório quando em uma situação se sabe quais os resultados que podem acontecer, mas não se sabe qual resultado particular irá acontecer. Por exemplo, quando uma moeda é lançada, se conhece que a probabilidade de o desfecho cara ocorrer é de 50%, mas se desconhece o que irá ocorrer até que a moeda esteja no chão.\nO número de caras que podem surgir em vários lançamentos da moeda é chamado de variável aleatória, ou seja, uma variável que pode assumir mais de um valor com determinadas probabilidades (3). Da mesma forma, um dado lançado pode mostrar seis faces, numeradas de um a seis, com igual probabilidade de 16,7%. Portanto, quando a probabilidade é associada a todos os conjuntos de valores possíveis de uma variável, diz-se que ela é aleatória. O conjunto de todos os possíveis resultados de um experimento aleatório é denominado espaço amostral.\nNa área da saúde, trabalha-se com uma infinidade de variáveis aleatórias, por exemplo, o número de filhos de uma mulher, o número de mortos diários em uma epidemia, o número de vacinados em uma campanha, etc. Essas variáveis são variáveis aleatórias discretas, pois apenas permitem ser quantificadas por processo de contagem. Por outro lado, o peso ou a altura de uma mulher são ditos variáveis aleatórias contínuas, pois podem assumir qualquer valor real entre uma medida e outra, dependendo da precisão do aparelho usado.\nEm geral, variáveis aleatórias são representadas por letras maiúsculas, como X, Y e Z e sua a probabilidade, por exemplo, pode ser denotada por: \\(P(X)\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#definição-frequentista",
    "href": "07-probabilidades.html#definição-frequentista",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.4 Definição frequentista",
    "text": "7.4 Definição frequentista\nA probabilidade se relaciona a eventos futuros ou que ainda não ocorreram, desta forma a probabilidade pode ser entendida como uma medida de incerteza em relação ao evento. A probabilidade de um evento ocorrer, em determinadas circunstâncias, pode ser definida como a proporção de vezes que o evento é observado quando o experimento é repetido um número infinitamente grande de vezes (2). Pode-se dizer que a visão frequentista define a probabilidade como uma frequência de longo prazo.\nA chamada Lei dos Grandes Números diz que à medida que múltiplas observações são coletadas, a proporção observada de ocorrências de um determinado desfecho, após n ensaios, converge para a probabilidade real P desse desfecho. Ou seja, quanto mais vezes for repetido uma experiência, a melhor estimativa de probabilidade tende a ocorrer. Suponha que seja lançada uma moeda honesta repetidas vezes. Por definição, essa é uma moeda que tem \\(P(cara)=0,5\\). O que se observaria? O autor fez 20 lançamentos seguidos com uma mesma moeda e obteve o seguinte resultado, onde 1 = cara (Figura 7.1):\n\n\n\n\n\n\n\n\nFigura 7.1: 20 lançamentos seguidos de uma moeda\n\n\n\n\n\nNeste caso, 10 (50%) desses lançamentos deram cara. Agora, suponha que foram feitos registros do número de caras (\\(n_1\\)) dos primeiros lançamentos (N) e calculadas as proporções de caras (\\(n_1⁄N\\)) todas as vezes. O resultado está na ?fig-propmoeda.\n\n\n\n\n\nProporção em 20 lançamentos de moeda\n\n\n\n\nObserva-se, nessa sequência, que a proporção de caras flutua muito, variando de 0,17 a 0,75. Se o número de lançamentos for aumentando tem-se a sensação de que a proporção se aproxima da “correta”. Por exemplo, com 100 jogadas, obteve-se 53 caras (0,53); com 150 jogadas, 79 (0,53) e com 200 jogadas, 111 (0,56). Quando N se aproximar do infinito (\\(N \\to\\infty\\)) a proporção de caras convergirá para 0,50. A definição frequentista de probabilidade segue essa definição. Ninguém consegue um número infinito de lançamentos de moedas, mas um computador pode simular milhares de lançamentos. A Figura 7.2 mostra o que acontece com a proporção \\(n_1⁄N\\) à medida que N aumenta em lançamentos de moedas. As simulações foram repetidas 4 vezes somente para ter certeza de que o que aconteceu não foi obra do acaso.\n\n\n\n\n\n\n\n\nFigura 7.2: Proporção à medida que N aumenta em lançamentos de moedas\n\n\n\n\n\nEmbora nenhuma das simulações tenha realmente terminado com um valor exato de 0,5, elas se aproximaram, oscilando muito pouco em torno desse valor.\n\n7.4.1 Aplicando a visão frequentista no dia a dia\nA definição frequentista também pode ser aplicada no cotidiano. Utilizando a altura de 1368 mulheres, uma medida numérica contínua, incluída no conjunto de dados dadosMater.xlsx (veja Seção 5.3). Essas alturas serão selecionadas e colocadas em um objeto, denominado dados.\n\n dados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nUsando a função summary(), será feito um resumo da variável altura:\n\nsummary(dados$altura)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.400   1.550   1.600   1.598   1.650   1.850 \n\n\nA mediana da altura das gestantes é 1.6 m. Em um longo conjunto de sorteios, a probabilidade de uma mulher ter altura acima devalor é 50%. O percentil 75 (3º quartil) é igual a 1.65 m, a probabilidade de estar acima deste valor, portanto, é 25%. É possível encontrar a probabilidade de a altura estar acima, abaixo ou entre quaisquer valores. Quando se faz a mensuração de uma variável contínua, fica-se limitado ao método usado. Portanto, quando se diz que uma mulher tem 160 cm, significa dizer que está entre 159,5 e 160,5 cm, dependendo da precisão do instrumento de medição. Dessa maneira, o interesse está na probabilidade de a variável aleatória assumir valores entre certos limites.\nA probabilidade de encontrar um valor exatamente igual à média (159.8) cm é quase igual a zero. Como se verá a seguir, isto pode ser verificado, no R, com bastante facilidade,calculando a distância que esta medida está da média em número de desvios padrão (escore Z):\n\nZ &lt;- (1.60 - mean(dados$altura))/sd(dados$altura)\nZ\n\n[1] 0.03103551\n\n\nObserve que o valor de 1,60 m está muito próximo da média e isto é um indicativo de que essa variável tem uma distribuição praticamente simétrica. Sabendo a distância, em números de desvios padrão, que 1,60 m está da média, qual a probabilidade de encontrar, na maternidade do HGCS 1, uma parturiente que tenha exatamente esta altura?\nPara responder a essa pergunta, será usada a função pnorm() (veja adiante na Seção 7.7.2) que utiliza o escore Z, a média e o desvio padrão para encontrar essa proporção que, multiplicada por 100, fornece a percentagem.\n\n p &lt;- pnorm (Z, mean(dados$altura),sd(dados $altura))\n p\n\n[1] 7.387473e-127\n\n\nO R por padrão retorna números grandes como notação científica. O resultado dessa operação é um número tão grande que para escrevê-lo sem este tipo de notação, seriam necessários 127 dígitos decimais. O resultado não caberia em apenas uma linha. Ficaria assim, suprimindo a notação científica 2:\n\n options(scipen =999)\n p\n\n[1] 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000007387473\n\n options(scipen = 0)\n\nOu seja, um número tão próximo de zero que poderia muito bem ser zero!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#propriedades-das-probabilidades",
    "href": "07-probabilidades.html#propriedades-das-probabilidades",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.5 Propriedades das probabilidades",
    "text": "7.5 Propriedades das probabilidades\nAs seguintes propriedades simples decorrem da definição de probabilidade.\nSendo E um evento aleatório, a \\(P[E]\\) está entre 0 e 1, ou seja \\(0\\le P[E]\\le 1\\). Quando o evento certamente não ocorre, a probabilidade é 0, quando sempre ocorre a probabilidade é 1. Quando a probabilidade for igual a 0,50 tem-se máxima incerteza.\n\nRegra de adição (regra do “ou”)\n\nDois eventos A e B são mutuamente exclusivos, ou seja, quando A acontece, B não pode acontecer. Então, a probabilidade de que um ou outro aconteça é a soma de suas probabilidades. Por exemplo, um dado lançado pode mostrar um ou dois, mas não ambos. A probabilidade de mostrar um ou dois é igual a \\(1/6 + 1/6 = 1/3\\).\n\\[\nP[A ou B]=P[A]+P[B]\n\\]\nSe A e B não são mutuamente exclusivos, ou seja, quando A acontece pode também ocorrer B. Por exemplo, o nascimento de uma menina pode ser concomitante com o fato de ser branca.\n\\[\nP[A ou B]=P[A]+P[B]-P[A \\space e \\space B]\n\\]\n\nRegra de multiplicação (regra do “e”)\n\nSuponha que dois eventos (A e B) sejam independentes, ou seja, saber que um aconteceu não nos diz nada sobre se o outro aconteceu. Então, a probabilidade de que ambos aconteçam é o produto de suas probabilidades. Por exemplo, suponha que jogamos duas moedas. Uma moeda não influencia a outra, portanto os resultados dos dois lançamentos são independentes e a probabilidade de ocorrerem duas caras é 050 × 0,50 = 0,25.\n\\[\nP[A \\quad e\\quad B]=P[A]×P[B]\n\\]\nSe os eventos são dependentes, a probabilidade que ambos aconteçam é igual a:\n\\[\nP[A \\quad e \\quad B]=P[A]×P[B \\rvert A]\n\\] Com essas propriedades simples e outras mais complexas, é possível construir algumas ferramentas matemáticas extremamente poderosas, mas isso não faz parte do objetivo deste livro e não se entrará em detalhes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-de-probabilidades",
    "href": "07-probabilidades.html#distribuição-de-probabilidades",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.6 Distribuição de Probabilidades",
    "text": "7.6 Distribuição de Probabilidades\nUm conjunto de eventos que são mutuamente excludentes e que inclui todos os eventos que podem acontecer, é chamado de exaustivo. A soma de suas probabilidades é 1. O conjunto dessas probabilidades constitui uma distribuição de probabilidade.\nExistem diversos modelos probabilísticos que procuram descrever vários tipos de variáveis aleatórias discretas ou contínuas. Estas distribuições também são chamadas de modelos probabilísticos estocásticos que são definidas por duas funções matemáticas: a função de probabilidade (fp) para variáveis discretas, que atribui a cada valor a sua probabilidade de ocorrência (P(X=x)) e função densidade de probabilidade (fdp) para variáveis contínuas.\nA função de probabilidade é a função que atribui probabilidades a cada um dos possíveis valores da variável aleatória discreta, usando, em geral, as frequências relativas, apresentadas em uma tabela de frequência. O modelo de Bernoulli ou Binomial e o modelo de Poisson são exemplos de modelo probabilístico de variáveis discretas.\nA função densidade de probabilidade é a função que atribui probabilidade a qualquer intervalo de número reais, ou seja, um conjunto de valores não enumerável (infinito). Não é possível atribuir probabilidades para um determinado valor, é possível apenas para um intervalo. Por exemplo, o peso dos recém-nascidos. Para atribuir probabilidade a intervalos de valores é utilizada uma função e as probabilidades são representadas por áreas. Existem diversos modelos contínuos de probabilidade, mas o mais importante deles, é o modelo normal, também conhecido como modelo gaussiano.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#sec-normal",
    "href": "07-probabilidades.html#sec-normal",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.7 Distribuição Normal",
    "text": "7.7 Distribuição Normal\nO modelo probabilístico normal ou gaussiano é extremamente importante em estatística, pois serve como um fundamento para técnicas de inferência. Variáveis como os pesos dos recém-nascidos a termo, as alturas das mulheres adultas, a renda familiar em reais e muitas outras variáveis, na natureza, se ajustam ao modelo da distribuição normal.\nO modelo de distribuição normal sempre descreve uma curva simétrica, unimodal e em forma de sino (Figura 7.3).\n\n\n\n\n\n\n\n\nFigura 7.3: Curva normal.\n\n\n\n\n\nUma distribuição normal é descrita por meio de dois parâmetros: a média da distribuição \\(\\mu\\) e o desvio padrão da distribuição \\(\\sigma\\). Em função dessa informação, observe como a distribuição normal funciona se esses parâmetros forem alterados.\nComo é fácil prever, alterar a média desloca a curva de sino para a esquerda ou para a direita, enquanto a alteração do desvio padrão estende ou achata a curva, ou seja, muda a dispersão da distribuição.\nA Figura 7.4, mostra a distribuição normal com média 0 e desvio padrão 1, na curva à direita, a distribuição normal com média 1.5 e desvio padrão 1. Sobrepondo-se à curva da esquerda observa-se uma curva mais achatada (verde) que tem média 0 e desvio padrão 1.5. Observa-se, como mencionado, que modificando os parâmetros da curva, altera-se a posição ou o formato da mesma.\n\n\n\n\n\n\n\n\nFigura 7.4: Curvas normais com modificação dos parâmetros.\n\n\n\n\n\n\n7.7.1 Características da distribuição normal\nA curva normal apresenta as seguintes características:\n\nA média e o desvio padrão descrevem exatamente uma distribuição normal, eles são chamados de parâmetros da distribuição. Se uma distribuição normal tem média \\(\\mu\\) e desvio padrão \\(\\sigma\\), pode-se escrever a distribuição como \\(N (\\mu,\\sigma)\\). As três distribuições dos gráficos da Figura 7.4 podem ser escritas como:\n\nCurva azul \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1)\\)\nCurva verde \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1.5)\\)\nCurva vermelha \\(\\to\\) \\(N(\\mu = 1.5,\\sigma = 1)\\)\n\nNa distribuição normal, a média, a mediana e a moda coincidem.\nA curva normal é simétrica em torno da média (\\(\\mu\\)).\nAs extremidades da curva, em ambos os lados da média, se estendem cada vez mais próximas do eixo x (abscissa) sem jamais tocá-lo. É assintótica.\nOs pontos de inflexão da curva são \\(\\mu - \\sigma\\) e \\(\\mu + \\sigma\\).\nA área total sob a curva é 1 ou 100%.\n\n\n\n7.7.2 Distribuição normal padronizada\nCada variável aleatória contínua tem a sua média e seu desvio padrão e, portanto, a sua curva normal correspondente.\nPara facilitar a comparação entre variáveis, foi criado o conceito de curva normal padronizada, que é uma curva normal com média 0 e desvio padrão 1. A distribuição normal padrão também pode ser chamada de distribuição normal centrada ou reduzida.\nPara calcular probabilidades associadas a distribuição normal, costuma-se converter a variável aleatória original X, em unidades reduzidas ou padronizadas, denominadas de escore Z ou escore padrão. Essa transformação é realizada pela equação que indica o número de desvios padrão envolvidos no afastamento do valor x em relação à média da população:\n\\[\nZ =\\frac{x-\\mu}{\\sigma}\n\\] onde:\n\nZ \\(\\to\\) escore Z\n\nx \\(\\to\\) valor qualquer da variável aleatória X\n\n\\(\\mu\\) \\(\\to\\) média da variável X\n\n\\(\\sigma\\) \\(\\to\\) desvio padrão da variável X\n\nQualquer distribuição de uma variável aleatória normal pode ser padronizada, usando o escore Z. Isto permite que se calcule a probabilidade de se encontrar determinados intervalos de valores (4).\nComo exemplo, se retornará à altura das mulheres. É, praticamente, impossível saber o valor da média populacional, por isso. costuma-se usar a média aritmética como um estimador da média populacional. Dessa forma, a variável dados$altura poderá utilizada com estimativa da média populacional. Em primeiro lugar, se construirá um tibble de nome resumo:\n\n resumo &lt;- dados %&gt;% \n   dplyr::summarise(n = n(),\n                    media = mean(altura, na.rm = TRUE),\n                    dp = sd(altura, na.rm = TRUE),\n                    min = min(altura, na.rm = TRUE),\n                    max = max(altura, na.rm = TRUE))\n resumo\n\n# A tibble: 1 × 5\n      n media     dp   min   max\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1368  1.60 0.0655   1.4  1.85\n\n\nAssim, pode-se verificar quantos desvios padrão uma mulher, pertencente a essa amostra, com 1,725m está afastada da média:\n\nZ &lt;- (1.725 - resumo$media)/resumo$dp\nround(Z, 2)\n\n[1] 1.94\n\n\nEsta mulher está distante praticamente 2 desvios padrão acima da média da sua população. Portanto, ela é considerada alta. Por que?\nPara responder a essa pergunta, há necessidade de calcular a probabilidade de encontrar uma mulher com esta altura, nesta população. Primeiro, calcula-se a probabilidade de encontrar uma mulher com esta altura, nesta amostra. No R, existem as funções dnorm(), pnorm() e qnorm(), que permitem calcular a densidade de probabilidade, a distribuição cumulativa e a função quantílica da distribuição normal para um conjunto de valores. Além dessas, há a função rnorm() que permite obter observações aleatórias que seguem uma distribuição normal@jain_2022norm.\n\n7.7.2.1 Função pnorm()\nA função pnorm() fornece a Função de Distribuição Cumulativa (CDF) da distribuição Normal, que é a probabilidade de que a variável X contenha um valor menor ou igual a x.\nArgumentos:\n\nq \\(\\rightarrow\\) vetor de quantis\n\nmean \\(\\rightarrow\\) média\n\nsd \\(\\rightarrow\\) desvio padrão\n\nlower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são \\(P(X\\le x)\\), caso contrário \\(P(X &gt; x)\\)\n\nSe for usado \\(mean = 0\\) e \\(sd = 1\\), o valor de q = Z, caso contrário, toma-se os valores da média, o desvio padrão da população e o valor de x. Com esta função, é possível responder a pergunta feita anteriormente em relação a probabilidade de encontrar uma mulher com mais de 1,725m, equivalente a 1.94 desvios padrão acima da média, em uma população com média = 1.5979678 e desvio padrão = 0.0654787.\n\np &lt;- pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\np\n\n[1] 0.02618654\n\n\nOu, usando os valores:\n\npnorm(1.725, mean = resumo$media, sd = resumo$dp, lower.tail = FALSE)\n\n[1] 0.02618654\n\n\nObserva-se que, nesta amostra, apenas 2.6% das mulheres têm acima de 1,725m, razão de ser considerada uma mulher alta. Ou seja, é pouco provável encontrar mulheres acima dessa altura, nesta amostra.\nPara representar graficamente essa pequena probabilidade, será construída uma curva com essa pequena área sombreada, colorida em vermelho. Para isso, será feito uso de uma função própria, denominada normal_area(). Ela pode ser obtida aqui para ser baixada em seu diretório de trabalho para uso posterior 3.\nA Figura 7.5 representa com clareza esta pequena probabilidade. Foi usada a função text() para escrever o valor da probabilidade.\n\nsource(\"dados/normal_area.R\")\nnormal_area(media = 0, dp = 1, linf = 1.94, lsup = 3, cor = \"tomato\", lwd = 2 )\ntext(2.6, 0.05, \"2.6%\")\n\n\n\n\n\n\n\nFigura 7.5: Probabilidade de encontrar mulheres com mais de 1,725m\n\n\n\n\n\n\n\n7.7.2.2 Função qnorm()\nA função qnorm() permite encontrar o quantil q para qualquer probabilidade p. Portanto, a função qnorm é o inverso da função pnorm().\nArgumentos:\n\np \\(\\to\\) vetor de probabilidades\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nNo exemplo anterior, a probabilidade de se encontrar mulheres, na maternidade, com mais de 1,725m foi de 2.6%. Poderia ser calculado com a função qnorm() qual o escore Z correspondente:\n\nqnorm(p, mean = 0, sd = 1, lower.tail = FALSE)\n\n[1] 1.940054\n\n\n\n\n7.7.2.3 Função dnorm()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição normal dada uma certa variável aleatória X, uma média populacional \\(\\mu\\) e o desvio padrão populacional \\(\\sigma\\).\nArgumentos:\n\nx \\(\\to\\) vetor de quantis\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nEmbora x represente a variável independente da pdf para a distribuição normal, também é útil pensar em x como um escore Z. Por exemplo, a densidade de probabilidade quando x = 0 é igual:\n\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\nPara se construir uma curva de densidade de probabilidades normal ( \\(X \\sim N(μ=0,σ=1)\\)), basta aplicar a função dnorm() a uma sequência contínua de escores Z. O vetor de escores Z é obtido com a função seq(), como mostrado a seguir:\n\nescores_z &lt;- seq(-3,3, by = 0.05)\nescores_z\n\n  [1] -3.00 -2.95 -2.90 -2.85 -2.80 -2.75 -2.70 -2.65 -2.60 -2.55 -2.50 -2.45\n [13] -2.40 -2.35 -2.30 -2.25 -2.20 -2.15 -2.10 -2.05 -2.00 -1.95 -1.90 -1.85\n [25] -1.80 -1.75 -1.70 -1.65 -1.60 -1.55 -1.50 -1.45 -1.40 -1.35 -1.30 -1.25\n [37] -1.20 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65\n [49] -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05\n [61]  0.00  0.05  0.10  0.15  0.20  0.25  0.30  0.35  0.40  0.45  0.50  0.55\n [73]  0.60  0.65  0.70  0.75  0.80  0.85  0.90  0.95  1.00  1.05  1.10  1.15\n [85]  1.20  1.25  1.30  1.35  1.40  1.45  1.50  1.55  1.60  1.65  1.70  1.75\n [97]  1.80  1.85  1.90  1.95  2.00  2.05  2.10  2.15  2.20  2.25  2.30  2.35\n[109]  2.40  2.45  2.50  2.55  2.60  2.65  2.70  2.75  2.80  2.85  2.90  2.95\n[121]  3.00\n\n\nAgora, usando a função dnorm(), será construído conjunto de valores de densidade de probabilidade correspondentes aos escores Z obtidos anteriormente:\n\nvalores_d &lt;- dnorm(escores_z, mean = 0, sd = 1)\n\nEstes valores serão plotados para construir a curva normal (Figura 7.6):\n\nplot(valores_d,\n     type = \"l\",                          \n     lwd = 2,                             \n     col = \"steelblue\",                  \n     xaxt = \"n\",                          \n     ylab = \"Densidade de Probabilidade\",\n     xlab = \"Escores Z\")\n\n# Rótulos do eixo x\naxis(side = 1, at = which(valores_d == dnorm(0)), labels = c(0))\naxis(side = 1, at=which(valores_d == dnorm(1)), labels=c(-1, 1))\naxis(side = 1, at=which(valores_d == dnorm(2)), labels=c(-2, 2))\naxis(side = 1, at=which(valores_d == dnorm(3)), labels=c(-3, 3))\n\n\n\n\n\n\n\nFigura 7.6: Função densidade de probabilidade.\n\n\n\n\n\nOs argumentos básicos a serem informados da função axis() são: side=, at= e labels=. Esses argumentos determinam qual eixo será preenchido, qual a posição dos valores no eixo e a sequência de valores a ser preenchida, respectivamente. O argumento side= recebe valores que vão de 1 a 4: 1 = eixo inferior, 2 = eixo lateral esquerdo, 3 = eixo superior, 4= eixo lateral direito. Ou seja, partindo do eixo inferior (eixo x), os valores aumentam até 4 seguindo o sentindo horário para os quatros lados do gráfico. No exemplo, foi modificado o eixo x, logo side = 1. O argumento at = estabelece os pontos (densidades de probabilidade) do eixo x que receberão os rótulos, especificados no argumento label =.\nComo se pode ver, dnorm() fornece a “altura” do pdf da distribuição normal em qualquer escore Z que se forneça como argumento.\n\n\n7.7.2.4 Função rnorm()\nA função rnorm() gera n números aleatórios com distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\sigma\\).\nArgumentos:\n\nn \\(\\to\\) número de observações a serem geradas\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nCom esta função é possível, por exemplo, gerar 10 observações de uma distribuição normal:\n\nrnorm(10)\n\n [1] -1.29768591 -2.39036387  0.03470234 -0.42183934  0.43562959  2.31995999\n [7] -0.77847225  0.62877376  0.91265429  0.90807352\n\n\nNo entanto, deve-se notar que, se uma “semente” (seed) não for especificada, a saída não será reproduzível, ou seja, cada vez que o comando for executado, retornará um novo conjunto de observações:\n\nrnorm(10)\n\n [1]  0.5912671  0.3262756  0.1554009  2.1154360  0.7452745 -1.8290293\n [7]  1.1480081 -0.9114279 -0.7808653 -0.6445455\n\n\nCada vez que este comando for reproduzido, retornará uma nova série de 10 números diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada, no exemplo, pela função rnorm(). O valor do número (“semente”) não é importante, é apenas um identificador. Para ilustrar, será construído dois conjuntos de 10 números que serão recebidos pelos objetos x e y. Para gerar o conjunto de números x, será usado o número 123 como “semente”. A “semente” funciona como uma espécie de marca. Para o y não será usado a função set.seed():\n\nn &lt;- 10\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny &lt;- rnorm(n)\ny\n\n [1]  1.2240818  0.3598138  0.4007715  0.1106827 -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n\nComparando os conjuntos com a função identical() do R base, observa-se que os conjuntos são diferentes:\n\nidentical(x, y)\n\n[1] FALSE\n\n\nAgora, repetindo os mesmos comandos, mas usando antes a mesma “semente”, observa-se que os conjuntos são idênticos.\n\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nset.seed (123)\ny &lt;- rnorm(n)\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nidentical(x, y)\n\n[1] TRUE\n\n\nOutros usos da função rnorm():\nA função rnorm()será usada para gerar três vetores diferentes de números aleatórios de uma distribuição normal.\n\nset.seed(1234)\nn10 &lt;- rnorm(10, mean = 0, sd = 1)\nn100 &lt;- rnorm(100, mean = 0, sd = 1)\nn10000 &lt;-  rnorm(10000, mean = 0, sd = 1)\n\nEmn sequência, serão construídos histogramas (Figura 7.7), onde se pode observar que, aumentando o número de observações, tem-se gráficos que irão progressivamente se aproximando da verdadeira função de densidade normal.\nA função par(mfrow(1,3)) coloca os gráficos gerados em uma mesma linha e em três colunas. No final, se repete a função, restaurando as configurações basais de plotagem (uma linha e uma coluna).\n\n# Este comando coloca os gráficos em uma mesma linha, o argumento mfrow(c(1,3)) diz ao R para construir uma linha e três colunas:\npar(mfrow=c(1,3))\n\n# Histogramas\nhist(n10, breaks = 5, main = \"n =10\", ylab = \"Frequência\")\nhist(n100, breaks = 20, main = \"n =100\", ylab = \"Frequência\")\nhist(n10000, breaks = 50, main = \"n =10000\", ylab = \"Frequência\")\n\n# Restaura as configurações basais de plotagem\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\nFigura 7.7: Histogramas construídos com amostras geradas pela função rnorm.\n\n\n\n\n\nObserve também que à medida que n aumenta, a distribuição dos dados caracteriza-se como uma distribuição normal. Na Seção 9.2.2, este assunto voltará à cena.\n\n\n\n7.7.3 Regra Empírica 68-95-99.7\nA regra empírica diz que, se uma população de um conjunto de dados tem uma distribuição normal com média 0 e desvio padrão 1 (\\(X\\sim N(\\mu=0,\\sigma=1)\\)) pode-se afirmar que aproximadamente, 68%, 95% e 99,7% dos valores encontram-se, respectivamente, dentro de \\(\\pm\\) 1, 2 e 3 desvio padrão acima e abaixo média.\nEssa regra pode ser usada para descrever uma população e ajudar a decidir se uma amostra de dados veio de uma distribuição normal. Se uma amostra é grande o suficiente e a observação do histograma tem um formato parecido com um sino, é possível verificar se os dados seguem as especificações 68-95-99,7%. Se sim, é razoável concluir que os dados vieram de uma distribuição normal.\nExemplo:\nCom a função rnorm(), será gerado um vetor de 1000 números e um histograma com curva normal sobreposta:\n\n\n\n\n\n\n\n\nFigura 7.8: Histograma com curva de densidade de probabilidade sobreposta\n\n\n\n\n\nNo histograma (Figura 7.8), a probabilidade entre os escores Z - 1 e + 1 (entre as duas linhas tracejadas A e B) é igual a aproximadamente 68%. Pode-se calcular isto facilmente, usando a função pnorm():\nProbabilidade abaixo de z = 1, abaixo do ponto B:\n\nB &lt;- pnorm (1, 0, 1)\nB &lt;- round(B, 3)*100\nB\n\n[1] 84.1\n\n\nProbabilidade abaixo de z = -1, abaixo do ponto A:\n\n A &lt;- pnorm (-1, 0, 1)\n A &lt;- round(A, 3)*100\n A\n\n[1] 15.9\n\n\nLogo , a área abaixo da curva entre A e B é igual a:\n\n prob &lt;- B - A\n prob\n\n[1] 68.2\n\n\n\n\n7.7.4 Calculando probabilidades em uma distribuição normal\nComo visto na Seção 7.4.1, a variável dados$altura tem uma distribuição praticamente simétrica. Usando esses dados (\\(X\\sim N(\\mu=1,598,\\sigma=0,065)\\)), pode-se calcular probabilidades, dadas pela área sob a curva.\nExemplo 1: Qual a probabilidade de se encontrar mulheres com altura entre 1,47 e 1,73 m?\n\n# Dados\n mu &lt;- 1.598\n sigma &lt;- 0.065\n x1 &lt;- 1.47\n x2 &lt;- 1.73\n \n # Solução\n z1 &lt;-  (x1 - mu)/sigma\n z2 &lt;-  (x2 - mu)/sigma\n\n p1 &lt;- pnorm(x1, mu, sigma)\n p2 &lt;- pnorm(x2, mu, sigma)\n \n p2 - p1\n\n[1] 0.9543975\n\n\n\n\n\n\n\n\n\n\nFigura 7.9: Probabilidade de alturas entre 1,47 e 1,73m.\n\n\n\n\n\nA probabilidade de alturas entre 1,47 m e 1,73m é igual a 95,4% (Figura 7.9).\nExemplo 2: Os dados de uma pesquisa mostram informações sobre o tempo de cirurgia para reconstrução do ligamento cruzado anterior (LCA). A distribuição de probabilidades se a justa à normal com o tempo médio de cirurgia de 129 minutos com um desvio padrão de 14 minutos.\n\nQual a probabilidade de uma cirurgia de reconstrução do LCA requerer um tempo menor do que 100 minutos?\n\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 100\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.01915938\n\n\n\n\n\n\n\n\n\n\nFigura 7.10: Probabilidade do tempo de cirurgia de LCA menor ou igual a 100 minutos.\n\n\n\n\n\nDe acordo com a distribuição, 1,92%% das cirurgias irão demandar quantidade de tempo menor do que 100 minutos (Figura 7.10).\n\nSe uma cirurgia demorar 160 minutos, o que se conclui em relação a essa informação?\n\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 160\n\n # Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = FALSE)\n p\n\n[1] 0.01340457\n\n\n\n\n\n\n\n\n\n\nFigura 7.11: Probabilidade do tempo de cirurgia de LCA maior ou igual a 160 minutos.\n\n\n\n\n\nDe acordo com a distribuição, 1,34% das cirurgias irão demandar quantidade de tempo \\(\\ge 160\\) minutos (Figura 7.11). Ou seja, é uma probabilidade muito pequena!\nExemplo 3: Suponha-se que em uma determinada ilha hipotética existam duas populações etnicamente diferentes onde as mulheres têm as seguintes medidas de altura: população 1 tem μ = 160 cm e σ = 6,6 cm e a população 2 tem μ = 140 cm e σ = 6,6 cm. As alturas de ambas as populações têm distribuição normal. Essas duas populações têm o mesmo aspecto físico, podendo ser distinguidas apenas geneticamente.\n\nQual a probabilidade de uma mulher com 150 cm pertencer a população 1?\n\n\n# Dados\n mu &lt;- 160\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.06486702\n\n\n\n\n\n\n\n\n\n\nFigura 7.12: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 160 cmm.\n\n\n\n\n\nNa população 1, apenas 6.5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.12). Em outras palavras, existe pouca probabilidade dessa mulher pertence à população 1.\n\nQual a probabilidade de uma mulher com 150 cm pertencer a população 2?\n\n\n# Dados\n mu &lt;- 140\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n \n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.935133\n\n\n\n\n\n\n\n\n\n\nFigura 7.13: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 140 cmm.\n\n\n\n\n\nNa população 2, 93,5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.13). Concluindo, ela pode pertencer a qualquer uma das populações. Pode ser uma mulher alta da população 2 ou uma “baixinha” da população 1!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-binomial",
    "href": "07-probabilidades.html#distribuição-binomial",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.8 Distribuição Binomial",
    "text": "7.8 Distribuição Binomial\nA distribuição normal padrão é apenas um dos exemplos de distribuição de probabilidade. Uma boa parte das situações se ajustam a ela. Entretanto, diversas situações reais muitas vezes se aproximam de outras distribuições estocásticas definidas por algumas hipóteses. Daí a importância de se conhecer e manipular algumas destas distribuições. Entre elas, a distribuição binomial.\nQuando um experimento aleatório resulta em um de dois, mutuamente exclusivos, desfechos, tais como vivo/morto, positivo/negativo, sim/não, masculino/feminino é denominado de Ensaio de Bernoulli. Recebeu esta denominação em homenagem ao matemático suíço, Jacob Bernoulli (1654-1705), considerado fundador do cálculo e da teoria da probabilidade (5).\nA distribuição de frequências que descreve as proporções de um ensaio de Bernoulli, chama-se Distribuição Binomial. A probabilidade binomial dá a probabilidade de determinado desfecho ocorrer em determinado número de ensaios independentes. Uma sequência de ensaios de Bernoulli forma um Processo de Bernoulli.\nA distribuição binomial é importante para variáveis discretas. Existem poucas condições que precisam ser atendidas para distribuição binomial:\n\nCada ensaio resulta em um de dois desfechos, mutuamente exclusivos, denominados, arbitrariamente, de sucesso e fracasso;\n\nA probabilidade de sucesso é fixa, igual a p, constante em cada ensaio, e a probabilidade de fracasso é igual a 1 – p;\nO número de repetições n em um ensaio é fixo.\n\nOs ensaios são independentes\n\nA distribuição binomial é na verdade uma família de distribuições, cujos membros são definidos pelos valores de n e p (parâmetros da distribuição binomial).\nA probabilidade de sucesso 4, em uma distribuição binomial, é dada pela fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nonde n = ensaios, x = sucessos, p = probabilidade de um sucesso e C representa o número possível de combinações em um ensaio.\nO número de combinações, C de x sucessos entre n repetições podem ser computado pela fórmula:\n\\[\nC = \\frac{n!}{x!(n - x)!}\n\\]\nou, no R, com a função choose (n, x).\nO modelo de distribuição binomial trata de encontrar a probabilidade de sucesso de um evento que tem apenas dois resultados possíveis em uma série de experimentos. Usando dados de uma distribuição binomial, é possível calcular os valores esperados de uma variável aleatória conforme ela passa por tentativas independentes. Em outras palavras, é possível prever o número exato de caras ou coroas que se deve esperar ao jogar uma moeda um certo número de vezes.\nTambém, pode-se usar a probabilidade binomial cumulativa para encontrar a probabilidade de obter um determinado intervalo de resultados. Por exemplo, saber a probabilidade do nascimento de até três meninos em 10 nascimentos consecutivos quando a probabilidade de nascer um menino é 0,50.\nO R tem quatro funções embutidas para gerar distribuição binomial. Ela são descritas a seguir.\n\n7.8.1 Funções da distribuição binomial\n\n7.8.1.1 Função pbinom()\nEsta função retorna o valor da função de densidade cumulativa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob).\nArgumentos:\n\nq \\(\\to\\) vetor de quantis\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, qual é a probabilidade de nascer até três meninos em cinco nascimentos, sabendo que a probabiliade de nascer um menino é igual a 0.50?\n\npbinom (3, 5, 0.50)\n\n[1] 0.8125\n\n\nIsso corresponde a soma das probabilidades de nascer nenhum menino, um menino, dois meninos e três meninos (Figura 7.14). Isto é calculado pela equação \\(P(X = x)\\), vista anteriormente.\nColocando no R:\n\nn = 5\np = 0.50\nx &lt;- 0:5\n# Probabilidades de meninos \nFx &lt;- (factorial(n)/(factorial(x)*factorial(n-x)))* p^x *(1-p)^(n-x)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n[1] 0.8125\n\n\n\n\n\n\n\n\nFigura 7.14: Distribuição binomial, mostrando a P (x &lt; 4) com n = 5 e p = 0.50\n\n\n\n\n\n\n\n7.8.1.2 Função qbinom()\nEsta função retorna o valor da função de densidade cumulativa inversa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). Com o uso desta função, podemos descobrir o quantil da distribuição binomial.\nArgumentos:\n\np \\(\\to\\) probabilidade ou vetor de probabilidades\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, quantos meninos nascerão em 5 partos com 81.25% de probabilidade cumulativa?\n\nqbinom (0.8125, size = 5, prob = 0.50)\n\n[1] 3\n\n\n\n\n7.8.1.3 Função rbinom()\nA função rbinom() permite extrair n observações aleatórias de uma distribuição binomial. Os argumentos da função são descritos abaixo:\nArgumentos:\n\nn \\(\\to\\) número de observações aleatórias a ser gerado\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nPara fazer uma simulação de 1000 amostras, aleatoriamente, de tamanho 5 e com probabilidade de nascer menino igual a 0,50, usa-se 5:\n\nset.seed(23)\nmenino &lt;- rbinom(n = 1000, size = 5, prob = 0.5)\n\nCada amostra de n = 5 exibe o número de meninos nascidos. Pode-se fazer a média que representa o valor esperado do número de sucessos (nascimento de menino, no exemplo) em um conjunto de ensaios independentes:\n\nmean(menino)\n\n[1] 2.515\n\n\nQuanto maior o número de variáveis aleatória criadas, mais próximo a média do número de sucessos estará do número esperado de sucessos que é igual ao número de sucessos vezes a probabilidade de sucesso em cada ensaio (5 x 0,50 = 2,5).\nEstranho, não é? Dois meninos e meio, em média por ensaio! É, a média é assim, uma estimativa, expectativa matemática! Não é real…\n\n\n7.8.1.4 Função dbinom()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição binomial dada uma determinada variável aleatória X, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). A função tem a seguinte sintaxe:\nArgumentos:\n\nx \\(\\to\\) vetor de números\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nA função é usada para encontrar a probabilidade de um determinado valor para dados que seguem a distribuição binomial, ou seja, encontra \\(P(X=x)\\), probabilidade de x sucessos em tentativas de tamanho (size) n quando a probabilidade (p) de sucesso é prob. Obtém o mesmo resultado da fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nPor exemplo, no nascimento de uma criança, as duas possibilidades, menino ou menina, são mutuamente excludentes e esses são os únicos eventos que podem acontecer. A probabilidade de nascimento de menino, como visto, é 0,50, qual seria a probabilidade de nascerem 4 meninos em 5 partos consecutivos não gemelares (Figura 7.15)?\n\ndbinom(4, size = 5, prob = 0.50)\n\n[1] 0.15625\n\n\nAs probabilidades de nascerem meninos em 5 nascimentos são:\n\nFx &lt;- dbinom(0:5, 5, 0.50)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n\n\n\n\n\n\nFigura 7.15: Distribuição binomial para P (x = 4) com n = 5 e p = 0,50\n\n\n\n\n\n\n\n\n7.8.2 Média e desvio padrão da distribuição binomial\nQuando o número de repetições é grande, geralmente há necessidade de resumir as probabilidades. A distribuição binomial pode ser descrita por sua média e variância.\nA média é o valor médio da variável aleatória em um longo número de repetições. É também chamada de valor esperado ou expectativa. A expectativa de uma variável aleatória X, geralmente, é denotada por \\(E(X)\\) e obtida pela multiplicação do número de ensaios independentes (n) pela probabilidade (p) de sucesso em cada ensaio:\n\\[\n\\mu = E(X) = n \\times p\n\\]\nPortanto, a expectativa (esperança) de nascimento de meninos em 5 partos é \\(E(X)=5 \\times 0,50 = 2,5\\), como visto na função rbinom(). Observe que o valor esperado de uma variável aleatória discreta não tem um valor que a variável aleatória pode realmente assumir.\nPor exemplo, para o número médio de meninos em um parto, ou não se tem menino ou se tem 1 menino, cada uma possibilidade com probabilidade de 0,50 e o valor esperado é (0 × 0,50) + (1 × 0,50) = 0,50. O número de meninos deve ser 0 ou 1, mas o valor esperado é a metade, a média que se obteria no longo prazo.\nA variância de uma variável aleatória discreta X é igual a\n\\[\n\\sigma^2=var(X) = n\\times p \\times (1-p)\n\\]\nConsequentemente, o desvio padrão é igual a\n\\[\n\\sigma = \\sqrt{var(X)} = \\sqrt{n\\times p \\times (1-p)}\n\\]\nPara o exemplo de 5 nascimentos, a média foi de 2,5 meninos e o desvio padrão\n\\[\n\\sigma =\\sqrt{5\\times 0.50 \\times (1-0.50)}=\\sqrt{2.5 \\times 0.50}= 1.12\n\\]\nPortanto, se espera que ocorram em média 2,5 (\\(\\sigma\\) = 1,12) nascimentos de meninos em 5 partos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-de-poisson",
    "href": "07-probabilidades.html#distribuição-de-poisson",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.9 Distribuição de Poisson",
    "text": "7.9 Distribuição de Poisson\nA distribuição de Poisson é utilizada para descrever a probabilidade do número de ocorrências em um intervalo contínuo (de tempo ou espaço). No caso da distribuição binomial, a variável de interesse é o número de sucessos em um intervalo discreto (n ensaios de Bernoulli).\nA unidade de medida (tempo ou espaço) é uma variável contínua, mas a variável aleatória, o número de ocorrências, é discreta. Esta distribuição segue as mesmas premissas da distribuição binomial:\n\nas tentativas são independentes;\na variável aleatória é o número de eventos em cada amostra;\na probabilidade é constante em cada intervalo\n\nEla é utilizada para modelar eventos discretos que ocorrem com pouca frequência no tempo ou espaço, por isso é algumas vezes denominada de distribuição de eventos raros. Pode-se usar a distribuição de Poisson como uma aproximação da distribuição Binomial quando n, o número de tentativas, for grande e p ou (1 – p) for pequeno (eventos raros).\nUm bom princípio básico é usar a distribuição de Poisson quando \\(n \\ge 20\\) e \\(n \\times p\\) ou \\(n \\times (1- p)\\) &lt; 5% (6). Nessas condições, a probabilidade que uma variável aleatória X adote um valor x é\n\\[\nP(X = x) = \\frac {e^{-\\lambda} \\times \\lambda^x}{x!}\n\\]\nonde \\(\\lambda\\) (lambda) representa o número de ocorrências de um evento em um intervalo de tempo e é conhecida como parâmetro da distribuição de Poisson e é igual em média a \\(n \\times p\\).\nNo R, essa probabilidade é dada pela função dpois(x, lambda).\nExemplo:\nSuponha que a probabilidade de uma puérpera ter infecção congênita (rubéola) seja igual a 0,0009. Qual seria a probabilidade, em uma população de 6000 gestantes, de que 5 estejam infectadas?\n\np &lt;- 0.0009\nx &lt;- 5\nn &lt;- 6000\nlambda &lt;- n * p\nP &lt;- dpois(x, lambda)\nround (P, 3)\n\n[1] 0.173\n\n\nPortanto, a probabilidade de se encontrar 5 mulheres com infecção congênita é de aproximadamente 17%.\n\n\n\n\n1. Debnath L, Basu K. A short history of probability theory and its applications. International Journal of Mathematical Education in Science and Technology. 2015;46(1):13–39. \n\n\n2. Menezes RX de. Introdução à Probabilidade. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 151–87. \n\n\n3. Pagano M, Kimberly G. Theoretical Probability Distributions. Em: Principles of Biostatistics. Second Edition. CRC Press; 2000. p. 162. \n\n\n4. Gonzalez JCS. Normal distribution in R [Internet]. R CODER. 2021. Disponível em: https://r-coder.com/\n\n\n5. Robertson E, O’Connor J. Jacob (Jacques) Bernoulli [Internet]. Maths History. School of Mathematics; Statistics, University of St Andrews; 2022. Disponível em: https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/\n\n\n6. Fisher LD, Van Belle G. Poisson Random Variables. Em: Biostatistics: A Methodology for the Health Sciences. New York, NY: John Wiley & Sons; 1993. p. 211–8.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#footnotes",
    "href": "07-probabilidades.html#footnotes",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "Hospital Geral de Caxias do Sul, Hospital de Ensino da Universidade de Caxias do Sul, RS↩︎\nPara remover a notação científica, usar a função options (scipen = 999) e, para desfazer essa ação, trocar o 999 por 0.↩︎\nVeja na Seção Seção 4.8.1.↩︎\nSucesso, aqui, não está no sentido de vitória, êxito, triunfo, glória e sim com a conotação de obter o desfecho esperado. Por exemplo, se uma moeda é lançada e se espera obter cara, sucesso significa um resultado igual a cara.↩︎\nDeve ser especificado uma “semente” (seed) antes de executar a função, senão será obtido um conjunto diferente de observações aleatórias a cada execução. Teste para verificar↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html",
    "href": "08-assimetria_curtose.html",
    "title": "8  Assimetria e Curtose",
    "section": "",
    "text": "8.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools,\n               dplyr, \n               e1071, \n               ggplot2, \n               ggpubr, \n               grDevices, \n               moments, \n               readxl, \n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#dados",
    "href": "08-assimetria_curtose.html#dados",
    "title": "8  Assimetria e Curtose",
    "section": "8.2 Dados",
    "text": "8.2 Dados\nSerá usada a mesma variável altura de 1368 mulheres do conjunto de dadosdadosMater.xlsx, já mostrado anteriormente (Seção 7.4.1).\n\n8.2.1 Exploração dos dados\nO resumo dos dados pode ser realizado, usando a função summarise() do pacote dplyr. A moda será calculada usando com função Mode() do pacote DescTools.\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nresumo &lt;- dados %&gt;% \n  summarise(n = n(),\n            media = mean(altura, na.rm = TRUE),\n            dp = sd(altura, na.rm = TRUE),\n            mediana = median(altura, na.rm = TRUE),\n            moda = Mode(altura),\n            Q1 = quantile (altura, 0.25),\n            Q3 = quantile (altura, 0.75),\n            CV = dp/media)\nresumo\n\n# A tibble: 1 × 8\n      n media     dp mediana  moda    Q1    Q3     CV\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1368  1.60 0.0655     1.6   1.6  1.55  1.65 0.0410\n\n\nPara a exploração visual dos dados, será construído um histograma com um boxplot sobreposto (Figura 8.1). A função layout() tem o formato layout(mat) onde mat é um objeto da classe matriz que permite dividir a janela de plotagem em áreas com tamanhos personalizados. Abaixo, cria-se uma matriz com uma coluna e duas linhas com uma relação de 1:8 entre as linhas. A função par() é utilizada para alterar as margens. Para mais detalhes acesse aqui.\n\n# Estruturação do layout do gráfico\nlayout(matrix(c(1,2), nrow = 2 , ncol = 1, \n              byrow = TRUE), heights = c(1, 8))\n\n# Boxplot\npar (mar=c (0, 4.3, 1.1, 2))\nboxplot (dados$altura, \n         horizontal = TRUE, \n         ylim = c (1.4, 1.9), \n         xaxt = \"n\", \n         col = \"lightblue\", \n         frame = FALSE)\n\n#Histograma\npar (mar=c (4, 4.3, 1.1, 2))\nhist (dados$altura, \n      breaks=15,\n      col = \"lightblue\",\n      border = \"black\",\n      main = \"\",\n      xlab = \"Altura (m)\",\n      ylab = \"Frequência\",\n      xlim = c(1.4,1.9),\n      las = 1)\nbox(bty = \"L\")\n# Restauração do padrão\npar (mar = c(5, 4, 4, 2) + 0.1)\n\n\n\n\n\n\n\nFigura 8.1: Histograma da altura das gestantes com boxplot sobreposto.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#assimetria",
    "href": "08-assimetria_curtose.html#assimetria",
    "title": "8  Assimetria e Curtose",
    "section": "8.3 Assimetria",
    "text": "8.3 Assimetria\nA assimetria analisa a proximidade ou o afastamento de um conjunto de dados quantitativos em relação à distribuição normal. Mede o grau de afastamento de uma distribuição em relação a um eixo central (geralmente a média).\nQuando a curva é simétrica, a média, a mediana e a moda coincidem, num mesmo ponto, havendo um perfeito equilíbrio na distribuição. Quando o equilíbrio não acontece, isto é, a média, a mediana e a moda recaem em pontos diferentes da distribuição esta será assimétrica; enviesada a direita ou esquerda. podendo-se caracterizar como curvas assimétricas à direita ou à esquerda. Quando a distribuição é assimétrica à esquerda ou assimetria negativa, a cauda da curva localiza-se à esquerda, desviando a média para este lado (Figura 8.2). Na assimetria positiva, ocorre o contrário, a cauda está localizada à direita e da mesma forma a média (1).\n\n\n\n\n\n\n\n\nFigura 8.2: Assimetria\n\n\n\n\n\n\n8.3.1 Avaliação da assimetria\nO R dispões de diversas maneiras para o cálculo do coeficiente de assimetria. O coeficiente de assimetria é um método numérico estatístico para medir a assimetria da distribuição ou conjunto de dados. Ele fala sobre a posição da maioria dos valores de dados na distribuição em torno do valor central.\n\n8.3.1.1 Cálculo do coeficiente de assimetria\nVárias medidas de coeficientes de assimetria amostrais foram propostas. O coeficiente de assimetria pode ser calculado no R, usando a função skewness() do pacote e1071 (2). Esta função usa os seguintes argumentos:\n\nx \\(\\to\\) vetor numérico que contém os valores\nna.rm \\(\\to\\) um valor lógico que indica se os valores NA devem ser eliminados antes que o cálculo prossiga.\ntype \\(\\to\\) número inteiro entre 1 e 3 selecionando um dos algoritmos para calcular assimetria detalhados abaixo.\n\nOs três tipos são os seguintes:\n\nTipo 1, g1 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_1=\\frac{m_3}{m_2^\\frac{3}{2}}\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 3, tem-se:\n\\[\nm_3=\\frac{\\sum(x_i - \\overline{x})^3}{n}\n\\] Para r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m3 &lt;- (sum((dados$altura - (mean(dados$altura)))^3))/resumo$n\n m3\n\n[1] 5.081924e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g1 no R, chega-se ao resultado:\n\n g1 &lt;- m3/(m2)^(3/2)\n g1\n\n[1] 0.1812196\n\n\nUsando a função skewness() do pacote e1071, chega-se ao mesmo resultado:\n\ne1071::skewness(dados$altura, type = 1)\n\n[1] 0.1812196\n\n\n\nTipo 2, G1 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_1=\\frac{g_1 \\sqrt{n(n-1)}}{n-2}\n\\]\nColocando os dados na fórmula na linguagem do R, tem-se:\n\n G1 &lt;- (g1*sqrt((resumo$n*(resumo$n-1))))/(resumo$n-2)\n G1\n\n[1] 0.1814186\n\n\nCalculando com a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 2)\n\n[1] 0.1814186\n\n\n\nTipo 3, b1 \\(\\to\\) É o padrão da função skewness() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_1= \\frac {m_3}{s^3}\n\\]\nonde s é o desvio padrão da amostra. Na linguagem R, tem-se:\n\n b1 &lt;- m3/(resumo$dp)^3\n b1\n\n[1] 0.1810209\n\n\nUsando a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 3)\n\n[1] 0.1810209\n\n\nPara amostras grandes, há muito pouca diferença entre as várias medidas (3). Todas as três medidas de assimetria são imparciais sob normalidade.\nInterpretação do coeficiente de assimetria\nQuando a \\(assimetria = 0\\), tem-se uma distribuição simétrica e a média, a mediana e a moda coincidem; quando a \\({assimetria} &lt; {0}\\), \\({média} &lt; {mediana} &lt; {moda}\\), a distribuição tem assimetria negativa e quando a \\({assimetria} &gt; {0}\\), \\({média} &gt; {mediana} &gt; {moda}\\), a distribuição tem assimetria positiva.\nA Tabela 8.1 sugere uma forma de interpretar o coeficiente de assimetria (4).\n\n\n\n\nTabela 8.1: Interpretação do Coeficiente de Assimetria\n\n\n\n\n\n\n\nValor do Coeficiente\nAssimetria\n\n\n\n\n-1 a +1\nleve\n\n\n-1 a -2 e +1 a +2\nmoderada\n\n\n-2 a -3 e +2 a +3\nimportante\n\n\n&lt; -3 ou &gt; +3\ngrave\n\n\n\n\n\n\n\n\n\n\nObservando o formato da distribuição no histograma e no boxplot, na Figura 8.1, e no resultado do coeficiente de assimetria, conclui-se que a variável altura tem uma assimetria positiva leve, não preocupante. É possível aceitar essa variável como praticamente simétrica.\n\n\n8.3.1.2 Avaliação da assimetria com o gráfico QQ\nOutra ferramenta gráfica que permite avaliar a simetria dos dados é o gráfico QQ (gráfico quantil-quantil). Ele permite observar se a distribuição se ajusta a distribuição normal. O gráfico QQ é um gráfico de dispersão que compara os quantis 1 da amostra com os quantis teóricos de uma distribuição de referência. Se os pontos do gráfico QQ formarem uma reta, isso indica que os dados têm a mesma distribuição da referência. Se os pontos se afastarem da reta, isso indica que os dados têm uma distribuição diferente da referência. Para construir um gráfico QQ, pode-se usar a função ggqqplot()do pacote ggpubr. Ele apresenta uma linha de referência, acompanhada de uma area sombreada, correspondente ao Intervalo de Confiança de 95% (veja o Capítulo 10):\n\nggqqplot(data = dados, \n         x = \"altura\",\n         conf.int = TRUE,\n         shape = 19,\n         xlab = \"Quantis teóricos\",\n         ylab = \"Altura (m)\",\n         color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 8.3: Gráfico QQ\n\n\n\n\n\nA Figura 8.3 exibe que a linha formada pelos pontos, praticamente, formam uma linha reta. É mais uma informação mostrando que os dados têm uma distribuição simétrica aceitável.\n\n\n8.3.1.3 Pesquisa de valores atípicos\nOs valores atípicos atraem as caudas da dispersão aumentando a possibilidade de assimetria. No boxplot da Figura 8.1, verifica-se a presença de outliers que devem ser avaliados.\nPara examinar os outliers, as estatísticas do boxplot são úteis, pois mostram a quantidade e os respectivos valores. A função boxplot.stats() do pacote grDevices, entregam as estatísticas dos 5 números (min, P25, mediana, P75 e max), o total de observações, o limite inferior e superior do intervalo de confiança de 95% e os valores atípicos (outliers)::\n\nboxplot.stats(dados$altura)\n\n$stats\n[1] 1.42 1.55 1.60 1.65 1.78\n\n$n\n[1] 1368\n\n$conf\n[1] 1.595728 1.604272\n\n$out\n[1] 1.40 1.82 1.80 1.40 1.40 1.85 1.80\n\n\nOutra maneira de identificar os outliers é através da função indentify_outliers() do pacote rstatix:\n\n dados %&gt;% \n   rstatix::identify_outliers(altura)\n\n# A tibble: 7 × 3\n  altura is.outlier is.extreme\n   &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1   1.4  TRUE       FALSE     \n2   1.82 TRUE       FALSE     \n3   1.8  TRUE       FALSE     \n4   1.4  TRUE       FALSE     \n5   1.4  TRUE       FALSE     \n6   1.85 TRUE       FALSE     \n7   1.8  TRUE       FALSE     \n\n\nAmbas as funções identificaram 7 valores atípicos (acima ou abaixo 1,5 vezes o intervalo interquartil), mas, como mostra a função identify_outliers, eles exercem pouca influência, pois não são extremos, ou seja, acima de três vezes o intervalo interquartil.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#curtose",
    "href": "08-assimetria_curtose.html#curtose",
    "title": "8  Assimetria e Curtose",
    "section": "8.4 Curtose",
    "text": "8.4 Curtose\nÉ o grau de achatamento de uma distribuição, em relação a distribuição normal. A curtose indica como o pico e as caudas de uma distribuição diferem da distribuição normal. A assimetria mede essencialmente a simetria da distribuição, enquanto a curtose determina o peso das caudas da distribuição. Portanto, é uma medida dos tamanhos combinados das duas caudas; mede a quantidade de probabilidade nas caudas. A curtose pode ser de três tipos (Figura 8.4):\n\nMesocúrtica \\(\\to\\) quando a distribuição é normal;\nLeptocúrtica \\(\\to\\) quando a distribuição é mais pontiaguda e concentrada que a normal, mostrando caudas pesadas em ambos os lados;\nPlaticúrtica \\(\\to\\) quando a distribuição é mais achatada e dispersa que a normal, com caudas planas.\n\nUma curtose em excesso é uma medida que compara a curtose de uma distribuição com a curtose de uma distribuição normal. A curtose de uma distribuição normal é igual a 3. Portanto, o excesso de curtose é determinado subtraindo 3 da curtose:\n\\[\nExcesso \\space de \\space curtose = curtose - 3\n\\]\nA distribuição normal tem uma curtose de zero e é chamada de mesocúrtica. Uma distribuição com curtose maior que zero (ou três) é mais alta e concentrada que a normal, mostrando caudas pesadas em ambos os lados, e é chamada de leptocúrtica. Uma distribuição com curtose menor que zero é mais achatada e dispersa que a normal, com caudas planas, e é chamada de platicúrtica.\nOs dados que seguem uma distribuição mesocúrtica mostram um excesso de curtose de zero ou próximo de zero. Isso significa que se os dados seguem uma distribuição normal, eles seguem uma distribuição mesocúrtica. A distribuição leptocúrtica mostra caudas pesadas em ambos os lados, indicando grandes valores discrepantes. Uma distribuição leptocúrtica manifesta uma curtose excessiva positiva. Uma distribuição platicúrtica mostra uma curtose excessiva negativa, revela uma distribuição com cauda plana.\n\n\n\n\n\n\n\n\nFigura 8.4: Assimetria\n\n\n\n\n\n\n8.4.1 Avaliação da curtose\n\n8.4.1.1 Cálculo do coeficiente de curtose\nO coeficiente de curtose pode ser calculado no R usando a função kurtosis() do pacote e1071. Esta função usa os mesmos argumentos da função skewness(), vista acima. Calcula três tipos de coeficientes:\n\nTipo 1, g2 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_2=\\frac{m_4}{m_2^2} - 3\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 4, tem-se:\n\\[\nm_4=\\frac{\\sum(x_i - \\overline{x})^4}{n}\n\\]\nPara r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m4 &lt;- (sum((dados$altura - (mean(dados$altura)))^4))/resumo$n\n m4\n\n[1] 5.734699e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g2 no R, chega-se ao resultado:\n\ng2 &lt;- (m4/(m2)^2)-3\ng2\n\n[1] 0.1242567\n\n\nUsando a função do pacote e1071, chega-se ao mesmo resultado:\n\n e1071::kurtosis(dados$altura, type = 1)\n\n[1] 0.1242567\n\n\n\nTipo 2, G2 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_2=\\left (\\left (n + 1 \\right )g_2 + 6 \\right )\\frac{\\left (n - 1 \\right)}{\\left ( \\left(n-2 \\right)\\left (n-3 \\right) \\right )}\n\\] Colocando os dados na fórmula na linguagem do R, tem-se:\n\n G2 &lt;- ((resumo$n+1)*g2 + 6)*(resumo$n-1)/((resumo$n-2)*(resumo$n-3))\n G2\n\n[1] 0.1291109\n\n\nCom a função kurtosis() do pacote e1071:\n\n e1071::kurtosis(dados$altura, type = 2)\n\n[1] 0.1291109\n\n\n\nTipo 3, b2 \\(\\to\\) É o padrão da função kurtosis() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_2=\\frac{m_4}{s^4}-3\n\\] onde s é o desvio padrão da amostra.\nNa linguagem R, tem-se:\n\n b2 &lt;- m4/(resumo$dp)^4 - 3\n b2\n\n[1] 0.1196907\n\n\nCom a função kurtosis():\n\ne1071::kurtosis(dados$altura, type = 3)\n\n[1] 0.1196907\n\n\nNovamente, para amostras grandes, há muito pouca diferença entre as várias medidas, principalmente entre G2 e b2 (3).\n\n\n8.4.1.2 Interpretação do coeficiente de curtose\nOs coeficientes calculados pela função do pacote e1071 retornam um resultado equivalente ao excesso de curtose. A curva normal tem um excesso de curtose próximo a zero e a curva é dita mesocúrtica. Se o coeficiente for positivo, os dados são leptocúrticos e se for negativo, os dados são platicúrticos. O resultado do exemplo aponta para uma distribuição leptocúrtica, pois existe um pequeno excesso de curtose (g2 = 0.1242567). Os valores que contribuem para a curtose são aqueles fora da região do pico, ou seja, ou outliers. A curva mesocúrtica tem um coeficiente de 3. Portanto, os valores calculados anteriormente referem-se ao excesso de curtose. O resultado da g2 = 0,1242567 pode ser escrito como b2 = 3,1242567. Daí o termo excesso de curtose.\nA função kurtosis() do pacote moments retorna um resultado ao redor de 3, para o coeficiente tipo 1. Para chegar ao mesmo resultado do coeficiente tipo 1 da função do pacote e1071, deve-se subtrair 3 do resultado.\n\nmoments::kurtosis(dados$altura)\n\n[1] 3.124257",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#exercício",
    "href": "08-assimetria_curtose.html#exercício",
    "title": "8  Assimetria e Curtose",
    "section": "8.5 Exercício",
    "text": "8.5 Exercício\n\nCriar um conjunto de dados com distribuição normal com média 0 e desvio padrão 1 e n = 10000 que será atribuído ao um objeto denominado meusDados.\n\n\nset.seed(1234)\nmeusDados &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\nConstrua um histograma (Figura 8.5) com curva normal sobreposta:\n\n\nggplot() +\n  geom_histogram(aes(x = meusDados,\n                     y =after_stat(density)), \n                 bins = 20,\n                 fill='tomato',\n                 col=alpha('gray40',0.5)) + \n  geom_function(fun=dnorm,\n                args=list(mean=0,sd=1), \n                col='dodgerblue4',\n                lwd=1,\n                lty=2) + \n  labs(x='X',    \n       y='Densidade de probabilidade')+\n  scale_x_continuous(limits = c(-3, 3),\n                      n.breaks = 6) +\n  theme_bw() \n\n\n\n\n\n\n\nFigura 8.5: Histograma com curva normal\n\n\n\n\n\n\nObserve a skewness e a kurtosis\n\n\ne1071::skewness(meusDados)\n\n[1] 0.008609517\n\ne1071::kurtosis(meusDados)\n\n[1] -0.003450388\n\n\nComo era de se esperar, usando a rnorm(), a distribuição é um exemplo de distribuição normal, \\(skewness \\approx 0\\) e \\(kurtosis \\approx 0\\). Observe que a cada vez que os comandos forem executados, os resultados serão discretamente diferentes. Para evitar isso, deve-se usar set.seed(), veja a seção Seção 7.7.2. Faça o teste!\n\n\n\n\n1. Peat J, Barton B. Descriptive statistics. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 24–51. \n\n\n2. Meyer D, Dimitriadou E, Hornik K, Weingessel A, Leisch F, Chang C-C, et al. Package «e1071». The R Journal. 2019;1–67. \n\n\n3. Joanes D, Gill C. Comparing Measures of Sample Skewness and Kurtosis. Journal of the Royal Statistical Society. 1998;47(1):183–9. \n\n\n4. George D, Mallery P. Descriptive Statistics. Em: IBM SPSS Statistics 26 Step by Step: A Simple Guide and Reference. New York, NY: Taylor & Francis Group; 2020. p. 114–20.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#footnotes",
    "href": "08-assimetria_curtose.html#footnotes",
    "title": "8  Assimetria e Curtose",
    "section": "",
    "text": "Sobre os quantis, veja na Seção 6.3.3.4.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html",
    "href": "09-distAmostrais.html",
    "title": "9  Distribuições Amostrais",
    "section": "",
    "text": "9.1 Pacotes necessários para este capítulo\npacman::p_load(dplyr, \n               e1071, \n               ggplot2, \n               ggpubr, \n               kableExtra, \n               knitr, \n               readxl)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#distribuições-populacional-e-amostral",
    "href": "09-distAmostrais.html#distribuições-populacional-e-amostral",
    "title": "9  Distribuições Amostrais",
    "section": "9.2 Distribuições populacional e amostral",
    "text": "9.2 Distribuições populacional e amostral\nMétricas como a média, a mediana e o desvio padrão são medidas numéricas de resumo. Quando calculadas a partir de dados de uma amostra são denominadas estatísticas amostrais. Por outro lado, as mesmas medidas numéricas de resumo calculadas para dados populacionais são chamadas de parâmetros populacionais.\nUm parâmetro populacional é sempre uma constante, enquanto uma estatística de amostra é sempre uma variável aleatória. Como cada variável aleatória deve possuir uma distribuição de probabilidade, cada estatística de amostra possui uma distribuição de probabilidade. A distribuição de probabilidade de uma estatística de amostra é mais comumente chamada de distribuição amostral. Os conceitos abordados neste capítulo são a base da estatística inferencial.\n\n9.2.1 Distribuição populacional\nA distribuição populacional é a distribuição de probabilidade derivada das informações sobre todos os elementos de uma população.\nPara fins de raciocínio didático, o conjunto de dados de 1368 observações de puérperas e recém-nascidos da Maternidade-escola do Hospital Geral de Caxias do Sul, RS, será considerado uma população. O gráfico da Figura 8.1, da Seção 8.2.1, mostra a distribuição da altura das puérperas dessa ‘população’. Os parâmetros (\\(\\mu\\) e \\(\\sigma\\)) dessa “população” são:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\n media = mean(dados$altura, na.rm =TRUE)\n round(media, 3)\n\n[1] 1.598\n\n dp = sd(dados$altura, na.rm =TRUE)\n round(dp, 3)\n\n[1] 0.065\n\n\n\n\n9.2.2 Distribuição amostral\nConforme mencionado no início deste capítulo, o valor de um parâmetro da população é sempre constante. Por exemplo, para qualquer conjunto de dados populacionais, há apenas um valor para a média populacional, \\(\\mu\\).\nNo entanto, não se pode dizer o mesmo sobre a média amostral. Amostras diferentes do mesmo tamanho, retiradas da mesma população, produzem valores diferentes da média amostral, \\(\\bar{x}\\). O valor da média amostral, para qualquer amostra, dependerá dos elementos incluídos nessa amostra. Em decorrência, a média amostral é uma variável aleatória. Portanto, como outras variáveis aleatórias, a média amostral possui uma distribuição de probabilidade, que é mais comumente chamada de distribuição amostral da média.\nOutras estatísticas de amostra, como mediana, moda e desvio padrão, também possuem distribuições amostrais. Em geral, a distribuição de probabilidades de uma amostra é denominada de distribuição amostral.\nUsar a variável altura das puérperas da Maternidade do HGCS como a população de interesse é apenas uma estratégia didática. Raramente, na vida real, é possível obter dados da população inteira. Reunir essa informação costuma ser muito custoso ou impossível. Por essa razão, a prática é selecionar apenas uma amostra da população e a usar para compreender as suas características.\nA função slice_sample() do pacote dplyrextrairá uma amostra 1 de n = 30 da população. As funções mean() e sd() calcularão a média e o desvio padrão, repectivamente:\n\nset.seed(234)\namostra1 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia1 &lt;- mean(amostra1$altura, na.rm =TRUE)\ndp1 &lt;-  sd(amostra1$altura, na.rm =TRUE)\nprint(c(media1, dp1))\n\n[1] 1.59266667 0.06073875\n\n\nSe este processo for repetido várias vezes, a cada amostra aleatória 2, serão gerados médias e desvios padrão diferentes.\n\nset.seed(236)\namostra2 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia2 &lt;- mean(amostra2$altura, na.rm =TRUE)\ndp2 &lt;-  sd(amostra2$altura, na.rm =TRUE)\nprint(c(media2, dp2))\n\n[1] 1.60633333 0.06960397\n\n\nÀ medida que o número de amostras possíveis forem aumentando, elas constituem uma distribuição cuja média, média das médias, \\(\\bar{x}_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\). Essa distribuição, no caso da média, recebe o nome de distribuição amostral das médias.\nAgora, para exemplificar este conceito, serão geradas 5000 amostras e calculada a média de cada uma das amostras de n = 30 que constituirão a distribuição, mostrada no gráfico da Figura 9.1.\n\n# extraindo 5000 amostras\namostras5000 &lt;- rep (0, 5000)\nfor (i in 1:5000) {\n  amostra &lt;- dados %&gt;% dplyr::slice_sample (n = 30) \n  amostras5000 [i] &lt;- mean(amostra$altura)\n}\n\nMedia e desvio padrão das 5000 amostras:\n\nmu &lt;- round (mean (amostras5000), digits = 3)\nsigma &lt;- round (sd (amostras5000), digits = 3)\nprint(c(mu, sigma))\n\n[1] 1.598 0.012\n\n\n\n\n\n\n\n\n\n\nFigura 9.1: Distribuição amostral das médias de 5000 amostras de n = 30\n\n\n\n\n\nSe a média, \\(\\bar{x}_{\\bar{x}}\\), dessas 5000 amostras de n = 30, for comparada com a média populacional, \\(\\mu\\), observa-se que até 3 dígitos decimais não há uma diferença. Entretanto, o desvio padrão é bem menor (0.012) que o da população (0.065).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "href": "09-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "title": "9  Distribuições Amostrais",
    "section": "9.3 Erros amostrais e não amostrais",
    "text": "9.3 Erros amostrais e não amostrais\nAmostras diferentes selecionadas da mesma população darão resultados diferentes porque contêm elementos diferentes. Isso é evidente nas medias das amostra1 e amostra2, 1.593m e 1.606m, respectivamente, comparadas com a média da população igual a 1.598m .\n\nerro1 &lt;- abs(mean(amostra1$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nerro2 &lt;- abs(mean(amostra2$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nprint(c(erro1, erro2), digits = 2)\n\n[1] 0.0053 0.0084\n\n\nSe outras amostras forem extraídas, o resultado obtido de qualquer amostra geralmente será diferente do resultado obtido da população correspondente. A diferença entre o valor de uma estatística amostral obtida de uma amostra e o valor do parâmetro populacional correspondente, é chamado de erro amostral. Observe que essa diferença representa o erro amostral apenas se a amostra for aleatória e não houver nenhum erro não amostral. Caso contrário, apenas uma parte dessa diferença será devido ao erro amostral.\n\\[\nerro \\quad amostral = \\bar{x}_{i} - \\mu  \n\\]\nÉ importante lembrar que o erro amostral ocorre devido ao acaso. Não é possível evitar o erro amostral. É possível limitar o seu valor através da seleção de uma amostra adequada. Os erros que ocorrem por outros motivos, como erros cometidos durante a coleta, registro e tabulação dos dados, são chamados de erros não amostrais. Esses erros ocorrem, em geral, por causa de erros humanos e não por acaso.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#média-e-desvio-padrão-da-média",
    "href": "09-distAmostrais.html#média-e-desvio-padrão-da-média",
    "title": "9  Distribuições Amostrais",
    "section": "9.4 Média e desvio padrão da média",
    "text": "9.4 Média e desvio padrão da média\nA média e o desvio padrão calculados para a distribuição amostral da média são chamados de média (\\(\\mu_{\\bar{x}}\\)) e desvio padrão (\\(\\sigma_{\\bar{x}}\\)) da média. Na verdade, a média e o desvio padrão da média são, respectivamente, a média e o desvio padrão das médias de todas as amostras do mesmo tamanho selecionadas de uma população. O desvio padrão da média é, comumente, chamado de erro padrão da média (\\(\\sigma_{\\bar{x}}\\)).\nA média amostral, \\(\\bar{x}\\), é chamada de estimador da média da população, \\(\\mu\\). Quando o valor esperado (ou média) de uma estatística amostral é igual ao valor do parâmetro populacional correspondente, essa estatística amostral é considerada um estimador não enviesado, consistente.\nPara a média amostral \\(\\bar{x}\\), \\(\\mu_{\\bar{x}} = \\mu\\). Logo, \\(\\bar{x}\\), é um estimador imparcial de \\(\\mu\\). Esta é uma propriedade muito importante que um estimador deve possuir. No entanto, o desvio padrão da média, \\(\\sigma_{\\bar{x}}\\), não é igual ao desvio padrão, \\(\\sigma\\), da distribuição populacional (a menos que n = 1). O desvio padrão da média amostral é igual ao desvio padrão da população dividido pela raiz quadrada do tamanho amostral:\n\\[\n\\sigma_{\\bar{x}} = \\frac {\\sigma}{\\sqrt{n}}\n\\]\nA dispersão da distribuição amostral da média é menor do que dispersão da distribuição populacional correspondente, como mostrado acima. Em outras palavras, \\(\\sigma_{\\bar{x}} &lt; \\sigma\\). Isso é visível na fórmula do \\(\\sigma_{\\bar{x}}\\) . Quando n é maior que 1, o que geralmente é verdadeiro, o denominador em \\(\\frac {\\sigma}{\\sqrt{n}}\\) é maior que 1. Desta forma, \\(\\sigma_{\\bar{x}}\\) é menor que \\(\\sigma\\). O desvio padrão da distribuição amostral da média diminui à medida que o tamanho amostral aumenta.\nSempre que o n for grande, em geral &gt; 30 (1), pode ser assumido que a distribuição será uma curva normal e que o desvio padrão da amostra (s) é um estimador não enviesado do desvio padrão populacional (\\(\\sigma\\)). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\):\n\\[\nEP_{\\bar{x}} = \\frac {s}{\\sqrt{n}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#teorema-do-limite-central",
    "href": "09-distAmostrais.html#teorema-do-limite-central",
    "title": "9  Distribuições Amostrais",
    "section": "9.5 Teorema do Limite Central",
    "text": "9.5 Teorema do Limite Central\nNa maioria das vezes, a população da qual as amostras são extraídas não é normalmente distribuída. Em tais casos, a forma da distribuição amostral de X é inferida de um teorema muito importante chamado teorema do limite central. De acordo com este teorema para um grande tamanho de amostra (&gt; 30), a distribuição amostral da média é aproximadamente normal, independentemente da forma da distribuição da população (1). Esta aproximação tornar-se-á mais acurada à medida que aumenta o tamanho amostral:\n\na média da distribuição amostral, \\(\\mu_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\);\ndesvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}\\), é igual a \\(\\frac {\\sigma}{\\sqrt{n}}\\);\no erro padrão da média, \\(\\sigma_{\\bar{x}}\\), é sempre menor que o desvio padrão populacional, \\(\\sigma\\) (Figura 9.2).\n\n\n\n\n\n\n\n\n\nFigura 9.2: Erro padrão versus desvio padrão.\n\n\n\n\n\nAgora, será tomado como exemplo a variável renda, do conjunto de dados dadosMater.xlsx, que representa a renda familiar em salários mínimos (sm). Como foi feito anteriormente, suponha que essa variável seja a “população” de estudo. Ela tem as seguintes medidas resumidoras e de assimetria:\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\")\nresumo &lt;- dados %&gt;% \n  select (renda) %&gt;% \n  dplyr::summarise (media.sm = mean (dados$renda, na.rm = TRUE),\n                    dp.sm = sd(dados$renda, na.rm = TRUE),\n                    mediana.sm = median(dados$renda, na.rm = TRUE),\n                    assimetria = e1071::skewness(dados$renda),\n                    curtose = e1071::kurtosis(dados$renda))\nresumo\n\n# A tibble: 1 × 5\n  media.sm dp.sm mediana.sm assimetria curtose\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1     2.22  1.23       1.92       2.22    8.21\n\n\nO desvio padrão é grande em relação à média, com um coeficiente de variação de 55.1% e uma mediana &lt; média. Estas métricas junto com os coeficientes de assimetria e curtose apontam para a assimetria positiva da variável renda. O gráfico da Figura 9.3 confirma esta afirmação:\n\n\n\n\n\n\n\n\nFigura 9.3: Distribuição assimétrica positiva\n\n\n\n\n\nOs valores da média e do desvio padrão calculados para a distribuição de probabilidade dessa população fornecem os valores dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Esses valores são \\(\\mu\\) =2.22sm 3 e \\(\\sigma\\) =1.23sm.\nSe extrairmos múltiplas amostras dessa população, observa-se a modificação do formato da distribuição à medida que aumenta o tamanho amostral, se aproximando progressivamente do modelo normal, com um número grande de amostras.\nExtração de múltiplas amostras(1000)\n\namostras1000 &lt;- rep (0, 1000)\nfor (i in 1:1000) {\n  amostra.sm &lt;- sample (dados$renda, 30) \n  amostras1000 [i] &lt;- mean(amostra.sm)\n}\n\nMedia e desvio padrão das 1000 amostras\n\nmu &lt;- round (mean (amostras1000), digits = 3)\nsigma &lt;- round (sd (amostras1000), digits = 3)\nmd &lt;- round (median(amostras1000), digits = 3)\nprint(c(mu, sigma, md))\n\n[1] 2.231 0.236 2.224\n\n\nAssimetria e curtose\n\nb1 &lt;- e1071::skewness(amostras1000)\nb2 &lt;- e1071::kurtosis(amostras1000)\nprint(c(b1, b2))\n\n[1] 0.4357889 0.2241975\n\n\n\n\n\n\n\n\n\n\nFigura 9.4: Distribuição praticamente normal\n\n\n\n\n\nOu seja, extraindo-se 1000 amostras de n = 30 e calculando as mesmas métricas anteriores, observa-se que, embora a distribuição populacional original seja assimétrica, a distribuição amostral da média se aproxima bastante da distribuição gaussiana (Figura 9.4).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#sec-popamostra",
    "href": "09-distAmostrais.html#sec-popamostra",
    "title": "9  Distribuições Amostrais",
    "section": "9.6 Proporções populacional e amostral",
    "text": "9.6 Proporções populacional e amostral\nO conceito de proporção é o mesmo que o conceito de frequência relativa e o conceito de probabilidade de sucesso em um experimento binomial, discutidos anteriormente, na distribuição binomial.\nA frequência relativa de uma categoria ou classe dá a proporção da amostra ou população que pertence a essa categoria ou classe. Da mesma forma, a probabilidade de sucesso em um experimento binomial representa a proporção da amostra ou população que possui uma determinada característica.\nA proporção populacional, representada por p, é obtida considerando a razão entre o número de elementos em uma população com uma característica específica e o número total de elementos na população. A proporção amostral, denotada por \\(\\hat{p}\\) (pronuncia-se p-chapéu), fornece uma proporção semelhante para uma amostra.\n\\[\np = \\frac{X}{N} \\quad e \\quad \\hat{p}= \\frac{x}{n}\n\\] onde,\n\nN \\(\\to\\) número total de elementos em uma população\nn \\(\\to\\) número total de elementos em uma amostra\nX \\(\\to\\) número de elementos na população que possui determinada característica\nx \\(\\to\\) número de elementos na amostra que possui determinada característica\n\nComo no caso da média, a diferença entre a proporção amostral e a proporção populacional correspondente, determina o erro amostral, assumindo que a amostra é aleatória e nenhum erro não amostral foi cometido. Ou seja,\n\\[\nerro \\quad amostral = \\hat{p} - p\n\\]\nA distribuição amostral de uma proporção é a distribuição das proporções de todas as amostras possíveis de tamanho n retiradas de uma população. De acordo com o Teorema Central do Limite: * Considerando m o número de vezes que o processo de repetição das amostras de tamanho n, a média das proporções, quando m $m$, tende para a verdadeira proporção populacional; * A distribuição amostral das proporções segue aproximadamente uma distribuição normal.\nAssim,\n\\[E(\\hat{p})=\\mu_\\hat{p}\\]\n\\[Var(\\hat{p})=\\sigma^2_\\hat{p}=\\frac{\\hat{p}(1-\\hat{p})}{n}\\] Logo,\n\\[E(\\hat{p})=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nDessa forma, a distribuição amostral de \\(\\hat{p}\\) será:\n\\[\n\\hat{p} \\sim N(\\hat{p}, \\frac{\\hat{p}(1-\\hat{p})}{n})\n\\]\nQuando não conhecemos a proporção populacional p, pode-se usar \\(\\hat{p}\\) como estimativa dessa proporção, desde que as seguintes condições sejam satisfeitas:\n\n\\(n \\times p ≥5\\)\n\\(n \\times(1-p) ≥5\\)\n\nDessa forma, pode-se calcular probabilidades aproximadas por uma distribuição normal com média \\(μ = n \\times p\\) e \\(σ = \\sqrt{(n×p(1-p))}\\) (veja também Seção 7.8.2).\nConsiderando a “população” que está sendo usada neste capítulo, o conjunto de dados dadosMater.xlsx, será verificado a proporção de mulheres fumantes. Inicialmente, a variável fumo, que está como variável numérica, será transformada em fator, pois, na realidade, é categórica:\n\ndados$fumo &lt;- factor (dados$fumo, \n                      levels = c (1,2), \n                      label = c (\"sim\", \"não\"))\n\nA proporção de fumantes, frequência relativa (fr) é:\n\nfumo &lt;- with(dados, table(fumo))\nfr.fumo &lt;- prop.table(fumo)\nfr.fumo\n\nfumo\n      sim       não \n0.2200292 0.7799708 \n\n\nA saída retorna que a proporção de fumantes entre as mulheres desse arquivo é 0.22. Esta será considerada a proporção p da ‘população’. Agora, imagine que esse resultado fosse desconhecido. Então, para saber a qual a proporção de fumantes dessa ‘população’, seria necessário extrair uma amostra adequada. Foi selecionada uma amostra de n = 100 da ‘população’ alvo:\n\n set.seed(134)\n amostra.fumo &lt;- dados %&gt;% dplyr::slice_sample(n = 100)\n\nUsando a amostra.fumo, calcula-se a proporção de fumantes:\n\n tabagismo &lt;- with(amostra.fumo, table(fumo))\n fr &lt;- prop.table(tabagismo)\n fp &lt;- fr*100\n\n tab.fumo &lt;- cbind(n = tabagismo,\n                   fr = round(fr, 2),\n                   fp = round(fp, 2))\n tab.fumo\n\n     n  fr fp\nsim 20 0.2 20\nnão 80 0.8 80\n\n\nA proporção de uma amostra é uma variável aleatória: varia de amostra para amostra de uma forma que não pode ser prevista com certeza. O Teorema Central do Limite se aplica em proporções. À medida que novas amostras forem extraídas, o valor da proporção amostral \\(\\hat{p}\\) se aproxima da proporção populacional p. Na “população” p = 0,22; na amostra de n = 100, \\(\\hat{p}\\) = 0.2. Para amostras grandes, a proporção amostral tem distribuição aproximadamente normal com as seguinte características mencionadas acima em relação a \\(\\mu_\\hat{p}\\) e \\(\\sigma_\\hat{p}\\).\nComo verificar se uma amostra é grande?\nUma amostra é grande se o intervalo\n\\[\n[\\hat{p}-3 \\times \\sigma_\\hat{p} , \\quad \\hat{p}-3 \\times \\sigma_\\hat{p}]\n\\]\nestiver totalmente dentro do intervalo [0,1].\nNa prática, p não é conhecido, portanto, \\(\\sigma_\\hat{p}\\) também não é. Nesse caso, para verificar se a amostra é suficientemente grande, substitui-se o valor de p pelo valor conhecido de \\(\\hat{p}\\). Isso significa verificar se o intervalo\n\\[\n\\hat{p}-3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\\quad \\hat{p}+3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nencontra-se totalmente dentro do intervalo [0,1].\nTransportando os dados da amostra de gestantes, para a fórmula e usando o R para o cálculo, tem-se:\n\np.chapeu &lt;- tab.fumo[1,2]\nn &lt;- tab.fumo[1,1] + tab.fumo[2,1]\n\nli &lt;- p.chapeu - 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nls &lt;- p.chapeu + 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nprint(c(li, ls), digits = 3)\n\n[1] 0.08 0.32\n\n\nComo os limites ficam no intervalo [0, 1], chega-se à conclusão de que a amostra de n = 100 é aceitável para estimar a proporção populacional.\nComo exercício, verificar se uma amostra de n = 40 é aceitável.\n\n\n\n\n1. Pagano M, Gavreau K. The Central Limit Theorem. Em: Principles of Biostatistics. Second Edition. Pacific Grove, CA: Duxbury; 2000. p. 197–8.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#footnotes",
    "href": "09-distAmostrais.html#footnotes",
    "title": "9  Distribuições Amostrais",
    "section": "",
    "text": "Para que a cada nova amostragem retorne o mesmo conjunto de dados, é usado a função set.seed()(veja Seção 7.7.2.4)↩︎\nObserve que se modificou o número da “semente”↩︎\nSalários mínimos↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html",
    "href": "10-estimacao.html",
    "title": "10  Estimação",
    "section": "",
    "text": "10.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools, \n               dplyr,\n               ggplot2, \n               kableExtra,\n               knitr,\n               readxl, \n               Rmisc, \n               tidyr)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#sec-dadoscap10",
    "href": "10-estimacao.html#sec-dadoscap10",
    "title": "10  Estimação",
    "section": "10.2 Dados",
    "text": "10.2 Dados\nOs dados deste capítulo são os mesmo usados no Capítulo 9, incluídos no arquivo dadosMater.xlsx, considerando apenas os recém-nascidos a termo e a variável altura e pesoRN:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(altura, pesoRN)\nstr(dados)\n\ntibble [1,085 × 2] (S3: tbl_df/tbl/data.frame)\n $ altura: num [1:1085] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ pesoRN: num [1:1085] 3285 3100 3100 2800 3270 ...",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#introdução",
    "href": "10-estimacao.html#introdução",
    "title": "10  Estimação",
    "section": "10.3 Introdução",
    "text": "10.3 Introdução\nA estatística inferencial é a parte da estatística que usa os resultados da amostra para tomar decisões e tirar conclusões sobre a população de onde a amostra foi retirada. A estimação e o teste de hipóteses, tomados em conjunto, constituem a inferência estatística.\nEstimação é um procedimento pelo qual um valor ou valores numéricos são atribuídos a um parâmetro populacional com base nas informações de uma amostra. Na estatística inferencial, \\(\\mu\\) é chamada de média populacional e p é chamada de proporção populacional. Existem muitos outros parâmetros populacionais, como mediana, moda, variância e desvio padrão, como observado na Seção 9.2.2.\nSe houvesse possibilidade de realizar um censo (pesquisa incluindo toda a população de interesse), não haveria necessidade dos procedimentos de estimação. Seria equivalente ao que ocorre em uma eleição, basta contar os votos, para declarar os vencedores da eleição. No entanto, em saúde, realizar censo é um procedimento caro, demorado ou virtualmente impossível. Portanto, geralmente é utilizada uma amostra da população e calculada o valor das estatísticas da amostra apropriada. Baseado nessas estatísticas, é atribuído valores ao parâmetro.\nA estatística usada para estimar um parâmetro é chamada de estimador. Assim, a média da amostra, \\(\\bar{x}\\), é um estimador da média da população, \\(\\mu\\); e a proporção da amostra, \\(\\hat{p}\\), é um estimador da proporção da população, p. Estimativa é um valor que a função estimador assume.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "href": "10-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "title": "10  Estimação",
    "section": "10.4 Estimativa Pontual e Intervalo de Confiança",
    "text": "10.4 Estimativa Pontual e Intervalo de Confiança\nA partir do dataframe dados, serão calculados a média e o desvio padrão da variável pesoRN (peso dos recém-nascidos em g) que, para fins didáticos, serão considerados os parâmetros dessa “população”:\n\n mu &lt;- round(mean(dados$pesoRN, na.rm = TRUE))\n sigma &lt;- round(sd(dados$pesoRN, na.rm = TRUE))\n print(x = c(mu, sigma))\n\n[1] 3216  462\n\n\nA seguir, será extraída, dessa população, uma amostra de n = 30 1 e calculado os mesmas medidas resumidoras, que se constituirão nas estimativas da amostra:\n\nset.seed (1234)\namostra &lt;- dados %&gt;% slice_sample(n = 30)\n\n# Média amostral\nx_barra &lt;- round(mean(amostra$pesoRN, na.rm = TRUE))\n\n# Desvio padrão amostral  \ns &lt;- round(sd(amostra$pesoRN, na.rm = TRUE))\n\nprint(c(x_barra, s))\n\n[1] 3222  407\n\n\nO valor de 3222g é a média amostral, \\(\\bar{x}\\), usado como um estimativa da \\(\\mu\\), é denominado de estimativa pontual. Como já mencionado anteriormente, espera-se que cada amostra selecionada produza um valor diferente da estatística amostral. Assim, o valor atribuído a uma média populacional, \\(\\mu\\), com base em uma estimativa pontual depende de qual das amostras está sendo usada. Consequentemente, a estimativa pontual atribui um valor a \\(\\mu\\) que quase sempre difere da mesma.\nPara melhorar a precisão, usa-se uma estimativa de intervalo. Em vez de atribuir um único valor para o parâmetro populacional, é construído um intervalo, acrescentando ou subtraindo um valor, chamado de margem de erro, à estimativa pontual.\nEste procedimento é conhecido como estimação por intervalo e o intervalo construído, estabelecendo um limite inferior e um limite superior em torno da estimativa amostral, é denominado de intervalo de confiança. Desta forma, é possível afirmar que o intervalo de confiança, provavelmente, contém o parâmetro populacional correspondente (Figura 10.1).\n\n\n\n\n\n\n\n\nFigura 10.1: Intervalo de Confiança.\n\n\n\n\n\nA construção do intervalo de confiança depende da obtenção da margem de erro. Este processo necessita de dois fatores:\n\ndo desvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}=\\frac{\\sigma }{\\sqrt{n}}\\), que em decorrência do Teorema do Limite Central, pode ser escrito \\(EP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\\);\ndo nível de confiança (NC) atribuído ao intervalo.\n\nPrimeiro, quanto maior for o desvio padrão de \\(\\bar{x}\\), maior será a margem de erro subtraída e adicionada à estimativa pontual. Consequentemente, o intervalo de confiança se modifica de acordo com a margem de erro. Quanto maior a margem de erro mais amplo o intervalo de confiança.\nEm segundo lugar, a quantidade subtraída e adicionada à estimativa se modifica de acordo o nível de confiança. Para ter uma maior confiança, deve-se aumentar a margem de erro, de acordo com a probabilidade declarada. Quanto maior o nível de confiança, maior a probabilidade. O nível de confiança é mostrado como \\((1 - \\alpha) \\times 100\\)%, onde \\(\\alpha\\) é o nível de significância. Tradicionalmente, o valor de \\(\\alpha\\) é igual a 0,05, mas qualquer outro valor pode ser usado.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "href": "10-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "title": "10  Estimação",
    "section": "10.5 Estimação da média populacional: \\(\\sigma\\) conhecido",
    "text": "10.5 Estimação da média populacional: \\(\\sigma\\) conhecido\nA margem de erro para a estimativa da média populacional, \\(\\mu\\), quando se conhece o desvio padrão populacional,\\(\\sigma\\), e \\(n \\ge 30\\) ou, mesmo que \\(n &lt; 30\\), mas a população de onde amostra foi selecionada tem distribuição normal, é a quantidade que é subtraída ou adicionada ao valor da média da amostra, \\(\\bar{x}\\), para obter o intervalo de confiança para \\(\\mu\\). Desta forma, a margem de erro é igual a:\n\\[\nmargem \\quad de\\quad erro\\quad(me)= z_{(1-\\frac{\\alpha}{2})} \\times \\sigma_{\\bar{x}}\n\\]\nOu,\n\\[\nme = z_{(1-\\frac{\\alpha}{2})} \\times \\frac{\\sigma }{\\sqrt{n}}\n\\]\nLogo, o intervalo de confiança para a média populacional, \\(\\mu\\), para um nível de confiança (1 - \\(\\alpha \\times 100\\))%, é igual a:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  me\n\\]\nSe objetivo é construir um intervalo de confiança de 95%, a última equação passa a ser:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  z_{(0,975)} \\times me\n\\]\nOnde Z é o valor crítico para o nível de confiança escolhido, obtido da tabela de distribuição normal padrão, e me é a margem de erro (\\(z_{0,975} \\times erro \\quad padrao\\)). Um intervalo de confiança de 95% significa que a área total sob a curva normal entre dois pontos em torno da média populacional, \\(\\mu\\), é igual a 95%, ou 0,95. A área das caudas é \\(\\alpha\\), ou seja, cada cauda á igual a \\(\\frac{\\alpha}{2}\\) (Figura 10.2)).\n\n\n\n\n\n\n\n\nFigura 10.2: Intervalo de Confiança de 95%.\n\n\n\n\n\nPara encontrar o valor de Z para um nível de confiança de 95%, primeiro encontram-se as áreas à esquerda desses dois pontos, \\(z_1\\) e \\(z_2\\). Esses dois valores de Z serão iguais, mas com sinais opostos. A área total sob a curva é igual a 1. A área entre \\(z_1\\) e \\(z_2\\) é igual a \\(1 - \\alpha = 0,95\\).\nA área a esquerda de \\(z_1\\) é igual a 0,025 e a área a esquerda de \\(z_2\\) é igual a 1 – 0,025 = 0,975. No R, os valores \\(z_1\\) e \\(z_2\\) podem facilmente ser obtidos com a função qnorm():\n\nprint(c(qnorm(0.025),qnorm(0.975)), 3)\n\n[1] -1.96  1.96\n\n\nDessa maneira, para uma confiança de 95%, é usado um \\(Z = 1.96\\), onde:\n\\[\np(-1,96 \\le z \\le 1,96) = 0,95\n\\] Logo,\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\sigma_{\\bar{x}})\n\\] ou\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\frac{\\sigma}{\\sqrt{n}})\n\\]\n\n10.5.1 Cálculo do intervalo de confiança com \\(\\sigma\\) conhecido\nUsando a média dos pesos dos recém-nascidos da amostra (n = 30), \\(\\bar{x}\\)= 3222 g, e o desvio padrão populacional conhecido, \\(\\sigma\\)= 462 g, tem-se que o intervalo de confiança de 95% (IC95%), para o peso dos recém-nascidos a termo na ‘população’ de onde esta amostra é proveniente:\nDados do exemplo para o cálculo\n\n n &lt;- 30\n x_barra &lt;- 3222\n sigma &lt;- 462\n\nCom 95% de confiança a margem de erro é igual a 1,96 vezes o erro padrão da média:\n\n n &lt;- 30\n me &lt;- 1.96 * sigma/sqrt(n)\n round(me,2)\n\n[1] 165.32\n\n\nBasta, agora, adicionar e subtrair a margem de erro da média:\n\n lim_inf &lt;- x_barra - me\n lim_sup &lt;- x_barra + me\n ic95 &lt;- c(lim_inf, lim_sup)\n round(ic95, 1)\n\n[1] 3056.7 3387.3\n\n\nAssim, tem-se uma confiança de 95% de que a verdadeira média, esteja incluída no intervalo. O nome para isso é intervalo de confiança de 95% para a média populacional.\n\n\n10.5.2 Função para calcular IC com \\(\\sigma\\) conhecido\nO cálculo manual é simples, mas enfadonho, nos tempos dos computadores. Em decorrência, como o R não tem uma função para encontrar os intervalos de confiança para a média de dados com distribuição normal quando o desvio padrão da população é conhecido, foi criada uma função para cumprir essa ação. Ela necessita dos seguintes argumentos:\n\nx \\(\\to\\) conjunto de números da amostra\ns \\(\\to\\) desvio padrão populacional\nnc \\(\\to\\) nível de confiança. Padrão: nc = 0.95\n\n\nIC_z &lt;- function (x, s, nc = 0.975)\n{\n  `%&gt;%` &lt;- dplyr::`%&gt;%`\n   n &lt;- length(x)\n   me &lt;- abs(qnorm((1-nc)/2))* sigma/sqrt(n)\n   df_out &lt;- data.frame( tamanho_amostral = n, \n                         media_amostral = mean(x), \n                         margem_erro = me,\n                         'IC limite inferior'=(mean(x) - me),\n                         'IC limite superior'=(mean(x) + me)) %&gt;%\n    tidyr::pivot_longer(names_to = \"Medidas\", values_to =\"valores\", 1:5 )\n  return(df_out)\n}\n\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.95)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           165.\n4 IC.limite.inferior   3056.\n5 IC.limite.superior   3387.\n\n\nEssa função pode ser salva no seu diretório e, quando necessária, pode ser ativada com a função source(), como visto na Seção 4.8.1. Com essa função fica fácil alterar o nível de confiança, por exemplo, para 99%. Isso mudará o Z crítico para:\n\n alpha &lt;- 0.01\n p &lt;- 1-(alpha/2)\n p\n\n[1] 0.995\n\n z_critico &lt;- qnorm(p)\n round(z_critico, 2)\n\n[1] 2.58\n\n\nCom a função IC_z():\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.995)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           237.\n4 IC.limite.inferior   2985.\n5 IC.limite.superior   3458.\n\n\nObservando o IC95% e o IC99%, verifica-se que a amplitude do intervalo aumentou com o crescimento da confiança de 95% para 99%, porque houve um aumento na margem de erro (Figura 10.3).\n\n\n\n\n\n\n\n\nFigura 10.3: Comparação do IC95% e IC99%.\n\n\n\n\n\n\n\n10.5.3 Interpretação do intervalo de confiança\nSe fossem extraídas todas as possíveis amostras de n = 30 da população de recém-nascidos a termo e construído para cada uma delas um intervalo de confiança de 95% em torno de cada média amostral, espera-se que 95% desses intervalos incluirão a média populacional e 5% não incluirão.\nO IC95% informa sobre a precisão com que a média amostral estima a média populacional desconhecida 2.\nNa Figura 10.4, são mostradas 20 amostras diferentes de tamanho n = 30, dessa população. Junto aparecem os intervalos de confiança de 95% construídos em torno dessas amostras. Observa-se que apenas uma amostra (em vermelho) não inclui a média populacional (linha tracejada vertical em azul). Pode-se afirmar com 95% de confiança que se forem extraídas muitas amostras do mesmo tamanho de uma população e construído intervalos de confiança de 95% em torno das médias dessas amostras, 95% desses intervalos de confiança incluirão a média populacional.\n\n\n\n\n\n\n\n\nFigura 10.4: Intervalos de confiança de 95% que mostra 20 replicações simuladas de amostras de n = 30 do peso do recém-nascido. Apenas um intervalo (em vermelho) não inclui a média populacional (linha vertical azul).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "href": "10-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "title": "10  Estimação",
    "section": "10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido",
    "text": "10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido\nCom amostras pequenas, usar o modelo normal para construir intervalos de confiança, pode gerar um erro, pois os pressupostos do teorema do limite central não são respeitados. Quando o desvio padrão populacional, \\(\\sigma\\), é desconhecido e o tamanho amostral é pequeno (&lt; 30), a estimação da média populacional é feita usando a distribuição t.\n\n10.6.1 Distribuição t\nA distribuição t, desenvolvida por William Sealy Gosset, em 1908, é semelhante à distribuição normal. Como a curva de distribuição normal, a curva de distribuição t é unimodal, simétrica (em forma de sino) em torno da média e nunca encontra o eixo horizontal. A área total sob uma curva de distribuição t é 1 ou 100%. A curva da distribuição t é mais plana do que a curva de distribuição normal padrão. Em outras palavras, ela é mais achatada e mais espalhada. No entanto, conforme o tamanho da amostra aumenta, a distribuição t aproxima-se da distribuição normal padrão.\nO formato de uma curva de distribuição t particular depende do número de graus de liberdade. O número de graus de liberdade (gl) para uma distribuição t é igual ao tamanho da amostra menos um, ou seja, \\(gl=n-1\\), veja Seção 6.3.4.3.\nO número de graus de liberdade é o único parâmetro da distribuição t. Há uma diferente distribuição t para cada número de graus de liberdade, portanto, a distribuição t se constitui em uma família de distribuições (Figura 10.5).\n\n\n\n\n\n\n\n\nFigura 10.5: Curvas de distribuição t conforme o grau de liberdade comparadas à distribuição normal.\n\n\n\n\n\nDa mesma maneira que a distribuição normal padrão, a média da distribuição padrão t é 0. Entretanto, ao contrário da distribuição normal padrão, cujo desvio padrão é 1, o desvio padrão de uma distribuição t é \\(\\sqrt{\\frac{gl}{gl-2}}\\) , para gl &gt; 2, sempre é maior do que 1. Assim, o desvio padrão de uma distribuição t é maior do que o desvio padrão da distribuição normal padrão.\nOs valores de \\({t}_{crítico}\\) podem ser obtidos usando a função qt() que usa os seguintes argumentos:\n\np \\(\\to\\) probabilidade, igual a \\(1 - \\frac{\\alpha}{2}\\), considerando-se bicaudal e \\(1 - \\alpha\\) quando unicaudal;\ndf \\(\\to\\) graus de liberdade;\nlower.tail \\(\\to\\) lógico; se TRUE, informa a probabilidade da cauda inferior. O padrão é TRUE.\n\nAssim, o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é:\n\nalpha  &lt;-  0.05\np &lt;- 1 - (alpha/2)\ngl = 10\nt &lt;- qt(p = p, df = 10, lower.tail = TRUE)\nround(t, digits = 2)\n\n[1] 2.23\n\n\nA área compreendida entre \\(\\pm\\) 2.23$ é igual a 95% (Figura 10.6):\n\\[\np(-2,23\\le t\\le 2,23)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 10.6: Distribuição t com gl = 10, bilateral.\n\n\n\n\n\nQuando se considera apenas uma das caudas (unicaudal ou unilateral), o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é\n\nt1 &lt;- qt(p = 0.95, df = 10, lower.tail = TRUE)\nround(t1, digits = 2)\n\n[1] 1.81\n\n\nAssim, a área abaixo de 1.81 é igual a 95% (Figura 10.7).\n\\[\np(t\\le 1,81)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 10.7: Distribuição t com gl = 10, unilateral.\n\n\n\n\n\n\n\n10.6.2 Cálculo do intervalo de confiança com \\(\\sigma\\) desconhecido\nSerão utilizados nesta seção, os dados da altura de mulheres, obtidos na Seção 10.2. Suponha-se que os parâmetros sejam desconhecidos. Para estimar esses parâmetros, selecionou-se uma amostra de n = 30 desse conjunto dados. Tomando essa amostra, calcula-se a sua média e o seu desvio padrão:\n\nset.seed(2345)\namostra1 &lt;- dados %&gt;%\n  slice_sample(n = 30)\n\nx_barra1 &lt;- mean(amostra1$altura, na.rm = TRUE)\ns1 &lt;- sd(amostra1$altura, na.rm = TRUE)\nprint(round(c(x_barra1, s1),3))\n\n[1] 1.577 0.062\n\n\nA maneira mais intuitiva de estimar a média da população com base na amostra, é, simplesmente, calcular a média e o desvio padrão. Entretanto, para uma maior precisão, é sempre importante calcular o intervalo de confiança.\n\n10.6.2.1 Cálculo manual do IC\nQuando o desvio padrão da população (\\(\\sigma\\)) não é conhecido, pode-se usar o seu estimador que é o desvio padrão da amostra (s), respeitando os pressupostos (1). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\).\n\\[\nEP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\n\\]\nO intervalo de confiança para a \\(\\mu\\) para um nível de confiança (NC) de \\((1 – \\alpha) \\times100\\)% é igual a:\n\\[\nIC_{NC}(\\mu)\\rightarrow x\\pm (t_{({1-\\frac{alpha}{2})} } \\times \\frac {s}{\\sqrt{n}})\n\\]\nQuando o tamanho amostral é grande, o valor de t se aproxima do valor de z, portanto, em situações em que não se conhece o desvio padrão populacional, não há muita diferença se houver uma aproximação de t para z (Tabela 10.1).\n\n\n\n\nTabela 10.1: Comparação dos valores z e t(gl).\n\n\n\n\n\n\n\nz\nn\ngl\nt\n\n\n\n\n1,96\n5\n4\n2,57\n\n\n1,96\n10\n9\n2,23\n\n\n1,96\n30\n29\n2,04\n\n\n1,96\n50\n49\n2,01\n\n\n1,96\n100\n99\n1,98\n\n\n1,96\n200\n199\n1,97\n\n\n1,96\n500\n499\n1,96\n\n\n1,96\n1000\n999\n1,96\n\n\n\n\n\n\n\n\n\n\nA amostra1 de n = 30, \\(\\overline x\\) = 1.577m e \\(s\\) = 0.062m. Essas estimativas servirão para o cálculo do intervalo de confiança, usando uma distribuição t bicaudal e um nível de significância \\(\\alpha = 0,05\\).\n\nn1 &lt;-  length(amostra1$altura)\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\n# Graus de liberdade  \ngl &lt;- n1 - 1\n# Valor t crítico  \ntc &lt;-  qt(p, gl, lower.tail = TRUE)\n# Erro padrão\nEP1 &lt;- round(s1/sqrt(n1),3)\nprint(round(c(tc, EP1),3))\n\n[1] 2.045 0.011\n\n\nCom esses dados, calcula-se o intervalo de confiança de 95%:\n\nme1 &lt;- tc*EP1\nlim_inf &lt;- x_barra1 - me1\nlim_sup &lt;- x_barra1 + me1\nic95 &lt;- c(lim_inf, lim_sup)\nround(ic95, 2)\n\n[1] 1.55 1.60\n\n\n\n\n10.6.2.2 Cálculo usando uma função do R\nO R possui algumas funções que calculam o intervalo de confiança para variáveis numéricas, baseadas na distribuição t. Entre elas, a função CI(), incluída no pacote Rmisc. Esta função tem dois argumentos:\n\nx ⟶ vetor de dados;\nci ⟶ intervalo de confiança a ser calculado\n\n\nIC95 &lt;- CI(amostra1$altura, ci = 0.95)\nround(IC95, 2)\n\nupper  mean lower \n 1.60  1.58  1.55",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "href": "10-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "title": "10  Estimação",
    "section": "10.7 Intervalo de Confiança para uma proporção populacional",
    "text": "10.7 Intervalo de Confiança para uma proporção populacional\n\n10.7.1 Dados para estimar a proporção populacional\nAqui, será utilizada uma amostra aleatória de n = 60 do conjunto de dados dadosMater.xlsx (sem filtro para os recém-nascidos a termo) para estimar a proporção de mulheres fumantes.\n\nset.seed(2346)\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select (fumo) %&gt;% \n  slice_sample(n = 60)\n\nstr(dados)\n\ntibble [60 × 1] (S3: tbl_df/tbl/data.frame)\n $ fumo: num [1:60] 2 2 1 2 2 2 2 2 2 1 ...\n\n\nA seleção mostra que temos 60 observações da variável fumo e que a mesma está classificada como numérica (num), 1 e 2. Onde 1 representa as mulheres fumantes. A variável é categórica e deve ser transformada para fator.\n\ndados$fumo &lt;- factor (dados$fumo, \n                      levels = c (1,2), \n                      label = c (\"fumante\", \"não fumante\"))\n\n\n\n10.7.2 Cálculo da estimativa pontual da proporção\nNessa amostra, a proporção de fumantes é:\n\ntab &lt;- table(dados$fumo)\ntab\n\n\n    fumante não fumante \n         14          46 \n\ntabFumo &lt;- round (prop.table (tab), 3)\ntabFumo\n\n\n    fumante não fumante \n      0.233       0.767 \n\n\n\n\n10.7.3 Cálculo do intervalo de confiança para a proporção\nCálculo manual com aproximação normal\n1ª etapa: verificar a premissa de que quando a proporção populacional é desconhecida a proporção pontual (\\(\\hat p\\)) e o seu complemento (\\(\\hat q = 1 - \\hat p\\)) multiplicados, cada um, por \\(n\\), devem ser maior do que 5.\n\nn &lt;- length(dados$fumo)\n(tabFumo) * n\n\n\n    fumante não fumante \n      13.98       46.02 \n\n\nComo se observa, ambos os valores são maiores do que 5.\n2ª Etapa: O intervalo pode ser estimado pela distribuição normal e é necessário calcular o z_crítico:\n\nalpha &lt;- 0.05\np &lt;-  1 - alpha/2\nzc &lt;- qnorm (p, mean = 0, sd = 1)\nround(zc, 2)\n\n[1] 1.96\n\n\n3ª Etapa: Cálculo do erro padrão da proporção (\\(\\sqrt \\frac {\\hat p \\times \\hat q}{n}\\)) e da margem de erro (veja também a Seção 9.6):\n\n# Extração da proporção amostral do tabFumo\nprop &lt;- tabFumo [1]\n# Cálculo do EP amostral\nEP &lt;- sqrt((prop * (1 - prop))/n)\n# Cálculo da margem de erro(me)\nme &lt;- zc * EP\n# dados necessários para o cálculo do IC95%\nprint(c(prop, me), digits = 3)\n\nfumante fumante \n  0.233   0.107 \n\n\n4ª Etapa: Intervalo de confiança\n\nic_prop &lt;- c((prop - me), (prop + me))\nround(ic_prop, 3)\n\nfumante fumante \n  0.126   0.340 \n\n\nCálculo usando uma função\nO chamado Intervalo de Confiança Exato corrigem as deficiências da aproximação normal. O R tem uma função para este cálculo: BinomCI() do pacote DescTools(2). É preferível usar o método de Clopper e Pearson que fornece o IC exato.\nOs argumentos da função BinomCI() são:\n\nx \\(\\to\\) é o número de desfechos, sucessos;\nn \\(\\to\\) é o tamanho da amostra, número de ensaios;\np \\(\\to\\) probabilidade, hipótese nula; se ignorada o padrão é 0,50;\nconf.level \\(\\to\\) nível de confiança, o padrão é 0.95;\nmethod \\(\\to\\) possui vários métodos para calcular intervalos de confiança para uma proporção binomial como: “clopper-pearson” (exact interval), “wilson”, “wald”, “agresti-coull”, “jeffreys”, “modified wilson”, “modified jeffreys”, “arcsine”, “logit”, “witting”, “pratt”. O método padrão é o de “wilson”. Qualquer outro método, há necessidade de solicitar;\nsides \\(\\to\\) hipótese alternativa padrão “two.sided” (bilateral), mas pode ser “right” ou “left” (unilateral a direita ou a esquerda, respectivamente).\n\n\nx &lt;-  tab[1]\nIC &lt;- BinomCI (x, \n               n, \n               conf.level = 0.95, \n               method = \"clopper-pearson\")\nround(IC, 3)\n\n       est lwr.ci upr.ci\n[1,] 0.233  0.134   0.36\n\n\nObserve que existe uma pequena diferença entre os valores da aproximação normal e o exato, com o método de “clopper-pearson”\n\n\n\n\n1. Motulsky H. The Theory of Confidence Intervals. Em: Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking. Second Edition. New York, NY: Oxford University Press; 2010. p. 96–102. \n\n\n2. Signorell A et al. DescTools: Tools for Descriptive Statistics [Internet]. 2022. Disponível em: https://cran.r-project.org/package=DescTools",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#footnotes",
    "href": "10-estimacao.html#footnotes",
    "title": "10  Estimação",
    "section": "",
    "text": "Repetindo, é importante lembrar que toda vez que for extraída uma nova amostra, o resultado será um conjunto de números diferentes e, em consequência, a média será diferente. Por isso, se for importante repetir o mesmo resultado, deve-se usar a função set.seed(). Consulte a Seção 7.7.2.4.↩︎\nAnteriormente, mostrou-se a media populacional por uma questão didática. A regra é não se conhecer a média populacional, razão da importância do intervalo de confiança↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html",
    "href": "11-testeHipoteses.html",
    "title": "11  Teste de Hipóteses",
    "section": "",
    "text": "11.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr,\n               lsr,\n               pwr,\n               readxl,\n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#sec-dadosth",
    "href": "11-testeHipoteses.html#sec-dadosth",
    "title": "11  Teste de Hipóteses",
    "section": "11.2 Dados do exemplo",
    "text": "11.2 Dados do exemplo\nConsidere o mesmo arquivo dadosMater.xlsx, usado várias vezes neste livro e disponível para consulta na Seção 5.3. Após a leitura do arquivo com a função read_excel() do pacote readxl, serão filtrados os recém-nascidos a termo (37 a 42 semanas de gestação) e selecionadas as varáveis sexo e pesoRN. Considerando esses dados como uma “população” para fins didáticos, será extraída uma amostra de 200 observações e atribuido o resultado ao objeto dados:\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  dplyr::filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(sexo, pesoRN) %&gt;%\n  slice_sample(n = 200)\n\n\n11.2.1 Exploração e transformação dos dados\nInicialmente, para ter uma visão da estrutura dos dados, usa-se:\n\nstr(dados)\n\ntibble [200 × 2] (S3: tbl_df/tbl/data.frame)\n $ sexo  : num [1:200] 1 1 1 2 2 2 1 2 1 1 ...\n $ pesoRN: num [1:200] 3585 2675 3435 3480 2880 ...\n\n\nA seguir, transformar a variável sexo em fator:\n\ndados$sexo &lt;- factor(dados$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\nEste conjunto de dados fica restrito, portanto, a 200 casos, contendo duas variáveis sexo e pesoRN, necessárias neste capítulo e assim resumidas:\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr::summarise (n = n(),\n                    media = mean(pesoRN, na.rm = TRUE),\n                    dp = sd(pesoRN, na.rm = TRUE))\nresumo\n\n# A tibble: 2 × 4\n  sexo      n media    dp\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 masc    122 3321.  462.\n2 fem      78 3146.  476.\n\n\nEsta amostra de 122 meninos e 78 meninas, informa que os meninos têm, em média, 3321 g ao nascer e as meninas 3146 g. Esta diferença de peso entre os sexos pode ter ocorrido devido ao acaso. Portanto, há necessidade de realizar um teste de hipóteses para tomar uma decisão sobre o parâmetro populacional. Esta diferença é grande o suficiente para rejeitar a hipótese de igualdade entre os pesos e concluir que existe uma diferença real entre eles?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#introdução",
    "href": "11-testeHipoteses.html#introdução",
    "title": "11  Teste de Hipóteses",
    "section": "11.3 Introdução",
    "text": "11.3 Introdução\nNo capítulo anterior, foi discutido aspectos relacionados à estimação, que se constitui, junto com o teste de hipótese, em procedimentos básicos da estatística inferencial. Em um teste de hipóteses, testa-se uma teoria ou crença sobre um parâmetro populacional (1). Na maioria das vezes, como mencionado anteriormente, obtém-se informações a partir de uma amostra em função da impossibilidade ou dificuldade de se conseguir essas informações a partir da população. Portanto, extrapolar ou estender os resultados, obtidos de uma amostra, para a população, significa aceitá-los como representações adequadas da mesma.\nSabe-se que as estimativas amostrais diferem dos valores reais (populacionais) e o objetivo dos testes de hipóteses é estabelecer a probabilidade de essa diferença ser explicada pelo acaso. O teste de hipóteses fornece um sistema referencial para a tomada de decisão sobre a adequação ou não dos dados amostrais serem representativos de uma população. Este sistema referencial é a distribuição de probabilidade do evento observado (2).\nInicialmente é importante fazer uma distinção entre hipótese de pesquisa e hipótese estatística. Uma hipótese de pesquisa é uma afirmação que expressa a relação esperada entre as variáveis de um estudo científico. Ela é baseada em uma pergunta de pesquisa e serve para orientar a coleta e análise dos dados. Uma hipótese de pesquisa pode ser confirmada ou refutada pelos resultados do estudo. Um exemplo de hipótese de pesquisa é: “O tabagismo durante a gestação interfere sobre o peso dos conceptos”. Uma hipótese de pesquisa corresponde àquilo que se quer acreditar sobre o mundo. Uma hipótese estatística é uma afirmação relacionada aos parâmetros de uma população. Baseia-se em uma hipótese de pesquisa e serve para testar a validade da mesma usando técnicas estatísticas. Uma hipótese estatística pode ser aceita ou rejeitada com um certo nível de confiança. A hipótese estatística deve ter uma relação clara com as hipóteses de pesquisa Por exemplo: “A média de peso dos recém-nascidos de mães fumantes é menor do que o das não fumantes”; “A média de peso dos recém-nascidos masculinos é igual ao peso dos recém-nascidos femininos”, ou ainda, “A média de peso dos recém-nascidos masculinos é diferente do peso dos recém-nascidos femininos”. Todos esses exemplos são legítimos de uma hipótese estatística porque são afirmações sobre um parâmetro populacional e estão significativamente relacionados à hipótese de pesquisa.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#hipótese-nula-e-alternativa",
    "href": "11-testeHipoteses.html#hipótese-nula-e-alternativa",
    "title": "11  Teste de Hipóteses",
    "section": "11.4 Hipótese nula e alternativa",
    "text": "11.4 Hipótese nula e alternativa\nEm função da hipótese de pesquisa, mencionada anteriormente, foram gerados os dados do exemplo. A hipótese de pesquisa corresponde ao que se quer acreditar, “o sexo interfere no peso dos neonatos”. Para refutar ou não essa afirmação constrói-se um teste de hipótese para verificar se ela é compatível ou não com os dados disponíveis (3).\nNo teste de hipóteses (TH), existem dois tipos de hipóteses, definidas como:\nHipótese nula(\\(H_{0}\\)): hipótese que afirma a não existência de diferença entre os grupos e, portanto, a diferença observada é atribuível ao acaso. É a hipótese a ser testada, aquela que se busca afastar, demonstrando que é, provavelmente 1, falsa, não válida. É denotada como:\n\\[\nH_{0}: \\mu_{1}= \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2}=0\n\\]\nHipótese alternativa (\\(H_{1} \\quad ou \\quad H_{a}\\)): é a hipótese contrária, como o nome diz, alternativa à \\(H_{0}\\). Representa a posição de uma nova perspectiva, a conclusão que será apoiada se \\(H_{0}\\) for rejeitada. Ela supõe que realmente exista uma diferença entre os grupos. É a hipótese que o pesquisador pretende comprovar. É denotada, em geral, simplesmente como havendo uma diferença entre os grupos, sem indicar uma direção, hipótese bilateral ou bicaudal:\n\\[\nH_{1}: \\mu_{1} \\neq  \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} \\neq  0\n\\]\nOu, se houver uma suspeita, através de um conhecimento prévio, apontar uma direção para a diferença, ou seja, usar uma hipótese unilateral ou monocaudal. Neste caso existe duas possibilidade:\n\nUnilateral à direita:\n\n\\[\nH_{1}: \\mu_{1} &gt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &gt; 0\n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\le \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\le 0\n\\] 2)\n\nUnilateral à esquerda:\n\n\\[\nH_{1}: \\mu_{1} &lt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &lt; 0   \n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\ge \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\ge 0\n\\]\nA \\(H_{0}\\) e \\(H_{1}\\) são opostas e mutuamente exclusivas. No teste de hipótese calcula-se a probabilidade de obter os resultados encontrados caso não haja efeito na população, ou seja, caso a \\(H_{0}\\) seja verdadeira. Portanto, o TH é um teste de significância para a \\(H_{0}\\).\n\n11.4.1 Exemplo\nVoltando à hipótese de pesquisa, usando os dados da Seção 11.2, as hipóteses estatísticas seriam escritas da seguinte maneira, considerando uma hipótese alternativa bilateral.\n\\[\nH_{0}: \\mu_{peso_{masc}} = \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}}=0\n\\]\n\\[\nH_{1}: \\mu_{peso_{masc}} \\neq \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}} \\neq 0\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "href": "11-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "title": "11  Teste de Hipóteses",
    "section": "11.5 Escolha do teste estatítico e regra de decisão",
    "text": "11.5 Escolha do teste estatítico e regra de decisão\n\n11.5.1 Teste estatístico\nUsa-se um teste estatístico para testar as hipóteses estabelecidas. Este depende do tipo de distribuição da variável, por exemplo, teste z, teste t, teste F, qui-quadrado (\\(\\chi^2\\)). Cada teste fornece um valor para dirigir a decisão de rejeitar ou não a hipótese nula. Essa decisão depende da magnitude do teste valor. O nome para esse indicador, calculado para orientar a escolha, é estatística de teste. Para fazer isso, há necessidade de determinar qual seria a distribuição amostral da estatística de teste se a hipótese nula fosse realmente verdadeira. Depois de analisar esse valor, decide-se se a hipótese nula está correta ou, caso contrário, ela é rejeitada em favor da alternativa.\nÉ fundamental lembrar que cada teste estatístico tem suas características e seus pressupostos que devem ser analisados para garantir a validade das estatísticas de teste. Para uma boa parte deles, por exemplo, deve-se verificar se os dados se ajustam à distribuição normal (normalidade), a igualdade das variâncias (homocedasticidade), independência entre os grupos, tipo de correlação, etc.\n\n\n11.5.2 Regra de decisão\nRealizado o teste estaístico, para rejeitar ou não rejeitar a \\(H_{0}\\), partindo do pressuposto de que ela é verdadeira, há necessidade de determinar uma regra de decisão que permita uma declaração fundamentada. Essa regra de decisão cria duas regiões, uma região de rejeição e uma região de não rejeição da \\(H_{0}\\), demarcadas por um valor crítico.\nEste valor de referência é determinado pelo nível de significância, \\(\\alpha\\), e deve ser explicitamente mencionado antes de se iniciar a pesquisa, pois é baseado nele que se fundamentam as conclusões da mesma. O nível de significância corresponde a probabilidade de rejeitar uma hipótese nula verdadeira. Quando a hipótese alternativa não tem uma direção definida, a área de rejeição, \\(\\alpha\\), é colocada nas duas caudas (Figura 11.1), superior), dividindo a probabilidade (\\(\\frac {\\alpha}{2}\\)); quando houver indicação prévia de um sentido, a área de rejeição ficará a direita (Figura 11.1), inferior) ou a esquerda dependendo da direção escolhida.\n\n\n\n\n\n\n\n\nFigura 11.1: Regiões bicaudais (acima) e monocaudal à direita (abaixo) de rejeição e não rejeição da hipótese\n\n\n\n\n\nQuais valores exatos da estatística de teste deve-se associar à hipótese nula e quais valores exatos devem ser associados à hipótese alternativa? Para encontrar a região de rejeição, deve-se levar em consideração:\n\nA estatística do teste deve ser muito grande ou muito pequena para que a hipótese nula seja rejeitada;\n\nDistribuição da variável de teste, que depende da distribuição da população em estudo e do tamanho da amostra;\n\nNível se significância adotado, em geral, usa-se um \\(\\alpha\\) = 0,05, o que equivale a dizer que a região de rejeição abrange 5% da distribuição.\n\nÉ importante entender bem este último ponto. A região de rejeição corresponde aos valores da estatística de teste para os quais se rejeita a hipótese nula e a distribuição amostral em questão descreve a probabilidade de obtermos um determinado valor da estatística de teste se a hipótese nula for efetivamente verdadeira. Agora, suponha-se que foi escolhido uma região de rejeição que cobre 10% da distribuição amostral e que a hipótese nula é realmente verdadeira. Qual seria a probabilidade de rejeitar incorretamente a hipótese nula? Obviamente, a resposta é 10%! E o teste usado teria um nível \\(\\alpha\\) = 0,10. Ou seja, se a hipótese nula é verdadeira e for rejeitada, foi cometido um erro.\n\n11.5.2.1 Erros de decisão\nComo se observa, ao se tomar uma decisão existe a possibilidade de se cometer erros. O primeiro erro é denominado de erro tipo I e ocorre quando, baseado na regra de decisão escolhida, uma hipótese nula verdadeira é rejeitada. Nesse caso, tem-se um resultado falso positivo. Há uma conclusão de que existe um efeito quando na verdade ele não existe. A probabilidade de cometer esse tipo de erro é \\(\\alpha\\), o mesmo usado como nível de significância no estabelecimento da regra de decisão.\n\\[\nP(rejeitar \\quad H_{0}|H_{0} \\quad verdadeira) = \\alpha\n\\]\nQual o valor de \\(\\alpha\\) que pode representar forte evidencia contra \\(H_{0}\\), reduzindo a possibilidade de erro tipo I?\nO valor de \\(\\alpha\\) escolhido, apesar de arbitrário, deve corresponder a importância do que se pretende demonstrar, quanto mais importante, menor deve ser o valor de \\(\\alpha\\). Nesses casos, não se quer rejeitar incorretamente \\(H_{0}\\) mais de 5% das vezes. Isso corresponde ao nível de significância mais usado de 0,05 (\\(\\alpha = 0,05\\)). Em algumas situações também são utilizados 0,01 e 0,10. Como mencionado, o valor de \\(\\alpha\\) deve ser escolhido antes de iniciar o estudo.\nExiste uma outra possibilidade de erro, denominado de erro tipo II, que ocorre quando a hipótese nula é realmente falsa, mas com base na regra de decisão escolhida, não se rejeita essa hipótese nula. Nesse caso, o resultado é um falso negativo; não se conseguiu encontrar um efeito que realmente existe. A probabilidade de cometer esse tipo de erro é chamada de \\(\\beta\\).\n\\[\nP(não \\quad rejeitar \\quad H_{0}|H_{0} \\quad falsa) = \\beta\n\\]\nNa construção de um teste de hipótese, o erro tipo II é considerado menos grave que o erro tipo I. Entretanto, ele é bastante importante. Tradicionalmente, adota-se o limite de 0,10 a 0,20 para o erro tipo II.\nNa Figura 11.2 estão resumidas as possíveis consequências na tomada de decisão em um teste de hipótese (4).\n\n\n\n\n\n\n\n\nFigura 11.2: Tomada de decisão e erros.\n\n\n\n\n\n\n\n\n11.5.3 Exemplo (continuação)\nContinuando com o exemplo da Seção 11.4.1, aceita-se que os pesos dos recém-nascidos de ambas as amostras tenham distribuição normal e que as variâncias são semelhantes. Apesar de o desvio padrão (\\(\\sigma\\)) da população-alvo ser conhecido (462, 476g), será suposto que ele é desconhecido 2. Portanto, o teste t de amostras independentes será o teste escolhido como o teste estatístico. A hipótese alternativa é bilateral e o \\(\\alpha\\) = 0,05.\nA distribuição t é dependente dos grau de liberdade, que para duas amostras independentes é igual \\(gl=n_1+n_2-2\\). Para os dados em uso, tem-se:\n\n n1 &lt;- resumo$n[1]\n n2 &lt;- resumo$n[2]\n gl &lt;- n1 + n2 - 2\n gl\n\n[1] 198\n\n\nPara o nível de significância escolhido, o valor crítico de t para gl = 198 e uma hipótese alternativa bilateral pode ser obtido da seguinte maneira:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round(qt(p, gl),3)\ntc\n\n[1] 1.972\n\n\nA partir do cálculo do valor crítico de t, podemos estabelecer a regra de decisão para as hipóteses estatísticas:\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\n|t_{calculado}| \\ge |t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\nO teste t pode ser calculado no R, usando a função t_teste() do pacote rstatix. Esta função usa, entre outros, os seguintes argumentos:\n\ndata \\(\\to\\) dataframe contendo as variáveis da formula;\nformula \\(\\to\\) uma fórmula da forma x ~ grupo onde x é uma variável numérica que fornece os valores dos dados e grupo é um fator;\npaired \\(\\to\\) lógico; indicando se o teste é pareado. Padrão é FALSE;\nvar.equal \\(\\to\\) lógico: se TRUE, uma variância combinada é usada; caso contrário, a aproximação de Welch dos graus de liberdade é usada\nalternative \\(\\to\\) two.sided (padrão) ou greater ou less.\n\n\nteste &lt;- rstatix::t_test(data = dados, \n                         formula = pesoRN~sexo, \n                         alternative = \"two.sided\",\n                         detailed = TRUE)\nteste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.    group1 group2    n1    n2 statistic      p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     175.     3321.     3146. pesoRN masc   fem      122    78      2.56 0.0113\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída do teste mostra uma estatística de teste 3 igual a 2.563. Esta é maior do que o t_crítico = 1.972, consequentemente, rejeita-se a hipótese nula e conclui-se, com uma confiança de 95%, que existe uma diferença estatisticamente significativa no peso dos recém-nascidos entre os sexos. Esta diferença é em média igual a 175 g (IC95%: 40, 310), \\(peso_{meninos} &gt; peso_{meninas}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#sec-valorp",
    "href": "11-testeHipoteses.html#sec-valorp",
    "title": "11  Teste de Hipóteses",
    "section": "11.6 Valor P do teste",
    "text": "11.6 Valor P do teste\nNas seções anteriores, foi discutido um procedimento onde se encontrou o valor de probabilidade tal que uma dada hipótese nula é rejeitada ou não é rejeitada, de acordo com o nível de significância, \\(\\alpha\\), fixado, pelo pesquisador, no início da pesquisa.\nEssa abordagem do valor de probabilidade, mais comumente chamada de abordagem do valor P, fornece esse valor. Uma vez realizada a pesquisa, o pesquisador calcula a probabilidade de obter um resultado tão ou mais extremo que o observado, uma vez que a hipótese nula é verdadeira. O valor P também é conhecido como nível descritivo do teste (5).\nO objetivo de um teste estatístico é transformar em probabilidade a magnitude do desvio verificado em relação ao valor esperado, fornecendo o valor P. A partir daí pode-se, também, definir a regra de decisão, usando esse valor P. Toma-se o valor predeterminado (em geral, 0,05) de \\(\\alpha\\) e, então, compara-se o valor P com \\(\\alpha\\) e toma-se a decisão. Usando essa abordagem, rejeita-se a \\(H_{0}\\) se o valor P &lt; \\(\\alpha\\) e não se rejeita se o valor P &gt; \\(\\alpha\\). Costuma-se dizer que se o valor P &lt; \\(\\alpha\\), o resultado é significativo e não significativo quando P &gt; \\(\\alpha\\).\nUma boa parte dos pesquisadores, principalmente no início da carreira, ficam empolgados pelo conhecimento do valor P. Entretanto, deve ser sempre lembrado que encontrar o valor P não é o único foco da pesquisa. O foco deve estar dirigido ao tamanho do efeito (effect size). O valor P obtido pelo teste estatístico, vai informar apenas sobre a probabilidade de se cometer erro ao rejeitar ou não rejeitar a hipóteses nula.\n\n11.6.1 Exemplo (continuação)\nO teste realizado, t_test(), fornece o valor P = 0.0113. Este valor é menor do que \\(\\alpha\\) e leva as mesmas conclusões da Seção 11.5.3.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#poder-do-teste",
    "href": "11-testeHipoteses.html#poder-do-teste",
    "title": "11  Teste de Hipóteses",
    "section": "11.7 Poder do teste",
    "text": "11.7 Poder do teste\nO poder do teste estatístico é a probabilidade de que um teste de hipótese rejeite corretamente a hipótese nula quando uma hipótese alternativa específica é verdadeira. É denotado comumente por \\(1 - \\beta\\) e representa a capacidade de um teste para detectar um efeito, se esse efeito realmente existir. O poder varia de 0 a 1 e, à medida que o poder do teste aumenta, a probabilidade \\(\\beta\\) de cometer um erro tipo II diminui.\n\\[\nPoder \\quad do \\quad teste = P(rejeitar \\quad H_{0}|H_{0} \\quad falsa)\n\\]\n\nNa Figura Figura 11.3, visualiza-se o poder em verde mais escuro. Em um teste de hipótese, o valor \\(\\alpha\\) sempre é estabelecido com antecedência, que geralmente é definido como 0,05, de modo que a taxa de erro do Tipo I é definida antes mesmo de se iniciar o teste. Em seguida, pode-se calcular o valor crítico mínimo necessário para rejeitar \\(H_0\\). É possível traçar uma linha da distribuição da hipótese nula até a distribuição da hipótese alternativa e separar a área sob a curva em duas partes. Se o valor t calculado cair à esquerda da linha tracejada, não se consegue rejeitar \\(H_0\\) quando \\(H_1\\) for verdadeira e é cometido um erro do Tipo II. Se o valor calculado cair à direita, rejeita-se \\(H_0\\) quando \\(H_1\\) é verdadeira e a decisão é correta. Portanto, a área à direita da curva é o poder.\n\n\n\n\n\n\n\n\nFigura 11.3: Nível de significância, probabilidade de erro tipo II, poder e nível de confiança em um teste de hipótese e a região de rejeição da hipótese nula (à direita da linha vertical tracejada).\n\n\n\n\n\nO poder do teste depende de vários fatores, como:\n\nO nível de significância do teste, que é a probabilidade de rejeitar a hipótese nula quando ela é verdadeira (erro tipo I).\nA magnitude do efeito, que é a diferença entre o valor real do parâmetro e o valor considerado na hipótese nula.\nA variabilidade da população, que é medida pelo desvio padrão ou pela variância dos dados.\nO tamanho da amostra, que é o número de observações coletadas para o teste.\n\nEm geral, pode-se dizer:\n\nquanto maior o nível de significância, maior o poder do teste;\n\nquanto maior a magnitude do efeito, maior o poder do teste;\n\nquanto menor a variabilidade da população, maior o poder do teste;\n\nquanto maior o tamanho da amostra, maior o poder do teste.\nExistem diferentes métodos para calcular o poder do teste, dependendo do tipo de teste e da distribuição dos dados. Por exemplo, para um teste de uma média com variância desconhecida, usa-se a distribuição t de Student com \\(n - 1\\) graus de liberdade. Para um teste de duas proporções, usa-se a distribuição normal aproximada. A análise de poder é uma ferramenta útil para planejar um estudo e determinar o tamanho da amostra necessário para obter um poder desejado. Ela também pode ser usada para avaliar a qualidade de um estudo realizado e verificar se o teste foi capaz de detectar um efeito relevante.\n\n\n11.7.1 Exemplo (continuação)\nO teste t retornou um resultado significativo, com valor de t = 2.563 &gt; 1.972, com P = 0.0113. Um resultado significativo não informa sobre a magnitude do efeito. Para isso, lançamos mão do teste d de Cohen que pode ser calculado, usando a função cohensD() do pacote lsr:\n\nd &lt;- lsr::cohensD (data = dados, formula = pesoRN ~ sexo)\nd\n\n[1] 0.373996\n\n\nNa Seção 12.2.6.1, se entrará em maiores detalhes, por enquanto, será assumido que a magnitude do efeito é pequena.\nDe posse do valor do d de Cohen, é possível calcular, através da função pwr.t.test() do pacote pwr, o poder do teste estatístico. Os argumentos dessa função são:\n\nn \\(\\to\\) número de observações por amostra;\nd \\(\\to\\) magnitude do efeito, d de Cohen;\nsig.level \\(\\to\\) nível de significância (padrão = 0.05);\npower \\(\\to\\) poder do teste;\ntype \\(\\to\\) tipo de teste (one- , two- ou paired-samples);\nalternative \\(\\to\\) hipótese alternativa, deve ser “one-sided” ou “two-sided (padrão).\n\nO parâmetro que se quer calcular deve ser passado como NULL. Assim, o poder do teste estatístico do exemplo é:\n\npoder &lt;- pwr::pwr.t.test(n = 150,\n                         d = d,\n                         sig.level = 0.05, \n                         power = NULL,\n                         type = \"two.sample\",\n                         alternative = \"two.sided\")\npoder\n\n\n     Two-sample t test power calculation \n\n              n = 150\n              d = 0.373996\n      sig.level = 0.05\n          power = 0.8976869\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nA saída mostra que no lugar do NULL, aparece o poder do teste estatístico. Ou seja, o poder foi de 0.898 , consequentemente, como \\(\\beta = 1 – Poder\\), então, \\(\\beta\\) = 0.102.\nO poder geralmente é definido em 0,80 (ou 0,90). Isto significa que se existirem efeitos verdadeiros a serem encontrados em 100 estudos diferentes com 80% de poder, apenas 80 em 100 testes estatísticos irão realmente detectá-los. Se não for garantido poder suficiente, é possível que nenhum efeito seja detectado, por isso, deve-se calcular o tamanho amostral necessário, antes de iniciar qualquer estudo, para garantir o poder pretendido.\n\n\n\n\n1. Kelen GD, Brown CB, Ashton J. Statistical reasoning in clinical trials: hypothesis testing. Am J Emerg Med. 1988;1(1):52–61. \n\n\n2. Menezes RX de, Burattini MN. Testes de Hipótese e intervalos de Confiança. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n3. Guyatt G, Jaeschke R, Heddle N, et al. Basic statistics for clinicians: 1. Hypothesis testing. CMAJ: Canadian Medical Association Journal. 1995;152(1):27. \n\n\n4. Fletcher RH, Fletcher SW, Fletcher GS. Acaso. Em: Epidemiologia Clínica: Elementos Essenciais. Quinta Edição. Artmed Editora; 2014. p. 108–9. \n\n\n5. Menezes RX de, Burattini MN. Testes de Hipótese e intervalos de Confiança. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 225–41.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#footnotes",
    "href": "11-testeHipoteses.html#footnotes",
    "title": "11  Teste de Hipóteses",
    "section": "",
    "text": "Ter em mente que nunca se pode saber com total certeza se existe um efeito na população.↩︎\nEsta foi uma suposição inicial! Fingiu-se que os dados do arquivo dadosMater.xlsx como filtro para os recém-nascidos a termo é a “população”↩︎\nPara ver todas as estatísticas do teste, basta escrever teste$ e apertar a tecla TAB do teclado e surgirá um menu para escolha.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html",
    "href": "12-teste-t.html",
    "title": "12  Comparação entre duas médias",
    "section": "",
    "text": "12.1 Pacotes necessários para este capítulo\npacman::p_load(car, \n               effectsize, \n               ggpubr, \n               ggsci, \n               kableExtra,\n               knitr,\n               readxl, \n               rstatix, \n               tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#teste-t-para-amostras-independentes",
    "href": "12-teste-t.html#teste-t-para-amostras-independentes",
    "title": "12  Comparação entre duas médias",
    "section": "12.2 Teste t para amostras independentes",
    "text": "12.2 Teste t para amostras independentes\nO teste t de amostras independentes é usado para comparar duas médias de amostras de grupos não relacionados. Isso significa que há pessoas diferentes fornecendo escores para cada grupo. O objetivo desse teste é determinar se as amostras são diferentes uma da outra\n\n12.2.1 Dados usados nesta seção\nSuponha que. em uma determinada Universidade, tenham sido coletadas as notas de Bioestatística de uma turma de 40 alunos. Estes dados estão aqui. Salve o mesmo no seu diretório de trabalho para a leitura dos dados.\n\n12.2.1.1 Leitura dos dados\nPara a leitura dos dados, será usada a função read_excel() incluída no pacote readxl, que precisa ser instalado e carregado. Os dados serão recebidos por um objeto que será denominado de dados:\n\ndados &lt;- readxl::read_excel(\"dados/dadosNotas.xlsx\")\n\nPara visualizar os dados, pode-se usar a função str():\n\nstr(dados)\n\ntibble [40 × 2] (S3: tbl_df/tbl/data.frame)\n $ notas: num [1:40] 63.1 76.3 57.7 66.9 73.1 70.3 63.6 75.7 73.5 83 ...\n $ sexo : chr [1:40] \"F\" \"F\" \"F\" \"F\" ...\n\n\nObserva-se que existem 40 alunos, sendo 20 mulheres e 20 homens. A variável altura é uma variável numérica que corresponde a a nota centesimal e sexo é uma variável categórica, onde F são as mulheres e M os homens.\n\n\n12.2.1.2 Exploração e resumo dos dados\nInicialmente, a variável sexo será transformada em fator:\n\ndados$sexo &lt;- as.factor(dados$sexo)\n\nA seguir, calcular a média e o desvio padrão da variável notas de acordo com sexo, usando a função group_by () e summarise do pacote dplyr\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr:: summarise(n = n(),\n                    media = mean(notas, na.rm = TRUE),\n                    dp = sd(notas, na.rm = TRUE),\n                    mediana = median(notas, na.rm = TRUE),\n                    Q1 = quantile(notas,0.25, na.rm = TRUE),    \n                    Q3 = quantile(notas, 0.75, na.rm = TRUE),\n                    me = 1.96 * dp/sqrt(n)) \nresumo\n\n# A tibble: 2 × 8\n  sexo      n media    dp mediana    Q1    Q3    me\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 F        20  68.4  7.79    68.6  63.2  73.8  3.41\n2 M        20  59.9  7.34    59.2  54.3  63.5  3.22\n\n\nA saída informa que a média das notas das mulheres é 68.4, bem acima da notas dos homens, mostrando uma diferença de 8.5. Parece que o desempenho das mulheres em Bioestatística é melhor do que o dos homens!\nAlém do resumo numérico, é interessante construir um gráfico do tipo boxplot (Figura 12.1), usando o pacote ggplot2 (veja Seção 6.6) para observar a distribuição dos dados:\n\nggplot2::ggplot(data = dados, aes(x = sexo, \n                                  y = notas, \n                                  fill = sexo)) + \n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  labs (x = \"Sexo\", \n        y = \"Notas\") + \n  theme_bw() + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 12.1: Boxplot dos dados\n\n\n\n\n\nOs boxplot sugerem que as notas dos alunos diferem, de acordo o sexo.\n\n\n\n12.2.2 Definição das hipóteses estatísticas\nAs hipóteses comparam as médias dos dois grupos. Para um teste bicaudal, as hipóteses são escritas como:\n\\[\nH_{0}: \\mu_{F} = \\mu_{M}\n\\]\n\\[\nH_{1}: \\mu_{F} \\neq \\mu_{M}\n\\]\n\n\n12.2.3 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0.05. A distribuição t é dependente dos graus de liberdade, dados por:\nNo exemplo,\n\nn1 &lt;- resumo$n[1]\nn2 &lt;- resumo$n[2]\ngl &lt;- n1 + n2 - 2\ngl\n\n[1] 38\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl =38 para uma hipótese alternativa bicaudal é obtido com a função qt (p, df), onde \\(df = gl\\) e \\(p = 1 - \\alpha/2\\)\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round (qt((1-alpha/2), gl), 3)\ntc\n\n[1] 2.024\n\n\nPortanto, se\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\nt_{calculado}| \\ge t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\n\n\n12.2.4 Teste estatístico\nPara determinar se existe uma diferença estatisticamente significativa entre as médias das notas de dois grupos independentes, será usado o teste t para duas amostras independentes, também conhecido como teste t de Student, baseado na distribuição de mesmo nome.\n\n12.2.4.1 Lógica do teste t\nO teste t compara as médias de duas amostras independentes, usando o erro padrão como métrica da diferença entre essas médias. Quanto maior o valor de t , maior a probabilidade de que as amostras pertençam a grupos diferentes, ocorrendo nessas circunstâncias a rejeição da hipótese nula (1).\nCalcula-se o teste t com a seguinte equação:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{EP_{d}}\n\\]\nOnde \\(EP_d\\) é o erro padrão da diferença entre a médias \\(\\bar{x}_1 - \\bar{x}_2\\). Se a hipótese nula for verdadeira, as amostras foram retiradas da mesma população e, portanto, \\(\\mu_1 - \\mu_2 = 0\\). Assim, a equação fica:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{EP_d}\n\\]\nO erro padrão da diferença \\(\\bar{x}_1 - \\bar{x}_2\\) é calculado de maneiras diferentes:\n\nSe a variâncias nos dois grupos forem iguais, usa-se:\n\n\\[\nEP_d = \\sqrt{s_o^2(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\nOnde \\(s_o^2\\) é a variância combinada ou conjugada que é, simplesmente, a média ponderada das variância dos grupos:\n\\[\ns_0^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{(n_1 -1)+ (n_2-1)}\n\\] Quando os grupos têm o mesmo tamanho (\\(n_1 = n_2\\)), \\(s_o^2\\) é simplesmente a média aritmética da variância dos grupos:\n\\[\ns_0^2 = \\frac {s_1^2 + s_2^2}{2}\n\\]\n\\[\nEP_d = \\sqrt{\\frac{2 s_o^2}{n}}\n\\]\n\nSe as variâncias dos dois grupos forem diferentes:\n\n\\[\nEP_d = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\n\\]\nEsta explicação da lógica e dedução da estatística de teste serve para uma melhor compreensão de como o teste funciona, mas para executar um teste t não há necessidade disso, basta saber como encaminhar ao R e como interpretar o resultado fornecido por ele.\n\n\n12.2.4.2 Pressupostos do teste t\nO teste t assume que:\n\nAs amostras são independentes;\nDeve haver distribuição normal. Entretanto, quando as amostras são grandes (teorema do limite central), isso não é muito importante;\nExista homocedasticidade, ou seja, as variâncias dos grupos devem ser iguais.\n\nViolar o pressuposto de número 3 tem importância se os tamanhos dos grupos forem diferentes. Se os grupos tiverem o mesmo tamanho e a amostra for grande, este pressuposto torna-se menos importante, não preocupando muito se essa hipótese foi violada (2). O pressuposto tem mais importância em grupos pequenos e desiguais. Existe um teste, denominado teste t de Welch que corrige essa violação. É possível portanto, esquecer esse pressuposto e fazer o teste de Welch sempre.\nAvaliação da normalidade\nUma boa parte dos procedimentos estatísticos são testes paramétricos 1 com base na distribuição normal. Ou seja, se assume que a distribuição dos dados segue o modelo da distribuição normal. Se essa suposição não for atendida, a lógica por trás do teste de hipóteses pode ser violada.\nPode-se verificar a normalidade de maneira visual, observando o comportamento dos dados através de gráficos como o histograma (Figura 12.2) e o gráfico Q-Q (Figura 12.3). É útil também sobrepor uma distribuição normal no histograma, para fins de comparação com a distribuição normal. Além disso, nos histogramas, pode-se observar como as duas populações se sobrepõem.\n\nmu1 &lt;- resumo$media[1]\ndp1 &lt;- resumo$dp[1]\nmu2 &lt;- resumo$media[2]\ndp2 &lt;- resumo$dp[2]\n\nmasc &lt;- dados %&gt;% filter (sexo == \"M\")\n\nfem &lt;- dados %&gt;% filter (sexo == \"F\")\n\nggplot(dados) +                       \n  geom_histogram(aes(x = notas, fill = sexo,\n                     y = after_stat(density)), \n                 col= \"white\", \n                 alpha = 0.5, \n                 bins = 15) +\n  stat_function(data = masc,\n                fun = dnorm,\n                color = \"red\",\n                lty = \"dashed\",\n                lwd = 1,\n                args = list(mean = mu1,\n                            sd = dp1)) +\n  stat_function(data = fem,\n                fun = dnorm,\n                color = \"darkred\",\n                lty = \"dashed\",\n                lwd = 1,\n                args = list(mean = mu2,\n                            sd = dp2)) +\n  labs(x=\"Notas\", y=\"Densidade\") +\n  scale_fill_manual(values = c(\"pink3\", \"lightblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\nFigura 12.2: Histogramas das notas dos alunos de acordo com o sexo\n\n\n\n\n\nO gráfico QQ (ou gráfico quantil-quantil) desenha a correlação entre uma determinada amostra e a distribuição normal. Uma linha de referência de 45 graus também é plotada. Um gráfico Q-Q é um gráfico de dispersão criado plotando dois conjuntos de quantis um contra o outro. Se ambos os conjuntos de quantis vierem da mesma distribuição, observa-se os pontos formando uma linha aproximadamente reta.\nSe os valores caírem na diagonal do gráfico, a variável é normalmente distribuída. Os desvios da diagonal mostram desvios da normalidade. Para desenhar um gráfico Q-Q pode ser usado a função ggqqplot ()2 do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%.\n\nggqqplot(dados, x = \"notas\", color = \"sexo\") +\n  labs(y = \"Notas\",\n       x = \"Quantis teóricos\")\n\n\n\n\n\n\n\nFigura 12.3: Gráficos Q-Q\n\n\n\n\n\nObservando os gráficos, verifica-se que a variável notas tem uma distribuição visualmente normal aceitável em ambas populações, pois o histograma se ajusta à curva normal e os gráficos Q-Q mostram que os dados seguem aproximadamente a linha diagonal.\nOutra maneira de analisar a normalidade é verificar se a distribuição como um todo se desvia de uma distribuição normal comparável. Para isso, usam-se testes estatísticos de normalidade. Os dois principais são o teste de Shapiro-Wilk e o teste de Kolmogorov-Smirnov (K-S).\nEsses testes comparam os dados da amostra com um conjunto de valores normalmente distribuídos com a mesma média e desvio padrão. Se o teste não for significativo (P &gt; 0,05), informa-se que a distribuição da amostra não é significativamente diferente de uma distribuição normal. Se, no entanto, o teste for significativo (P \\(\\le\\) 0,05), a distribuição em questão será significativamente diferente de uma distribuição normal.\nO método de Shapiro-Wilk é amplamente recomendado para teste de normalidade (3), (4), (5).\n\nsw &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;%\n  rstatix::shapiro_test(notas)\nsw\n\n# A tibble: 2 × 4\n  sexo  variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 F     notas        0.973 0.812\n2 M     notas        0.972 0.802\n\n\nA saída mostra que ambos valores P do teste, 0.812 e 0.802, estão acima de 0,05, corroborando com a não rejeição da normalidade dos dados.\nHomogeneidade da Variância\nNa visualização da Figura 12.1, nos dois grupos de alunos, observa-se que há, entre os limites inferior e superior, uma dispersão das notas em torno da região central. Esta dispersão parece ser semelhante nos grupos. Isto sugere que haja homogeneidade das variâncias.\nPortanto, homogeneidade da variância é o pressuposto de que a dispersão das medidas é aproximadamente igual em diferentes grupos de casos, ou que a dispersão dos valores são aproximadamente iguais em pontos diferentes da variável preditora.\nAlém do aspecto visual, a homogeneidade da variância pode ser testada com o teste de Levene. Neste teste, a \\(H_{0}\\) é todas as variâncias são iguais. No R, a função que calcula o teste é leveneTest() do pacote car (6). Os argumentos são:\n\ny \\(\\to\\) variável de resposta para o método padrão ou um objeto lm ou fórmula. Se y for um objeto de modelo linear ou uma fórmula, as variáveis do lado direito do modelo devem ser todas fatores e devem ser completamente cruzadas;\ngroup \\(\\to\\) fator que define os grupos;\ncenter \\(\\to\\) O nome de uma função para calcular o centro de cada grupo; mean fornece o teste de Levene original; o padrão, median, fornece um teste mais robusto;\ndata \\(\\to\\) conjunto de dados para avaliar a formula.\n\n\nlevene &lt;- car::leveneTest(notas~sexo, \n                          center = mean, \n                          data = dados)\nlevene\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  0.4575 0.5029\n      38               \n\n\nA saída do teste de Levene retorna um valor P &gt; 0,05, confirma a impressão visual dos boxplots de que os grupos têm homogeneidade das variâncias, portanto a hipótese nula de igualdade das variâncias não pode ser rejeitada.\nUm outro teste que compara duas variância poderia ser usado. É o teste F que pode ser calculado com a função var.test() do pacote stats, incluído no R base. Seus argumentos pode ser consultados na ajuda do R.\n\nteste.Var &lt;- var.test(notas~sexo, alternative = \"two.sided\" , data = dados)\nteste.Var\n\n\n    F test to compare two variances\n\ndata:  notas by sexo\nF = 1.1274, num df = 19, denom df = 19, p-value = 0.7966\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4462295 2.8482624\nsample estimates:\nratio of variances \n          1.127377 \n\n\nA saída do teste permite uma conclusão igual ao teste de Levene, pois o valor P = 0.7966.\n\n\n12.2.4.3 Execução do teste t de Student\nOs pressupostos do teste não foram violados, portanto ele pode ser realizado com confiança. Será utilizado a função t_test() do pacote rstatix (7) para calcular o teste t para amostras independentes. Ele fornece uma estrutura compatível com operador pipe %&gt;% (pipe-friendly) para executar testes t de uma e duas amostras. Para consultar os argumentos, consulte a Seção 11.5.3 ou a ajuda do RStudio.\n\n teste &lt;- dados %&gt;% rstatix::t_test(formula = notas ~ sexo,\n                                    detailed = TRUE,\n                                    var.equal = TRUE)\n teste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1 group2    n1    n2 statistic       p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     8.51      68.4      59.9 notas F      M         20    20      3.56 0.00102\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída retorna a estimativa da diferença média (8.515), as estimativas das médias dos grupos (arredondadas), a estatística do teste (3.5586328) o valor P (0.00102), graus de liberdade (38)3 e outras métricas.\nTambém é possível ver os resultados do teste t , usando o objeto teste que os recebeu. Por exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa da diferença entre as médias.\n\nIC95 &lt;- round(c(teste$conf.low, teste$conf.high),3)\nIC95\n\n[1]  3.671 13.359\n\n\n\n\n\n12.2.5 Conclusão\nComo \\(|t_{calculado}|\\) = 3.559 &gt; \\(|t_{0,05;58}|\\) = 2.024, rejeita-se \\(H_{0}\\). Observa-se que o valor P é muito pequeno (0.00102) e, portanto, a diferença observada nas médias dos dois grupos deve ser assumida como significativa.\nAssim, pode-se admitir que as médias das notas são diferentes, com probabilidade de erro extremamente pequena. A estimativa da diferença média (\\(\\mu_1 - \\mu_2\\)) é fornecida pelo intervalo de confiança de 95% (3.671, 13.359). Observe que o valor zero não está contido no intervalo e isto confirma a não significância estatística da diferença.\nConcluindo, as notas de Bioestatística das mulheres e as notas dos homens são diferentes, a diferença (\\(\\mu_1 - \\mu_2\\)) encontrada é estatisticamente significativa (t = 3.559, gl = 38, P = 0.00102), com uma confiança de 95%.\nEsta conclusão pode ser visualizada em um gráfico (Figura 12.4) que exibirá a saída do teste t:\n\nConstruir dois boxplots, usando o ggplot2 com cores do New England Journal of Medicine (NEJM), do pacote ggsci . Atribuir a um objeto bp:\n\n\nbp &lt;- ggplot(dados, aes(x=sexo, y=notas)) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1)+\n    geom_boxplot(aes(fill = sexo),\n                 color = \"black\")+\n    scale_color_nejm() +\n    theme_bw() +\n    theme(legend.position=\"none\")\n\n\nAdicionar ao boxplot novos rótulos e os testes realizados:\n\n\n bp +\n   labs(x = \"Sexo\", \n        y = \"Notas\", \n        title = \"Notas de Bioestatística\",\n        subtitle = rstatix::get_test_label(stat.test = teste,\n                                           correction = \"none\",\n                                           detailed = TRUE,\n                                           type = \"expression\",\n                                           p.col = \"p\"))\n\n\n\n\n\n\n\nFigura 12.4: Boxplots comparando os dois grupos\n\n\n\n\n\n\n\n12.2.6 Tamanho do Efeito\nA significância estatística deve ter uma atenção relativa do pesquisador, pois ela apenas mede a probabilidade de rejeitar uma hipótese nula, uma vez que ela seja verdadeira. Ajudam a determinar, em uma pesquisa, a significância dos resultados encontrados em relação à hipótese nula, mas não informam nada em relação a magnitude do efeito. Por exemplo, mostra se determinado tratamento afeta as pessoas, mas não dizem quanto isso as afeta.\nO tamanho do efeito (effect size) é uma medida quantitativa da magnitude do efeito. Quanto maior o tamanho do efeito, mais forte é a relação entre duas variáveis. É possível observar o tamanho do efeito ao comparar dois grupos quaisquer para ver quão substancialmente diferentes eles são.\nNormalmente, em ensaios clínicos tem-se um grupo de tratamento e um grupo de controle. O grupo de tratamento é uma intervenção que se espera efetue um resultado específico. O valor do tamanho do efeito mostrará se a terapia teve um efeito pequeno, médio ou grande. Isso tem mais relevância do que simplesmente informar o tamanho do valor P.\n\n12.2.6.1 d de Cohen\nTambém conhecida como diferença média padronizada, o d de Cohen (8) (9) é uma medida adequada e bastante popular para encontrar a magnitude do efeito na comparação entre duas médias.\nPara calcular a diferença média padronizada se verifica a diferença entre as médias dos dois grupos e se divide pelo desvio padrão conjugado:\n\\[\nd = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_{o}}\n\\]\nOnde,\n\\[\ns_o =\\sqrt \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2 - 2}\n\\]\nVoltando ao exemplo das notas dos alunos de Bioestatística, o d de Cohen é calculado, usando a função cohensD() do pacote lsr que usa os seguintes argumentos:\n\nx \\(\\to\\) um vetor numérico de valores de dados, variável preditora;\ny \\(\\to\\) um vetor numérico de valores de dados, variável resposta;\nformula \\(\\to\\) Fórmula na forma variável resposta ~ grupo;\ndata \\(\\to\\) dataframe ou matriz;\nmethod \\(\\to\\) Qual versão da estatística d devemos calcular? Os valores possíveis são pooled(padrão), x.sd, y.sd, corrected, raw, paired e unequal.;\nmu \\(\\to\\) O valor “nulo” contra o qual o tamanho do efeito deve ser medido. Quase sempre é 0 (padrão); raramente especificado.\n\nAssim, o d de Cohen pode ser obtido da seguinte forma:\n\nd &lt;- lsr::cohensD (notas ~ sexo, data = dados)\nd\n\n[1] 1.125339\n\n\nBastante simples! Agora, como interpretar este resultado de d = 1,3 (arredondado)? Sua interpretação não é intuitiva, recomenda-se usar a Tabela 12.1 para interpretar (8).\n\n\n\n\nTabela 12.1: Tamanho do Efeito\n\n\n\n\n\n\n\nd Cohen\nInterpretação\n\n\n\n\n&lt; 0,2\ninsignificante\n\n\n0,2 &lt; 0.5\npequeno\n\n\n0.5 &lt; 0.8\nmédio\n\n\n&gt;= 0,8\ngrande\n\n\n\n\n\n\n\n\n\n\nAssim, as notas dos alunos diferem significativamente (P &lt; 0,0001) de acordo com a sexo, sendo que as mulheres têm notas mais altas do que os homens e a magnitude dessa diferença é grande (d = 1.13).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#teste-t-para-grupos-pareados",
    "href": "12-teste-t.html#teste-t-para-grupos-pareados",
    "title": "12  Comparação entre duas médias",
    "section": "12.3 Teste t para grupos pareados",
    "text": "12.3 Teste t para grupos pareados\nUm teste t pareado é usado para estimar se as médias de duas medidas relacionadas são significativamente diferentes uma da outra. Esse teste é usado quando duas variáveis contínuas são relacionadas porque são coletadas do mesmo participante em momentos diferentes (antes e depois), de locais diferentes na mesma pessoa ao mesmo tempo ou de casos e seus controles correspondentes.\n\n12.3.1 Dados usados nesta seção\nO banco de dados é constituído por uma amostra de 15 escolares portadores de asma não controlada. Fizeram avaliação da sua função pulmonar no início do uso de um novo corticoide inalatório. Após 60 dias, repetiram a avaliação da função pulmonar. Para baixar o banco de dados, clique aqui. Faça o downloado para o seu diretório de trabalho.\n\n12.3.1.1 Leitura e transformação dos dados\nLeia o arquivo dadosPar.xlsx a partir do diretório de trabalho, usando a função read_excel() do pacote readxl. Atribuir os dados a um objeto com o nome dados.\n\ndados &lt;- readxl::read_excel(\"dados/dadosPar.xlsx\")\n\nA estrutura dos dados podem ser visualizada, usando a função str():\n\nstr(dados)\n\ntibble [15 × 3] (S3: tbl_df/tbl/data.frame)\n $ id   : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ basal: num [1:15] 1.3 1.47 2.06 1.95 1.47 1.13 1.48 0.94 1.05 0.87 ...\n $ final: num [1:15] 1.53 1.63 2.35 2.7 2.01 1.53 1.66 1.59 1.5 1.61 ...\n\n\nO dataframe dados encontra-se no formato amplo (wide), ou seja, com as colunas basal e final colocadas lado a lado como se fossem duas variáveis distintas, quando, na realidade, constituem-se em apenas uma variável contendo as medidas de VEF1 (Volume Expiratório Forçado no primeiro segundo).\nA função pivot_longer() do pacote tidyr fará a transformação do formato amplo para o longo (long). Este processo não é obrigatório, mas será realizado para fins de treinamento. O novo banco de dados será atribuído ao objeto dadosL. A função pivot_longer() necessita dos seguintes argumentos:\n\ndados \\(\\to\\) dataframe a ser pivotado, tranformado;\ncols \\(\\to\\) colunas a serem transformadas no formato longo;\nnames_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos nomes das colunas de dados;\nvalues_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos valores das células;\n… \\(\\to\\) possui outros argumento. Ver ajuda.\n\n\ndadosL &lt;- dados %&gt;% \n  tidyr::pivot_longer(c(basal, final), \n                      names_to = \"momento\",\n                      values_to = \"medidas\")\nstr(dadosL)\n\ntibble [30 × 3] (S3: tbl_df/tbl/data.frame)\n $ id     : num [1:30] 1 1 2 2 3 3 4 4 5 5 ...\n $ momento: chr [1:30] \"basal\" \"final\" \"basal\" \"final\" ...\n $ medidas: num [1:30] 1.3 1.53 1.47 1.63 2.06 2.35 1.95 2.7 1.47 2.01 ...\n\n\n\n\n12.3.1.2 Medidas Resumidoras\nPara resumir as variáveis, serão usadas as funções group_by() e summarise() do pacote dplyr, aplicadas ao formato longo dadosL:\n\nresumo &lt;- dadosL %&gt;% \n  dplyr::group_by(momento) %&gt;% \n  dplyr::summarise(n = n (),\n                   media = mean(medidas, na.rm = TRUE),\n                   dp = sd (medidas, na.rm = TRUE),\n                   mediana = median (medidas, na.rm = TRUE),\n                   IIQ = IQR (medidas, na.rm =TRUE),\n                   ep = dp/sqrt(n),\n                   me = ep * qt(1 - (0.05/2), n - 1)) \nresumo\n\n# A tibble: 2 × 8\n  momento     n media    dp mediana   IIQ    ep    me\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 basal      15  1.31 0.427    1.26  0.48 0.110 0.236\n2 final      15  1.69 0.471    1.59  0.38 0.122 0.261\n\n\n\n\n12.3.1.3 Visualização dos dados\n\nTabela\n\nÉ possível exibir os dados, tanto o banco de dados dados como o dadosL, de uma maneira mais elegante (Tabela 12.2), usando a função kable() do pacote knitr 4 e a função kable_styling() do pacote kableExtra. A função kable () usa a função head() embutida. Ao executar os códigos, se não for especificado, é mostrado apenas 6 linhas. Será mostrado o formato amplo e todas as suas 15 linhas:\n\n\n\n\nTabela 12.2: Função pulmonar de 15 escolares asmáticos antes-e-depois \\ do uso de um corticoide inalatório\n\n\n\n\n\n\n\nId\nBasal\nFinal\n\n\n\n\n1\n1.30\n1.53\n\n\n2\n1.47\n1.63\n\n\n3\n2.06\n2.35\n\n\n4\n1.95\n2.70\n\n\n5\n1.47\n2.01\n\n\n6\n1.13\n1.53\n\n\n7\n1.48\n1.66\n\n\n8\n0.94\n1.59\n\n\n9\n1.05\n1.50\n\n\n10\n0.87\n1.61\n\n\n11\n0.75\n1.17\n\n\n12\n1.26\n1.30\n\n\n13\n1.21\n1.41\n\n\n14\n0.78\n1.00\n\n\n15\n1.99\n2.37\n\n\n\n\n\n\n\n\n\n\n\nGráficos\n\nApenas, por uma questão didática, serão apresentadas várias maneiras de mostrar os dados visualmente. Podem ser usados qualquer um dos tipos a seguir, pois todos dão, praticamente, a mesma informação.\nGráfico de barra de erro\n\nresumo %&gt;% \n  ggplot2::ggplot(aes(x=momento, y=media, fill=momento)) + \n  geom_bar(stat=\"identity\", width = 0.4, color=\"black\") +\n  geom_point() +\n  geom_errorbar(aes(ymin=media, ymax=media+me), width=0.1,\n                position=position_dodge(.9)) +\n  labs(title=\"Avaliação de um corticoide inalatório\", \n       x=\"Momento\", y = \"Volume Forçado em 1 seg (L)\")+\n  theme_classic() +\n  scale_fill_manual(values=c(\"cyan4\",\"cyan3\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +                    \n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\nFigura 12.5: Gráfico de barra de erro comparando o grupo antes-e-depois\n\n\n\n\n\nNesse gráfico (Figura 12.5), a altura da barra representa a média do Volume Forçado em 1 seg (VEF1) nos diferentes momentos (basal e final). O erro corresponde a margem de erro (me) a partir do ponto (média), ou seja, é o intervalo de confiança de 95%. O limite inferior do IC95% foi suprimido.\nBoxplot\n\ndadosL %&gt;% \n  ggplot2::ggplot(aes(x = momento, y = medidas, fill = momento)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot (outlier.color = \"red\", \n                outlier.shape = 1,\n                outlier.size = 1) +\n  scale_fill_manual(values = c(\"cyan4\",\"cyan3\")) +\n  ylab(\"Volume Forçado em 1 seg (L)\") +\n  xlab(\"Momento\") +\n  stat_summary(fun = mean, \n               geom = \"point\", \n               shape = 19, size = 2,  color=\"red\") +\n  theme_bw() + \n  theme(text = element_text(size = 12)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 12.6: Boxplots comparando o grupo antes-e-depois\n\n\n\n\n\nA altura da caixa dos boxplots (Figura 12.6)) é o intervalo interquartil (IIQ) e corresponde a 50% dos dados. A linha que corta horizontalmente a caixa é a mediana. Os bigodes da caixa (whiskers) em suas extremidades são os limites inferior e superior dos dados, excluindo os valores atípicos (outliers), representado no boxplot final por um ponto vermelho, acima do limite superior. Os pontos em vermelho (dentro das caixas) representam as médias.\nGráfico de linha\n\nresumo %&gt;%\n  ggplot2::ggplot(aes(x=momento, y=media, group=1)) +\n  geom_line(linetype ='dashed') +\n  geom_errorbar(aes(ymin=media - me,\n                    ymax=media + me),\n                width=0.1,\n                linewidth = 1,\n                col = c(\"cyan4\",\"cyan3\")) +\n  geom_point(size = 3, color = c(\"cyan4\", \"cyan3\")) +\n  theme_classic()+\n  labs(x='Momento',\n       y='Volume Forçado em 1 seg (L)')\n\n\n\n\n\n\n\nFigura 12.7: Gráfico de linha comparando o grupo antes-e-depois\n\n\n\n\n\nEste gráfico de linha (Figura 12.7) com representação da margem de erro tem a mesma interpretação do gráfico de barra de erro. A escolha do tipo de gráfico depende da ênfase do autor sobre os dados.\n\n\n12.3.1.4 Criação de uma variável que represente a diferença entre as médias\nA diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados):\n\ndados$D &lt;- dados$basal - dados$final\nhead (dados)\n\n# A tibble: 6 × 4\n     id basal final     D\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.3   1.53 -0.23\n2     2  1.47  1.63 -0.16\n3     3  2.06  2.35 -0.29\n4     4  1.95  2.7  -0.75\n5     5  1.47  2.01 -0.54\n6     6  1.13  1.53 -0.4 \n\n\nAtenção, agora, o banco de dados apresenta uma nova variável D, pois o foco do teste t pareado é essa diferença entre as médias, basal e final, a média das diferenças.\nResumo da variável D\nAo resumo será atribuído ao nome sumario (sem acento):\n\nresumoD &lt;- dados %&gt;% \n  dplyr::summarise(media = mean (D),\n                   dp = sd (D),\n                   mediana = median (D),\n                   IIQ = IQR (D),\n                   min = min (D),\n                   max = max (D))\nresumoD\n\n# A tibble: 1 × 6\n   media    dp mediana   IIQ   min     max\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 -0.377 0.218   -0.38 0.285 -0.75 -0.0400\n\n\nExiste uma diferença de 0.38L entre o VEF1 basal e o final. A pergunta que se faz é: Esta diferença tem significância estatística? Os gráficos sugerem que sim!\n\n\n\n12.3.2 Definição das hipóteses estatísticas\nSerá usado um teste bicaudal. Se a intervenção não produz efeito, então:\n\\[\nH_0: \\mu_D = 0  \n\\] Se a intervenção produz efeito, então:\n\\[\nH_1: \\mu_D \\neq 0\n\\]\n\n\n12.3.3 Regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição t que é dependente dos graus de liberdade. O número de graus de liberdade á igual ao número de observações menos 1, neste caso são o número de pares menos 1.\n\nn &lt;- length(dados$D)\ngl &lt;- n - 1\ngl\n\n[1] 14\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl = 14 para uma hipótese alternativa bicaudal:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\nround(qt(p, 14), 3)\n\n[1] 2.145\n\n\nPortanto, se\n\\[\n\\mid t_{calculado}\\mid &lt; \\mid t_{crítico}\\mid -&gt; não \\quad rejeitar \\quad H_{0} \\\\ \\mid t_{calculado}\\mid &gt; \\mid t_{crítico}\\mid -&gt; rejeitar \\quad H_{0}\n\\]\n\n\n12.3.4 Teste estatístico\n\n12.3.4.1 Lógica do teste\nA estatística do teste t dependente é a mesma do teste t independente r dada por:\n\\[\nT = \\frac{\\bar{D} - \\mu_{D}}{EP_{D}}\n\\]\nComo na equação do teste t para amostras independentes, sob a hipótese nula igual a zero, \\(\\mu_{D} = 0\\), assim, a equação fica:\n\\[\nT = \\frac{\\bar{D}}{EP_{D}}\n\\]\nA estimativa do erro padrão das diferenças é dada por:\n\\[\nEP_{D}=\\frac{s_{D}}{\\sqrt{n}}\n\\]\nO desvio padrão das diferenças, \\(s_{D}\\) , é dado por:\n\\[\ns_{D}=\\sqrt\\frac{\\Sigma(D_{i} - \\bar{D})^2}{n - 1}\n\\]\nOnde \\(D_{i}\\) são as diferença individuais (\\(x_1 - y_1, x_2 - y_2, ..., x_n - y_n\\)).\nDa mesma maneira que no teste t para grupos independentes, essa demonstração serve para uma melhor compreensão de como o teste funciona, mas para executar este teste t não há necessidade disso, basta saber como encaminhar ao R, como será visto adiante.\n\n\n12.3.4.2 Pressupostos do teste\nO teste t pareado assume que os seguintes pressupostos devem ser atendidos:\n\nOs dados devem ser dependentes;\nA variável desfecho deve estar em uma escala contínua;\nAs diferenças entre os pares devem ter distribuição normal.\n\nAo usar um teste t pareado, a variação entre os pares de medidas é a estatística mais importante e a variação entre os participantes, como no teste t de duas amostras independentes, é de pouco interesse, não havendo necessidade de se verificar se as variâncias dos grupos são iguais.\nPara testar o pressuposto de normalidade das diferenças, usa-se a variável criada da diferença entre os pares, D. Verifica-se a normalidade dessa variável com o teste Shapiro-Wilk, usando a função shapiro_test() do pacote rstatix, já usada no teste t de amostras independentes.\n\nshapiro &lt;- dados %&gt;% \n   rstatix::shapiro_test(D)\n shapiro\n\n# A tibble: 1 × 3\n  variable statistic     p\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 D            0.942 0.410\n\n\nO teste de Shapiro-Wilk retorna um valor P &gt; 0,05, mostrando que a variável D que não se pode rejeitar a hipóteses nula de sua normalidade.\nAlém disso, um gráfico Q-Q (Figura 12.8) pode ser usado para avaliar a normalidade, com a função ggqqplot() do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%\n\nggpubr::ggqqplot (dados$D, color = \"steelblue\") +\n  labs(y = \"Diferença Basal-Inicial\", \n       x = \"Quantis teóricos\") +\n  theme_bw() \n\n\n\n\n\n\n\nFigura 12.8: Gráfico Q-Q para avaliar a normalidade\n\n\n\n\n\nOs resultados do teste de Shapiro-Wilk e ográfico QQ, mostram que a \\(H_{0}\\) de normalidade da variável D não é rejeitada, apesar de haver uma pequena assimetria à esquerda que não impede o prosseguimento da análise.\n\n\n12.3.4.3 Execução do teste estatístico\nO cálculo do teste t pareado pode usar a mesma função do teste t para amostras independentes, t_test(), do pacote rstatix, mudando o argumento paired =FALSE(padrão) por paired =TRUE. Assim:\n\nteste_par &lt;- dadosL %&gt;% \n  rstatix:: t_test(formula = medidas ~ momento,\n                   paired = TRUE,\n                   detailed = TRUE) \nteste_par\n\n# A tibble: 1 × 13\n  estimate .y.     group1 group2    n1    n2 statistic         p    df conf.low\n*    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.377 medidas basal  final     15    15     -6.70 0.0000102    14   -0.497\n# ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObserve que foi usado o conjunto de dados de formato longo (dadosL) para usar a fórmula (x ~ grupo). Da mesma maneira do que o teste t para amostras independentes, é possível ver os resultados do teste t , usando o objeto teste_par que os recebeu.\nPor exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa de diferença (D) entre as médias\n\nIC95 &lt;- round(c(teste_par$conf.low, teste_par$conf.high),3)\nIC95\n\n[1] -0.497 -0.256\n\n\n\n\n\n12.3.5 Conclusão\nConclui-se que o VEF1 dos escolares asmáticos se modificou significativamente entre o início e após 60 dias do uso de um novo medicamento com uma confiança de 95%. A diferença (\\(\\mu_{basal} - \\mu_{final}\\)) encontrada é estatisticamente significativa (t = -6.6969, gl = 14, P = 1.02^{-5}), com uma confiança de 95%.\nObserve que o intervalo de confiança de 95% da diferença de -0.38 está todo abaixo de zero (-0.497, -0.256), confirmando a significância.\n\n\n12.3.6 Tamanho do Efeito\nO tamanho do efeito pode ser determinado, também, com o teste d de Cohen, usando a função cohensD() do pacote lsr:\n\nd_par &lt;- lsr::cohensD (dados$basal, dados$final)\nd_par\n\n[1] 0.8379499\n\n\nDessa forma, o uso do novo corticoide inalatório modificou significativamente o VEF1 dos escolares asmáticos com o uso de um novo corticoide inalatório (P = 1.02^{-5}), mostrando um aumento deste e que a magnitude dessa diferença é grande (d = 0.84).\nOs resultados podem ser apresentados usando um gráfico de linha (Figura 12.9)), aproveitando o resultado da função t_test().\n\nresumo %&gt;% \n    ggplot2::ggplot(aes(x=momento, y=media, group=1)) +\n    geom_line(linetype ='dashed') +\n    geom_errorbar(aes(ymin=media - me, \n                      ymax=media + me), \n                  width=0.1,\n                  size = 1,\n                  col = c(\"cyan4\",\"cyan3\")) +\n    geom_point(size = 2) +\n   labs(title=\"Avaliação do Uso de Corticosteroide Inalatório\",\n       subtitle = rstatix::get_test_label(stat.test = teste_par,\n                                          correction = \"none\",\n                                          detailed = TRUE,\n                                          type = \"expression\"),\n       x=\"Momento\", \n       y = \"Volume Expiratório Forçado em 1 seg (L)\",\n       caption = \"d Cohen = 0,84\")+\n   theme_bw() + \n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 12.9: Gráfico de linha comparando um grupo de escolares asmáticos antes e depois do uso de um corticosteroide inalatório\n\n\n\n\n\n\n\n\n\n1. Pagano M, Kimberly G. Comparison of Two Means. Em: Principles of Biostatistics. Second Edition. CRC Press; 2000. p. 262–72. \n\n\n2. Zimmerman DW. A note on preliminary tests of equality of variances. Br J Math Stat Psychol. 2004;57(1):173–81. \n\n\n3. Razali NM, Wah YB, et al. Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics. 2011;2(1):21–33. \n\n\n4. Ghasemi A, Zahediasl S. Normality tests for statistical analysis: a guide for non-statisticians. International journal of endocrinology and metabolism. 2012;10(2):486. \n\n\n5. Yap BW, Sim CH. Comparisons of various types of normality tests. Journal of Statistical Computation and Simulation. 2011;81(12):2141–55. \n\n\n6. Fox J, Weisberg S. An R Companion to Applied Regression [Internet]. Third. Thousand Oaks CA: Sage; 2019. Disponível em: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n7. Kassambara A. rstatix: Pipe-Friendly Framework for Basic Statistical Tests [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=rstatix\n\n\n8. Cohen J. Statistical power analysis for the behavioral sciences. 2nd Edition. Routledge; 1988. \n\n\n9. Lindenau JD, Guimaraes LSP. Calculating the Effect Size in SPSS. Revista HCPA [Internet]. 2012;32(3):363–81. Disponível em: https://seer.ufrgs.br/hcpa",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#footnotes",
    "href": "12-teste-t.html#footnotes",
    "title": "12  Comparação entre duas médias",
    "section": "",
    "text": "Teste paramétricos são testes estatísticos que se baseiam nos padrões da distribuição populacional da variável em estudo, por exemplo, a distribuição normal é descrita por dois parâmetros – média e desvio padrão – que são suficientes para se conhecer as probabilidades. Os testes que não requerem a especificação da forma de distribuição da população, ou seja, têm distribuição livre, são denominados de não paramétricos.↩︎\nVeja também a Seção 8.3.1.2.↩︎\nSe as variâncias forem diferentes (var.equal = FALSE), o teste calcula os graus de liberdade pela fórmula de Welch, bem mais complicada.↩︎\nVeja também a Seção 6.4.2.2.↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "13-anova.html",
    "href": "13-anova.html",
    "title": "13  Análise de Variância",
    "section": "",
    "text": "13.1 Pacotes necessários para este capítulo\npacman::p_load(car,\n               dplyr,\n               effectsize,\n               emmeans,\n               fastGraph,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               kableExtra,\n               knitr,\n               readxl,\n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#por-que-realizar-uma-anova",
    "href": "13-anova.html#por-que-realizar-uma-anova",
    "title": "13  Análise de Variância",
    "section": "13.2 Por que realizar uma ANOVA?",
    "text": "13.2 Por que realizar uma ANOVA?\nInicialmente, para analisar os grupos, se ficaria tentado a fazer comparações por pares usando um teste t de amostras independentes. Com existem quatro grupos, é possível compará-los realizando seis testes, grupo 1 versus grupo 2, grupo 1 versus grupo 3, grupo 1 versus grupo 4, grupo 2 versus grupo 3, grupo 2 versus grupo 4 e grupo 3 versus grupo 4. Se os dados têm k grupos são necessários \\(\\frac {k!}{2!(k-2)!}\\) testes.\nA probabilidade de um erro do tipo I não ocorrer para cada teste t é de 0,95 (isto é, 1 – 0,05), supondo um \\(\\alpha\\) = 0,05. Os três testes são independentes; portanto, a probabilidade de um erro do tipo I não ocorrer nos seis testes é de \\((0,95)^6 = 0,735\\). Dessa maneira, a probabilidade de ocorrer pelo menos um erro do tipo I nos seis testes t de duas amostras é de 1 – 0,735 ou 0,265 (26,5%), o que é mais alto do que o nível de significância definido de 0,05 (1).\nLogo, uma ANOVA de um fator é usada para verificar as diferenças entre vários grupos dentro de um fator, reduzindo assim o número de comparações em pares e a probabilidade de ocorrer um erro tipo I.\n\n13.2.1 Lógica do Modelo da ANOVA\nO procedimento de ANOVA é utilizado para testar a hipótese nula de que as médias de três 1 ou mais populações são as mesmas contra hipótese alternativa de que nem todas as médias são iguais.\nNa Seção 12.2.4.2, foram comparadas duas variâncias, usando um teste, denominado de teste F. Este teste, é uma razão entre duas variâncias e recebeu este nome em homenagem a Sir Ronald Aylmer Fisher (veja a Seção 1.2. A variância é uma medida de dispersão que mensura como os dados estão espalhados em torno da média (veja Seção 6.3.4.3). Quanto maior o seu valor, maior a dispersão.\nConsidere a Figura 13.1, onde está representada a distribuição de uma variável X em três grupos independentes. Pode-se, claramente, distinguir observações provenientes dessas distribuições, pois a sobreposição delas é pequena. Cada uma dela se dispersa pouco em torno da média.\n\n\n\n\n\n\n\n\nFigura 13.1: Três distribuições diferentes\n\n\n\n\n\nAgora, observe o Figura 13.2, onde a distribuição da variável X é mostrada, mantendo as mesmas médias, mas com variâncias maiores. Isto torna claro que se o objetivo é distinguir observações provenientes desses grupos não basta avaliar suas médias, há necessidade de comparar a variação entre os grupos com a variação dentro de cada grupo (2).\n\n\n\n\n\n\n\n\nFigura 13.2: Distribuições com mesmas médias da figura anterior, mas variâncias maiores\n\n\n\n\n\nSe a variação entre os grupos for grande quando comparada à variação dentro de cada grupo, aumenta a probabilidade de reconhecer a proveniência das observações (Figura 13.1). Entretanto, se a variação entre os grupos for pequena comparada à variação dentro do grupo, torna difícil a distinção de observações provenientes dos grupos (Figura 13.2).\nPortanto, usar o teste F para determinar se as médias de grupo são iguais é apenas uma questão de incluir as variâncias corretas na razão. Na ANOVA com um fator, a estatística F é a razão dos estimadores das variância entre e dentro dos grupos.\n\\[\nF = \\frac{variância \\quad ENTRE \\quad os \\quad grupos}{variância \\quad DENTRO \\quad dos \\quad grupos}\n\\]\nQuando o valor de F fica próximo de 1, significa que as variâncias são muito próximas; quando F é significativamente maior do que 1, é possível distinguir os indivíduos de diferentes grupos. Ou seja, se o objetivo for mostrar que as médias são diferentes, será bom que a variância dentro dos grupos seja baixa. Pode-se pensar na variância dentro do grupo como o ruído que pode obscurecer a diferença entre os sons (as médias). No gráfico da Figura 13.1, o valor de F seria alto; no da Figura 13.2, seria baixo.\nComo saber se o valor de F é alto o suficiente? Um único valor F é difícil de interpretar sozinho. Há necessidade de colocá-lo em um contexto maior antes que seja possível interpretá-lo. Para fazer isso, usa-se a distribuição F para calcular as probabilidades.\n\n\n13.2.2 Distribuição F\nA razão entre a variabilidade entre os grupos e a variabilidade dentro do grupo segue uma distribuição F quando a hipótese nula é verdadeira. Quando se realiza uma ANOVA com um fator obtém-se um valor F. No entanto, se forem extraídas várias amostras aleatórias do mesmo tamanho da mesma população e fosse repetida a mesma análise, o resultado seriam muitos valores F diferentes, constituindo uma distribuição amostral, denominada de distribuição F.\nDessa forma, como a distribuição F assume que a hipótese nula é verdadeira, é possível colocar o resultado de qualquer valor F, resultante do teste de ANOVA, e determinar quão consistente ele é com a hipótese nula e calcular a probabilidade. A probabilidade que se quer calcular é a probabilidade de observar uma estatística F que é pelo menos tão alta quanto o valor que o estudo obteve. Essa probabilidade permite determinar quão comum ou raro é o valor F, sob a suposição de que a hipótese nula é verdadeira. Se a probabilidade for pequena o suficiente, pode-se concluir que dados são inconsistentes com a hipótese nula. Como já foi mostrado em outros momentos, essa probabilidade é o valor P (Seção 11.6).\nO formato de uma curva de distribuição F depende do número de graus de liberdade. No entanto, a distribuição F tem dois números de graus de liberdade: graus de liberdade para o numerador (variância entre) e graus de liberdade para o denominador (variância dentro). Esses dois graus de liberdade são os parâmetros da distribuição F. Cada combinação de graus de liberdade fornece uma curva de distribuição F diferente. As unidades de uma distribuição F são denotadas por F, que assume apenas valores positivos. Como as distribuições normal, t e qui-quadrado (veja ?sec-qui), a distribuição F é uma distribuição contínua. A forma de uma curva de distribuição F é inclinada para à direita, mas a assimetria diminui à medida que o número de graus de liberdade aumenta, conforme observado na Figura 13.3.\n\n\n\n\n\n\n\n\nFigura 13.3: Distribuições F.\n\n\n\n\n\n\n13.2.2.1 Funções do R para trabalhar com a distribuição F\nNo R, existem quatro funções principais para trabalhar com a distribuição F:\n\ndf(x, gl1,gl2) calcula a densidade de probabilidade da distribuição F no ponto x; df1 e df2 são os graus de liberdade do numerador e denominador, respectivamente 2 ;\npf(x, gl1, gl2) calcula a função de probabilidade acumulada da distribuição F no ponto x;\nqf(p, gl1, gl2) calcula o quantil da distribuição F correspondente a uma probabilidade p;\nrf(n, gl1, gl2) gera n valores aleatórios da distribuição F com os parâmetros gl1 e gl2.\n\nEssas funções são úteis para resolver problemas de probabilidade envolvendo a distribuição F. Por exemplo, se o objetivo é saber qual é a probabilidade de uma variável aleatória F com 10 e 20 graus de liberdade no numerador e no denominador, respectivamente, ser menor que 1, pode-se usar a função pf() da seguinte forma:\n\n x &lt;- 1\n p &lt;- pf(x, 10, 20,lower.tail = TRUE)\n round(p, 3)\n\n[1] 0.524\n\n\nOu seja, ao se observar a curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20), a probabilidade abaixo de x = 1 é igual a 52,4%.\nPara calcular a densidade de probabilidade quando x = 1, pode-se usar a função df():\n\n x &lt;- 1\n d &lt;- df(x, 10, 20)\n round(d, 3)\n\n[1] 0.714\n\n\nA saída da função df() corresponde à altura da curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20) quando x é igual a 1.\nPara encontrar o valor da distribuição F(10,20) que corresponde ao percentil 50%, ou seja, o valor que deixa 50% da área da curva à esquerda, usa-se a função qf() com lower.tail=TRUE. Assim:\n\n p &lt;- 0.50\n x &lt;- qf(p, 10, 20, lower.tail = TRUE)\n round(x,3) \n\n[1] 0.966\n\n\nPara representar, graficamente, esse resultado, foi construido o gráfico da Figura 13.4 com a função shadeDist() do pacote fastGraph (3) , verifica-se que a área sob a curva abaixo de 0,97 é igual a 50%. Consulte a ajuda do RStudio para maiores detalhes dos argumentos da função.\n\n\n\n\n\n\n\n\nFigura 13.4: Área da curva da distribuição F (10,20) abaixo de x = 0,97 é igual a 50%\n\n\n\n\n\nPara gerar 100.000 valores aleatórios da distribuição F com gl1=10 e gl2=20,será usada a função rf(). Em seguida, plota-se um histograma (Figura 13.5) com curva da distribuição F sobreposta (linha vermelha) e compara-se com a função de densidade de probabilidade da distribuição F (curva verde da Figura 13.3).\n\nx &lt;- rf(100000, df1 = 10, df2 = 20)\nhist(x, \n     breaks = 'Scott', \n     freq = FALSE, \n     xlim = c(0,3), \n     ylim = c(0,1),\n     ylab = \"Densidade\",     \n     xlab = '', \n     main = 'Histograma para uma distribuição F(10,20)', \n     cex.main=0.9)\n\ncurve(df(x, df1 = 10, df2 = 20), \n      from = 0, \n      to = 4, \n      n = 5000, \n      col= 'red', \n      lwd=2, add = T)\n\n\n\n\n\n\n\nFigura 13.5: Histograma com curva sobreposta de uma distribuição F (10,20)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#anova-de-um-fator",
    "href": "13-anova.html#anova-de-um-fator",
    "title": "13  Análise de Variância",
    "section": "13.3 ANOVA de um fator",
    "text": "13.3 ANOVA de um fator\nA análise de variância (ANOVA) de um fator, também conhecida como ANOVA de uma via, é uma extensão do teste t independente para comparar duas médias em uma situação em que há mais de dois grupos. Dito de outra forma, o teste t para uso com duas amostras independentes é um caso especial da análise de variância de uma via.\nA ANOVA de um fator compara o efeito de uma variável preditora (variável independente, fator) sobre uma variável contínua (desfecho). Por exemplo, verificar se a intensidade do tabagismo na gestação (não fumantes, fumantes leves, moderados ou pesados) afetam o peso dos recém-nascidos Figura 13.6.\n\n13.3.1 Dados do exemplo\nPara testar a hipótese de que a intensidade do tabagismo materno tem efeito sobre o peso do recém-nascido, foram selecionados aleatoriamente 200 recém-nascidos classificados em quatro grupos de n = 50 cada grupo, conforme a quantidade de cigarros fumados por dia por suas mães.\n\nGrupo 1: recém-nascidos de mães não fumantes;\nGrupo 2: recém-nascidos de mães que fumavam até 10 cigarros/dia – categorizado como tabagismo leve;\nGrupo 3: recém-nascidos de mães que fumavam de 11 a 19 cigarros/dia – categorizado como tabagismo moderado;\nGrupo 4: recém-nascidos de mães que fumavam \\(\\ge\\) 20 cigarros por dia – categorizado como tabagismo pesado.\n\nEstes dados estão no arquivo dadosFumo.xlsx. Para baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho.\n\n13.3.1.1 Leitura dos dados\nA leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função str().\n\ndados &lt;- readxl::read_excel(\"dados/dadosFumo.xlsx\")\nstr (dados)\n\ntibble [200 × 3] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:200] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN: num [1:200] 3458 2723 4125 2905 3608 ...\n $ fumo  : num [1:200] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n13.3.1.2 Exploração e resumo dos dados\nComo a variável fumo encontra-se como uma variável numérica, será transformada em fator que é a sua verdadeira classe com 4 níveis.\n\ndados$fumo &lt;- factor (dados$fumo, \n                      ordered = TRUE,\n                      levels = c(1, 2, 3, 4),\n                      labels = c (\"não\", \n                                  \"leve\", \n                                  \"moderado\",\n                                  \"pesado\"))\n\nAs medidas resumidoras serão obtidas, usando as funções group_by () e summarise () do pacote dplyr.\n\nalpha = 0.05\nresumo &lt;- dados %&gt;%\n  dplyr::group_by(fumo) %&gt;%\n  dplyr::summarise(n = n(),\n                   media = mean(pesoRN, na.rm = TRUE),\n                   dp = sd (pesoRN, na.rm = TRUE),\n                   ep = dp/sqrt(n),\n                   me = qt ((1-alpha/2), n-1)*ep,\n                   IC_Inf = media - me,\n                   IC_sup = media + me)\nresumo\n\n# A tibble: 4 × 8\n  fumo         n media    dp    ep    me IC_Inf IC_sup\n  &lt;ord&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 não         50 3395.  405.  57.3  115.  3280.  3510.\n2 leve        50 3102.  431.  60.9  122.  2980.  3225.\n3 moderado    50 3151.  495.  70.0  141.  3011.  3292.\n4 pesado      50 2954.  443.  62.6  126.  2828.  3080.\n\n\n\n\n13.3.1.3 Visualização gráfica dos dados\nOs boxplots (Figura 13.6) são uma maneira interessante de visualizar os dados, principalmente com o pacote ggplot23 :\n\n\n\n\n\n\n\n\nFigura 13.6: Boxplots do impacto do tabagismo materno no peso ao nascer\n\n\n\n\n\nObserva-se que há uma tendência de o peso ao nascer diminuir à medida que quantidade de cigarros fumados aumenta. Entretanto, esta diferença pode ser pelo acaso.\n\n\n\n13.3.2 Definição das hipóteses estatísticas\nPara testar a igualdade entre as médias, \\(H_{0}: \\mu_{1} = \\mu_{2} =  \\mu_{3} =  \\mu_{4}\\), supondo homocedasticidade, isto é, as variâncias \\(σ_1^2=σ_2^2=σ_3^3=σ_4^2\\).\nA hipótese alternativa, \\(H_1\\), diz que, pelo menos, uma das médias é diferente das demais. Ela não é unilateral ou bilateral, é multifacetada porque permite qualquer relação que não seja todas as médias iguais. Por exemplo, a \\(H_1\\) inclui o caso em que \\(μ_1=μ_2=μ_3\\), mas \\(μ_4\\) tem um valor diferente.\n\n\n13.3.3 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), geralmente escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição F. O número de graus de liberdade total \\((n – 1)\\) é dividido em dois componentes:\n\nGrau de liberdade do numerador (ENTRE) é dado por \\(gl_{E} = k - 1\\), onde k é o número de grupos.\nGrau de liberdade do denominador (DENTRO ou residual) é dado por \\(gl_{D} = n - k\\), onde, \\(n = \\sum n_{i}\\).\n\nNo exemplo, para um \\(\\alpha = 0,05\\), tem-se:\n\nalpha &lt;- 0.05\nk &lt;-  length(resumo$media)\nn &lt;- nrow(dados)\nglE &lt;-  k - 1\nglE\n\n[1] 3\n\nglD &lt;- n - k\nglD\n\n[1] 196\n\n\nCom esses dados, usando a a função qf()calcula-se o valor crítico de F (Figura 13.7) que é igual:\n\nFc &lt;- qf(1 - alpha, glE, glD)\nround(Fc, 2)\n\n[1] 2.65\n\n\nPortanto, se\n\\[\n|F_{calculado}| &lt; |F_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0} \\\\F_{calculado}| \\ge F_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\n\n\n\n\n\n\n\n\nFigura 13.7: Curva da Distribuição F 3,196 = 2,65\n\n\n\n\n\n\n\n13.3.4 Teste Estatístico\nA estatística de teste é obtida calculando duas estimativas da variância populacional, \\(\\sigma^2\\): a variância entre os grupos (\\(s_{E}^2\\)) e a variância dentro dos grupos (\\(s_{D}^2\\)).\nA variância entre os grupos também é chamada de quadrado médio entre os grupos (\\(QM_{E}\\)) e é igual a soma dos quadrados entre (\\(SQ_{E}\\)) ou do fator dividida pelos graus de liberdade entre:\n\\[\nQM_{E} = \\frac{SQ_{E}}{gl_{E}}\n\\]\nA variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual (\\(QM_{D}\\)) e é igual a soma dos quadrados dentro dividida pelos graus de liberdade dentro:\n\\[\nQM_{D} = \\frac {SQ_{D}}{gl_{D}}\n\\]\nA variância entre os grupos, \\(QM_{E}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação entre as médias das amostras extraídas de diferentes populações. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{E}\\) será baseado nos valores das médias dos pesos dos recém-nascidos nos quatro grupos diferentes. Se as médias de todas as populações em consideração forem iguais, as médias das respectivas amostras ainda serão diferentes, mas a variação entre elas deverá ser pequena e, consequentemente, espera-se que o valor do \\(QM_{E}\\) seja pequeno. No entanto, se as médias das populações consideradas não são todas iguais, espera-se que a variação entre as médias das respectivas amostras seja grande e, consequentemente, o valor de \\(QM_{E}\\) seja grande.\nA variância dentro das amostras, \\(QM_{D}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação dos dados de diferentes amostras. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{D}\\) será baseado nas médias individuais dos pesos dos recém-nascidos incluídos nas quatro amostras retiradas de quatro populações. O conceito de \\(QM_{D}\\) é semelhante ao conceito de desvio padrão conjugado ou agrupado, \\(s_{o}\\), para duas amostras.\nA estatística de teste é, como visto, a razão das variâncias entre e dentro do grupo. Dessa maneira,\n\\[\nF = \\frac {s_{E}^2}{s_{D}^2} = \\frac {\\frac {SQ_{E}}{gl_{E}}}{\\frac {SQ_{D}}{gl_{D}}} = \\frac {QM_{E}}{QM_{D}}\n\\]\n\n13.3.4.1 Avaliação dos pressupostos do teste\nAo realizar um teste de ANOVA de um fator deve-se assumir que:\n\nAs populações das quais as amostras são retiradas são normalmente distribuídas;\nAs populações das quais as amostras são retiradas têm a mesma variância (homocedasticidade);\nAmostras aleatórias e independentes;\nTodos os grupos devem ter tamanho amostral adequado. Grupos com menos de 10 participantes são problemáticos por reduzirem a precisão da média. Na prática, deve-se evitar menos de 30 participantes. A relação entre os grupos não deve ser maior do que 1:4 (4);\nNão devem existir valores atípicos (outliers);\nA mensuração dos dados deve ser em nível intervalar ou de razão.\n\nPortanto, antes iniciar com o teste de hipótese, verifica-se se as suposições mencionadas para o teste de hipótese ANOVA unidirecional foram atendidas. As amostras são amostras aleatórias e independentes. Isto já é um bom começo!\nAvaliação da normalidade\nVerifica-se a premissa de normalidade, usando o teste de Shapiro-Wilk para os múltiplos grupos e desenhando um gráfico de probabilidade normal (gráficos Q-Q) para cada grupo.\n\n dados %&gt;% \n  dplyr::group_by(fumo) %&gt;% \n  shapiro_test(pesoRN)\n\n# A tibble: 4 × 4\n  fumo     variable statistic     p\n  &lt;ord&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 não      pesoRN       0.976 0.385\n2 leve     pesoRN       0.979 0.499\n3 moderado pesoRN       0.985 0.776\n4 pesado   pesoRN       0.971 0.257\n\n\nPara o gráfico Q-Q (Figura 13.8), pode ser usado a função ggqqplot () do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%.\n\nggpubr::ggqqplot(dados, \n                 x=\"pesoRN\", \n                 facet.by = \"fumo\") +\n  labs(y = \"Peso ao nascer (g) (m)\",\n       x = \"Quantis teóricos\")\n\n\n\n\n\n\n\nFigura 13.8: Gráficos Q-Q\n\n\n\n\n\nO resultado do teste de Shapiro-Wilk entregou todos os resultados com valor P acima de 0.05 e os gráficos Q-Q, não são perfeitos, mas pode-se assumir que os dados para cada grupo caem aproximadamente em uma linha reta.\nAvaliação da homogeneidade das variâncias\nEm seguida, testa-se a suposição de que as variâncias são iguais, usando o Teste de Levene através da função leveneTest () do pacote car.\n\ncar::leveneTest(pesoRN~fumo, center = mean, data = dados)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value Pr(&gt;F)\ngroup   3  0.6306 0.5961\n      196               \n\n\nO teste de Levene exibe como resultado um valor P &gt; 0,05, mostrando que não é possível rejeitar a \\(H_0\\) de igualdade das variâncias.\nVerificação da presença de outliers\nPode-se aqui, além de verificar nos boxplots, usar a função by_group() do pacote dplyr junto com a função identify_outliers() do pacote rstatix :\n\ndados %&gt;% \n  dplyr::group_by(fumo) %&gt;% \n  rstatix::identify_outliers(pesoRN)\n\n# A tibble: 1 × 5\n  fumo      id pesoRN is.outlier is.extreme\n  &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 pesado   168  1611. TRUE       FALSE     \n\n\nComo mostrado nos boxplots, existe um valor atípico, ou seja, está abaixo de 1,5 IIQ. Entretanto, ele não é extremo (&gt; 3 IIQ).\nDa mesma maneira que no teste t, os pressupostos têm mais importância em grupos pequenos e desiguais. Para o exemplo em análise, os pressupostos foram verificados e pode-se assumir que os grupos são independentes e as médias têm distribuição normal e existe homocedasticidade, além disso, os grupos têm o mesmo tamanho (n = 50). Portanto, a análise pode ser continuada.\nO que fazer se os pressupostos são violados?\nSe a homogeneidade da variância é o problema, um teste possível de ser implementado no R é o F de Welch, aplicando a funçãowelch.test(), incluída no pacote onewaytests (5). Existem também testes não paramétricos, como o Teste de Kruskal-Wallis, que será visto mais adiante (?sec-kuskalwallis).\n\n\n13.3.4.2 Execução do teste estatístico\nPara realizar um teste de hipótese ANOVA unidirecional, aplica-se a função aov() do R base. Esta função espera a chamada notação de fórmula, portanto, os dados são incluídos separando as duas variáveis de interesse separadas por ~ (til) e os dados, no qual as variáveis especificadas na fórmula, são encontradas. Além da fórmula e dos dados, a função aov() pode necessitar outros argumentos:\n\neffect.size \\(\\to\\) tamanho do efeito a ser calculado e mostrado nos resultados da ANOVA. Os valores permitidos podem ser “ges” (eta ao quadrado) ou “pes” (eta parcial ao quadrado) ou ambos. O padrão é “ges”;\ncontrasts \\(\\to\\) uma lista de contrastes a ser usada para alguns dos fatores da fórmula\n\n\nmodelo.aov &lt;- aov(pesoRN ~ fumo, dados)\n\nsummary(modelo.aov)\n\n             Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \nfumo          3  5030606 1676869   8.482 2.52e-05 ***\nResiduals   196 38748837  197698                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA saída, liberada pela função summary(), é bem reduzida, relatando as informações específicas da Tabela da ANOVA, a estatística F junto com o valor P e os graus de liberdade, soma dos quadrados (Sum Sq) e quadrados médios (Mean Sq), que com frequência se necessita para o relatório do modelo.\nA variância entre os grupos também é chamada de quadrado médio entre os grupos e é igual à soma dos quadrados entre ou do fator dividida pelos graus de liberdade entre. A variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual e é igual à soma dos quadrados dentro dividida pelos graus de liberdade dentro.\nA ANOVA detectou um efeito significativo do fator, que neste caso é o fumo, o valor \\(F_{calculado} = 8,48 &gt; F_{crítico} = 2,65\\) e o valor P &lt; 0,0001.\nPode-se simplesmente relatar isso e encerrar, mas é provável que se queira saber quais grupos diferem uns dos outros. Lembre-se de que não se pode apenas inferir isso a partir de uma visão dos dados, existem testes estatísticos para ajudar a entender as diferenças dos grupos.\n\n\n\n13.3.5 Testes post-hoc\nOs testes de comparações múltiplas constituem-se em uma análise após a realização da ANOVA. Se houve uma diferença, indicada pela ANOVA, os testes de comparações múltiplas ou também conhecidos como teste post hoc, ajudam a quantificar as diferenças entre os grupos para determinar quais grupos diferem significativamente uns dos outros.\nAqui será usado o HSD de Tukey, que é conservador. HSD vem da expressão em inglês - Honest Significant Difference. Este teste requer um objeto aov no qual executa seu procedimento, que chamaremos de pwc. O procedimento de Tukey HSD executará uma comparação de pares de todas as combinações possíveis dos grupos e testará esses pares para diferenças significativas entre suas médias, tudo enquanto ajusta o valor P a um limite superior de significância para compensar o fato de que muitos testes estatísticos estão sendo realizados e a probabilidade de um falso positivo aumenta com o aumento do número de testes. A função a ser usada é a tukey_hsd(), do pacote rstatix.\n\npwc &lt;- rstatix::tukey_hsd (modelo.aov)\npwc\n\n# A tibble: 6 × 9\n  term  group1   group2   null.value estimate conf.low conf.high      p.adj\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 fumo  não      leve              0   -292.     -523.     -61.8 0.00654   \n2 fumo  não      moderado          0   -243.     -474.     -12.8 0.0341    \n3 fumo  não      pesado            0   -441.     -671.    -210.  0.00000915\n4 fumo  leve     moderado          0     49.0    -181.     279.  0.946     \n5 fumo  leve     pesado            0   -149.     -379.      81.9 0.342     \n6 fumo  moderado pesado            0   -198.     -428.      32.8 0.121     \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nCom base nos valores P &lt; 0,05 tem-se três combinações de grupos que diferem: leve-não, moderado-não e pesado-não. Isto mostra que o grupo que difere é o das mães não fumantes.\nPode-se visualizar isso na Figura 13.9 obtida com a função plot(), usando os resultados da função TukeyHSD() disponível no R base. Esta função gera o teste de Tukey com as diferença entre os pares e os intervalos de confiança que permitem a construção do gráfico. A função par()é empregada para adaptar as margens da figura ao tamanho da mesma e depois é usada novamente para retornar ao padrão par(mar=c(5.1, 4.1, 4.1, 2.1)). O argumento mar é um vetor numérico que define os tamanhos das margens na seguinte ordem: inferior, esquerda, superior e direita.\n\npar(mar=c(3,8,3,3)) # Adaptar o tamanho das margens\nplot(TukeyHSD(modelo.aov, conf.level = 0.95), las = 1)\npar(mar=c(5.1, 4.1, 4.1, 2.1)) # Retorna as margens ao padrão\n\n\n\n\n\n\n\nFigura 13.9: Gráficos do Teste de Tukey\n\n\n\n\n\n\n\n13.3.6 Tamanho do efeito\nUma das medidas de tamanho de efeito mais comumente relatadas para a ANOVA é o eta ao quadrado (\\(\\eta^2\\)), que é um índice da força da associação entre um fator e uma variável dependente. Eta ao quadrado é a proporção da variação total atribuível ao fator. É calculado como a razão da variância do fator para a variância total e os valores variam de 0 a 1.\nEsta medida pode ser obtida com o pacote effectsize (6), usando a função eta_squared()com um objeto da classe tipo modelo.aov.\n\neffectsize::eta_squared (modelo.aov, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\nfumo      | 0.11 | [0.05, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nO eta quadrado é uma estimativa tendenciosa da força da associação, na medida em que superestima os efeitos, especialmente para amostras pequenas. Uma outra medida do tamanho do efeito menos tendenciosa é o ômega ao quadrado (\\(\\omega^2\\)). O ômega ao quadrado é uma medida corrigida, menos enviesada e menos inflacionada. Ela pode ser calculada com a função omega_squared(), também do pacote effectsize:\n\neffectsize::omega_squared (modelo.aov, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 |       95% CI\n---------------------------------\nfumo      |   0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nApesar de ser controverso, pode-se seguir a orientação da Tabela 13.1), para a interpretação (7):\n\n\n\n\nTabela 13.1: Interpretação do tamanho do efeito\n\n\n\n\n\n\n\nResultado\nEffectsize\n\n\n\n\n0.01\npequeno\n\n\n0,06\nmédio\n\n\n0,14\ngrande\n\n\n\n\n\n\n\n\n\n\n\n\n13.3.7 Conclusão\nO peso dos recém-nascidos foi estatisticamente diferente entre os diferentes grupos, F(3, 196) = 8,48, P = 0.0000252, \\(\\eta^2\\) = 0,11.\nAs análises post-hoc de Tukey revelaram que o peso dos recém-nascidos a termo no grupo das gestantes não fumantes apresentou uma diferença estatisticamente significativa do grupo de tabagismo leve (-292 g, IC95%: -523 a -62 g; P = 0,0065); do grupo de tabagismo moderado (-243 g, IC95%: -474 a -13 g; P = 0,0341) e do grupo de tabagismo pesado (-441 g, IC95%: -671 a -210 g; P &lt; 0,0001), mas entre os grupos de fumantes não houve diferença estatisticamente significativa.\n\n13.3.7.1 Apresentação dos resultados\nSerão apresentados boxplots (Figura 13.10)), com ggboxplot(), do pacote ggpubr, utilizando, para cores, a pallete = \"jama\", do pacote ggsci. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix.\n\ntab.aov &lt;- anova_test(dados, \n                       pesoRN ~ fumo, \n                       type = 2)\n \npwc &lt;- tukey_hsd(dados,pesoRN~fumo)\npwc &lt;- pwc %&gt;% add_xy_position (x = \"fumo\")\np &lt;- ggplot2::ggplot(dados, aes(x=fumo, y=pesoRN)) +\n  stat_boxplot(geom = \"errorbar\", \n               width = 0.1) + \n  geom_boxplot(aes(color = fumo), size = 0.8) +\n  scale_color_nejm() +\n  labs(x = \"Tabagismo\", \n       y = \"Peso ao nascer (g)\",\n       subtitle = get_test_label (tab.aov, detailed = TRUE),\n       caption = get_pwc_label(pwc)) +\n  stat_pvalue_manual (pwc,\n                      label = \"p.adj.signif\",\n                      label.size = 3.5,\n                      hide.ns = TRUE) + \n  theme (text = element_text (size = 12)) +\n  theme_classic()\np +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\nFigura 13.10: Efeito do tabagismo na gestação sobre o peso do recém-nascido.([*]: P entre 0,01 e 0,05; [**]: P entre 0,001 e 0,01; [****]: P &lt; 0,0001).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#anova-de-dois-fatores",
    "href": "13-anova.html#anova-de-dois-fatores",
    "title": "13  Análise de Variância",
    "section": "13.4 ANOVA de dois fatores",
    "text": "13.4 ANOVA de dois fatores\nA ANOVA de dois fatores é uma extensão da ANOVA de um fator. Neste tipo de ANOVA, ao invés de observar o efeito de um fator sobre a variável desfecho contínua, é analisado simultaneamente o efeito de duas variáveis de agrupamento. Outros sinônimos para a ANOVA de dois fatores são: ANOVA fatorial ou ANOVA de duas vias.\nQuando se tem dois ou mais fatores, além de observar o efeito desses fatores sobre a variável desfecho, há necessidade de verificar se eles não interagem entre si. Portanto, é um objetivo importante da ANOVA fatorial avaliar se há um efeito de interação estatisticamente significativo entre os fatores.\n\n13.4.1 Dados usados nesta seção\nO conjunto de dados dadosMemoria.xlsx que contém informações de um teste de memória realizado em homens e mulheres, após o consumo de álcool, categorizado em três grupos (nenhum, 3 latas e 6 latas de cerveja tipo pilsen com 4,5% de álcool). O grupo sem consumo de álcool (cerveja sem álcool) serve como controle. Após o consumo de álcool, foi avaliada a memória para a realização de uma tarefa cognitiva.\nNeste exemplo, modificado de Andy Field (8), o efeito do álcool sobre a memória do indivíduo é a variável focal, a principal preocupação. Acredita-se que o efeito de álcool depende de outro fator, sexo, que são chamados de variáveis moderadoras.\nPara baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho.\n\n13.4.1.1 Leitura dos dados\nA leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função str().\n\ndados &lt;- readxl::read_excel(\"dados/dadosMemoria.xlsx\")\n\nstr(dados)\n\ntibble [48 × 4] (S3: tbl_df/tbl/data.frame)\n $ sexo    : chr [1:48] \"Feminino\" \"Feminino\" \"Feminino\" \"Feminino\" ...\n $ alcool  : chr [1:48] \"nenhum\" \"nenhum\" \"nenhum\" \"nenhum\" ...\n $ escore  : num [1:48] 65 70 60 60 60 55 60 55 70 65 ...\n $ latencia: num [1:48] 2.5 3 1.4 1.5 1.8 2.2 2.3 1.6 3.9 4 ...\n\n\n\n\n13.4.1.2 Exploração e sumarização dos dados\nNa saída da função str(), verifica-se que as variáveis alcool e sexo estão como &lt;chr&gt;e o ideal é que estejam como fatores. Portanto, serão colocadas as categorias do consumo de álcool como fator e em uma ordem lógica (nenhum consumo, três latas e 6 latas). A variável sexo será apenas colocada como fator porque não tem uma ordem lógica. As demais variáveis, id (identificação) e escore(escore de memória) podem permanecer com dbl (numérica).\n\ndados$alcool &lt;- factor(dados$alcool,\n                       levels = c(\"nenhum\",\n                                  \"3 latas\",\n                                  \"6 latas\")) \ndados$sexo &lt;- as.factor(dados$sexo)\n\nA sumarização dos dados será feita com as funções group_by() e summarise() do pacote dplyr para a variável escore por grupos, sexo e alcool.\n\nalpha &lt;- 0.05\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo, alcool) %&gt;% \n  dplyr::summarise(n = n(),\n            media = mean(escore, na.rm=TRUE),\n            dp = sd(escore, na.rm=TRUE),\n            ep = dp/sqrt(n),\n            me = qt((1 - alpha/2),n-1)*ep,\n            linf = media - me,\n            lsup = media + me)\nresumo\n\n# A tibble: 6 × 9\n# Groups:   sexo [2]\n  sexo      alcool      n media    dp    ep    me  linf  lsup\n  &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Feminino  nenhum      8  60.6  4.96  1.75  4.14  56.5  64.8\n2 Feminino  3 latas     8  62.5  6.55  2.31  5.47  57.0  68.0\n3 Feminino  6 latas     8  57.5  7.07  2.5   5.91  51.6  63.4\n4 Masculino nenhum      8  66.9 10.3   3.65  8.64  58.2  75.5\n5 Masculino 3 latas     8  66.9 12.5   4.43 10.5   56.4  77.3\n6 Masculino 6 latas     8  35.6 10.8   3.83  9.06  26.6  44.7\n\n\nOs dados estão estruturados com um desenho onde as células tem um formato 2 x 3 (Tabela 13.2) com os fatores sexo e alcool e 8 indivíduos em cada célula. O fator sexo tem dois níveis (feminino e masculino) e o fator alcool tem três níveis (nenhum, 3 latas e 6 latas). Observe que o desenho é balanceado, pois todas as células têm o mesmo número de indivíduos. Esta estrutura é o caso mais simples; desenhos não balanceados são mais complexos.\n\n\n\n\nTabela 13.2: Distribuição Balanceada do Sexo e Consumo de Álcool\n\n\n\n\n\n\n\n\nnenhum\n3 latas\n6 latas\n\n\n\n\nFeminino\n8\n8\n8\n\n\nMasculino\n8\n8\n8\n\n\n\n\n\n\n\n\n\n\n\n\n13.4.1.3 Visualização gráfica dos dados\nPara visualizar os dados, será construido um gráfico com boxplots (Figura 13.11), usando o pacote ggpubr(9), com a função ggboxplot(), que fornece algumas funções fáceis de usar para criar e personalizar gráficos prontos para publicação baseados em ‘ggplot2’. O boxplot irá plotar os dados agrupados pelas combinações dos níveis dos dois fatores.\n\nggpubr::ggboxplot (dados,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.2,\n                   x = \"alcool\", \n                   y = \"escore\", \n                   color = \"black\",\n                   fill = \"sexo\",\n                   palette = \"bmj\",\n                   ylab = \"Escore da Memória\",\n                   xlab = \"\",\n                   legend.title = \"Sexo\",\n                   legend = \"top\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 13.11: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nAlém dos boxplot, é interessante desenhar um gráfico de linhas (Figura 13.12)) que plota a média (ou outro resumo) da variável escore (resposta) para combinações bidirecionais de fatores, ilustrando assim possíveis interações. Aqui, pode-se usar a função ggline(), também pertencente ao interessante pacote ggpubr.\n\nggpubr::ggline(dados, \n               x = \"alcool\", \n               y = \"escore\", \n               color = \"sexo\",\n               size = 0.7,\n               linetype = \"dashed\",\n               position = position_dodge(width = 0.2),\n               add = \"mean_ci\",\n               palette = c(\"red\", \"dodgerblue4\"))\n\n\n\n\n\n\n\nFigura 13.12: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nO gráfico sugere um possível efeito do álcool sobre a memória, bem como uma interação entre os sexos.\n\n\n\n13.4.2 Hipóteses estatísticas\nComo mencionado acima, uma ANOVA de duas vias é usada para avaliar simultaneamente o efeito de duas variáveis categóricas em uma variável quantitativa contínua. Ela é chamada de ANOVA de duas vias porque compara grupos formados por duas variáveis categóricas independentes.\nNo exemplo, o objetivo é saber se a memória depende da álcool e/ou do sexo. Em particular, estamos interessados em:\n\nmedir e testar a relação entre a alcool e a memória,\nmedir e testar a relação entre sexo e memória, e\npossivelmente verificar se a relação entre álcool e memória é diferente para mulheres e homens (o que é equivalente a verificar se a relação entre sexo e memória depende da álcool)\n\nAs duas primeiras relações são chamadas de efeitos principais, enquanto o item 3 é conhecido como efeito de interação.\nOs efeitos principais testam se pelo menos um grupo é diferente de outro (durante o controle da outra variável independente). Por outro lado, o efeito de interação tem como objetivo testar se a relação entre duas variáveis difere dependendo do nível de uma terceira variável. Em outras palavras, se a variação entre a resposta e a primeira variável categórica não depender das modalidades da segunda variável categórica, então não há interação entre as duas variáveis. Se, ao contrário, houver uma modificação dessa variação, seja por um aumento no efeito da primeira variável, seja por uma diminuição, então há uma interação.\nVoltando ao exemplo, tem-se os seguintes testes de hipótese:\nEfeito principal do sexo no escore de memória:\n\\(H_{0}\\): o escore de memória médio é igual entre mulheres e homens.\n\\(H_{1}\\): o escore de memória médio é diferente entre mulheres e homens.\nEfeito principal do álcool no escore de memória:\n\\(H_{0}\\): o escore de memória médio é igual entre as categorias de ingesta de álcool.\n\\(H_{1}\\): o escore de memória médio é diferente entre as categorias de ingesta de. álcool\nInteração entre sexo e álcool:\n\\(H_{0}\\): não há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é a mesma para mulheres e homens (da mesma forma, a relação entre sexo e memória é a mesma para todas as três categorias de ingesta de álcool).\n\\(H_{1}\\): há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é diferente para mulheres e homens (da mesma forma, a relação entre sexo e memória depende da ingesta de álcool).\n\n\n13.4.3 Pressupostos do modelo\nPara usar uma ANOVA de duas vias, os dados devem atender a certos pressupostos. A ANOVA de duas vias faz todas as suposições usuais de um teste paramétrico de diferença:\n\nIndependência de observações\n\nAs variáveis respostas não devem ser dependentes umas das outras (ou seja, uma não deve causar a outra). Isso é impossível de testar com variáveis categóricas - só pode ser garantido por um bom projeto experimental.\nAlém disso, a variável dependente deve representar observações únicas - não devem ser agrupadas em locais ou indivíduos. Se esta premissa for violada, você pode incluir uma variável de bloqueio e/ou usar uma ANOVA de medidas repetidas.\n\nNormalidade\n\nVariável desfecho normalmente distribuída em todos os grupos.\n\nAusência de valores atípicos (outliers)\n\nUm valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais da série, \\(\\pm 1,5\\) o intervalo interquartil (IIQ) e extremo se estiver \\(\\pm 3\\) IIQ. A existência de outliers implica, tipicamente, em prejuízos à interpretação dos resultados.\n\nHomogeneidade de variância (homocedasticidade)\n\nA variação em torno da média para cada grupo sendo comparado deve ser semelhante entre todos os grupos. Se os dados não atenderem a essa suposição, é possível usar uma alternativa não paramétrica, como o teste de Kruskal-Wallis.\n\n\n13.4.4 Verificação dos pressupostos nos dados brutos\nExiste uma discussão se os pressupostos devem ser avaliados nos dados brutos ou apenas nos resíduos. Aqui serão realizadas as duas abordagens que frequentemente resultam no mesmo resultado.\n\n13.4.4.1 Normalidade\nA variável dependente (escore) deve apresentar distribuição aproximadamente normal dentro de cada grupo. Os grupos aqui serão formados pela combinação das duas variáveis independentes (sexo e alcool). A normalidade será avaliada pelo teste de Shapiro-Wilk, com a função shapiro_test() do pacote rstatix (10), separando os grupos com a função group_by() do pacote dplyr, encadeadas com o operador pipe (%&gt;%):\n\ndados %&gt;% \n     dplyr::group_by (sexo, alcool) %&gt;% \n     rstatix::shapiro_test (escore)\n\n# A tibble: 6 × 5\n  sexo      alcool  variable statistic     p\n  &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Feminino  nenhum  escore       0.872 0.156\n2 Feminino  3 latas escore       0.899 0.283\n3 Feminino  6 latas escore       0.897 0.273\n4 Masculino nenhum  escore       0.941 0.622\n5 Masculino 3 latas escore       0.967 0.870\n6 Masculino 6 latas escore       0.951 0.720\n\n\nOs resultados suportam a conclusão de não rejeição da hipótese nula de que os dados se ajustam a distribuição normal.\n\n\n13.4.4.2 Pesquisa de valores atípicos\nA forma mais simples de verificar a presença de um valor atípico é observar o boxplot, mostrado anteriormente. Se observa a presença de valores atípicos entre as mulheres que não ingeriram álcool e nas que ingeriram 3 latas de cerveja. Agora, para confirmar esse achado, será usado a função identify_outliers (), do pacote rstatix:\n\ndados %&gt;% \n      dplyr::group_by (sexo, alcool) %&gt;% \n      rstatix::identify_outliers(escore)\n\n# A tibble: 2 × 6\n  sexo     alcool  escore latencia is.outlier is.extreme\n  &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 Feminino nenhum      70      3   TRUE       TRUE      \n2 Feminino 3 latas     50      3.2 TRUE       FALSE     \n\n\nA saída do teste confirma a existência dos dois valores atípicos, sendo um deles extremo, entretanto como estes valores são possíveis e, relativamente, próximos da média do sexo feminino, portanto, causam pouca preocupação, principalmente porque o teste de ANOVA é bastante robusto.\n\n\n13.4.4.3 Verificação da homogeneidade das variâncias\nPara verificar a homocedasticidade, como os dados têm distribuição normal, é possível usar o teste de Levene, o leveneTest() do pacote car (11).\n\ncar::leveneTest (escore ~ sexo*alcool, \n                 data = dados, \n                 center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  5  1.5268 0.2021\n      42               \n\n\n\n\n\n13.4.5 Verificação dos pressupostos nos resíduos\nO modelo da ANOVA pode ser considerado como um modelo de regressão. Desta forma, este modelo de regressão vai usar os dados brutos para criar um modelo de previsão para esses dados. Este modelo de regressão não é perfeito, existe uma diferença entre os valores previstos e os valores observados, são os resíduos. Faz sentido, então, preocupar-se com os resíduos quando se analisa fatores tentando explicar uma variável dependente contínua, como na ANOVA, pensando em uma regressão linear simples.\nA ANOVA prevê que todos os valores do grupo sejam iguais a média do grupo. Ou seja, um homem que ingere 3 latas de cerveja tem um valor de seu escore de memória igual ao deste grupo. Por este motivo, fazer a análise dos resíduos é praticamente o mesmo que a análise dos valores brutos.\nPara analisar os resíduos (diferença entre os valores observados e o previsto pelo modelo), em primeiro lugar se constrói o modelo da ANOVA com efeito da interação, usando a função lm() do pacote stats, incluído no R base:\n\nmod.int.lm &lt;- lm(formula = escore ~ alcool * sexo,\n              data = dados)\n\nAo se executar o comando, tem-se a impressão que nada ocorreu, entretanto foi criado o modelo da ANOVA com uma série de variáveis, entre elas os resíduos (residuals). Para observar os resíduos, basta digitar:\n\nmod.int.lm$residuals\n\n      1       2       3       4       5       6       7       8       9      10 \n  4.375   9.375  -0.625  -0.625  -0.625  -5.625  -0.625  -5.625   7.500   2.500 \n     11      12      13      14      15      16      17      18      19      20 \n -2.500   7.500   2.500  -2.500  -2.500 -12.500  -2.500   7.500  12.500  -2.500 \n     21      22      23      24      25      26      27      28      29      30 \n -2.500   2.500  -7.500  -7.500 -16.875 -11.875  13.125  -1.875   3.125   8.125 \n     31      32      33      34      35      36      37      38      39      40 \n  8.125  -1.875 -21.875  -6.875  18.125  -1.875   3.125   3.125  13.125  -6.875 \n     41      42      43      44      45      46      47      48 \n -5.625  -5.625  -5.625  19.375  -0.625 -15.625   9.375   4.375 \n\n\nFunção summary() fornece um resumo estatístico dos resíduos:\n\nsummary(mod.int.lm$residuals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-21.875  -5.625  -0.625   0.000   5.156  19.375 \n\n\n\n13.4.5.1 Avaliação da normalidade dos resíduos\nUma das suposições de uma ANOVA é que os resíduos são normalmente distribuídos. A normalidade dos resíduos, inicialmente, será verificada, usando o teste de Shapiro-Wilk com a função shapiro.test(), também pertencente ao pacote stats.\n\nshapiro_test (mod.int.lm$residuals)\n\n# A tibble: 1 × 3\n  variable             statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n1 mod.int.lm$residuals     0.982   0.664\n\n\nO teste entrega um valor P &gt; 0.05, indicando que não é possível rejeitar \\(H_{0}\\) de normalidade dos resíduos.\nUma outra maneira comum de verificar essa suposição é criando um gráfico Q-Q. Se os resíduos forem normalmente distribuídos, os pontos em um gráfico Q-Q ficarão em uma linha diagonal reta. Este gráfico (Figura 13.13) pode ser contruído com a função ggqqplot() do pacote ggpubr.\n\nggpubr::ggqqplot(mod.int.lm$residuals,\n                 conf.int = TRUE,\n                 shape = 19,\n                 xlab = \"Quantis teóricos\",\n                 ylab = \"Resíduos\",\n                 color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 13.13: Normalidade dos resíduos - QQ plot.\n\n\n\n\n\nO gráfico QQ de normalidade, mostra que os resíduos seguem aproximadamente uma linha reta, permitindo assumir a normalidade dos mesmos.\n\n\n13.4.5.2 Pesquisa de valores atípicos nos resíduos\nPara a verificação da presença de valores atípicos entre os resíduos, cria-se uma variável que será denominada de residuos (observe o banco de dados com a função str() para ver o acréscimo dessa variável):\n\ndados$residuos &lt;- mod.int.lm$residuals\nstr (dados)\n\ntibble [48 × 5] (S3: tbl_df/tbl/data.frame)\n $ sexo    : Factor w/ 2 levels \"Feminino\",\"Masculino\": 1 1 1 1 1 1 1 1 1 1 ...\n $ alcool  : Factor w/ 3 levels \"nenhum\",\"3 latas\",..: 1 1 1 1 1 1 1 1 2 2 ...\n $ escore  : num [1:48] 65 70 60 60 60 55 60 55 70 65 ...\n $ latencia: num [1:48] 2.5 3 1.4 1.5 1.8 2.2 2.3 1.6 3.9 4 ...\n $ residuos: Named num [1:48] 4.375 9.375 -0.625 -0.625 -0.625 ...\n  ..- attr(*, \"names\")= chr [1:48] \"1\" \"2\" \"3\" \"4\" ...\n\n\nPara identificar os outliers, usa-se função identify_outliers() do pacote rstatix:\n\ndados %&gt;% \n  dplyr::group_by(sexo, alcool) %&gt;% \n  rstatix::identify_outliers(residuos)\n\n# A tibble: 2 × 7\n  sexo     alcool  escore latencia residuos is.outlier is.extreme\n  &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 Feminino nenhum      70      3       9.38 TRUE       TRUE      \n2 Feminino 3 latas     50      3.2   -12.5  TRUE       FALSE     \n\n\nObservando os resultados com os dados brutos, verifica-se que eles são iguais aos atuais, confirmando, que neste caso, tanto faz avaliar os dados brutos como os resíduos.\n\n\n13.4.5.3 Verificação da homogeneidade da variância nos resíduos\nA verificação da homogeneidade da variância entre os resíduos pode ser feita com o teste de Levene, como feito com os dados brutos.\n\ncar::leveneTest (residuos ~ sexo*alcool, \n                 data = dados, \n                 center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  5  1.5268 0.2021\n      42               \n\n\nUma outra maneira de avaliar a homogeneidade da variância, é construir um gráfico diagnóstico4 (Figura 13.14)) do modelo com a função plot(), tipo 1, resíduos versus ajustes (Residuals vs Fitted).\n\nplot(mod.int.lm, 1)\n\n\n\n\n\n\n\nFigura 13.14: Resíduos versus ajuste\n\n\n\n\n\nNão há correlações óbvias entre resíduos e valores ajustados (a média de cada grupo) no gráfico abaixo,onde a linha vermelha tracejada (@fig-residuals) segue praticamente uma linha horizontal em torno de 0, o que é bom. Como resultado, pode-se, assim como no teste de Levene, assumir que as variâncias são homogêneas.\nVerica-se o mesmo ocorrido com a normalidade, os resultados nos resíduos não diferem daqueles realizados com os dados brutos.\n\n\n\n13.4.6 Realização do teste de ANOVA de dois fatores\nMostrou-se que praticamente todos os pressupostos foram atendidos, portanto, agora pode-se prosseguir com a implementação da ANOVA de duas vias.\nA inclusão de um efeito de interação em uma ANOVA de duas vias não é obrigatória. Entretanto, para evitar conclusões errôneas, recomenda-se verificar primeiro se a interação é significativa ou não e, dependendo dos resultados, incluí-la ou não.\nSe a interação não for significativa, é seguro removê-la do modelo final. Por outro lado, se a interação for significativa, ela deverá ser incluída no modelo final que será usado para interpretar os resultados.\nPortanto, deve-se começar com um modelo que inclui os dois efeitos principais (ou seja, sexo e alcool) e a interação:\n\nmod.aov &lt;- aov(formula = escore ~ alcool * sexo,\n                        data = dados)\nsummary (mod.aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nalcool       2   3332  1666.1  20.065 7.65e-07 ***\nsexo         1    169   168.7   2.032    0.161    \nalcool:sexo  2   1978   989.1  11.911 7.99e-05 ***\nResiduals   42   3488    83.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSemelhante a uma ANOVA de uma via, o princípio de uma ANOVA de duas vias baseia-se na dispersão total dos dados e em sua decomposição em quatro componentes:\n\na parcela atribuível ao primeiro fator\na parcela atribuível ao segundo fator\na parcela atribuível à interação dos dois fatores\na parte não explicada ou residual.\n\nA soma dos quadrados (coluna Sum Sq) mostra esses quatro componentes. A ANOVA de duas vias consiste em usar um teste estatístico para determinar se cada componente de dispersão (atribuível aos dois fatores estudados e à interação deles) é significativamente maior do que o componente residual. Se esse for o caso, concluímos que o efeito considerado (fator A, fator B ou a interação) é significativo.\nVê-se que a variável alcool explica uma grande parte da variabilidade da memória. Ela é o fator mais importante para explicar essa variabilidade. Os valore P são exibidos na última coluna do resultado acima (Pr(&gt;F)). A partir desses valores conclui-se que, no nível de significância de 5%:\n\ncontrolando para o alcool, o escore de memória não é significativamente diferente entre os dois sexos (P = 0,161),\ncontrolando para o sexo, o escore memória é significativamente diferente (P &lt; 0,0001) para pelo menos uma categoria de ingesta de álcool, e\na interação entre sexo e álcool (exibida na linha alcool:sexo no resultado) é significativa (P &lt; 0,0001).\n\nPortanto, com base no efeito de interação significativo, se observa que relação entre os escores de memória e álcool é diferente entre os sexos. Como ela é significativa, deve-se mantê-la no modelo e interpretar os resultados desse modelo.\nSe, ao contrário, a interação não for significativa (ou seja, se o valor P &gt; 0,05), esse efeito de interação do modelo seria removido. Abaixo, segue o código de uma ANOVA de dois fatores sem interação, chamada de modelo aditivo:\n\n mod.aov2 &lt;- aov(formula = escore ~ alcool * sexo, \n                 data = dados)\n\nNa Seção 13.4.5, foi construído um modelo para analisar os resíduos. Este modelo está baseado na semelhança dos modelos de regressão linear (veja ?sec-rls) com o modelo da ANOVA. Observa-se que o código é bem semelhante, usando a fórmula variável dependente ~ variável independentes, o sinal + é usado para incluir variáveis independentes sem interação e o sinal * quando há interação. Ou seja, a ANOVA, como todas as ANOVAs, é na verdade um modelo linear. Observe que o código a seguir, usando a função lm() e após Anova() do pacote car, também funciona e retorna os mesmos resultados 5 :\n\n mod.int.lm &lt;- lm(formula = escore ~ sexo * alcool,\n                   data = dados)\n Anova(mod.int.lm)\n\nAnova Table (Type II tests)\n\nResponse: escore\n            Sum Sq Df F value    Pr(&gt;F)    \nsexo         168.7  1  2.0323    0.1614    \nalcool      3332.3  2 20.0654 7.649e-07 ***\nsexo:alcool 1978.1  2 11.9113 7.987e-05 ***\nResiduals   3487.5 42                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nObserve que a função aov() pressupõe um projeto balanceado, o que significa tamanhos de amostra iguais dentro dos níveis das variáveis de agrupamento independentes. Para verificar se os dados estão balanceados, proceda como mostrado na Tabela 13.2. Os resultados mostram o mesmo número de indivíduos em todas as células, portanto, não importa qual o tipo de ANOVA a ser usado. Os resultados serão iguais. Além disso, aov() usa as somas de quadrados do tipo I.\nPara delineamentos não balanceados, ou seja, números desiguais de indivíduos em cada subgrupo, os métodos recomendados são:\n\na ANOVA do tipo II, quando não há interação significativa, que pode ser feita no R com Anova(mod, type = “II”) ou Anova(mod, type = 2), em que mod é o nome do seu modelo salvo, e\na ANOVA do tipo III, quando há uma interação significativa, que pode ser feita no R com Anova(mod, type = “III”) ou Anova(mod, type = 3).\n\nFundamentalmente, a diferença entre um método e outro é como o R calcula a soma dos quadrados ao calcular a ANOVA. Quando os dados são balanceados, os três tipos dão o mesmo resultado 6.\n\n\n13.4.7 Testes post hoc\nNeste estágio, chegou-se ao ponto em que se constatou que o efeito principal do sexo não é significativo e que o efeito principal do álcool é significativo. Além disso, mais importante, existe uma interação entre o álcool e o sexo, o efeito do álcool depende do sexo.\nNão é possível saber exatamente qual categoria da variável alcool é diferente da outra em termos de escore de memória. Para saber isso, há que comparar cada categoria duas a duas graças aos testes post-hoc, também conhecidos como comparações entre pares (13) (14) (15). Há vários testes post-hoc, sendo os mais comuns o Tukey HSD, que testa todos os pares possíveis de grupos. Será utilizada a função TukeyHSD(), usando como argumento o modelo com interação, mod.aov:\n\nTukeyHSD(mod.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = escore ~ alcool * sexo, data = dados)\n\n$alcool\n                    diff        lwr        upr     p adj\n3 latas-nenhum    0.9375  -6.889643   8.764643 0.9544456\n6 latas-nenhum  -17.1875 -25.014643  -9.360357 0.0000105\n6 latas-3 latas -18.1250 -25.952143 -10.297857 0.0000040\n\n$sexo\n                    diff       lwr      upr     p adj\nMasculino-Feminino -3.75 -9.058607 1.558607 0.1613818\n\n$`alcool:sexo`\n                                       diff        lwr        upr     p adj\n3 latas:Feminino-nenhum:Feminino      1.875 -11.726381  15.476381 0.9983764\n6 latas:Feminino-nenhum:Feminino     -3.125 -16.726381  10.476381 0.9825753\nnenhum:Masculino-nenhum:Feminino      6.250  -7.351381  19.851381 0.7432243\n3 latas:Masculino-nenhum:Feminino     6.250  -7.351381  19.851381 0.7432243\n6 latas:Masculino-nenhum:Feminino   -25.000 -38.601381 -11.398619 0.0000306\n6 latas:Feminino-3 latas:Feminino    -5.000 -18.601381   8.601381 0.8796489\nnenhum:Masculino-3 latas:Feminino     4.375  -9.226381  17.976381 0.9277939\n3 latas:Masculino-3 latas:Feminino    4.375  -9.226381  17.976381 0.9277939\n6 latas:Masculino-3 latas:Feminino  -26.875 -40.476381 -13.273619 0.0000080\nnenhum:Masculino-6 latas:Feminino     9.375  -4.226381  22.976381 0.3286654\n3 latas:Masculino-6 latas:Feminino    9.375  -4.226381  22.976381 0.3286654\n6 latas:Masculino-6 latas:Feminino  -21.875 -35.476381  -8.273619 0.0002776\n3 latas:Masculino-nenhum:Masculino    0.000 -13.601381  13.601381 1.0000000\n6 latas:Masculino-nenhum:Masculino  -31.250 -44.851381 -17.648619 0.0000003\n6 latas:Masculino-3 latas:Masculino -31.250 -44.851381 -17.648619 0.0000003\n\n\nQuando se tem muitos grupos para fazer a comparação, fica mais fácil interpretar, usando gráficos (Figura 13.15):\n\n# Definir as margens do eixo para que os rótulos não sejam cortados\npar(mar = c(4.1, 15, 4.1, 2.1))\n# Criar intervalo de confiança para cada comparação\nplot(TukeyHSD(mod.aov, which = \"alcool:sexo\"), las = 1)\n# Retorna as margens ao padrão\npar(mar=c(5.1, 4.1, 4.1, 2.1)) \n\n\n\n\n\n\n\nFigura 13.15: Resíduos versus ajuste\n\n\n\n\n\nAs Saída exibe resultados onde aparece que o consumo de álcool não afetou a memória das mulheres, mas o consumo de 6 latas de cerveja diminuiu o escore de memória dos homens quando comparados com homens que consumiram cerveja sem álcool ou que ingeriram apenas 3 latas de cerveja. Em outras palavras, os termos de interação dizem como o efeito dos álcool muda quando quem o ingere é do sexo masculino ou feminino. O efeito do álcool na memória das mulheres é pequeno, ficando praticamente estável nas três condições. Por outro lado, os homens permanecem estáveis no seu escore de memória quando quantidades pequenas de álcool são ingeridas, declina rapidamente quando ingerem 6 latas de cerveja.\n\n\n13.4.8 Relatando os resultados de uma ANOVA de dois fatores\nPode-se relatar os resultados da ANOVA de dois fatores da seguinte maneira:\n\nUma ANOVA de dois fatores foi realizada para avaliar se a memória de homens e mulheres era afetada pelo consumo do álcool avliado em três níveis:\n\nNão consumiram álcool\nConsumiram 3 latas de cerveja (~ 1L)\nConsumiram 6 latas de cerveja (~ 2L)\n\nOs dados são apresentados como média e desvio padrão, na Tabela 13.3).7\n\n\n\n\n\nTabela 13.3: Efeito do Álcool sobre a Memória - Escore médio (desvio padrão)\n\n\n\n\n\n\nSexo\nSem_alcool\nUm_litro\nDois_litros\n\n\n\n\nFeminino\n60,6 (5,0)\n62,5 (6,6)\n57,5 (7,1)\n\n\nMasculino\n66,9 (10,3)\n66,9 (12,5)\n35,6 (10,8)\n\n\nValor P\n0,145\n0,396\n0,0003\n\n\n\n\n\n\n\n\n\n\n\nO efeito principal do sexo na memória foi não significativo (F(1,42) = 2,03, P = 0,1614).\nHouve um efeito principal significativo de acordo com a quantidade de álcool consumida na memória dos participantes (F(2,42) = 20,07, P &lt;0,0001).\nAs análises posteriores (teste de Tukey, Figura 13.15) revelaram que a memória não foi afetada nas mulheres pelo consumo de álcool, mas o consumo de 6 latas de cerveja afetou a memória dos homens quando comparados os homens que não consumiram álcool ou que consumiram até 3 latas de cerveja.\nVisualização dos resultados:\n\nSerão apresentados gráficos de barra de erro (Figura 13.16), com ggbarplot(), do pacote ggpubr, utilizando, para cores tonalidades de cinza. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix.\n\n# Construção de um gráfico de barra de erro\nbe &lt;- ggpubr::ggbarplot(dados, \n                        x = \"alcool\", y = \"escore\", \n                        add = \"mean_ci\",\n                        error.plot = \"upper_errorbar\",\n                        fill = \"sexo\", \n                        palette = c(\"gray60\", \"gray40\"),\n                        position = position_dodge(0.8)) +\n  theme(legend.key.size = unit(0.3, 'cm')) +\n  theme(legend.position = \"right\")\n\n# Comparações por pares (pairwise comparisons)\npwc &lt;- dados %&gt;%\n   dplyr::group_by(alcool) %&gt;%\n   rstatix::tukey_hsd(formula = escore ~ sexo)\n\n# Calcular e adicionar as posições x e y.\npwc &lt;- pwc %&gt;%\n  add_xy_position(fun = \"mean_ci\", \n                  x = \"alcool\", \n                  dodge = 0.8) \n\n# Cálculo do teste estatístico com pacote rstatix\nanova &lt;-  anova_test(mod.aov)\n\n# Acrescentar o teste e o valor P ajustado ao gráfico\nbe + stat_pvalue_manual(pwc,  \n                        label = \"p.adj\", \n                        tip.length = 0.01,\n                        y.position = 85) +\n  labs (x = \"Ingestão de álcool\",\n        y = \"Média escore de memória\",\n        subtitle = rstatix::get_test_label (anova, detailed = TRUE),\n        caption = rstatix::get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 13.16: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nUma opção, é apresentar os resultados como um gráfico de linhas (Figura 13.17), já mostrado anteriormente , usando a função ggline() do pacote ggpubr, com as cores do periódico Lancet:\n\n# Construção de um gráfico linha\ngl &lt;- ggpubr::ggline(dados, \n                     x = \"alcool\", y = \"escore\", \n                     add = \"mean_ci\",\n                     color = \"sexo\",\n                     size = 0.7,\n                     linetype = \"dashed\", \n                     palette = \"lancet\",\n                     position = position_dodge(0.2)) +\n  theme(legend.key.size = unit(0.3, 'cm')) +\n  theme(legend.position = \"right\")\n\n# Comparações por pares (pairwise comparisons)\npwc &lt;- dados %&gt;%\n  dplyr::group_by(alcool) %&gt;%\n  rstatix::tukey_hsd(formula = escore ~ sexo)\n\n# Calcular e adicionar as posições x e y.\npwc &lt;- pwc %&gt;%\n  add_xy_position(fun = \"mean_ci\", \n                  x = \"alcool\", \n                  dodge = 0.8)\n\n# Cálculo do teste estatístico com pacote rstatix\nanova &lt;-  anova_test(mod.aov)\n\n# Acrescentar o teste e o valor P ajustado ao gráfico\ngl + stat_pvalue_manual(pwc,  \n                        label = \"p.adj\", \n                        tip.length = 0.01,\n                        y.position = 85) +\n  labs (x = \"Ingestão de álcool\",\n        y = \"Média escore de memória\",\n        subtitle = rstatix::get_test_label (anova, \n                                            detailed = TRUE),\n        caption = rstatix::get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 13.17: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\n\n\n\n\n1. Field A, Miles J, Field Z. Comparing several means: ANOVA (GML 1). Em: Discovering Statistics Using R. Sage Publications, Ltd; 2012. p. 399–400. \n\n\n2. Menezes RX de. Análise de Variância. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 297–300. \n\n\n3. Garren ST. Package fastgraph [Internet]. CRAN. Comprehensive R Archive Network (CRAN); 1919. Disponível em: https://CRAN.R-project.org/package=fastGraph\n\n\n4. Peat J, Barton B. Continuous variables: analysis of variance. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 114. \n\n\n5. Dag O, Dolgun A, Konar NM. Onewaytests: An R Package for One-Way Tests in Independent Groups Designs. R Journal. 2018;10(1):175–99. \n\n\n6. Ben-Shachar MS, Lüdecke D, Makowski D. effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software. 2020;5(56):2815. \n\n\n7. Watson P. Rules of thumb on magnitudes of effect sizes [Internet]. MRC Cognition and Brain Sciences Unit. Cambridge University; 2021. Disponível em: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n8. Field A, Miles J, Field Z. Factorial ANOVA (GLM3). Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 513–4. \n\n\n9. Kassambara A. ggpubr:’ggplot2’ based publication ready plots [R package ggpubr version 0.5.0] [Internet]. The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN); 2022. Disponível em: https://cloud.r-project.org/web/packages/ggpubr/index.html\n\n\n10. Kassambara A. rstatix: Pipe-Friendly Framework for Basic Statistical Tests [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=rstatix\n\n\n11. Fox J, Weisberg S. An R Companion to Applied Regression [Internet]. Third. Thousand Oaks CA: Sage; 2019. Disponível em: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n12. Patterson R, Coffman J, Goldstein-Greenwood J, Others. Understanding Diagnostic Plots for Linear Regression Analysis [Internet]. Research Data Services + Sciences. University of Virginia Library; 2015. Disponível em: https://data.library.virginia.edu/diagnostic-plots/\n\n\n13. Wickens TD, Keppel G. Two-way factorial experiments. Em: Design and analysis: A researcher’s handbook. Pearson Prentice-Hall; 2004. p. 193–286. \n\n\n14. Maxwell SE, Delaney HD, Kelley K. Two-way Between-Subject Factorial Designs. Em: Designing experiments and analyzing data: A model comparison perspective. Third Edition. Routledge; 2017. p. 312–82. \n\n\n15. Lenth R, Singmann H, Love J, Buerkner P, Herve M. Emmeans: Estimated marginal means, aka least-squares means. R package version. 2018;1(1):3.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#footnotes",
    "href": "13-anova.html#footnotes",
    "title": "13  Análise de Variância",
    "section": "",
    "text": "Pode ser usada também para comparar a média de duas populações e o resultado será o mesmo de um teste t para amostras independentes.↩︎\nNo texto, df1 e df2 (em inglês, df = degree of freedom) foram traduzidos para o português como gl1 e gl2 (gl = graus de liberdade).↩︎\nVolte à Seção 6.6 para mais informações sobre o como fazer gráficos no ggplot2.↩︎\nOutros gráficos diagnósticos podem ser obtidos para analisar resíduos em um modelo de regressão (12)↩︎\nA função Anova() do pacote car, usada para testar efeitos principais e de interação em modelos lineares gerais, não deve ser confundida com a função anova(), da base do R, porque esta fornece resultados sequenciais que dependem da ordem em que as variáveis aparecem no modelo.↩︎\nPara os interessados, pode-se obter maiores informações sobre os diferentes tipos de ANOVA em https://www.r-bloggers.com/2011/03/anova-%E2%80%93-type-iiiiii-ss-explained/↩︎\nlembrar que um litro de cerveja (4,5%) = 5 unidades de alcool↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "14  Summary",
    "section": "",
    "text": "Em construção",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Armitage P, Berry G, Matthews JNS. Statistical\nmethods in medical research. John Wiley & Sons; 2008. \n\n\n2. Massad E, Silveira PSP, Menezes RX de, Ortega\nNRS. Métodos quantitativos em medicina. Editora Manole Ltda; 2004.\n\n\n\n3. Kendall MG. Studies in the history of\nprobability and statistics. Where shall the history of statistics begin?\nBiometrika. 1960;47(3/4):447–9. \n\n\n4. Breve história dos censos [Internet]. Instituto\nNacional de Estatistica. Statistics Portugal; 2014. Available from: https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria\n\n\n5. Salgado-Neto G, Salgado A. Sir francis galton e\nos extremos superiores da curva normal. Revista de Ciências Humanas.\n2011;45(1):223–39. \n\n\n6. Stolley PD, Lasky T. The beginnings of\nepidemiology. In: Investigating disease patterns. Scientific American\nLibrary; 2000. p. 23–49. \n\n\n7. Stolley PD, Lasky T. Lung cancer: New methods\nof studying disease. In: Investigating disease patterns. Scientific\nAmerican Library; 2000. p. 51–79. \n\n\n8. Editors History com. Florence\nNightingale.\nhttps://www.history.com/topics/womens-history/florence-nightingale-1;\n\n\n\n9. Moore DS. Topics in inferency. In: The basic\npractice of statistics. W.H. Freeman; 2000. p. 417. \n\n\n10. Hald A. Biography of fisher. In: A history of\nparametric statistics inference from bernoulli to fisher,1713-1935. John\nWiley & Sons; 2007. p. 159–63. \n\n\n11. Salsburg D. Uma senhora toma chá... In: Uma\nsenhora toma chá. Zahar; 2009. p. 17–23. \n\n\n12. Kruskal W. The significance of fisher: A review\nof r.a. Fisher: The life of a scientist. Journal of the American\nStatistical Association [Internet]. 1980;75(372):1019–30. Available\nfrom: https://doi.org/10.1080/01621459.1980.10477590\n\n\n13. Matthews R, Chalmers I, Rothwell P. Douglas g\naltman: Statistician, researcher, and driving force behind global\ninitiatives to improve the reliability of health research. British\nMedical Journal Publishing Group; 2018. \n\n\n14. Altman DG. The scandal of poor medical\nresearch. Vol. 308, Bmj. British Medical Journal Publishing Group; 1994.\np. 283–4. \n\n\n15. R\nCore Team. The r project for statistical computing | what is r?\nDisponível em: https://www.r-project.org/about.html; 2022. \n\n\n16. R\nCore Team. The r project for statistical computing | CRAN mirrors.\nDisponível em: https://cran.r-project.org/mirrors.html; 2022. \n\n\n17. Whitney L et al. R programming language\ncontinues to grow in popularity [Internet]. TechRepublic. 2020.\nAvailable from: https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity\n\n\n18. Oliveira Filho PF de. Natureza dos dados. In:\nEpidemiologia e bioestatística–fundamentos para a leitura crítica. 2ª\nedição. Editora Rubio; 2022. p. 3–6. \n\n\n19. Kirkwood BR, Sterne JA. Defining the data. In:\nEssential medical statistics. Second Edition. Blackwell Science Company;\n2003. p. 9–14. \n\n\n20. Sternbach GL. The glasgow coma scale. The\nJournal of emergency medicine. 2000;19(1):67–71. \n\n\n21. Pediatrics AA of, Obstetricians AC of. The\napgar score. Pediatrics. 2006;117(4):1444–7. \n\n\n22. Bowers D. First things first-the nature of\ndata. In: Medical statistics from scratch. Second Edition. John Wiley;\nSons; 2008. p. 3–13. \n\n\n23. Ribeiro Mendes F. O que é um trabalho\ncientífico. In: Iniciacão cientifica. Autonomia Editora; 2012. p. 17–26.\n\n\n\n24. Hulley SB, Cummings SR, Browner WS, Grady DG,\nNewman TB. Elaborando a questão de pesquisa e desenvolvendo o plano de\nestudo. In: Delineando a pesquisa clinica. Quarta Edição. Artmed\nEditora; 2015. p. 15–24. \n\n\n25. McCombes S. Sampling methods [Internet].\nhttps://www.scribbr.com/methodology/sampling-methods/. scribbr.com Team;\n2019. Available from: https://www.scribbr.com/\n\n\n26. Callegari-Jacques SM. Amostras. In:\nBioestatistica: Principios e aplicações. Artmed Editora; 2003. p. 146–7.\n\n\n\n27. Faul F, Erdfelder E, Lang A-G, Buchner A. G*\npower 3: A flexible statistical power analysis program for the social,\nbehavioral, and biomedical sciences. Behavior research methods.\n2007;39(2):175–91. \n\n\n28. Cohen J. Statistical power analysis for the\nbehavioral sciences. Lawrence Erlbaum Associates; 1988. \n\n\n29. Grimes DA, Schulz KF. An overview of clinical\nresearch: The lay of the land. The lancet. 2002;359(9300):57–61. \n\n\n30. Fletcher RH, Fletcher SW, Fletcher GS.\nPrognóstico. In: Epidemiologia clínica: Elementos essenciais. Artmed\nEditora; 2014. p. 108–9. \n\n\n31. Grimes DA, Schulz KF. Descriptive studies: What\nthey can and cannot do. The Lancet. 2002;359(9301):145–9. \n\n\n32. Fletcher RH, Fletcher SW, Fletcher GS. Risco:\nDa doença à exposição. In: Epidemiologia clínica: Elementos essenciais.\nArtmed Editora; 2014. p. 88. \n\n\n33. Grimes DA, Schulz KF. Compared to what? Finding\ncontrols for case-control studies. The Lancet. 2005;365(9468):1429–33.\n\n\n\n34. Ernster VL. Nested case-control studies.\nPreventive Medicine. 1994;23(5):587–90. \n\n\n35. Newman TB, Browner WS, Cummings SR, Hulley SB.\nDelineando estudos de caso-controle. In: Delineando a pesquisa clinica.\nQuarta Edição. Artmed Editora; 2015. p. 111. \n\n\n36. Grimes DA, Schulz KF. Cohort studies: Marching\ntowards outcomes. The Lancet. 2002;359(9303):341–5. \n\n\n37. Fletcher RH, Fletcher SW, Fletcher GS. Risco:\nDa doença à exposição. In: Epidemiologia clínica: Elementos essenciais.\nArtmed Editora; 2014. p. 68. \n\n\n38. Celentano DD, Szklo M. Cohort studies. In:\nGordis epidemiology. 6th Edition. Elsevier; 2019. p. 179. \n\n\n39. Kannel WB, McGee DL. Diabetes and\ncardiovascular risk factors: The framingham study. Circulation.\n1979;59(1):8–13. \n\n\n40. Coutinho M. Principios de epidemiologia clínica\naplicada a cardiologia. Arquivos Brasileiros de Cardiologia.\n1998;71:109–16. \n\n\n41. McCambridge J, Witton J, Elbourne DR.\nSystematic review of the hawthorne effect: New concepts are needed to\nstudy research participation effects. Journal of Clinical Epidemiology.\n2014;67(3):267–77. \n\n\n42. Bland JM, Altman DG. Statistic notes:\nRegression towards the mean. BMJ. 1994;308(6942):1499. \n\n\n43. Kabisch M, Ruckes C, Seibert-Grafe M, Blettner\nM. Randomized controlled trials: Part 17 of a series on evaluation of\nscientific publications. Deutsches Ärzteblatt\nInternational. 2011;108(39):663. \n\n\n44. Fletcher RH, Fletcher SW, Fletcher GS.\nTratamento. In: Epidemiologia clínica: Elementos essenciais. Artmed\nEditora; 2014. p. 143. \n\n\n45. Elander G, Hermerén G. Placebo effect and\nrandomized clinical trials. Theoretical Medicine. 1995;16(2):171–82.\n\n\n\n46. Schulz KF, Grimes DA. Blinding in randomised\ntrials: Hiding who got what. The Lancet. 2002;359(9307):696–700. \n\n\n47. Montori VM, Guyatt GH. Intention-to-treat\nprinciple. CMAJ. 2001;165(10):1339–41. \n\n\n48. Christensen E. Methodology of superiority vs.\nEquivalence trials and non-inferiority trials. Journal of hepatology.\n2007;46(5):947–54. \n\n\n49. Health Improvement O for, Disparities.\nCrossover randomised controlled trial: Comparative studies [Internet].\nOffice for Health Improvement and Disparities. UK Health improvement;\n2020. Available from: https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies\n\n\n50. Hennekens CH, Buring JE, et al. Lack of effect\nof long-term supplementation with beta carotene on the incidence of\nmalignant neoplasms and cardiovascular disease. New England Journal of\nMedicine. 1996;334(18):1145–9. \n\n\n51. Stanley K. Design of randomized controlled\ntrials. Circulation. 2007;115(9):1164–9. \n\n\n52. Chang W. Cookbook for r. Cookbook for R.\nhttp://www.cookbook-r.com; 2021. \n\n\n53. Verzani J. Using r for introductory statistics.\nChapman; Hall/CRC; 2004. \n\n\n54. Damiani A, Milz B, Lente C, al et. Ciência de\ndados em r [Internet]. R6 Consultoria; 2015. Available from: https://livro.curso-r.com/index.html\n\n\n55. Zuur AF, Ieno EN, Meesters EH. Getting data\ninto r. In: A beginner’s guide to r. Springer; 2009. p. 29–56. \n\n\n56. Wickham H, François R, Henry L, Müller K, et\nal. Dplyr: A grammar of data manipulation. R package version 04.\n2015;3:156. \n\n\n57. Wickham H, Grolemund G. 15 factors|r for data\nscience [Internet]. Welcome | R for Data Science. O’Reilly; 2017.\nAvailable from: https://r4ds.had.co.nz/factors.html\n\n\n58. Ooms J. Writexl: Export data frames to excel\n’xlsx’ format [Internet]. 2022. Available from: https://CRAN.R-project.org/package=writexl\n\n\n59. Team RC. Write.table: Data output/CSV files\n[Internet]. DataCamp; 2022. Available from: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table\n\n\n60. Wickham H, Averick M, Bryan J, Chang W, et al.\nWelcome to the tidyverse. Journal of Open Source Software.\n2019;4(43):1686. \n\n\n61. Wickham H. Tidy data. Journal of Statistical\nSoftware. 2014;59(10):11–23. \n\n\n62. Wickham H, Girlich M. Tidyr: Tidy messy data\n[Internet]. 2022. Available from: https://CRAN.R-project.org/package=tidyr\n\n\n63. Fisher RA. The use of multiple measurements in\ntaxonomic problems. Annals of eugenics. 1936;7(2):179–88. \n\n\n64. Grolemund G, Wickham H. Dates and times made\neasy with lubridate. Journal of Statistical Software [Internet].\n2011;40(3):1–25. Available from: https://www.jstatsoft.org/v40/i03/\n\n\n65. Field A, Miles J, Field Z. Everithing you ever\nwanted to know about statistics (well, sort of). In: Discovering\nstatistics using r. Sage Publications, Ltd; 2012. p. 38. \n\n\n66. Oliveira Filho PF de. Tabelas. In:\nEpidemiologia e bioestatística-fundamentos para a leitura crítica. 2ª\nedição. Editora Rubio; 2022. p. 9–12. \n\n\n67. Arango HG. Organização dos dados em tabelas.\nIn: Bioestatística: Teórica e computacional. 3ª edição. Guanabara\nKoogan; 2009. p. 32–57. \n\n\n68. Xie\nY. Knitr: A comprehensive tool for reproducible research in r. In:\nImplementing reproducible research. Chapman; Hall/CRC; 2018. p. 3–31.\n\n\n\n69. Zhu\nH et al. Construct complex table with “kable” and pipe\nsyntax [Internet]. 2021. Available from: https://CRAN.R-project.org/package=kableExtra\n\n\n70. Arango HG. Números de classes e intervalo de\nclasses. In: Bioestatística teórica e computacional. Terceira edição.\nGuanabara Koogan, RJ; 2009. p. 35–40. \n\n\n71. Rasmussen KM, Yaktine AL, et al. Weight gain\nduring pregnancy: Reexamining the guidelines. 2009; \n\n\n72. Field A, Miles J, Field Z. Exploring data with\ngraphs. In: Discovering statistics using r. Sage Publications, Ltd;\n2012. p. 117. \n\n\n73. Wickham H. Getting started with ggplot2. In:\nggplot2. Second edition. Springer; 2016. p. 11–31. \n\n\n74. Tufte ER. Aesthetics and technique in data\ngraphical design. In: The visual display of quantitative information.\nSecond edition. Graphics Press; 2001. p. 178. \n\n\n75. Lemon J, Bolker B, Oom S, et al. Package\n“plotrix.” Vienna: R Development Core Team. 2015; \n\n\n76. Kabacoff RI. Basic graphs. In: R in action:\nData analysis and graphics with r. Manning Publications Co.; 2011. p.\n120–4. \n\n\n77. Harrell FE, Dupont C. Hmisc: Harrell\nmiscellaneous [Internet]. R package version. 2022. Available from: https://cran.r-project.org/web/packages/Hmisc/index.html\n\n\n78. Wickham H. A layered grammar of graphics.\nJournal of Computational and Graphical Statistics. 2010;19(1):3–28.\n\n\n\n79. Wickham H. ggplot2: Elegant graphics for data\nanalysis [Internet]. Springer-Verlag New York; 2016. Available from: https://ggplot2.tidyverse.org\n\n\n80. Rinker TW, Kurkiewicz D. Pacman: Package\nmanagement for r [Internet]. Buffalo, New York; 2018. Available from: http://github.com/trinker/pacman\n\n\n81. Debnath L, Basu K. A short history of\nprobability theory and its applications. International Journal of\nMathematical Education in Science and Technology. 2015;46(1):13–39.\n\n\n\n82. Menezes RX de. Introdução à probabilidade. In:\nMassad E, Menezes RX de, Silveira PSP, Ortega NRS, editors. Métodos\nquantitativos em medicina. Barueri, São Paulo: Editora Manole Ltda.;\n2004. p. 151–87. \n\n\n83. Pagano M, Kimberly G. Theoretical probability\ndistributions. In: Principles of biostatistics. Second Edition. CRC\nPress; 2000. p. 162. \n\n\n84. Gonzalez JCS. Normal distribution in r\n[Internet]. R CODER. 2021. Available from: https://r-coder.com/\n\n\n85. Robertson E, O’Connor J. Jacob (jacques)\nbernoulli [Internet]. Maths History. School of Mathematics; Statistics,\nUniversity of St Andrews; 2022. Available from: https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/\n\n\n86. Fisher LD, Van Belle G. Poisson random\nvariables. In: Biostatistics: A methodology for the health sciences. New\nYork, NY: John Wiley & Sons; 1993. p. 211–8. \n\n\n87. Peat J, Barton B. Descriptive statistics. In:\nMedical statistics : A guide to SPSS, data analysis, and critical\nappraisal. New York, NY: John Wiley & Sons; 2014. p. 24–51. \n\n\n88. Joanes D, Gill C. Comparing measures of sample\nskewness and kurtosis. Journal of the Royal Statistical Society.\n1998;47(1):183–9. \n\n\n89. George D, Mallery P. Descriptive statistics.\nIn: IBM SPSS statistics 26 step by step: A simple guide and reference.\nNew York, NY: Taylor & Francis Group; 2020. p. 114–20. \n\n\n90. Pagano M, Gavreau K. The central limit theorem.\nIn: Principles of biostatistics. Second Edition. Pacific Grove, CA:\nDuxbury; 2000. p. 197–8. \n\n\n91. Motulsky H. The theory of confidence intervals.\nIn: Intuitive biostatistics: A nonmathematical guide to statistical\nthinking. Second Edition. New York, NY: Oxford University Press; 2010.\np. 96–102. \n\n\n92. Signorell A et al. DescTools: Tools for\ndescriptive statistics [Internet]. 2022. Available from: https://cran.r-project.org/package=DescTools\n\n\n93. Kelen GD, Brown CB, Ashton J. Statistical\nreasoning in clinical trials: Hypothesis testing. Am J Emerg Med.\n1988;1(1):52–61. \n\n\n94. Menezes RX de, Burattini MN. Testes de hipótese\ne intervalos de confiança. In: Massad E, Menezes RX de, Silveira PSP,\nOrtega NRS, editors. Métodos quantitativos em medicina. Barueri, São\nPaulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n95. Guyatt G, Jaeschke R, Heddle N, et al. Basic\nstatistics for clinicians: 1. Hypothesis testing. CMAJ: Canadian Medical\nAssociation Journal. 1995;152(1):27. \n\n\n96. Fletcher RH, Fletcher SW, Fletcher GS. Acaso.\nIn: Epidemiologia clínica: Elementos essenciais. Quinta Edição. Artmed\nEditora; 2014. p. 108–9. \n\n\n97. Menezes RX de, Burattini MN. Testes de hipótese\ne intervalos de confiança. In: Massad E, Menezes RX de, Silveira PSP,\nOrtega NRS, editors. Métodos quantitativos em medicina. Barueri, São\nPaulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n98. Pagano M, Kimberly G. Comparison of two means.\nIn: Principles of biostatistics. Second Edition. CRC Press; 2000. p.\n262–72. \n\n\n99. Zimmerman DW. A note on preliminary tests of\nequality of variances. Br J Math Stat Psychol. 2004;57(1):173–81. \n\n\n100. Razali NM, Wah YB, et al. Power comparisons of\nshapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests.\nJournal of statistical modeling and analytics. 2011;2(1):21–33. \n\n\n101. Ghasemi A, Zahediasl S. Normality tests for\nstatistical analysis: A guide for non-statisticians. International\njournal of endocrinology and metabolism. 2012;10(2):486. \n\n\n102. Yap BW, Sim CH. Comparisons of various types of\nnormality tests. Journal of Statistical Computation and Simulation.\n2011;81(12):2141–55. \n\n\n103. Fox J, Weisberg S. An r companion to applied\nregression [Internet]. Third. Thousand Oaks CA: Sage; 2019.\nAvailable from: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n104. Kassambara A. Rstatix: Pipe-friendly framework\nfor basic statistical tests [Internet]. 2022. Available from: https://CRAN.R-project.org/package=rstatix\n\n\n105. Cohen J. Statistical power analysis for the\nbehavioral sciences. 2nd Edition. Routledge; 1988. \n\n\n106. Lindenau JD, Guimaraes LSP. Calculating the\neffect size in SPSS. Revista HCPA [Internet]. 2012;32(3):363–81.\nAvailable from: https://seer.ufrgs.br/hcpa\n\n\n107. Field A, Miles J, Field Z. Comparing several\nmeans: ANOVA (GML 1). In: Discovering statistics using r. Sage\nPublications, Ltd; 2012. p. 399–400. \n\n\n108. Menezes RX de. Análise de variância. In: Massad\nE, Menezes RX de, Silveira PSP, Ortega NRS, editors. Métodos\nquantitativos em medicina. Barueri, São Paulo: Editora Manole Ltda.;\n2004. p. 297–300. \n\n\n109. Garren ST. Package fastgraph [Internet]. CRAN.\nComprehensive R Archive Network (CRAN); 1919. Available from: https://CRAN.R-project.org/package=fastGraph\n\n\n110. Peat J, Barton B. Continuous variables:\nAnalysis of variance. In: Medical statistics : A guide to SPSS, data\nanalysis, and critical appraisal. New York, NY: John Wiley & Sons;\n2014. p. 114. \n\n\n111. Dag O, Dolgun A, Konar NM. Onewaytests: An r\npackage for one-way tests in independent groups designs. R Journal.\n2018;10(1):175–99. \n\n\n112. Ben-Shachar MS, Lüdecke D, Makowski D.\nEffectsize: Estimation of effect size indices and standardized\nparameters. Journal of Open Source Software. 2020;5(56):2815. \n\n\n113. Watson P. Rules of thumb on magnitudes of\neffect sizes [Internet]. MRC Cognition and Brain Sciences Unit.\nCambridge University; 2021. Available from: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n114. Field A, Miles J, Field Z. Factorial ANOVA\n(GLM3). In: Discovering statistics using r. Sage Publications, Ltd;\n2012. p. 513–4. \n\n\n115. Kassambara A. Ggpubr:’ggplot2’ based\npublication ready plots [r package ggpubr version 0.5.0] [Internet]. The\nComprehensive R Archive Network. Comprehensive R Archive Network (CRAN);\n2022. Available from: https://cloud.r-project.org/web/packages/ggpubr/index.html\n\n\n116. Patterson R, Coffman J, Goldstein-Greenwood J,\nOthers. Understanding diagnostic plots for linear regression analysis\n[Internet]. Research Data Services + Sciences. University of Virginia\nLibrary; 2015. Available from: https://data.library.virginia.edu/diagnostic-plots/\n\n\n117. Wickens TD, Keppel G. Two-way factorial\nexperiments. In: Design and analysis: A researcher’s handbook. Pearson\nPrentice-Hall; 2004. p. 193–286. \n\n\n118. Maxwell SE, Delaney HD, Kelley K. Two-way\nbetween-subject factorial designs. In: Designing experiments and\nanalyzing data: A model comparison perspective. Third Edition.\nRoutledge; 2017. p. 312–82. \n\n\n119. Lenth R, Singmann H, Love J, Buerkner P, Herve\nM. Emmeans: Estimated marginal means, aka least-squares means. R package\nversion. 2018;1(1):3. \n\n\n120. Physicians’ Health Study Research Group* SC of\nthe. Final report on the aspirin component of the ongoing physicians’\nhealth study. New England Journal of Medicine. 1989;321(3):129–35.\n\n\n\n121. Meyer D, Dimitriadou E, Hornik K, Weingessel A,\nLeisch F, Chang C-C, et al. Package “e1071.” The R Journal.\n2019;1–67.",
    "crumbs": [
      "References"
    ]
  }
]