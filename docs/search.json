[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestatística Usando o R",
    "section": "",
    "text": "Prefácio\nEste livro foi escrito com o objetivo de consultoria educacional. Aqui você encontrará uma introdução à Bioestatística usando a linguagem R, salientando os assuntos que se constituem em uma instrumentação básica para a análise de dados na área da saúde.\nEste livro tem a ambição de ser um pouco mais tolerável e amistoso, a fim de estimular o estudo da Bioestatística e, assim, facilitar a crítica da literatura científica e , desta forma, ajudar a evitar a intoxicação causada pela pseudociência.\nPetrônio Fagundes de Oliveira Filho\nemail: petronioliveira@gmail.com\nWhatsApp: +55 54 9997 15499",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Importância da Bioestatística\nOs indivíduos variam em relação as suas características biológicas, psicológicas e sociais na saúde e na doença. Esta variabilidade gera uma grande quantidade de incertezas.\nA Bioestatística, estatística aplicada às ciências biológicas e da saúde, é a ferramenta utilizada pelos pesquisadores para trabalhar com essas incertezas advindas da variabilidade. Várias definições foram escritas para a estatística, uma delas é a seguinte (1):\nA Bioestatística lida com a variabilidade humana utilizando técnicas estatísticas quantitativas (2) que ajudam a diminuir a ignorância em relação a esta diversidade. A compreensão da variabilidade humana torna a medicina mais ciência, diminuindo as incertezas, na tentativa de verificar se os resultados encontrados de fato existem ou são apenas obra do acaso.\nNa década de 1990, houve um acesso maior aos computadores. Os profissionais da saúde não estatísticos passaram a ter mais interesse no campo da bioestatística. Isto gerou uma onda que facilitou o aparecimento de novas ferramentas estatísticas de ponta. Apesar disso, o conhecimento da Bioestatística permanece restrito aos especialistas na área.\nNos últimos anos, os pacotes de softwares foram aprimorados, tornando-se mais amigáveis e diminuindo significativamente o pânico ao se defrontar com uma série de números uma vez que a maioria deles exige apenas conhecimento básico de matemática.\nPara a tomada de decisão em saúde é fundamental o acúmulo de conhecimento adquirido através da prática clínica, geradora da experiência do profissional, do intercâmbio com os pares e da análise adequada das evidências científicas publicadas em periódicos de qualidade. Para atingir este objetivo, é fundamental o conhecimento de bioestatística, incluindo aqui que o pensamento que deve nortear os profissionais da saúde ao lidar com o ser humano é o pensamento probabilístico.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#importância-da-bioestatística",
    "href": "intro.html#importância-da-bioestatística",
    "title": "1  Introdução",
    "section": "",
    "text": "Estatística é a disciplina interessada com o tratamento dos dados numéricos obtidos a partir de grupos de indivíduos",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-historia",
    "href": "intro.html#sec-historia",
    "title": "1  Introdução",
    "section": "1.2 Pílulas históricas da Estatística",
    "text": "1.2 Pílulas históricas da Estatística\n\n A história deve começar em algum lugar, mas a história não tem começo (3) \n\nEntretanto, é natural, que se trace as raízes voltando ao passado, tanto quanto possível. Alguns referem-se à curiosidade em relação ao registro de dados à dinastia Shank, na China, possivelmente no século XIII a.c, com a realização de censos populacionais. Há relatos bíblicos de possíveis censos realizados por Moisés (1491 a.C.) e por Davi (1017 a.C.).\nOs romanos e os gregos já realizavam censos por volta do século VIII a IV a.C. Em 578-534 a.C., o imperador Servo Túlio mandou realizar um censo de população masculina adulta e suas propriedades que serviu para estabelecer o recrutamento para o exército, para o exercício dos direitos políticos e para o pagamento de impostos. Os romanos fizeram 72 censos entre 555 a.C. e 72 d.C. A punição para quem não respondia, geralmente era a morte! Na Idade Média, na Europa, existem registros de diversos censos: durante o domínio muçulmano, na Península Ibérica, nos séculos VII a XV; no reinado de Carlos Magno (712-814) e ainda o maior registro estatístico feito na época, o Domesday Book (Figura 1.1), realizado na Inglaterra, por Guilherme I (3) , o Conquistador, onde registravam nascimentos, mortes, batismos e casamentos. Houve, também, recenseamentos nas repúblicas italianas no século XII ao XIII (4).\n\n\n\n\n\n\n\n\nFigura 1.1: Domesday Book\n\n\n\n\n\nJohn Graunt (24/04/1620 - 18/04/1674) foi um cientista britânico a quem se deve vários estudos demográficos ingleses. Foi o precursor da construção de Tábuas de Mortalidade. Realizou estudos com William Petty (1623 - 1687), economista britânico que propôs a aritmética política.\nEm 1791, Sir John Sinclair (1754 - 1835) concebeu um plano de uma pesquisa empírica na Escócia para fornecer informações estatísticas. Foi a primeira vez que o termo estatística foi usado em inglês.\nGirolamo Cardano (24/09/1501 - 21/09/1576) foi um médico, matemático, físico e filósofo italiano. É tido como o primeiro a introduzir ideias gerais da teoria das equações algébricas e as primeiras regras da probabilidade, descritas no livro Liber de Ludo Aleae, publicado em 1663. Descreveu pela primeira vez a clínica da febre tifoide. Foi amigo de Leonardo da Vinci.\nPierre-Simon Laplace, Marquês Laplace (23/03/1749 - 05/03/1927) foi um matemático, astrônomo e físico francês. Embora conduzisse pesquisas substanciais sobre física, outro tema principal dos esforços de sua vida foi a teoria das probabilidades. Em seu Essai philosophique sur les probabilités, Laplace projetou um sistema matemático de raciocínio indutivo baseado em probabilidades, que hoje coincidem com as ideias bayesianas.\nAntoine Gombaud, conhecido como Chevalier de Méré (1607 - 1684) foi um nobre e jogador. Como não tinha mais sucesso nos jogos de azar, buscou ajuda de Blaise Pascal (19/06/1623 – 19/08/1662), matemático, físico francês, que se correspondeu com Pierre Fermat (matemático e cientista francês), nascendo desta colaboração a teoria matemática das probabilidades (1812). Blaise Pascal foi mais tarde chamado de o Pai da Teoria das Probabilidades.\nA moderna teoria das probabilidades foi atribuída a Abraham De Moivre (25/05/1667 – 27/11/1754), matemático francês, que adquiriu fama por seus estudos na trigonometria, teoria das probabilidades e pela equação da curva normal. Em 1742, Thomas Bayes (1701 – 07/04/1761, matemático e pastor presbiteriano, inglês, desenvolveu o Teorema de Bayes que descreve a probabilidade de um evento ocorrer, baseado em um conhecimento a priori.\nAdrien-Marie Legendre (18/09/1752 - 10/01/1833) foi um matemático francês. Em 1783, tornou-se membro adjunto da Academie des Sciences, instituição que esteve na vanguarda dos desenvolvimentos científicos dos séculos XVII e XVIII. Fez importantes contribuições à estatística, à teoria dos números e à álgebra abstrata.\nJohann Carl Friedrich Gauss (30/04/1777 - 23/02/1855) foi um matemático, astrônomo e físico alemão (Figura 1.2) que contribuiu em diversas áreas das ciências como teoria dos números, estatística, geometria diferencial, eletrostática, astronomia e ótica. Muitos referem-se a ele como o Príncipe da Matemática, o mais notável dos matemáticos. Descobriu o método dos mínimos quadrados e a lei de Gauss da distribuição normal de erros e sua curva em formato de sino, hoje tão familiar para todos que trabalham com estatística.\n\n\n\n\n\n\n\n\nFigura 1.2: Johann Carl Friedrich Gauss\n\n\n\n\n\nLambert Adolphe Jacques Quételet (22/02/1796 - 17/02/1874) foi um astrônomo, matemático, demógrafo e estatístico francês. Seu trabalho se concentrou em estatística social, criando regras de determinação de propensão ao crime\nFrancis Galton (16/02/1822 – 17/01/1911) foi um antropólogo, matemático e estatístico inglês. Entre muitos artigos e livros, criou o conceito estatístico de correlação e da regressão à média. Ele foi o primeiro a aplicar métodos estatísticos para o estudo das diferenças e herança humanas de inteligência. Criou o conceito de eugenia e afirmava que era possível a melhoria da espécie por seleção artificial. Acreditava que a raça humana poderia ser melhorada caso fossem evitados relacionamentos indesejáveis. Isto acompanhava o pensamento burguês europeu da época. Criou a psicometria, onde desenvolveu testes de inteligência para selecionar homens e mulheres brilhantes. Esta teoria teve papel importante na formação do fascismo e nazismo (5).\nWilliam Farr (30/11/1807 - 14/04/1883) foi um médico sanitarista e estatístico inglês, nascido na vila de Kenley, Shropshire. Foi o primeiro investigador a examinar séries temporais de morbimortalidade para longos períodos e, assim, considerado o criador da Estatística da Saúde Pública Moderna. Seus relatórios foram fundamentais para o desencadeamento das reformas sanitárias britânicas, em meados e final do século XIX (6).\nFlorence Nightingale (12/05/1820 – 13/08/1910) foi uma enfermeira (Figura 1.3) que ficou famosa por ser pioneira no tratamento de feridos, durante a Guerra da Criméia (7). Ficou conhecida na história pelo apelido de “A dama da lâmpada”, pelo fato de servir-se de uma lamparina para auxiliar no cuidado aos feridos durante a noite. Também contribuiu no campo da Estatística, sendo pioneira na utilização de métodos de representação visual de informações, como por exemplo gráfico de setores (habitualmente conhecido como gráfico do tipo “pizza”)\n\n\n\n\n\n\n\n\nFigura 1.3: Florence Nightingale\n\n\n\n\n\nJohn Snow (York, 15/03/1813 - Londres, 15/03/1858) foi um médico inglês (Figura 1.4)), considerado pai da Epidemiologia Moderna. Recebeu, em 1853, o título de Sir após ter anestesiado a rainha Vitória no parto sem dor de seu oitavo filho, Leopoldo de Albany. Este fato ajudou a divulgar a técnica entre os médicos da época. Demonstrou que a cólera era causada pelo consumo de águas contaminadas com matérias fecais, ao comprovar que os casos dessa doença se agrupavam em determinados locais da cidade de Londres, em 1854, onde havia fontes dessas águas (6).\n\n\n\n\n\n\n\n\nFigura 1.4: John Snow\n\n\n\n\n\nKarl Pearson (27/03/1857 - 27/04/1936) foi um importante estatístico inglês, fundador do Departamento de Estatística Aplicada da University College London em 1911. Juntamente com Weldon e Galton fundou, em 1901, a revista Biometrika com o objetivo era desenvolver as teorias estatísticas, editada até os dias de hoje. O trabalho de Pearson como estatístico fundamentou muitos métodos estatísticos de uso comum, nos dias atuais: regressão linear e o coeficiente de correlação, teste do qui-quadrado de Pearson, classificação das distribuições (8).\nCharles Edward Spearman (10/09/1863 - 17/09/1945) foi um psicólogo inglês conhecido pelo seu trabalho na área da estatística, como um pioneiro da análise fatorial e pelo coeficiente de correlação de postos de Spearman. Ele também fez bons trabalhos de modelos da inteligência humana.\nWilliam Sealy Gosset (13/07/1876 - 16/10/1937) foi um químico e estatístico inglês (Figura 1.5)). Em 1907, enquanto trabalhava químico da cervejaria experimental de Arthur Guinness & Son, criou a distribuição t que usou para identificar a melhor variedade de cevada, trabalhando com pequenas amostras. A cervejaria Guinness tinha uma política que proibia que seus empregados publicassem suas descobertas em seu próprio nome. Ele, então, usou o pseudônimo “Student” e o teste é chamado “t de Student” em sua homenagem (9).\n\n\n\n\n\n\n\n\nFigura 1.5: William Sealy Gosset\n\n\n\n\n\nRonald Aylmer Fisher (17/02/1890 - 29/07/1962) foi um estatístico, biólogo e geneticista inglês. Em 1919, Fisher se envolveu com pesquisa agrícola no centro de experimentos de Rothamsted Research, em Harpenden, Inglaterra, e desenvolveu novas metodologias e teoria no ramo de experimentos (10). Durante sua vida, Fisher (Figura 1.6) escreveu 7 livros e publicou cerca de 400 artigos acadêmicos em estatística e genética . Em um dos seus livros, The design of Experiments (1935), Fisher relata um experimento que surgiu de uma pergunta curiosa: o gosto do chá muda de acordo com a ordem em que as ervas e o leite são colocados? Essa simples questão resultou em um estudo pioneiro na área e serviu de sustentação para análise da aleatorização de dados experimentais (9). Ronald A. Fisher foi descrito (11) como “um gênio que criou praticamente sozinho os fundamentos para o moderno pensamento estatístico”. Era muito temperamental. Seus atritos com outros estatísticos ficaram famosos, entre eles encontra-se ninguém menos do que Karl Pearson, outro notável estatístico.\n\n\n\n\n\n\n\n\nFigura 1.6: Ronald A. Fisher\n\n\n\n\n\nAustin Bradford Hill (08/07/1897 - 18 /04/1991) foi um epidemiologista e estatístico inglês (Figura 1.7)), pioneiro no estudo do acaso nos ensaios clínicos e, juntamente com Richard Doll, foi o primeiro a demonstrar a ligação entre o uso do cigarro e o câncer de pulmão. Hill é amplamente conhecido pelos Critérios de Hill, conjunto de critérios para a determinação de uma associação causal (12).\n\n\n\n\n\n\n\n\nFigura 1.7: Bradford Hill\n\n\n\n\n\nJohn Wilder Tukey (16/06/1915 - 26/07/2000) foi um estatístico norte-americano. Desenvolveu uma filosofia para a análise de dados que mudou a maneira de pensar dos estatísticos, sugerindo que se faça uma visualização dos dados, interpretando o formato, centro, dispersão, presença de valores atípicos, sumarizar numericamente e por fim escolher um modelo matemático. Foi o criador do boxplot e introduziu a palavra “bit” como uma contração do termo binary digit.\nDouglas G. Altman (12 /07/1948 - 03/06/2018) foi um estatístico inglês (Figura 1.8)), conhecido por seu trabalho em melhorar a confiabilidade dos artigos de pesquisa médica (13) e por artigos altamente citados sobre metodologia estatística. Ele foi professor de estatística em medicina na Universidade de Oxford. Há praticamente 30 anos, Altman (14) escreveu um artigo sobre problema da qualidade da pesquisa em medicina que causou um grande impacto e permanece válido até hoje. Nesta publicação ele afirma:\n\n A má qualidade de muitas pesquisas médicas é amplamente reconhecida, mas, de forma perturbadora, os líderes da profissão médica parecem apenas minimamente preocupados com o problema e não fazem nenhum esforço aparente para encontrar uma solução.\n\n\n\n\n\n\n\n\n\nFigura 1.8: Douglas G. Altman",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#história-resumida-do-r",
    "href": "intro.html#história-resumida-do-r",
    "title": "1  Introdução",
    "section": "1.3 História resumida do R",
    "text": "1.3 História resumida do R\nO R é uma linguagem e um ambiente de desenvolvimento voltado fundamentalmente para a computação estatística. Foi inspirado em duas linguagens: S (John Chambers, do Bell Labs) que forneceu a sintaxe e Scheme (Hal Abelson e Gerald Sussman) implementou e forneceu a semântica.\nO nome R provém em parte das iniciais dos criadores, George Ross Ihaka e Robert Gentleman (Figura 1.9), e também de um jogo figurado com a linguagem S. Em 29 de Fevereiro de 2000, o software foi considerado com funcionalidades e estável o suficiente para a versão 1.0.\nO R é um projeto GNU 1. Software Livre significa que os usuários têm liberdade para executar, copiar, distribuir, estudar, alterar e melhorar o software. Foi desenvolvido em um esforço colaborativo de pessoas em vários locais do mundo (15).\nO projeto R fornece uma grande variedade de técnicas estatísticas e gráficas. É uma linguagem e um ambiente similar ao S. A linguagem do S que também é uma linguagem de computador voltada para cálculos estatísticos. Um dos pontos fortes de R é a facilidade com que produções gráficas de qualidade podem ser produzidas. O R é também altamente expansível com o uso dos pacotes, que são bibliotecas para sub-rotinas específicas ou áreas de estudo específicas. Um conjunto de pacotes é incluído com a instalação de R e muito outros estão disponíveis na rede de distribuição do R - Comprehensive R Archive Network (CRAN) (16).\n\n\n\n\n\n\n\n\nFigura 1.9: Robert Gentlemen (E) e George Ross (D)\n\n\n\n\n\nA linguagem R é largamente usada entre estatísticos e analistas de dados para desenvolver softwares de estatística e análise de dados. Pesquisas e levantamentos com profissionais da área da saúde mostram que a popularidade do R aumentou substancialmente nos últimos anos (17).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#sobre-o-autor",
    "href": "intro.html#sobre-o-autor",
    "title": "1  Introdução",
    "section": "1.4 Sobre o autor",
    "text": "1.4 Sobre o autor\nPetrônio Fagundes de Oliveira Filho nasceu em 04/10/1947, em Porto Alegre, Rio Grande do Sul, Brasil. Estudou no Ensino Médio do Colégio do Rosário, em Porto Alegre. Possui graduação em Medicina pela Universidade de Caxias do Sul (UCS), em 1973, residência em Pediatria no Hospital da Criança Conceição, Porto Alegre (1975) e mestrado em Saúde Pública Materno Infantil, Universidade de São Paulo (1998). Em 1980, obteve o Título de Especialista em Pediatria (TEP) e, em 2009, o título de especialista em Estatística Aplicada (UCS). Atuou como Pediatra no INAMPS até 2002 e em consultório privado até hoje. Aposentou-se como professor da Universidade de Caxias do Sul (UCS), em 2019, onde atuou desde 1975, nas áreas de Pediatria, Epidemiologia e Bioestatística, foi coordenador do Serviço e da Residência Médica em Pediatria, chefe de Departamento, coordenador do curso de Medicina e diretor de Ensino do Hospital Geral de Caxias do Sul (Hospital de Ensino da Universidade de Caxias do Sul) e membro de Conselho de Ética em Pesquisa da Universidade de Caxias do Sul, ligado ao CONEP (Conselho Nacional de pesquisa). Durante mais de 20 anos fez parte do Núcleo de Consultoria e Epidemiologia do Centro de Ciência da Saúde (UCS). É autor de dois livros: Epidemiologia e Bioestatística: Fundamentos para a leitura crítica (Editora Rubio, 2015/2018 e ,em 2ª edição, 2022) e SPSS - Análise de Dados Biomédicos, em coautoria com Valter Motta (MedBook, 2009). Além disso, participou de dezenas publicações e de capítulos de outros livros. Desde 1976, é casado com Lena Maria Cantergiani Fagundes de Oliveira. Tem duas filhas, Nathalia e Andressa, e dois lindos e inteligentes netos, Gabriel e Felix. Ah, teve um cão shitzu branco, marrom claro e com algumas manchas pretas, Floquinho, que ao acompanhar seus estudos e análises estatísticas, latia toda vez que ele mencionava o nome de Ronald Fisher. Infelizmente, faleceu em 2024 aos 17 anos, deixando um grande vazio…\n\n\n\n\n1. Armitage P, Berry G, Matthews JNS. Statistical methods in medical research. John Wiley & Sons; 2008. \n\n\n2. Massad E, Silveira PSP, Menezes RX de, Ortega NRS. Métodos quantitativos em medicina. Editora Manole Ltda; 2004. \n\n\n3. Kendall MG. Studies in the history of probability and statistics. Where shall the history of statistics begin? Biometrika. 1960;47(3/4):447–9. \n\n\n4. Breve História dos Censos [Internet]. Instituto Nacional de Estatistica. Statistics Portugal; 2014. Disponível em: https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria\n\n\n5. Salgado-Neto G, Salgado A. Sir Francis Galton e os extremos superiores da curva normal. Revista de Ciências Humanas. 2011;45(1):223–39. \n\n\n6. Stolley PD, Lasky T. The Beginnings of Epidemiology. Em: Investigating Disease Patterns. Scientific American Library; 2000. p. 23–49. \n\n\n7. Editors History com. Florence Nightingale. https://www.history.com/topics/womens-history/florence-nightingale-1; \n\n\n8. Moore DS. Topics in Inferency. Em: The basic practice of statistics. W.H. Freeman; 2000. p. 417. \n\n\n9. Salsburg D. Uma senhora toma chá... Em: Uma senhora toma chá. Zahar; 2009. p. 17–23. \n\n\n10. Hald A. Biography of Fisher. Em: A History of Parametric Statistics Inference from Bernoulli to Fisher,1713-1935. John Wiley & Sons; 2007. p. 159–63. \n\n\n11. Kruskal W. The Significance of Fisher: A Review of R.A. Fisher: The Life of a Scientist. Journal of the American Statistical Association [Internet]. 1980;75(372):1019–30. Disponível em: https://doi.org/10.1080/01621459.1980.10477590\n\n\n12. Stolley PD, Lasky T. Lung Cancer: New Methods of Studying Disease. Em: Investigating Disease Patterns. Scientific American Library; 2000. p. 51–79. \n\n\n13. Matthews R, Chalmers I, Rothwell P. Douglas G Altman: statistician, researcher, and driving force behind global initiatives to improve the reliability of health research. British Medical Journal Publishing Group; 2018. \n\n\n14. Altman DG. The scandal of poor medical research. Vol. 308, Bmj. British Medical Journal Publishing Group; 1994. p. 283–4. \n\n\n15. R Core Team. The R Project for Statistical Computing | What is R? Disponível em: https://www.r-project.org/about.html; 2022. \n\n\n16. R Core Team. The R Project for Statistical Computing | CRAN Mirrors. Disponível em: https://cran.r-project.org/mirrors.html; 2022. \n\n\n17. Whitney L et al. R programming language continues to grow in popularity [Internet]. TechRepublic. 2020. Disponível em: https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introdução",
    "section": "",
    "text": "Esta sigla está associada ao animal gnu africano, símbolo de software de distribuição livre, quer dizer is Not Unix, sigla recursiva muito comum entre nerds!↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "02-dados.html",
    "href": "02-dados.html",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "2.1 Variáveis e Dados\nAs pesquisas manuseiam dados referentes às variáveis que estão sendo estudadas. Variável é toda característica ou condição de interesse que pode de ser mensurada ou observada em cada elemento de uma amostra ou população. Como o próprio nome diz, seus valores são passíveis de variar de um indivíduo a outro ou no mesmo indivíduo. Em contraste com a variável, o valor de uma constante é fixo. As variáveis podem ter valores numéricos ou não numéricos. O resultado da mensuração ou observação de uma variável é denominado dado.\nA Tabela 2.1 mostra um conjunto de variáveis e suas medidas (dados) de um grupo de pacientes internados em uma determinada UTI. O termo medida deve ser entendido num sentido amplo, pois não é possível “medir” o sexo (observação) ou o estado geral (critérios) de alguém, ao contrário do peso e da pressão arterial que podem ser mensurados com instrumentos.\nTabela 2.1: Variáveis e dados\n\n\n\nIdNomeIdadeSexoPASPADEstado Geral1João45masculino14090bom2Maria32feminino11070regular3Pedro27masculino12080grave4Teresa18feminino10060bom",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#população-e-amostra",
    "href": "02-dados.html#população-e-amostra",
    "title": "2  Natureza dos Dados",
    "section": "2.2 População e Amostra",
    "text": "2.2 População e Amostra\nNa pesquisa em saúde, a não ser quando se realiza um censo, coleta-se dados de um subconjunto de indivíduos denominado de amostra, pertencente a um grupo maior, conhecido como população. A população de interesse é, geralmente, chamada de população-alvo. A amostra, para ser representativa da população, deve ter as mesmas características desta. A partir da análise dos dados encontrados na amostra, deduz-se sobre a população. Este processo é denominado de inferência estatística. O interesse na amostra não está propriamente nela, mas na informação que ela fornece ao investigador sobre a população de onde ela provém. A amostra fornece estimativas (estatísticas) da população (Figura 2.1).\n\n População ou população-alvo consiste em todos os elementos (indivíduos, itens, objetos) cujas características estão sendo estudadas.\nAmostra é a parte, subconjunto, da população selecionada para estudo. \n\nEm decorrência do acaso, diferentes amostras de uma mesma população fornecem resultados diferentes. Este fato deve ser levado em consideração ao usar uma amostra para fazer inferência sobre uma população. Este fenômeno é denominado de variação amostral ou erro amostral (Consulte também o Capítulo 9) e é a essência da estatística. O grau de certeza na inferência estatística depende da representatividade da amostra.\nO processo de obtenção da amostra é chamado de amostragem. Mesmo que este processo seja adequado, a amostra nunca será uma cópia perfeita da população de onde ela foi extraída. Desta forma, em qualquer conclusão baseada em dados de uma amostra, sempre haverá o erro amostral. Este erro deve ser tratado estatisticamente tendo em mente a teoria da amostragem, baseada em probabilidades.\n\n\n\n\n\n\n\n\nFigura 2.1: População, amostra e inferência estatística",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#estatística-e-parâmetro",
    "href": "02-dados.html#estatística-e-parâmetro",
    "title": "2  Natureza dos Dados",
    "section": "2.3 Estatística e Parâmetro",
    "text": "2.3 Estatística e Parâmetro\nEstatística é uma característica que resume os dados de uma amostra e o parâmetro é uma característica estabelecida da população. Os valores dos parâmetros são normalmente desconhecidos, porque, na maioria das vezes, é inviável medir uma população inteira. A estatística é um valor aproximado, uma estimativa, do parâmetro. As estatísticas são representadas por letras romanas1 e os parâmetros por letras gregas. Por exemplo, a media da população é representada por \\(\\mu\\) e a média da amostra por \\(\\bar{x}\\); o desvio padrão da população é denotado \\(\\sigma\\) e o desvio padrão da amostra por s.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#escalas-de-medição",
    "href": "02-dados.html#escalas-de-medição",
    "title": "2  Natureza dos Dados",
    "section": "2.4 Escalas de medição",
    "text": "2.4 Escalas de medição\nEm um estudo científico, há necessidade de registrar os dados para que eles representem acuradamente as variáveis observadas. Este registro de valores necessita de escalas de medição. Mensuração ou medição é o processo de atribuir números ou rótulos a objetos, pessoas, estados ou eventos de acordo com regras específicas para representar quantidades ou qualidades dos dados. Para a mensuração das variáveis são usadas as escalas nominal, ordinal, intervalar e de razão (1).\n\n2.4.1 Escala Nominal\nAs escalas nominais são meramente classificativas, permitindo descrever as variáveis ou designar os sujeitos, sem recurso à quantificação. É o nível mais elementar de representação. São usados nomes, números ou outros símbolos para designar a variável. Os números, quando usados, representam códigos e como tal não permitem operações matemáticas. As variáveis nominais não podem ser ordenadas. Podem apenas ser comparadas utilizando as relações de igualdade ou de diferença, através de contagens. Os números atribuídos às variáveis servem como identificação, ou para associá-la a uma dada categoria. As categorias de uma escala nominal são exaustivas e mutuamente exclusivas. Quando existem duas categorias, a variável é dita dicotômica e com três ou mais categorias, politômicas.\nOs nomes e símbolos que designam as categorias podem ser intercambiáveis sem alterar a informação essencial.\nExemplos: Tipos sanguíneos: A, B, AB, O; variáveis dicotômicas: morto/vivo, homem/mulher, sim/não; cor dos olhos, etc.\n\n\n2.4.2 Escala Ordinal\nAs variáveis são medidas em uma escala ordinal quando ocorre uma ordem, crescente ou decrescente, inerente entre as categorias, estabelecida sob determinado critério. A diferença entre as categorias não é necessariamente igual e nem sempre mensuráveis. Geralmente, designam-se os valores de uma escala ordinal em termos de numerais ou postos (ranks), sendo estes apenas modos diferentes de expressar o mesmo tipo de dados. Também não faz sentido realizar operações matemática com variáveis ordinais. Pode-se continuar a usar contagem.\nExemplos: classe social (baixa, média, alta); estado geral do paciente: bom, regular, mau; estágios do câncer: 0, 1, 2, 3 e 4; escore de Apgar: 0, 1, 2… 10.\n\n\n2.4.3 Escala Intervalar\nUma escala intervalar contém todas as características das escalas ordinais com a diferença de que se conhece as distâncias entre quaisquer números. Em outras palavras, existe um espectro ordenado com intervalos quantificáveis. Este tipo de escala permite que se verifique a ordem e a diferença entre as variáveis, porém não tem um zero verdadeiro, o zero é arbitrário.\nO exemplo clássico é a mensuração da temperatura, usando as escalas de: Celsius ou Fahrenheit. Aqui é legítimo ordenar, fazer soma ou médias. No entanto, 0ºC não significa ausência de temperatura, portanto a operação divisão não é possível. Uma temperatura de 40ºC não é o dobro de 20ºC. Se 40ºC e 20ºC forem transformados para a escala Fahrenheit, passarão, respectivamente, para 104ºF e 68ºF e, sem dúvida, 104 não é o dobro de 68!\n\n\n2.4.4 Escala de Razão\nHá um espectro ordenado com intervalos quantificáveis como na escala intervalar. Entretanto, as medidas iniciam a partir de um zero verdadeiro e a escala tem intervalos iguais, permitindo as comparações de magnitude entre os valores. Refletem a quantidade real de uma variável, permitindo qualquer operação matemática.\nOs dados tanto na escala intervalar como na de razão, podem ser contínuos ou discretos. Dados contínuos necessitam de instrumentos para a sua mensuração e assumem qualquer valor em um certo intervalo. Por exemplo, o tempo para terminar qualquer tarefa pode assumir qualquer valor, 10 min, 20 min, 35 min, etc., de acordo com o tipo de tarefa. Outros exemplos: peso, dosagem de colesterol, glicemia.\nDados discretos possuem valores iguais a números inteiros, não existindo valores intermediários. A mensuração é feita através da contagem. Por exemplo: número de filhos, número de fraturas, número de pessoas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#tipos-de-variáveis",
    "href": "02-dados.html#tipos-de-variáveis",
    "title": "2  Natureza dos Dados",
    "section": "2.5 Tipos de Variáveis",
    "text": "2.5 Tipos de Variáveis\nA primeira etapa na descrição e análise dos dados é classificar as variáveis, pois a apresentação dos dados e os métodos estatísticos variam de acordo com os seus tipos. As variáveis, primariamente, podem ser divididas em dois tipos: numéricas ou quantitativas e categóricas ou qualitativas (2).\n\n2.5.1 Variáveis Numéricas\nAs variáveis numéricas são classificadas em dois tipos de acordo com a escala de mensuração: continuas e discretas.\nAs variáveis contínuas são aquelas cujos dados foram mensurados em uma escala intervalar ou de razão, podendo assumir, como visto, qualquer valor dentro de um intervalo de números reais, dependendo da precisão do instrumento de medição. O tratamento estatístico tanto para variável intervalar como de a razão é o mesmo. A diferença entre elas está na presença do zero absoluto. As variáveis numéricas contínuas têm unidade de medida. Por exemplo, um menino de 4 anos tem 104 cm.\nUma variável numérica é considerada discreta quando é apenas possível quantificar os resultados possíveis através do processo de contagem. Também têm unidade de medida – número de elementos. Por exemplo, o número de fraturas, o número de acidentes, etc.\n\n\n2.5.2 Variáveis Categóricas\nAs variáveis categóricas ou qualitativas são de dois tipos: nominal e ordinal, de acordo com a escala de mensuração. Um tipo particularmente comum é uma variável binária (ou variável dicotômica), que tem apenas dois valores possíveis. Por exemplo, o sexo é masculino ou feminino. Este tipo de variável é bastante utilizado na área da saúde, em Epidemiologia. As variáveis nominais não têm quaisquer unidades de medida e a nominação das categorias é completamente arbitrária e pertencer a uma categoria não significa ter maior importância do que pertencer à outra. Uma variável ordinal tem uma ordem inerente ou hierarquia entre as categorias. Do mesmo modo que as variáveis nominais, as variáveis ordinais não têm unidades de medida. Entretanto, a ordenação das categorias não é arbitrária. Assim, é possível ordená-las de modo lógico. Um exemplo comum de uma variável categórica ordinal é a classe social, que tem um ordenamento natural da maioria dos mais desfavorecidos para os mais ricos. As escalas, como a escore de Apgar e a escala de coma de Glasgow (3), também são variáveis ordinais. Mesmo que pareçam numéricas, elas apenas mostram uma ordem no estado dos pacientes. O escore de Apgar (4) é uma escala, desenvolvida para a avaliação clínica do recém-nascido imediatamente após o nascimento. Originalmente, a escala foi usada para avaliar a adaptação imediata do recém-nascido à vida extrauterina. A pontuação pode variar de zero a 10. Uma pontuação igual ou maior do que oito, indica um recém-nascido normal. Uma pontuação de sete ou menos pode significar depressão do sistema nervoso e abaixo de quatro, depressão grave.\nAs variáveis ordinais, da mesma forma que as nominais, não são números reais e não convém aplicar as regras da aritmética básica para estes tipos de dados. Este fato gera uma limitação na análise dos dados.\n\n\n2.5.3 Como identificar o tipo da variável?\nA maneira mais fácil de dizer se os dados são numéricos é verificar se eles têm unidades ligadas a eles, tais como: g, mm, ºC, ml, número de úlceras de pressão, número de mortes e assim por diante. Se não, podem ser ordinais ou nominais – ordinais se os valores podem ser colocados em ordem. A Figura 2.2 é uma ajuda para o reconhecimento do tipo de variável (5).\n\n\n\n\n\n\n\n\nFigura 2.2: Caminho para identificar o tipo de variável\n\n\n\n\n\n\n\n2.5.4 Variáveis Dependentes e Independentes\nDe um modo geral as pesquisas são realizadas para testar as hipóteses dos pesquisadores e, para isso, eles medem variáveis com a finalidade de compará-las. A maioria das hipóteses podem ser expressas por duas variáveis: uma variável explicativa ou preditora e uma variável desfecho (2).\nA variável preditora ou explanatória é a que se acredita ser a causa e também é conhecida como variável independente, porque o seu valor não depende de outras variáveis. Em Epidemiologia, é com frequência referida como exposição ou fator de risco.\nA variável desfecho é aquela que é o efeito, consequência ou resultado da ação de outra variável, por isso, também chamada de variável dependente. Em um estudo que tenta verificar se o tabagismo, durante a gestação, pode interferir no peso do recém-nascido, tem o fumo (variável categórica) como variável preditora (exposição ou fator de risco) e o peso do recém-nascido (variável numérica contínua) como variável desfecho\nNa maioria dos estudos, são utilizadas amostras que fornecem estimativas que, para serem representativas da população, devem ser probabilísticas. Ou seja, a amostra deve ser recrutada de forma aleatória, permitindo que cada um dos membros da população tenha a mesma probabilidade de ser incluído na amostra. Além disso, uma amostra deve ter um tamanho adequado para permitir inferências válidas.\n\n\n\n\n1. Oliveira Filho PF de. Natureza dos Dados. Em: Epidemiologia e Bioestatística–Fundamentos para a Leitura Crítica. 2ª edição. Editora Rubio; 2022. p. 3–6. \n\n\n2. Kirkwood BR, Sterne JA. Defining the Data. Em: Essential Medical Statistics. Second Edition. Blackwell Science Company; 2003. p. 9–14. \n\n\n3. Sternbach GL. The Glasgow coma scale. The Journal of emergency medicine. 2000;19(1):67–71. \n\n\n4. Pediatrics AA of, Obstetricians AC of. The apgar score. Pediatrics. 2006;117(4):1444–7. \n\n\n5. Bowers D. First things first-the nature of data. Em: Medical Statistics from Scratch. Second Edition. John Wiley; Sons; 2008. p. 3–13.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "02-dados.html#footnotes",
    "href": "02-dados.html#footnotes",
    "title": "2  Natureza dos Dados",
    "section": "",
    "text": "Também podem ser representadas pela letra grega correspondente ao respectivo parâmetro com um acento circunflexo, por exemplo, a média amostral é \\(\\hat{\\mu}\\), dita (mü chapéu).↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natureza dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html",
    "href": "03-producaoDados.html",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "3.1 Processo de Pesquisa\nA pesquisa é um processo de construção do conhecimento. O objetivo deste processo é gerar um novo conhecimento e/ou confirmar ou refutar algum conhecimento prévio. A pesquisa é um processo de aprendizagem tanto do pesquisador quanto da sociedade que se beneficiará deste novo conhecimento. Para ser chamada de científica, a pesquisa deve obedecer aos princípios consagrados pela ciência (1).\nA pesquisa nasce de uma dúvida do pesquisador, de algum questionamento que ele considerou interessante sobre o mundo, ou seja, de algo que se costuma chamar de pergunta ou questão da pesquisa. Existem vários motivos que geram questões de pesquisa:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-pesquisa",
    "href": "03-producaoDados.html#processo-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "Avaliação crítica de pesquisas realizadas por outros pesquisadores.\nCondução de uma pesquisa primária com a finalidade de responder uma questão (ou questões), gerando um novo conhecimento ou ampliação do conhecimento existente.\nPara obter habilidades de pesquisa ou experiência, com frequência como parte de um programa educacional.\nTestar a viabilidade de um projeto ou técnica de pesquisa.\n\n\n3.1.1 Questão de Pesquisa\nA pesquisa visa estabelecer novos conhecimentos em torno de um tema específico. O tema de pesquisa pode surgir do próprio interesse ou experiência do pesquisador, ou partir da encomenda de alguma instituição financiadora. Algumas vezes, a pesquisa se origina de outros estudos realizados pelo próprio pesquisador ou outros pesquisadores.\nÀ medida que a ideia da pesquisa cresce, o pesquisador estabelece uma pergunta de pesquisa específica ou um conjunto de questões que ele deseja responder. Algumas vezes, o tema da pesquisa é tão amplo que o pesquisador tem que ter cuidado para não se perder do seu objetivo. Este objetivo é que vai guiá-lo no estabelecimento da pergunta ou perguntas a serem respondidas no estudo. Estes questionamentos são conhecidos como questão de pesquisa ou pergunta de partida.\nO foco da questão de pesquisa pode ser na descrição de um fenômeno clínico. Neste caso a pergunta é dita descritiva, por exemplo, pesquisa de prevalência de uma enfermidade, proporção de utilização de um serviço de saúde, características de um teste, etc. Quando a pergunta busca a explicação para um fenômeno, ela é dita analítica, por exemplo, comparação entre dois fenômenos. Em geral, perguntas analíticas são mais interessantes. Entretanto, as perguntas descritivas são fundamentais no início de um estudo analítico.\nUma boa pergunta de pesquisa deve ter as seguintes características (2):\n\nFactível: o pesquisador deve conhecer desde o início os limites e problemas práticos que podem interferir na pesquisa. A viabilidade está relacionada com o tamanho amostral, com o domínio técnico adequado, com o tempo e custos envolvidos e com um foco dirigido estritamente aos objetivos mais importantes.\nInteressante: a questão de pesquisa deve despertar o interesse não apenas do pesquisador, mas também de seus pares e agentes financiadores.\nNova: a pesquisa deve ser inovadora, original, em algum sentido, para que o estudo seja uma contribuição ao conhecimento ou amplie um conhecimento existente;\nÉtica: se o estudo impõe riscos físicos ou invasão de privacidade ou não traz nenhuma informação nova, o pesquisador deve suspendê-lo. É importante discutir previamente com pesquisadores mais experientes ou com algum representante do Comitê de Ética em Pesquisa da instituição.\nRelevante: nenhuma das características da questão de pesquisa é mais importante do que a sua relevância. Para isto basta pensar nos benefícios que os resultados da pesquisa trarão à Medicina atual.\n\nOu seja, antes de dedicar tempo e esforço para escrever um projeto de pesquisa deve-se avaliar se a questão de pesquisa é FINER (Factível, Interessante, Nova, Ética e Relevante).\n\n\n3.1.2 Hipótese de Pesquisa\nUma vez estabelecida a(s) pergunta(s) de pesquisa adequada(s), os pesquisadores formulam hipóteses para serem testadas. Enquanto a pergunta de pesquisa possa ser um pouco vaga em sua natureza como: “existe uma relação entre o tipo psicológico e a capacidade de parar de usar drogas?” Uma hipótese de pesquisa, necessita ser precisa. Há necessidade de especificar qual o tipo psicológico está relacionado à habilidade de parar de usar drogas.\nA precisão da hipótese é fundamental em um projeto de pesquisa, pois ela determinará o delineamento de pesquisa a ser seguido pelo pesquisador e as técnicas estatísticas apropriadas para a análise dos dados. A fonte e o tipo de dados são determinados pela característica do delineamento recomendado pela hipótese de pesquisa.\nO objetivo da pesquisa, usando o método científico, é refutar ou não as hipóteses de pesquisa. Se a hipótese do pesquisador não for rejeitada, houve a geração de um novo conhecimento.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#processo-de-amostragem",
    "href": "03-producaoDados.html#processo-de-amostragem",
    "title": "3  Produção dos Dados",
    "section": "3.2 Processo de Amostragem",
    "text": "3.2 Processo de Amostragem\nApós o estabelecimento das hipóteses a serem testadas, há necessidade de coletar os dados. Uma vez que é praticamente impossível analisar toda a população que constitui a população-alvo, extrai-se uma amostra desta população. Este processo é denominado de amostragem (3).\nUma amostra deve ser representativa da população, ou seja, deve ter características semelhantes às da população e ser fidedigna. A fidedignidade está relacionada à precisão dos dados que sofrem influência dos instrumentos de aferição, questionários não validados e falhas humanas. Uma amostra inadequada ameaça a validade da pesquisa. Os dados coletados de maneira não aleatória são chamados de evidência anedótica. O nível de confiança nos resultados de uma pesquisa está diretamente relacionado à qualidade da amostra. A amostra deve ser representativa.\nUma amostra deve conter apenas dados úteis que permitam a resposta da pergunta de pesquisa, evitando desperdício e fuga dos objetivos traçados. A aleatoriedade provoca uma diferença entre o resultado da amostra e o verdadeiro valor da população que é denominada erro amostral. Não importa quão bem a amostra seja coletada, os erros amostrais irão sempre ocorrer. Entretanto, não existe técnica estatística que salve amostras coletadas incorretamente, tendenciosas!\n\n3.2.1 Amostras probabilística\nPara evitar vieses, erros sistemáticos, que favorecem determinados desfechos, o ideal é coletar uma amostra probabilística. A amostra probabilística adota o princípio da equiprobabilidade, isto é, “todos os sujeitos da população têm a mesma probabilidade de fazerem parte da amostra”. Esta probabilidade é conhecida e diferente de zero. As amostras probabilísticas têm o potencial de ser possível a generalização para a população; ser imparcial e com menor erro amostral.\nAmostra aleatória simples: é a mais utilizada pois garante representatividade da amostra junto à população. A amostra aleatória simples não emprega nenhum critério particular para a definição da amostra. O mecanismo mais comum de obter este tipo de amostra é por um simples sorteio, em geral, usando programas de computador.\nAmostra aleatória estratificada: quando a população é constituída por subpopulações ou estratos e é razoável supor que a variável de interesse apresenta comportamento diferente nos diferentes estratos, pode-se usar este tipo de amostragem. Neste caso, a amostra deve ter a mesma estratificação da população para ser representativa. Um exemplo comum de estratificação é o nível socioeconômico. A partir do momento que os estratos estão definidos se procede uma amostra aleatória simples de cada estrato.\nAmostra aleatória sistemática: as unidades amostrais são selecionadas a partir de um esquema rígido preestabelecido de sistematização que tem o propósito de abranger toda a população-alvo. Para isso, ordena-se os indivíduos da população (por exemplo, um grande arquivo com 20000 fichas) e calcula-se uma constante conveniente, \\(c = N/n\\), onde \\(N\\) é tamanho da população e \\(n\\) é o tamanho da amostra. Se \\(n = 500\\), a constante será \\(40\\), ou seja, será selecionado aleatoriamente o primeiro membro da amostra (\\(k\\)), de maneira que \\(k\\) seja menor do que a constante e maior do que \\(1\\). A partir daí os sucessivos membros serão: \\(k + c\\) ; \\(k + 2c\\) ; \\(k + 3c\\) ; … até atingir \\(n\\).\nAmostra aleatória por conglomerados (clusters): este tipo de amostra é utilizada quando dentro da população são identificados agrupamentos (clusters) naturais, por exemplo, espaços, vilas, etc. Neste tipo de amostragem o elemento focal não é o sujeito, mas o cluster. Identificados estes, sorteiam-se os conglomerados e se analisa todos os indivíduos dos conglomerados sorteados.\n\n\n3.2.2 Amostras não probabilísticas\nNa amostragem não aleatória ou intencionada há uma escolha deliberada da amostra, subordinada a objetivos específicos do pesquisador. Não há garantia de representatividade da população. É importante averiguar, neste tipo de amostragem, a presença de conflitos de interesse.\nAmostra de conveniência: é uma técnica comum onde é selecionada uma mostra que esteja acessível. Em outras palavras, os indivíduos são recrutados porque eles estão prontamente disponíveis. Neste tipo de amostra há incapacidade de fazer afirmações gerais com rigor estatístico sobre a população.\nAmostra por cotas: é uma versão não probabilística da amostra estratificada. Tem três etapas:\n\nSegmentação, onde se divide em grupos, por exemplo, sexo, classe social, região, etc.;\nDefinição do tamanho das cotas;\nSeleção por meio de amostras de conveniência.\n\nAmostra de resposta voluntária: o pesquisador solicita aos membros de uma população-alvo para que eles participem da amostra e as pessoas decidem se entram ou não. Esses tipos de amostras são enviesados porque as pessoas podem ter interesses particulares ou opiniões negativas e tendem a querer participar.\n\n\n3.2.3 Tamanho amostral\nA determinação do tamanho de uma amostra é de suma importância, pois amostras desnecessariamente grandes acarretam desperdício de tempo e de dinheiro e amostras muito pequenas podem levar a resultados não confiáveis, ameaçando a validade da pesquisa.\nNão existe um número estabelecido para o tamanho da amostra. Há uma solução para cada caso. O tamanho da amostra depende (4):\n\ndo tipo de problema;\ndo tipo de variável;\nda magnitude do erro estatístico aceito pelo pesquisador;\nda diferença minimamente importante entre os grupos;\nda probabilidade de que a amostra identifique uma diferença verdadeira: Poder estatístico;\ndo tempo, dinheiro e pessoal disponível, bem como da dificuldade em se obterem dados e da complexidade da pesquisa.\n\nO tamanho amostral mínimo é determinado por fórmulas estatísticas complexas. Os cálculos são muito pesados, mas agora, felizmente, existem programas de computador disponíveis que realizam este trabalho, por exemplo o G-Power3 (5). Além disso, é possível acessar um site que fornece informações e ferramentas para o cálculo amostral em pesquisas da área da saúde 1. Existem tabelas extensas para calcular o número de participantes (6) para um determinado nível de poder (e vice-versa).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "href": "03-producaoDados.html#principais-delineamentos-de-pesquisa",
    "title": "3  Produção dos Dados",
    "section": "3.3 Principais Delineamentos de Pesquisa",
    "text": "3.3 Principais Delineamentos de Pesquisa\nEm geral, a pesquisa clínica, é dividida em dois tipos de investigação. O primeiro é aquele em que o observador apenas observa o doente, as características da sua doença e sua evolução, sem atuar de modo a modificar qualquer aspecto que esteja estudando. Trata-se de estudo observacional.\nO segundo corresponde aos estudos experimentais, onde o pesquisador não se limita a observar, mas promove uma intervenção com o objetivo de conhecer os efeitos dessa sobre os participantes da pesquisa. A intervenção pode ser a prescrição de um medicamento, uma dieta, atividade física ou repouso, ou simplesmente, o estabelecimento de um programa de atenção à saúde.\nOs estudos podem ser também classificados em primários ou secundários ou integrativos (7). Estudos primários correspondem a pesquisas originais que constituem a maioria das publicações encontradas nas revistas médicas. Estudos secundários são aqueles que procuram sumarizar e extrair conclusões de estudos primários\n\nEstudos Primários\n\nEstudos Observacionais\n\nRelato de Caso e Série de Casos\nEstudo Transversal\nEstudo Caso-controle\nEstudo de Coorte\n\nEstudos Experimentais\n\nExperimento laboratorial\nEnsaio Clínico\n\n\nEstudos Secundários\n\nRevisões não sistemáticas\nRevisões Sistemáticas\nDirerizes (Guidelines)\nAnálise de decisão\nAnálise Econômica\n\n\n\n3.3.1 Elementos básicos de um delineamento de pesquisa\nOs estudos contêm três elementos básicos:\n\nVariáveis componentes: Nas investigações das relações entre as variáveis identificam-se pelo menos duas variáveis nos estudos epidemiológicos.\n\nDesfecho: Aquilo que vai acontecer durante uma investigação na mensuração da condição de saúde-doença. Sinônimo: variável dependente.\nExposição: O fator que precede o desfecho. Sinônimos: fator em estudo, variável preditora, variável independente.\n\nTemporalidade: Quanto ao tempo os estudos podem ser contemporâneos, retrospectivos e prospectivos, de acordo como os dados são obtidos em relação ao momento atual.\nEnfoque: Um estudo pode ter vários enfoques. Na maioria deles, na área médica, eles relacionam-se à prevenção, ao diagnóstico, à terapêutica e ao prognóstico.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#estudos-observacionais",
    "href": "03-producaoDados.html#estudos-observacionais",
    "title": "3  Produção dos Dados",
    "section": "3.4 Estudos Observacionais",
    "text": "3.4 Estudos Observacionais\n\n3.4.1 Relato de Caso ou Série de casos\nNo relato de caso, descrevem-se casos raros, eventos não comuns ou inesperados, doenças desconhecidas ou raras. Um evento notável deve ser identificado. Um relato de caso tem a descrição de até dez casos. Acima deste número tem-se uma série de casos (8).\nMetodologicamente, faz-se um relato descritivo simples de características interessantes observadas em um paciente ou grupo de pacientes. Os indivíduos são acompanhados em um espaço de tempo curto e não possuem participantes-controles. A coleta dos dados é, na maioria das vezes, retrospectiva.\nUma série de casos não é planejada e não envolve quaisquer hipóteses investigativas. Pode ser empregada como precursor de outros estudos.\n\n\n3.4.2 Estudos Transversais ou Seccionais\nOs estudos transversais são também conhecidos como estudos seccionais. Este tipo de estudo fornece a informação sobre a prevalência, ou seja, a proporção dos indivíduos que tem a doença ou condição clínica em um determinado momento. Por este motivo são também conhecidos como estudos de prevalência (9).\nObservam dados coletados em um grupo de indivíduos em um único momento, sem um período de seguimento. O desfecho e exposição são avaliados no mesmo momento no tempo. Os dados são coletados apenas uma vez para cada indivíduo, podendo ser em dias diferentes em diferentes sujeitos. As informações são, em geral, obtidas em um curto espaço de tempo.\nÉ um estudo estático, representa a “fotografia” de um momento. Entretanto, se as variáveis preditora e de desfecho são definidas apenas com base nas hipóteses causa-efeito do investigador e não no delineamento do estudo, é possível também examinar associações.\nOs estudos de corte transversal, de um modo geral, são desenhados para determinar “O que está acontecendo?”. São usados para:\n\nDeterminar a prevalência de uma doença, como a prevalência de HIV em gestantes.\nPesquisar atitudes ou opiniões em relação a um determinado assunto (pesquisa de satisfação)\nVerificar interrelações entre variáveis, como observação das características de fumantes pesados em relação ao sexo, idade, etc.\nEnquetes\n\n\n3.4.2.1 Cuidados na interpretação de dados de estudos transversais\n\nEfeito temporal\n\nComo os dados (exposição e desfecho) são coletados no mesmo momento, fica difícil estabelecer qualquer relação temporal entre eles (dilema ovo/galinha). Por exemplo, não é possível estabelecer uma relação de causalidade entre hipertensão e doença cardíaca se os dados são coletados de forma a ficar impossível saber que surgiu em primeiro lugar.\n\nEstudos transversais repetidos\n\nOs estudos transversais, algumas vezes, são repetidos em outro momento ou em outros locais com a finalidade de verificar variabilidade nos achados. Por exemplo, medir a prevalência de uma doença em momentos diferentes ou em diferentes locais. Os indivíduos serão um pouco diferentes, devendo-se interpretar as diferenças destes resultados com cautela.\n\nEstudos transversais que parecem longitudinais\n\nUma armadilha comum é confundir um estudo seccional com um longitudinal porque os dados foram coletados através do tempo até completar o tamanho amostral previsto. O importante é que os dados (variável preditora e desfecho) foram coletados somente uma vez para cada indivíduo e no mesmo momento. Isto gera uma interpretação errônea se analisarmos como um estudo longitudinal.\n\n\n3.4.2.2 Análise dos Estudos Transversais\nQuando se compara a prevalência de doença em expostos e não expostos, a medida de associação usada é a Razão de Prevalência Pontual (RPP).\n\n\n\n3.4.3 Estudos Caso-Controle\nPara examinar a possível associação de uma exposição a uma determinada doença, identifica-se um grupo de doentes (casos) e, com a finalidade de comparação, um grupo de pessoas sem a doença (controles) e determina-se a chance (odds) de exposição e não exposição entre casos e entre controles.\nOs estudos caso-controle, portanto, partem da presença ou ausência de um desfecho e após olham para trás no tempo (retrospectivamente) para detectar possíveis fatores de risco (Figura 3.1)) (10). Analisam o que aconteceu e são usados para investigar fatores de risco de doenças raras onde um estudo prospectivo seria muito longo para identificar uma quantidade suficiente de casos.\nÉ útil também para investigar surtos agudos (infecção alimentar) para identificar se existe ou não associação entre a exposição e o desfecho investigado. Com frequência, os estudos caso-controle são o primeiro passo na busca de uma etiologia quando há suspeita de que alguma de várias exposições esteja associada a uma determinada doença.\n\n\n\n\n\n\n\n\nFigura 3.1: Desenho de um estudo caso controle.\n\n\n\n\n\n\n3.4.3.1 Seleção dos casos\nOs casos podem ser selecionados de várias fontes, incluindo indivíduos hospitalizados, de consultórios ou clínicas, principalmente quando registros adequados são mantidos.\nMuitos problemas podem ocorrer na seleção de casos, neste tipo de estudo. Se os casos forem selecionados de um único hospital, quaisquer fatores de risco identificados podem ser apenas daquele hospital, em decorrência do padrão de referência e nível de atendimento (um hospital terciário que apenas atende um determinado convênio, por exemplo, o Sistema Único de Saúde). Por isso, devem ser utilizados casos procedentes de vários hospitais da comunidade, pois aí os casos pertenceriam a diferentes grupos sociais e diferentes graus de gravidade da doença.\nCasos incidentes ou prevalentes\nOs casos usados nos estudos caso-controle podem ser casos incidentes (recém-diagnosticados) ou casos prevalentes da doença (pessoas que apresentaram a doença em algum período).\nO problema do uso de casos incidentes é que há necessidade de se esperar que novos casos sejam diagnosticados e isto pode requerer muito tempo. Enquanto os casos prevalentes já estão disponíveis havendo um maior número disponível para o estudo. Em ambos os modelos existem problemas, pois nos casos prevalentes algumas pessoas podem morrer logo após o diagnóstico e estarem pouco representadas no estudo. Por outro lado, nos casos incidentes, serão excluídos os pacientes que morreram antes do diagnóstico ser feito. Não existe uma solução fácil para este problema, mas é importante lembrar-se destas questões ao interpretar os resultados e tirar conclusões do estudo.\n\n\n3.4.3.2 Seleção dos controles\nDa mesma forma do que nos estudos experimentais, a escolha dos controles afeta a comparação com os casos (11). A escolha dos controles inclui:\n\nPacientes do mesmo hospital, mas com condições ou doenças não relacionadas;\nPacientes pareados um a um em relação a fatores prognósticos, tais como sexo e idade;\nUma amostra aleatória originária da mesma população de onde provêm os casos.\n\nSem dúvida, o melhor grupo controle é a terceira opção, mas esta é raramente possível. Por este motivo, alguns estudos caso-controle incluem mais de um grupo controle para tornar o estudo mais robusto\nControles pareados\nO emparelhamento é definido como processo de seleção dos controles para que sejam semelhantes aos casos em algumas características como, por exemplo, idade, gênero, raça, condição socioeconômica e ocupação.\nControles emparelhados são bastante comuns. O autor deve ter o cuidado de especificar cuidadosamente o modo como houve o pareamento. Por exemplo, “emparelhado por idade dentro de dois anos” mostra a amplitude do pareamento. É difícil realizar o emparelhamento para muitos fatores, pois um pareamento seguro não existe. Em um delineamento pareado, a análise estatística deve levar em conta o emparelhamento e os fatores usados por ele. Onde um indivíduo em um par tiver um dado perdido, ambos devem ser omitidos da análise estatística.\n\n\n3.4.3.3 Estudos caso-controle aninhados\nUm delineamento do tipo caso-controle aninhado é um estudo de caso-controle ’’aninhado” em um estudo de coorte (12). É um excelente desenho para variáveis preditoras que são caras para medir e que podem ser avaliadas no final do estudo em indivíduos que desenvolvem o resultado durante o estudo (casos) e em uma amostra daqueles que não o fazem (controles).\nO investigador começa com uma coorte adequada (Figura 3.2) (13) com casos suficientes ao final do acompanhamento para fornecer poder adequado para responder à pergunta de pesquisa. No final do estudo, aplica critérios que definem o resultado de interesse para identificar todos aqueles que desenvolveram o resultado (casos). Em seguida, seleciona uma amostra aleatória dos indivíduos que não desenvolveram o resultado (controles).\nA principal razão para usar delineamentos caso-controle aninhado é reduzir o trabalho e o custo na coleta de dados. A principal desvantagem desse projeto é que muitas questões e circunstâncias da pesquisa não são passíveis de armazenamento para posterior análise.\n\n\n\n\n\n\n\n\nFigura 3.2: Desenho de um estudo caso-controle aninhado.\n\n\n\n\n\n\n\n3.4.3.4 Estudo caso-controle de base populacional\nSão os estudos caso-controle onde os casos e controles são uma amostra completa ou probabilística de uma população definida.\n\n\n3.4.3.5 Limitações dos estudos caso-controle\nVárias limitações podem afetar os estudos caso-controle:\n\nA escolha do grupo controle afeta as comparações entre casos e controles;\nOs dados da exposição ao fator de risco são coletados retrospectivamente e dependem da memória dos participantes, registros médicos e, portanto, podem ser incompletos, sem acurácia ou enviesados (viés de memória);\nSe o processo que conduz à identificação dos casos está relacionado a um possível fator de risco, a interpretação dos resultados será difícil (viés averiguação).\n\nPor exemplo: suponha que os casos sejam mulheres jovens com hipertensão selecionadas de uma clínica de contracepção. Nesta situação, um possível fator de risco, o anticoncepcional oral (ACO), estará vinculado à seleção dos casos e, desta forma, o uso de ACO será mais comum entre os casos do que entre os controles populacionais.\n\n\n\n\n3.4.3.6 Análise dos Estudos Caso-controle\nA principal estratégia de análise é o cálculo da odds ratio (Razão de Chances), que pode ser interpretado como uma estimativa do Risco Relativo.\nO Risco Relativo somente pode ser calculado quando é possível o cálculo da incidência (ver Seção 18.5.2). Nos estudos caso-controle, isso não é possível, pois aqui o estudo começa com casos e controles em vez de indivíduos expostos e não expostos ao fator de risco. Desta maneira, se comparam as odds (chance) de uma exposição passada a um fator de risco suspeitado em indivíduos doentes e em controles não doentes. Esta relação é denominada de odds ratio (ver Seção 18.5.1).\n\n\n\n3.4.4 Estudos de Coorte\nOs estudos de coorte são considerados o padrão-ouro dos estudos observacionais. Seu nome se originou das coortes dos soldados romanos, cada uma delas constituída por 480 a 600 legionários. As coortes romanas eram distintas entre si e tinham sua identidade determinada por, ao menos, uma característica comum entre os indivíduos de cada grupo. Podia ser por características estratégicas no campo de batalha, por uma cor presente na indumentária, ou outras. Em Epidemiologia, o termo coorte permaneceu com significado semelhante.\nEm um estudo de coorte, um grupo de pacientes sadios (coorte), expostos ou não a um suspeitado fator de risco, é seguido através do tempo para determinar a incidência da doença em questão em cada um dos grupos (14).\nNeste modelo de estudo, a característica comum aos dois grupos é a exposição. Tem-se uma coorte de expostos e uma coorte de não expostos que são acompanhadas por um período de tempo que permita o aparecimento do desfecho. No final do estudo, compara-se a incidência do desfecho (doença) entre os expostos com a incidência do desfecho entre os não expostos. Se existe uma associação positiva entre a exposição e o desfecho, se espera que a incidência do desfecho entre os expostos seja maior do que a incidência de desfecho entre não expostos.\nUm esquema simplificado de um estudo de coorte é mostrado na Figura 3.3 (15).\n\n\n\n\n\n\n\n\nFigura 3.3: Desenho de um estudo de coorte sobre risco.\n\n\n\n\n\nObservar que como se identifica novos casos (incidência) à medida que eles ocorrem, é possível determinar uma relação temporal entre a exposição e a doença, isto é, se a exposição precedeu o início da doença. Isto é fundamental para estabelecer uma relação causal entre a exposição e a doença.\nOs estudos de coorte têm semelhança com os ensaios clínicos randomizados. Ambos os estudos comparam grupos expostos a grupos não expostos. Não havendo possibilidade de realizar a randomização, por exemplo, por motivos éticos quando a exposição é sabidamente prejudicial, é indicado um estudo de coorte. A diferença fundamental, portanto, é a ausência de randomização nos estudos de coorte.\nExistem duas maneiras básicas para formar os grupos:\n\nSeleciona-se a população-alvo baseado no fato dos indivíduos estarem expostos ou não ao fator em estudo (Figura 3.3);\nOu seleciona-se a população-alvo antes que qualquer um dos seus membros se torne exposto, ou antes, que a exposição seja identificada (Figura 3.4). Um exemplo típico deste modelo é o clássico Estudo de Framingham (16).\n\n\n\n\n\n\n\n\n\nFigura 3.4: Desenho de uma coorte com grupos expostos e não expostos. (17).\n\n\n\n\n\n\n3.4.4.1 Tipos de estudo de coorte\nDe acordo com as características do seguimento, as coortes podem ser:\n\nEstudo de Coorte Prospectivo (Coorte Concorrente ou Longitudinal), onde os grupos são montados no presente, coletados os dados basais deles e continua-se a coletar dados com o passar do tempo até a doença se desenvolver ou não.\nEstudo de Coorte Retrospectivo ou Histórico (Coorte não concorrente), onde a exposição é avaliada em dados passados e o desfecho (doença ou não) é verificado no momento do início do estudo. O problema aqui é que a averiguação da exposição depende dos registros pregressos.\nEstudo de Coorte Misto (Prospectivo e Retrospectivo), onde a exposição é verificada em registros objetivos no passado (como em uma coorte histórica) e o seguimento e a medida do desfecho se fazem no futuro.\n\n\n\n3.4.4.2 Vieses em estudos de coorte\nOs potenciais vieses nos estudos de coorte são os seguintes:\n\nViés de confusão – é a grande ameaça dos estudos observacionais. O confundimento causa um erro sistemático na inferência, podendo aumentar ou diminuir uma associação observada entre exposição e doença. Uma variável funciona como fator de confusão quando ela está associada com a exposição e ao mesmo tempo com a doença. Ela não deve fazer parte da cadeia causal da exposição à doença. Por exemplo, num estudo sobre fatores de risco, uma associação entre o hábito de beber café e a doença coronária é detectada. Porém, se não for considerado o fato de que os fumantes bebem mais café do que os não-fumantes, pode-se chegar à errônea conclusão de que o café é um fator de risco independente para doença coronária, o que não corresponde à realidade. Neste caso, o café é um fator de confusão e não um fator causal independente para a doença coronária (18).\nViés na avaliação dos desfechos – este viés pode ocorrer quando o pesquisador que avalia o desfecho também sabe sobre o status de exposição dos sujeitos da pesquisa. Evita-se este problema “cegando” a pessoa que faz a avaliação da doença.\nViés de informação – ocorrem principalmente em estudos históricos onde as informações dependem de registros passados e podem ser diferentes entre as pessoas expostas e não expostas.\nViés de não resposta e perdas de acompanhamento – a não participação e as perdas podem introduzir um grande viés, alterando o cálculo da incidência nos expostos e entre os não expostos.\nViés de análise – se os estatísticos tiverem alguma hipótese em relação aos dados que estão analisando, eles podem introduzir vieses em suas análises.\n\n\n\n3.4.4.3 Análise dos estudos de coorte\nPara verificar se existe associação entre certo desfecho (doença) e uma determinada exposição calcula-se o Risco Relativo (RR). Este é definido como a razão entre a incidência (risco) em expostos e a incidência (risco) em não expostos (ver Seção 18.5.2).\n\n\n3.4.4.4 Vantagens e desvantagens dos estudos de coorte\n\nVantagens\n\nAdequado para exposições raras\nBom poder para testar hipóteses\nImportante em estudos etiológicos e prognósticos\nSalienta os múltiplos desfechos de uma exposição\n\nDesvantagens\n\nInadequado em desfechos raros\nPerdas no seguimento levam a viés de seleção\nDemorado/elevado custo",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#ensaios-clínicos",
    "href": "03-producaoDados.html#ensaios-clínicos",
    "title": "3  Produção dos Dados",
    "section": "3.5 Ensaios Clínicos",
    "text": "3.5 Ensaios Clínicos\nExperimentos são estudos nos quais o pesquisador manipula a variável preditora (intervenção) e observa o efeito no desfecho que está sendo avaliado ao longo do tempo. A abordagem experimental, especificamente, o ensaio clínico randomizado controlado é a ferramenta de escolha para comparar terapêuticas ou intervenções.\nOs estudos experimentais podem também comparar os cuidados prestados por serviços de saúde, programas de educação em saúde e estratégias administrativas. Os estudos experimentais realizados com seres humanos são denominados de ensaios clínicos.\nNos ensaios clínicos não controlados os indivíduos servem como seus próprios controles (antes-e-depois). Os resultados destes estudos estão sujeitos vários problemas:\n\nMelhora previsível. Paciente melhora espontaneamente e não pelo tratamento.\nFlutuação na gravidade da doença.\nEfeito Hawthorne: o indivíduo melhora pela atenção e não pela terapêutica (19).\nRegressão à média: uma limitação importante surge quando se quer avaliar a evolução de um grupo que tenha sido selecionado por estar no extremo de uma distribuição sem que haja um grupo controle. Empiricamente, observa-se que indivíduos que se encontrem num determinado momento, em um dos extremos de uma distribuição, tendem a estarem menos distantes da média em um momento posterior, sem que qualquer intervenção tenha sido desenvolvida. Este fenômeno é conhecido como efeito de regressão à média. Por exemplo: uma pessoa com uma doença crônica tem dias piores e outros melhores. Se ela é medicada com gotas homeopáticas ou faz uso de florais nos dias em que se sente excepcionalmente mal vai notar que é frequente uma melhora, seguindo estes “tratamentos”. Não que eles funcionem, mas pela regressão à média (20).\n\n\n3.5.1 Características de um ensaio clínico\nUm ensaio clínico deve ter algumas características fundamentais (Figura 3.5) (21):\n\nOs indivíduos devem ser designados por randomização para os grupos de comparação.\n\nA randomização é a melhor abordagem no delineamento de um ensaio clínico (22).\nRandomizar significa sortear (por meio de computadores, tábua de números aleatórios) os indivíduos para decidir a alocação dos mesmos em um dos grupos de estudo. O elemento decisivo da randomização é a imprevisibilidade da próxima alocação.\n\nO pesquisador compara o grupo de estudo com um grupo controle apropriado.\nO investigador manipula a variável independente (preditora).\n\n\n\n\n\n\n\n\n\nFigura 3.5: Estrutura de um ensaio clínico randomizado.\n\n\n\n\n\n\n\n3.5.2 Elementos básicos de um ensaio clínico\n\nSeleção dos participantes\n\nOs pesquisadores devem determinar e explicar detalhadamente os critérios de inclusão e de exclusão:\n\nObjetivos dos critérios de inclusão e exclusão\n\nRestringir a heterogeneidade da amostra\nDiminuir o número de variáveis independentes\nFazer com que exista uma chance maior de que as diferenças nos desfechos estejam relacionadas aos tratamentos\nMelhorar a validade interna, ou seja, o grau em que os resultados do estudo são consistentes para aquela amostra particular de indivíduos. Esta validade depende basicamente do rigor metodológico usado para delinear o ensaio clínico, podendo ser ameaçada por dois tipos de erros: sistemático ou aleatório.\n\nTornar a generalização (validade externa) mais precisa. Entretanto deve-se ter cuidado com critérios de inclusão e exclusão muito rígidos, pois podem diminuir esta capacidade de generalização\n\n\nO grau de detalhamento deve ser suficientemente preciso para permitir que outros reproduzam o estudo. O tamanho da amostra deve ser claramente determinado pelo poder do teste estatístico. Poder é a habilidade de o teste estatístico detectar diferenças entre os grupos, dado que tais diferenças existam na população em estudo. Lembrar que resultados não significativos podem ser apenas uma evidência para um inadequado tamanho amostral.\nO grupo controle deve ser selecionado utilizando-se os mesmos critérios do grupo experimental. Prestar atenção em possíveis armadilhas que podem gerar vieses:\n\nUso de grupo controle histórico (não concorrente);\nGrupo controle selecionado de outros locais (outras clínicas, outros hospitais).\n\nO grupo controle adequado é um grupo controle concorrente, tratado no mesmo momento e no mesmo local do grupo experimental. O característico é o grupo controle não receber tratamento. Mais comumente recebem um placebo, indistinguível do tratamento experimental, mas sem componente ativo. Mesmo assim, pode haver melhora dos participantes do grupo controle (Efeito Placebo ) (23). Quando não for ético suspender o tratamento e administrar placebo, o grupo controle pode ser constituído por indivíduos que recebem o tratamento padrão.\n\nAlocação\n\nA alocação deve ser aleatória. A randomização é a principal técnica para reduzir o viés, criando grupos homogêneos. Como foi visto, é uma das características fundamentais dos ensaios clínicos. O poder da randomização depende da ocultação da sequência de alocação.\nA randomização pode ser:\n\nCompleta: os indivíduos que obedecem ao critério de inclusão e exclusão são randomizados de modo que todos têm a mesma probabilidade de pertencer a cada um dos grupos. Isto maximiza o poder. Pode ser feita por blocos para assegurar a igualdade numérica dos grupos (estudos multicêntricos).\nEstratificada: os participantes são estratificados de acordo com possíveis variáveis de confusão (gravidade da doença, idade, sexo, etc.) e a randomização é realizada dentro de cada estrato.\nRandomização e alocação desigual: os sujeitos têm uma maior probabilidade de ser randomizados em um grupo (em geral, grupo experimental) do que o outro (comparação). Este tipo de estudo tem menor poder.\n\n\nCondução/Seguimento/Avaliação\n\nEm um ensaio clínico deve estar assegurado de que o estudo tenha um tempo de seguimento adequado, pois nem todos os indivíduos participam conforme o plano original. Podem ocorrer perdas de alguns pacientes durante o acompanhamento, seja porque com o tempo se constata que eles não têm a doença em estudo ou porque não aderiram ao tratamento ou intervenção e abandonaram o estudo. Quanto maior o número de pacientes perdidos e menos informações sobre eles, menos confiança pode ser colocada nos resultados do estudo. De um modo geral, não se deve tolerar perdas que sejam maiores que a incidência do desfecho no estudo. Uma regra simples é que perdas menores que 5% produzem pouco viés e perdas maiores que 20% são uma ameaça importante à validade do estudo. As perdas entre 5 e 20% devem ser avaliadas com cuidado, se possível utilizando-se uma análise de sensibilidade (pior cenário), principalmente se as perdas forem diferentes nos grupos pelo maior risco de viés.\nNeste tipo de análise, nos estudos com resultado positivo, todos os pacientes perdidos no grupo experimental, inicialmente, são considerados como tendo o desfecho. Posteriormente, analisa-se como se nenhum dos indivíduos perdidos no grupo controle atingiu o desfecho. Se o resultado permanecer positivo, as perdas não afetaram a validade do estudo. Estudos sem relato adequado ou nenhum relato de perdas ou exclusões devem ser avaliados com muito cuidado.\nOutro aspecto importante, no seguimento dos sujeitos da pesquisa, é o tratamento igual de todos os grupos. Para garantir este princípio, utiliza-se da técnica de cegamento ou mascaramento (24). Esta técnica impede que os participantes da pesquisa (pesquisadores, avaliadores e participantes) tomem conhecimento de qual grupo de tratamento o participante se encontra. Este conhecimento antecipado pode influenciar as expectativas, as opiniões e as crenças em relação aos resultados do estudo. O cegamento tem como principal finalidade a eliminação do viés de aferição, além de melhorar a adesão ao tratamento, reduzir as perdas de seguimento e diminuir o viés causado por co-intervenções (assistência suplementar maior para um dos grupos).\nQuando o cegamento ocorre nos pacientes e nos pesquisadores, diz-se que o estudo é duplo-cego. Se ele também incluir os avaliadores do estudo, ele é triplo cego. Um ensaio clínico em que não há cegamento é dito aberto (open label, no caso de estudos com fármacos).\nA avaliação dos desfechos também pode afetar os resultados. É importante garantir-se que aqueles que registram os desfechos estejam cegados em relação a que grupo o sujeito da pesquisa pertence. Os autores devem estabelecer regras cuidadosas para decidir se um desfecho ocorreu ou não e despender esforços iguais para identificar desfechos para todos os pacientes no estudo.\n\nIntenção de tratar\n\nOs pesquisadores violam a randomização se omitirem da análise os pacientes que não receberam a intervenção designada ou, pior ainda, contarem eventos que ocorreram nos sujeitos não aderentes que foram designados para a intervenção contra o grupo controle. Os sujeitos de uma pesquisa, para evitar tal viés, devem ser analisados dentro do grupo para o qual eles foram alocados pela randomização (25). Este princípio é denominado intenção de tratar.\n\nAnálise da magnitude do efeito\n\nCalcula-se uma série de estimativas quantitativas para analisar a magnitude do efeito da intervenção em um ensaio clínico. Entre elas, destacam-se o Risco Relativo, Redução Relativa do Risco, Número Necessário para Tratar que serão estudados no capítulo @ref(sec-cap18).\nOutro método para avaliar resultados de um ensaio clínico para dados de tempo até o evento é a análise de sobrevida. Esta fornece informação sobre a rapidez com que os eventos ocorrem. A curva de sobrevida pode utilizar dados de pacientes acompanhados por diferentes períodos de tempo.\n\n\n3.5.3 Ensaios clínicos de equivalência e não inferioridade\nEnsaios clínicos controlados com placebo são ideais para avaliar a eficácia de um tratamento. Eles permitem o controle do efeito placebo e são mais eficientes, exigindo um menor número de pacientes para detectar um efeito do tratamento. Um ensaio clínico placebo controlado é eticamente justificado se não existe tratamento padrão, se o tratamento padrão não se mostrou eficaz, não há riscos associados com o retardo no tratamento e se a possiblidade de se retirar do estudo está incluída no protocolo. Sempre que possível e justificado, os ensaios clínicos placebo controlados devem ser a primeira escolha para avaliação de um tratamento.\nDado que um grande número de tratamentos eficazes comprovados está disponível, ensaios clínicos controlados por placebo são, muitas vezes, antiéticos. Nestas situações, ensaios clínicos com controle ativo são geralmente apropriados.\nSe o objetivo do ensaio clínico é testar se um novo tratamento é similar em eficácia a um tratamento já existente, ele é denominado de Estudo de Equivalência. O Ensaio Clínico é delineado de maneira que possa demonstrar que, dentro limites aceitáveis, os dois tratamentos são igualmente eficazes. Existe equivalência quando a diferença observada entre os dois tratamentos for menor que a máxima diferença aceitável, determinada previamente. Estes limites devem ser clinicamente apropriados. Se condição em investigação for muito grave, os limites para a equivalência devem ser estreitados. Quanto menor forem os limites de equivalência, maior o tamanho amostral. Este delineamento é útil se o novo tratamento trouxer benefícios, tais como menores efeitos colaterais, facilidade no uso e ser mais barato.\nEm muitos estudos com controle ativo, os pesquisadores desejam comprovar que o tratamento em estudo, no mínimo, não é substancialmente pior que o tratamento controle. Estes estudos são chamados de Estudos de Não Inferioridade. Um aspecto importante do delineamento e da interpretação desses estudos é a determinação da margem de não inferioridade. Os estudos de não inferioridade devem demonstrar, pelo menos, que o tratamento em estudo tem alguma eficácia, não inferior ao tratamento padrão. A análise dos estudos de não inferioridade é, por natureza, unidirecional.\nQuando um ensaio clínico busca evidenciar que um tratamento é melhor do que outro ele é denominado Estudos de Superioridade. Quando o ensaio clínico é delineado, ele deve ter uma hipótese bilateral e o tamanho da amostra definido de maneira que haja alto poder estatístico para detectar uma diferença clinicamente significativa entre os dois tratamentos. Os ensaios clínicos clássicos têm esta característica. Entretanto, nos dias atuais, este desenho de estudo pode não ser eticamente possível, uma vez que é pouco provável que não exista um tratamento com algum benefício comprovado. A comparação, portanto, deverá ser feita com o tratamento já existente, provando que o tratamento em estudo é similar ou, pelo menos, não seja inferior (26).\n\n\n3.5.4 Outros tipos de ensaios clínicos\n\n3.5.4.1 Ensaio clínico com delineamento cruzado\nNo delineamento cruzado (crossover design), os sujeitos da pesquisa são randomizados para um grupo e depois mudados para o outro grupo (Figura 3.6). Cada sujeito serve como seu próprio controle, diminuindo a variabilidade intragrupo, aumentando o poder e consequentemente, reduzindo o erro \\(\\beta\\) (erro que ocorre quando a análise estatística dos dados não consegue rejeitar uma hipótese, no caso desta hipótese ser falsa). É um tipo de delineamento bastante atrativo e útil (27).\nA maior desvantagem é o efeito residual (carryover), por isso os estudos cruzados devem ter um período de washout, período sem nenhum tratamento. Este período de tempo deve ser suficiente para a eliminação da droga para se ter certeza de que nenhum efeito da terapia permaneceu. Também pode haver um viés de acordo com a ordem de administração das terapias, pois os pacientes podem reagir de modo diferente como resultado do entusiasmo no início do tratamento que pode diminuir com o tempo.\n\n\n\n\n\n\n\n\nFigura 3.6: Ensaio clínico randomizado com delineamento cruzado.\n\n\n\n\n\n\n\n3.5.4.2 Delineamento Fatorial\nUma variação interessante de ensaio clínico é o delineamento fatorial. Este tipo de estudo permite que sejam testadas duas drogas em apenas um estudo, assumindo que os desfechos antecipados para as duas são diferentes e que seus modos de ação são independentes. Este desenho de estudo gera economia.\nUm exemplo de delineamento fatorial é observado no Physician’s Health Study onde usando um delineamento fatorial 2 x 2 foi testada a aspirina para a prevenção primária de doença cardiovascular (28), e betacaroteno para a prevenção primária de câncer.\nNo estudo da prevenção primária do câncer, os autores concluíram, após 12 anos de suplementação de betacaroteno, que o mesmo não produziu nem benefícios e nem prejuízos em termos de incidência de câncer (29).\n\n\n\n3.5.5 Fases de um ensaio clínico\nPara a realização de um ensaio clínico, a intervenção deve passar por várias fases (30).\n\n3.5.5.1 Fase Não Clínica\nAntes de começar a testar novos tratamentos em seres humanos, os cientistas testam as substâncias em laboratórios (in vitro) e em animais de experimentação. O objetivo principal desta fase é verificar como esta substância se comporta em um organismo. Assim, após esta fase se pode verificar se o medicamento é seguro para ser testado em seres humanos. Todo este processo é regido por leis da bioética em pesquisa em animais.\n\n\n3.5.5.2 Fase Clínica\nA fase clínica é a fase de testes em seres humanos. Esta etapa é constituída por quatro fases consecutivas e somente depois de finalizadas todas as fases, a droga poderá ser autorizada para comercialização e disponibilizada para uso em seres humanos. As sucessivas fases dentro da fase clínica são:\n\nFase I - Um estudo de fase I testa a droga pela primeira vez. O objetivo principal é avaliar a segurança do produto investigado. Nesta fase, o medicamento é testado em pequenos grupos (10 – 30 pessoas), geralmente, de voluntários sadios. Podemos ter exceções se estivermos avaliando medicamentos para câncer ou portadores de HIV-AIDS. Se a droga se mostrar segura, é possível ir para a Fase II.\nFase II - Nesta fase, o número de pacientes é maior (70 - 100). O objetivo é avaliar a eficácia da medicação, isto é, se ela funciona para tratar determinada doença, e também conseguir informações mais detalhadas sobre a segurança (toxicidade). Somente se os resultados forem bons é que o medicamento será estudado como um estudo clínico fase III.\nFase III - Nesta fase, o novo tratamento é comparado com o tratamento padrão existente. São os ensaios clínicos. O número de pacientes aumenta e depende da hipótese (em geral, 100 a 1.000). Devem de preferência utilizar desfechos clínicos, grupo controle, além de serem randomizados e duplo-cegos.\nFase IV - Estes estudos são realizados para se confirmar que os resultados obtidos na fase III são aplicáveis a grande parte dos doentes. Nesta fase, o medicamento já foi aprovado para ser comercializado. A vantagem dos estudos fase IV é que eles permitem acompanhar os efeitos dos medicamentos em longo prazo. É uma fase de vigilância pós-comercialização.\n\n\n\n\n\n1. Ribeiro Mendes F. O que é um trabalho científico. Em: Iniciacão Cientifica. Autonomia Editora; 2012. p. 17–26. \n\n\n2. Hulley SB, Cummings SR, Browner WS, Grady DG, Newman TB. Elaborando a questão de pesquisa e desenvolvendo o plano de estudo. Em: Delineando a pesquisa clinica. Quarta Edição. Artmed Editora; 2015. p. 15–24. \n\n\n3. McCombes S. Sampling Methods [Internet]. https://www.scribbr.com/methodology/sampling-methods/. scribbr.com Team; 2019. Disponível em: https://www.scribbr.com/\n\n\n4. Callegari-Jacques SM. Amostras. Em: Bioestatistica: principios e aplicações. Artmed Editora; 2003. p. 146–7. \n\n\n5. Faul F, Erdfelder E, Lang A-G, Buchner A. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods. 2007;39(2):175–91. \n\n\n6. Cohen J. Statistical power analysis for the behavioral sciences. Lawrence Erlbaum Associates; 1988. \n\n\n7. Grimes DA, Schulz KF. An overview of clinical research: the lay of the land. The lancet. 2002;359(9300):57–61. \n\n\n8. Fletcher RH, Fletcher SW, Fletcher GS. Prognóstico. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 108–9. \n\n\n9. Grimes DA, Schulz KF. Descriptive studies: what they can and cannot do. The Lancet. 2002;359(9301):145–9. \n\n\n10. Fletcher RH, Fletcher SW, Fletcher GS. Risco: da doença à exposição. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 88. \n\n\n11. Grimes DA, Schulz KF. Compared to what? Finding controls for case-control studies. The Lancet. 2005;365(9468):1429–33. \n\n\n12. Ernster VL. Nested case-control studies. Preventive Medicine. 1994;23(5):587–90. \n\n\n13. Newman TB, Browner WS, Cummings SR, Hulley SB. Delineando estudos de caso-controle. Em: Delineando a pesquisa clinica. Quarta Edição. Artmed Editora; 2015. p. 111. \n\n\n14. Grimes DA, Schulz KF. Cohort studies: marching towards outcomes. The Lancet. 2002;359(9303):341–5. \n\n\n15. Fletcher RH, Fletcher SW, Fletcher GS. Risco: da doença à exposição. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 68. \n\n\n16. Kannel WB, McGee DL. Diabetes and cardiovascular risk factors: the Framingham study. Circulation. 1979;59(1):8–13. \n\n\n17. Celentano DD, Szklo M. Cohort Studies. Em: Gordis Epidemiology. 6th Edition. Elsevier; 2019. p. 179. \n\n\n18. Coutinho M. Principios de epidemiologia clínica aplicada a cardiologia. Arquivos Brasileiros de Cardiologia. 1998;71:109–16. \n\n\n19. McCambridge J, Witton J, Elbourne DR. Systematic review of the Hawthorne effect: new concepts are needed to study research participation effects. Journal of Clinical Epidemiology. 2014;67(3):267–77. \n\n\n20. Bland JM, Altman DG. Statistic Notes: Regression towards the mean. BMJ. 1994;308(6942):1499. \n\n\n21. Fletcher RH, Fletcher SW, Fletcher GS. Tratamento. Em: Epidemiologia Clínica: Elementos Essenciais. Artmed Editora; 2014. p. 143. \n\n\n22. Kabisch M, Ruckes C, Seibert-Grafe M, Blettner M. Randomized controlled trials: part 17 of a series on evaluation of scientific publications. Deutsches Ärzteblatt International. 2011;108(39):663. \n\n\n23. Elander G, Hermerén G. Placebo effect and randomized clinical trials. Theoretical Medicine. 1995;16(2):171–82. \n\n\n24. Schulz KF, Grimes DA. Blinding in randomised trials: hiding who got what. The Lancet. 2002;359(9307):696–700. \n\n\n25. Montori VM, Guyatt GH. Intention-to-treat principle. CMAJ. 2001;165(10):1339–41. \n\n\n26. Christensen E. Methodology of superiority vs. equivalence trials and non-inferiority trials. Journal of hepatology. 2007;46(5):947–54. \n\n\n27. Health Improvement O for, Disparities. Crossover randomised controlled trial: comparative studies [Internet]. Office for Health Improvement and Disparities. UK Health improvement; 2020. Disponível em: https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies\n\n\n28. Physicians’ Health Study Research Group* SC of the. Final report on the aspirin component of the ongoing Physicians’ Health Study. New England Journal of Medicine. 1989;321(3):129–35. \n\n\n29. Hennekens CH, Buring JE, et al. Lack of effect of long-term supplementation with beta carotene on the incidence of malignant neoplasms and cardiovascular disease. New England Journal of Medicine. 1996;334(18):1145–9. \n\n\n30. Stanley K. Design of randomized controlled trials. Circulation. 2007;115(9):1164–9.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "03-producaoDados.html#footnotes",
    "href": "03-producaoDados.html#footnotes",
    "title": "3  Produção dos Dados",
    "section": "",
    "text": "http://calculoamostral.bauru.usp.br/calculoamostral/index.php↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Produção dos Dados</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html",
    "href": "04-ambienteR.html",
    "title": "4  Ambiente do R",
    "section": "",
    "text": "4.1 Instalação do R básico\nPara usar o R, há necessidade de carregar o programa básico que contém a sua linguagem de programação. O sistema é formado por um programa básico, Graphical User Interface (R-Gui) e muitos pacotes com procedimentos adicionais.\nO site oficial do R fornece as versões atualizadas do software e informações sobre este sofisticado projeto de computação estatística.\nPara baixar o R, usa-se um “CRAN Mirror”, clicando em CRAN (Comprehensive R Archive Network) na margem esquerda, abaixo de Download. O CRAN é central no uso do R: é o local de onde se carrega o software e todos os pacotes necessários para instalar e para expandir o R.\nEm vez de ter um único local, o CRAN é “espelhado” em diferentes locais do mundo. “Espelhado” significa simplesmente que existem versões idênticas do CRAN distribuídas por todo o mundo. É possível baixar o R diretamente da nuvem ou escolher uma origem mais próxima do seu local de atuação. No Brasil, encontram-se várias opções, como a Universidade Federal do Paraná, Fundação Oswaldo Cruz, RJ, Universidade de São Paulo, São Paulo e Universidade de São Paulo, Piracicaba\nApós escolher uma das alternativas acima (pode ser qualquer uma delas) surgirá a página The Comprehensive R Archive Network com as opções para escolher o sistema operacional. Escolha o sitema de acordo com o seu computador (Windows, macOS ou Linux). Ao clicar em uma dessas opções, se o sistema operacional escolhido é o Windows, aparecerá a página R for Windows. Nesta, deve-se clicar em base. No caso de outros sistemas operacionais, seguir as orientações mostradas no site do R.\nClicando em base, haverá um redirecionamento para a a página onde aparece a versão do R para o Windows mais atual. Clique no link que diz Download R-…for Window para baixar o instalador em um diretório do computador, em geral Downloads.\nPara instalar o programa básico, basta executar o instalador R-…-win.exe baixado no diretório. Ao fazer isso, aparece na tela do computador,no canto esquerdo, em baixo, o arquivo salvo. Execute este arquivo com um clique sobre ele. Aparecerá u,a janela perguntando “Deseja permitir que este aplicativo faça alterações no seu dispositivo?”. Clique em Sim. A seguir o instalador pedirá para escolher o Idioma. Selecione Português Brasileiro.\nEm sequência aparecerão informações sobre o diretório no qual o R será instalado em seu computador. Recomenda-se aceitar a configuração padrão sugerida pelo instalador do software.\nA próxima janela pedirá para personalizar os componentes que serão instalados. Recomenda-se usar as configurações sugeridas pelo instalador que irá reconhecer automaticamente a arquitetura do seu sistema Windows (32 e/ou 64 bits).\nA partir daqui, siga as recomendações padrão propostas pelo instalador até completar a instalação, clicando em Concluir.\nO R não precisa ser iniciado, pois o software que será usado, neste livro, é o RStudio. Este, para ser executado, necessita ter o R instalado no computador. Ou seja, o R é o programa “cérebro” necessário para as análises de dados que serão realizadas. Ele precisa estar instalado para permitir o funcionamento do RStudio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#rstudio",
    "href": "04-ambienteR.html#rstudio",
    "title": "4  Ambiente do R",
    "section": "4.2 RStudio",
    "text": "4.2 RStudio\nO RStudio é um ambiente de desenvolvimento integrado (IDE1). Ele serve para facilitar a escrita, execução e depuração de código R, bem como para gerenciar projetos, visualizar dados e criar gráficos.\nO RStudio é um membro ativo da comunidade R. Foi fundado em 2009 por Joseph J. Allaire, engenheiro de software americano. O RStudio, inspirado pelas inovações dos usuários de R em ciência, educação e indústria, desenvolveu ferramentas gratuitas e abertas para facilitar o uso do R.\nO RStudio é escrito em linguagem C++ e foi inicialmente focado apenas na linguagem R. Com o tempo o desenvolveu suporte para Python e VSCode. Em 2022, para acompanhar essa mudança, foi anunciada a mudança do nome da empresa que o desenvolve para Posit e, recentemente, introduzido um novo IDE, denominado Positron, um projeto inicial, com um ambiente semelhante ao RStudio e que continua em desenvolvimento. Talvez, no futuro, possa substituir o RStudio. Por enquanto, isso será difícil , pois o Positron não tem todas as funcionalidades do RStudio.\n\n4.2.1 Instalação do R Studio\nPara instalar o RStudio , acessar o site e clicar em Download para obter a versão desejada. Recomenda-se a versão RStudio Desktop – Open Source License que é gratuita. Esta versão entrega as ferramentas integradas para o R.\nA seguir, aparecerão os instaladores disponíveis, conforme a plataforma suportada pelo seu computador. As mais utilizadas são Windows e Mac OS X. Neste livro, como base, serão mostrados os passos para a plataforma Windows 2.\nEm sequência, executar o instalador baixado RStudio-2025.05.1-513.exe 3 e seguir as suas instruções.\n\n\n4.2.2 Iniciando o RStudio\nPara iniciar o RStudio basta clicar no ícone indicativo (Figura 4.1) que se encontra no menu Iniciar do Windows.\n\n\n\n\n\n\n\n\nFigura 4.1: Ícone do RStudio\n\n\n\n\n\nO RStudio abre como mostrado na Figura 4.2. O RStudio é uma interface mais funcional e amigável para o R. Contém um conjunto de ferramentas integradas projetadas para ajudá-lo a ser mais produtivo com o R.\n\n\n\n\n\n\n\n\nFigura 4.2: Tela inicial do RStudio\n\n\n\n\n\nInclui o Console , editor que suporta execução direta de códigos e uma variedade de ferramentas robustas para plotagem, exibição de histórico, depuração e gerenciamento de seu espaço de trabalho incluídos em uma interface que está, inicialmente, dividida em 3 paineis:\n\nConsole\nEnvironment, History, Connections, Tutorial\nFiles, Plots, Packages, Help\n\nConsole e R Script\nDo lado esquerdo fica o Console (Figura 4.2), em vermelho), onde os comandos podem ser digitados e aparecem os resultados da execução dos comandos. Ao abrir o RStudio , vê-se no Console uma série de informações sobre o R, como versão em uso e, por último, o diretório onde está armazenado o espaço de trabalho (workspace). Estas informações podem ser facilmente apagadas, clicando na barra de ferramentas, no menu Edit, e após em Clear Console ou, usando as teclas Ctrl+L.\nO Console é a principal parte do R. Aqui é onde o R realmente executa o comando. No início do Console, existe um caractere (&gt;). Este é um prompt que informa que o R está pronto para receber um novo código. Pode-se digitar o código diretamente no Console após o prompt e obter uma resposta imediata. Por exemplo, se for digitado 1 + 1 e pressionado Enter, o R imediatamente gera uma saída de 2 (Figura 4.3).\n\n\n\n\n\n\n\n\nFigura 4.3: Console\n\n\n\n\n\nRecomenda-se que a maior parte dos comandos sejam digitados no bloco de notas do RStudio , o R Script. Reservar o Console apenas para depurar ou fazer análises e cálculos rápidos. A razão para isso é simples: se o comando for digitado diretamente no Console, ele não será salvo e se for cometido um erro na digitação, haverá necessidade de digitar tudo novamente. Portanto, é melhor escrever os comandos no R Script e, quando estiver pronto para executar, enviar para o Console.\nO R Script é o quarto painel do RStudio e seu bloco de notas. Ele é criado através do menu File &gt; New File &gt; R Script ou clicando no botão verde com o sinal (+), na barra de ferramentas de acesso rápido, na parte superior à esquerda. Ao criar um novo R Script será aberto o painel do bloco de notas (Figura 4.4), em verde).\n\n\n\n\n\n\n\n\nFigura 4.4: R Script\n\n\n\n\n\nUm diferencial do RStudio é que os comandos são autocompletáveis. Basta começar a escrever o comando, inserindo 3 ou mais caracteres, por exemplo, summ referente a função summary (), usada para sumarizar um conjunto de dados, e surge um menu de opções, facilitando a digitação (Figura 4.5).\n\n\n\n\n\n\n\n\nFigura 4.5: Menu autocompletável\n\n\n\n\n\nApós digitar no Console, para que seja executado o comando há necessidade de clicar na tecla Enter; no RScript, clicar em Run, acima, na barra, no lado direito, ou usar o atalho Ctrl + Enter. Textos podem ser copiados e colados no script e linhas em branco podem ser inseridas. Além disso, no final da sua sessão, é possível salvar o arquivo, que poderá ser recarregado no futuro, se precisar refazer a análise.\nOs scripts do R são apenas arquivos de texto com a extensão (.R). Quando se cria um R Script, aparece como Sem título (Untitled). Antes de começar a digitar um novo script no R Sem título, recomenda-se salvar o arquivo com um novo nome de arquivo. Dessa forma, se algo no computador falhar durante o trabalho, o R terá o código protegido.\nAo digitar o código em um script, o R não executa o código enquanto se digita. Para que o R realmente avalie o código digitado, há necessidade de primeiro enviar o código para o Console, clicando no botão Run ou usando a tecla de atalho Crtl + Enter. Cada linha é marcada no início por um número em sequência.\nAlém da digitação de comandos, o R Script permite fazer comentários onde tudo que for escrito, após o símbolo \\(\\#\\), não é considerado, é apenas uma explicação, um esclarecimento. Os comentários são literais, escritos diretamente para explicar o comando executado. São repetidos na saída do Console sem aparecer nos resultados.\nAmbiente, História, Conexão e Tutorial\nNo lado superior direito há um painel com quatro abas (Figura 4.2), em azul):\n\nAmbiente (Environment) - onde ficam armazenados os objetos criados, as bases de dados importadas, etc., na sessão ativa. É possível visualizar informações como o número de observações e linhas dos bancos de dados ativos. A guia também tem algumas ações clicáveis, como Import Dataset, que permite importar arquivos csv, Excel, SPSS, etc.\nHistória (History) - onde fica o histórico dos comandos executados no Console. Estes comandos podem ser pesquisados nesta guia. Os comandos são exibidos em ordem (mais recentes na parte inferior) e agrupados por bloco de tempo.\nConexões (Connections) - mostra todas as conexões feitas com fontes de dados suportadas e permite saber quais conexões estão ativas no momento. O RStudio suporta múltiplas conexões de banco de dados simultâneas.\nTutorial - a partir da versão 1.3, o R Script ganhou um painel Tutorial dedicado, usado para executar tutoriais que ajudarão você a aprender e dominar a linguagem de programação R. Na primeira vez que se abre o programa, clicando nesta aba, o RStudio solicita que seja instalado o pacote learnr (Figura 4.6)). Isto permite acesso a vários tutoriais úteis que merecem ser explorados\n\n\n\n\n\n\n\n\n\nFigura 4.6: Tutoriais do RStudio\n\n\n\n\n\nArquivos, Gráficos, Pacotes, Ajuda e Apresentação\nNo lado direito, abaixo, existem outras abas muito úteis (Figura 4.2), em amarelo):\n\nArquivos (Files) - esta guia dá acesso ao diretório onde se encontram os seus arquivos. Um bom recurso do painel Files é que se pode usá-lo para definir seu diretório de trabalho. Para isso, clique em More e depois em Set As Working Directory.\nGráficos (Plots) - local onde ficam os gráficos gerados. Existem botões para abrir o gráfico em uma janela separada e exportar o gráfico como um .pdf ou .jpeg.\nPacotes (Packages) - mostra uma lista de todos os pacotes R instalados no seu computador e indica se eles estão atualmente carregados ou não. Pacotes que estão sendo executados na sessão atual, estão marcados, enquanto aqueles que estão instalados, mas ainda inativos, estão desmarcados.\nAjuda (Help) - menu de ajuda para as funções R. Você pode digitar o nome de uma função na janela de pesquisa (por exemplo, histogram ou usar o ?hist), no Console ou no R Script, para procurar ajuda sobre uma função (Figura 4.7)). A Ajuda no R Studio pode também ser acessada no menu Help da barra de ferramentas onde existem várias opções. Para complementar, alguns livros são muito uteis, como o R Cookbook (1) ou Using R* for introductory statistics* (2). No entanto, na maioria das vezes a forma mais prática de conseguir ajuda com uma dúvida específica é a busca em fóruns na internet, como o Stack Overflow: https://stackoverflow.com/.\nApresentação (Presentation) – é visualizador de apresentações. Nas últimas versões do Rstudio, é possível com o Quarto, editar um código em R Markdown para construir uma apresentação. Não faz parte do objetivo deste livro desenvolver este assunto. É possível encontrar um tutorial em https://quarto.org/docs/get-started/hello/rstudio.html.\n\n\n\n\n\n\n\n\n\nFigura 4.7: Ajuda do RStudio",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#pacotes",
    "href": "04-ambienteR.html#pacotes",
    "title": "4  Ambiente do R",
    "section": "4.3 Pacotes",
    "text": "4.3 Pacotes\nPara que o R desempenhe sua função de interagir com o usuário, realizar análises estatísticas e gerar gráficos, a instalação de pacotes é essencial.\nAo instalar o R básico, ele já vem acompanhado de diversos pacotes que possibilitam uma ampla gama de análises. No entanto, à medida que o uso do R se intensifica, torna-se necessário instalar novos pacotes desenvolvidos pela comunidade, que oferecem funcionalidades adicionais por meio de novas funções e comandos.\nUm pacote é um conjunto de funções, dados e documentação que expande os recursos do R base. O uso de pacotes é fundamental para aproveitar todo o potencial da ferramenta, sendo instalados conforme as demandas do trabalho realizado no R.\n\n4.3.1 Repositório de pacotes\nQuando se identifica a necessidade de um novo pacote, é fundamental saber onde ele se encontra. O principal repositório de pacotes é o CRAN (Comprehensible R Archive Network), já comentado anteriormente. Para acessar este repositório, use o link e escolha um espelho (0-Cloud ou o mais próximo geograficamente). Depois que o pacote for instalado, ele será mantido em sua biblioteca (library) R associada à sua versão principal atual do R. Haverá necessidade de atualizar e reinstalar os pacotes sempre que atualizar uma versão principal do R.\nEstando na página do CRAN, no menu, à esquerda, clique em Packages . Isto o colocará na página dos Contributed Packages, onde a maioria dos pacotes podem ser encontrados em Table of available packages, sorted by name . Também é possível clicar em CRAN Task Views , onde estão os pacotes separados por tópicos.\n\n\n4.3.2 Instalação de um pacote novo\nInstalar um pacote significa simplesmente baixar o código do pacote em um computador pessoal. Existem duas maneiras principais de instalar novos pacotes. O método mais comum é baixá-los do CRAN, usando a função install.packages(). Dentro dos parênteses, como argumento, coloca-se entre aspas (duplas ou simples) o nome do pacote. Como visto, deve-se, de preferência, digitar o comando no R Script. Por exemplo, será instalado o pacote ggplot2 que contém múltiplas funções gráficas como abaixo:\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nPara carregar o pacote, isto é, para fazer com que suas funções se tornem ativas para uso na na sessão, deve-se usar a função library(), como mostrado no comando acima. Se o RStudio for fechado e reaberto, o o pacote deverá ser novamente ativado. Observe que a função library() não requer que o nome do pacote seja digitado entre aspas. Isto acontece porque antes de o pacote ser instalado o R não o reconhece , portanto, há necessidade de indicar o nome (caracteres), para que o R procure na internet, por exemplo, o que ele deve baixar. Já, depois de instalado, o pacote é um objeto conhecido pelo R, logo as aspas não são mais necessárias.\nUma outra maneira de instalar pacotes no R, é usar o botão Install, localizado na aba Packages, no painel inferior, à direita. Clicando em Install, abre-se a caixa de diálogo da Figura 4.8. Digitar em Packages o nome do pacote (ggplot2) e o RStudio completará com opções para achar o pacote. Clicar em ggplot2 e verifique se Install dependencies foi selecionado. A seguir clicar em Install e aguardar aparecer no Console a mensagem que o pacote foi instalado com sucesso.\n\n\n\n\n\n\n\n\nFigura 4.8: Instalação do pacote ‘ggplot2’ usando a caixa de diálogo ‘Install Packages’\n\n\n\n\n\n\n\n4.3.3 Atualização dos pacotes\nPeriodicamente, há necessidade de atualizar os pacotes instalados. Essa necessidade advém do fato que, com o tempo, os autores de pacotes lançarão novas versões com correções de defeitos e novos recursos e, geralmente, é uma boa ideia manter-se atualizado. Para realizar a atualização proceda da seguinte maneira:\n\n# atualiza todos os pacotes disponíveis, solicitando permissão\nupdate.packages()\n# atualiza, sem solicitações de permissão/esclarecimento\nupdate.packages(ask = FALSE)\n# atualiza um pacote específico\nupdate.packages(\"ggplot2\")\n\n\n\n4.3.4 Instalando e carregando mais de um pacote\nPara carregar mais de um pacote simultaneamente, pode-se usar uma das funções: libraries() ou packages() do pacote easypackages. Em primeiro lugar, instalar e carregar o pacote:\n\ninstall.packages(\"easypackages\")\nlibrary(easypackages)\n\nPosteriormente, basta usar uma das funções do easypackages:\n\nlibraries(\"readxl\", \"dplyr\", \"ggplot2\", \"car\")\n\nOutro pacote que gerencia pacotes do R é o pacman (3). Este pacote tem a função p_load() que instala e carrega um ou mais pacotes. Usar esta função, escrevendo o nome dos pacotes sem necessidade de aspas:\n\ninstall.packages(\"pacman\")\nlibrary(pacman)\np_load(readxl, dplyr, ggplot2, car)\n\nOu, escrever diretamente:\n\npacman::p_load(readxl, dplyr, ggplot2, car)\n\nO pacote pacman tem outas funções, entre elas a função p_update() que atualiza o pacote e , se usada sem especificar o pacote , atualiza todos. Para saber mais sobre o pacote pacman, use a ajuda.\n\npacman::p_update(readxl, dplyr, ggplot2, car)\n\n\n\n4.3.5 Citação de pacotes em publicações\nNo R existe um comando que mostra como citar o R ou um de seus pacotes. Basta digitar a função citation() no Console ou no R Script e observar a saída. Para um pacote específico, basta colocar o nome do pacote entre aspas, na função.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation (\"ggplot2\")\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nUma entrada BibTeX para usuários(as) de LaTeX é\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#diretório-de-trabalho",
    "href": "04-ambienteR.html#diretório-de-trabalho",
    "title": "4  Ambiente do R",
    "section": "4.4 Diretório de trabalho",
    "text": "4.4 Diretório de trabalho\nO diretório de trabalho (Working Directory) é uma pasta onde o R lê e salva arquivos. Deve-se criar um diretório de trabalho para a sessão . Para isso, no RStudio siga o caminho: Session &gt; Set Working Directory &gt; Choose Directory ou use o atalho Ctrl + Shift + H e escolha o diretório desejado ou crie um novo.\nAo finalizar, aparecerá no Console (Figura 4.9):\n\n\n\n\n\n\n\n\nFigura 4.9: Diretório de trabalho\n\n\n\n\n\nNote que o R usou a função setwd() que significa “definir diretório de trabalho”. Também é possível usar esta função diretamente no R Script ou no Console, digitando conforme o caminho do diretório.\nPara saber qual é o diretório de trabalho que está sendo usado pelo R pode-se executar a função getwd(). A saída no Console mostrará o diretório de trabalho usado, portanto é recomendado que se faça isso no início da sessão para verificar se há ou não necessidade de modificar o diretório.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#projeto",
    "href": "04-ambienteR.html#projeto",
    "title": "4  Ambiente do R",
    "section": "4.5 Projeto",
    "text": "4.5 Projeto\nUma funcionalidade importante do RStudio é a possibilidade de se criar projetos. Um projeto nada mais é do que uma pasta no seu computador. Nessa pasta, estarão todos os arquivos que serão usados ou criados na sua análise.\nA principal razão de se utilizar projetos é simplesmente organização. Com eles, fica muito mais fácil importar conjunto de dados para dentro do R, criar análises reprodutíveis e compartilhar o trabalho realizado.\nAo se começar uma nova análise, é interessante criar um Novo Projeto. Para isso, clicar File &gt; New Project ou clicar no menu que está na parte superior, à direita, Project (none) &gt; New Project…. Abrirá a janela da Figura 4.10).\n\n\n\n\n\n\n\n\nFigura 4.10: Assistente de novo projeto.\n\n\n\n\n\nClique em New Directory para criar um novo diretório. Por exemplo, para as aulas de Bioestatística, pode-se criar um diretório com o nome de bioestatistica (evite usar acentos, maiúsculas ou caracteres especiais) ou qualquer outro nome.\nQuaisquer documentos Excel ou arquivos de texto associados podem ser salvos nesta nova pasta e facilmente acessados, indo ao menu Project (none) &gt; Open Project…. A partir daí, é possível realizar análises de dados ou produzir visualizações com seus dados importados.\nQuando um projeto estiver aberto no RStudio, o seu nome aparecerá no canto superior direito da tela. Na aba Files, aparecerão todos os arquivos contidos no projeto. Quando se clica no nome do projeto, abre um menu que torna muito fácil a navegação pelos projetos existentes. Basta clicar em qualquer um deles para trocar de projeto, isto é, deixar de trabalhar em uma análise e começar a trabalhar em outra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#o-r-como-calculadora",
    "href": "04-ambienteR.html#o-r-como-calculadora",
    "title": "4  Ambiente do R",
    "section": "4.6 O R como calculadora",
    "text": "4.6 O R como calculadora\nO R pode ser utilizado para uma série de operações matemáticas desde as mais simples às mais complexas. Para isso, basta digitar no Console ou no R Script, usando os operadores.\n\n4.6.1 Operadores\nOperadores são usados para realizar operações com variáveis e valores.\nOperadores aritméticos\nNo R, você pode usar operadores aritméticos para realizar operações matemáticas comuns.\n\n 10 + 5        # Adição\n\n[1] 15\n\n 10 - 5        # Subtração\n\n[1] 5\n\n 10 * 5        # Multiplicação\n\n[1] 50\n\n 10 / 5        # Divisão\n\n[1] 2\n\n 10 ^ 5        # Potência\n\n[1] 1e+05\n\n 10 %% 3       # Divisão modular (divisão com resto)\n\n[1] 1\n\n 10 %/% 3      # Divisão inteiro\n\n[1] 3\n\n\nObserve que o R repete a operação e coloca em baixo o resultado precedido por [1]. O resultado da operação de exponenciação é exibido como notação científica, onde \\(e+05\\) significa \\(10^5\\).\nOperadores de atribuição\nOperadores de atribuição são usados para atribuir valores a variáveis, como será visto na Seção 4.7, adiante.\nOperadores de comparação\nSão usados para comparar dois valores.\n\n# Igualdade\n3 == 3\n\n[1] TRUE\n\n3 == 4\n\n[1] FALSE\n\n# Não igual (diferente)\n3 != 4\n\n[1] TRUE\n\n# Maior\n6 &gt; 3\n\n[1] TRUE\n\n# Menor\n3 &lt; 4\n\n[1] TRUE\n\n# Maior ou igual\n5 &gt;= 3\n\n[1] TRUE\n\n# Menor ou igual  \n3 &lt;= 4\n\n[1] TRUE\n\n\nATENÇÃO, na linguagem R, o sinal de igualdade é escrito com duplo \\(=\\).\nOperadores lógicos\nOperadores lógicos são usados para combinar declarações condicionais:\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros \n6 == 6 & 7 == 8\n\n[1] FALSE\n\n# Conjunção lógica E, retorna TRUE se ambos elementos são  verdadeiros\n2 * 3 && 1 * 6\n\n[1] TRUE\n\n# Conjunção lógica OU, retorna TRUE se um dos elementos é verdadeiro\n(2 * 2) | sqrt(16)\n\n[1] TRUE\n\n6 == 6 | 7 == 8 \n\n[1] TRUE\n\n# Conjunção lógica NÃO, retorna FALSE se o  elemento é verdadeiro\n!6==6\n\n[1] FALSE\n\n!2==4\n\n[1] TRUE\n\n# Operador lógico que verifica se um elemento pertence a um conjunto (%in%)\n\n pares &lt;- c(0, 2, 4, 6, 8, 10)\n 5 %in% pares\n\n[1] FALSE\n\n\nOutros operadores\n\n# Logarítmo natural (base e)\nlog (10) \n\n[1] 2.302585\n\n# Logarítmo base 10\nlog10 (10)       \n\n[1] 1\n\n# Raiz quadrada\nsqrt (81)\n\n[1] 9\n\n# Resultado absoluto\nabs (3 - 6)\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-objetos",
    "href": "04-ambienteR.html#sec-objetos",
    "title": "4  Ambiente do R",
    "section": "4.7 Objetos",
    "text": "4.7 Objetos\nO R permite salvar valores dentro de um objeto. Os objetos são criados utilizando o operador de atribuição (&lt;-). Para digitar este operador, basta teclar o sinal menor que (&lt;), seguido de hífen (-) , sem espaços. Existe um atalho que é pressionar (Alt) \\(+\\) (-). O símbolo \\(=\\) pode ser usado no lugar de &lt;-.\nObjeto é um pequeno espaço na memória do computador onde o R armazenará um valor ou o resultado de um comando, utilizando um nome arbitrariamente definido. Tudo criado pelo R pode se constituir em um objeto, por exemplo: uma variável, uma operação aritmética, um gráfico, uma matriz ou um modelo estatístico. Através de um objeto torna-se simples acessar os dados armazenados na memória. Ao criar um objeto, se faz uma declaração. Isto significa que se está afirmando que uma determinada operação aritmética irá, agora, tornar-se um objeto que irá armazenar um determinado valor. As declarações são feitas uma em cada linha do R Script.\nOs objetos devem receber um nome e é obrigatório que ele comece por uma letra (ou um ponto) e não é permitido o uso do hífen. Pode-se usar o ponto ou underlines para separar palavras. Deve ser evitado o uso de nomes que sejam de objetos do sistema, ou outros objetos já criados, funções ou constantes. Por exemplo, não deve ser utilizado: c, q, r, s, t, C, D, F, I, T, diff, exp, log, mean, pi, range, rank, var, NA, NaN, NULL, FALSE, TRUE, break, else, if, break, function, in, while que devem ser reservados, pois têm significados especiais.\nQuando se usa um objeto com o nome pi, ele assumirá outro valor diferente de 3,141593. Preservando este nome, toda vez que usarmos a palavra pi, o R assume o valor pré-estabelecido. Além disso, o R faz a diferença entre letras maiúsculas e minúsculas. Ou seja, soma é um objeto diferente de Soma e ambos são diferentes de SOMA.\nPara exibir o conteúdo de um objeto, basta digitar seu nome no R Script ou no Console e executar. Em análises mais extensas, verificar se já há um objeto com o mesmo nome, pois seus valores serão substituídos ao executar o novo objeto. Para saber se já existe um objeto com o nome definido, digite as primeiras letras do objeto criado e o R Studio listará, usando a sua função de autocompletar, tudo que começar com essas letras no arquivo. Assim ficará fácil verificar se já existe um objeto com o nome desejado.\nNo comando abaixo, é criado um objeto que receberá a soma de dez números, utilizando a função sum(). O objeto foi denominado de soma. Para exibir o valor contido no objeto soma, é necessário digitar soma no R Script ou Console e executar:\n\nsoma &lt;- sum (2, 3, 12, 15, 21, 4, 8, 7, 13, 21)\nsoma\n\n[1] 106",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-funcoes",
    "href": "04-ambienteR.html#sec-funcoes",
    "title": "4  Ambiente do R",
    "section": "4.8 Funções",
    "text": "4.8 Funções\nA função é uma orientação ao R para que ele execute uma ação que é algum procedimento específico. Em decorrência, em geral, uma função ttem um nome sugestivo da ação que ela realiza. Por exemplo, a função mean () realiza a média aritmética de uma série de números. O resultado, como regra geral, deve ser colocado em um objeto que será armazenado na memória do computador.\nEsta série de números, concatenados na função c(), é armazenada por um objeto, nomeado dadose, posteriormente, se usa a função mean()com este objeto dados. O resultado da função mean, exibido no Console, será recebido por outro objeto media_dados e colocado na memória do computador.\n\ndados &lt;- c(3, 5, 7, 9, 6, 7)\nmedia_dados &lt;- mean(dados)\nmedia_dados\n\n[1] 6.166667\n\n\nAs funções podem ser criadas pelo pesquisador, de acordo com as suas necessidades. Entretanto, na maioria das vezes, elas são encontradas prontas, fazendo parte de um pacote. Pacotes contêm muitas funções que para serem executadas necessitam que estes estejam instalados e carregados. As funções para exercerem a sua ação devem receber dentro delas (entre parênteses) os argumentos que elas exigem. Os argumentos de uma função são sempre separados por vírgulas.\nPara se saber quais argumentos necessários para uma determinada função basta consultar a ajuda, onde se encontrará a documentação da mesma. Para isso basta digitar no Console, no caso da função mean(), help(mean) ou ?mean:\n\nhelp(mean)\n\nO resultado deste comando aparecerá na aba Help, na parte inferior, à direita (Figura 4.11):\n\n\n\n\n\n\n\n\nFigura 4.11: Ajuda para Média Aritmética.\n\n\n\n\n\nOs principais argumentos da função mean() são:\n\nx \\(\\to\\) vetor numérico\ntrim \\(\\to\\) fração das observações (varia de 0 a 0,5) extraída de cada extremidade de x para calcular a média aparada\nna.rm \\(\\to\\) valor lógico (TRUE ou FALSE) que indicam se os valores ausentes (NA) devem ser removidos antes que o cálculo continue\n\nEste último argumento é muito importante quando, na sequência de valores existe algum não informado ou inexistente. No R, eles são denominados de valores ausentes (missing values) e denotados por NA (Not Available).\nPor exemplo, em uma coleta de uma série de valores, correspondentes ao peso de 15 recém-nascidos, havendo a “falta” de um dos registros, ao calcular a média com a função mean(), ela retornará NA.\n\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,NA,3970,3060,3180,  \n             2865,2815,3245,2051,2630)\nmean (pesoRN)\n\n[1] NA\n\n\nColocando o argumento na.rm = TRUE, para remover os valores faltantes, a função retornará a média aritmética sem este valor:\n\nmean (pesoRN, na.rm = TRUE)\n\n[1] 3227.929\n\n\n\n4.8.1 Criando funções\nNo R, é possível criar funções pessoais que podem simplificar um código e, eventualmente, diminuir o tempo de execução das análises.\nFórmula geral\nAs funções têm uma fórmula geral:\n\nnome_da_funcao &lt;- function (x){transformar x}\n\nPor exemplo, a área de um circulo é igual a \\(\\pi\\times raio^2\\). Para calcular a área do círculo, pode-se criar uma função que faça este trabalho:\n\narea.circ &lt;- function(r){\n  area &lt;- pi*r^2\n  return(area)                \n}\n\nOu seja, foi usada a função function(), com o raio do círculo como argumento. A seguir, entre chaves {}, coloca-se a ação que a função realizará, no caso o cálculo da área do círculo. O resultado deste cálculo (pi*r^2) é recebido por um objeto denominado area.4 A seguir, usou-se a função return () para retornar o resultado do cálculo realizado.\nAo executar essa função, é possível usá-la para calcular a área de um círculo, cujo raio é igual a 5 cm:\n\narea.circ(5)\n\n[1] 78.53982\n\n\nOutros exemplos\nO Indice de Massa Corporal é igual ao peso (kg) dividido pela \\(altura^2\\), em metros. Uma função para fazer este cálculo é:\n\nimc &lt;- function(peso, altura){\n  res &lt;- peso/altura^2\n  return(res)\n}\n\nLogo, o IMC de um indivíduo que tenha 67 kg e 1,7 m é:\n\npeso &lt;-  67\naltura &lt;-  1.70\nimc(67, 1.70)\n\n[1] 23.18339\n\n\nOs exemplos mostrados são muito simples. Quase não haveria necessidade de construir uma função. Entretanto, quando se tem uma ação mais complexa, a função mostra a sua utilidade. Por exemplo, se for necessário realizar a comparação entre duas médias, usando um teste t e apresentar o resultado junto com boxplots, a função fica mais complexa. Sempre que for necessário cálculo semelhante, a função automatiza a ação, sem necessidade de repetir os códigos:\n\nplotBpT &lt;- function(df, var.x, var.y){\n  library(ggplot2)\n  library(ggpubr)\n  ggplot(df, aes(x = {{var.x}}, y = {{var.y}}, fill = {{var.x}})) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1) +\n    geom_boxplot() +\n    theme_classic() +\n    theme(legend.position = \"none\") +\n    stat_compare_means(method = \"t.test\", label.x = 0.5)\n}\n\nNeste momento, não serão discutidos os códigos da função. Ela será utilizada como uma função qualquer com os dados do arquivo dadosPop.xlsx^[Para maiores detalhes, consulte a o Capítulo 12. Os argumentos da função são o dataframe (df = dados), a variável x (var.x = pop) e a variável y (var.y = altura). Dessa forma, está se comparando a altura de mulheres de duas populações de duas regiões diferentes :\n\ndados &lt;- readxl::read_excel(\"dados/dadosPop.xlsx\")\ndados$pop &lt;- as.factor(dados$pop)\nplotBpT (df = dados, var.x = pop, var.y = altura)\n\n\n\n\n\n\n\nFigura 4.12: Comparação da altura de mulheres em duas populações\n\n\n\n\n\nObserve na Figura 4.12 onde aparece o resultado do teste t (\\(P = 2,2 \\times 10^-16\\)) e os boxplots em posições bem diferentes (veja Seção 6.5.5).\nAtivação de uma função criada\nPara ativar uma função previamente criada, usa-se a função nativa source (). O argumento desta função é o caminho (no exemplo, é o diretório do autor) onde se encontra a função buscada, por exemplo, a função imc() criada acima:\n\nsource('C:/Users/petro/Dropbox/Estatistica/Bioestatistica_usando_R/Funcoes/imc.R')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#classes",
    "href": "04-ambienteR.html#classes",
    "title": "4  Ambiente do R",
    "section": "4.9 Classes",
    "text": "4.9 Classes\nSão os atributos de um objeto e o seu conhecimento é de suma importância. É a partir do conhecimento do tipo de classe que as funções sabem o que extamente fazer com um objeto. Por exemplo, não é possivel somar duas letras e se for feita a tentativa de somar “a” e “b”, o Rretorna um erro:  Error in “a” + “b”: non-numeric argument to binary operator .\nNo R, os textos são escritos entre aspas simples ou duplas. As aspas servem para diferenciar nomes (objetos, funções, pacotes) de textos (letras e palavras). Os textos são muito comuns em variáveis categóricas e são popularmente chamados de strings ou character. Além desta classe, o R tem outras classes básicas que são a numeric e a logical. Um objeto de qualquer uma dessas classes é chamado de objeto atômico. Esse nome se deve ao fato de essas classes não se misturarem (4).\nPara saber qual o tipo de classe que um objeto pertence, basta usar a função class().\n\nidade &lt;- c(3, 5, 7, 9, 6, 7)\nclass (idade)\n\n[1] \"numeric\"\n\nnome &lt;- c(\"Pedro\", \"Maria\", \"Margarida\", \"Alice\", \"João\", \"Luís\")\nclass(nome)\n\n[1] \"character\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-vetores",
    "href": "04-ambienteR.html#sec-vetores",
    "title": "4  Ambiente do R",
    "section": "4.10 Vetores",
    "text": "4.10 Vetores\nUm vetor é uma variável com um ou mais valores do mesmo tipo. Por exemplo, o número de filhos em 10 famílias foi 4, 5, 3, 2, 2, 1, 2, 1, 3 e 2. O vetor nomeado de n.filhos é um objeto numérico de comprimento = 10. A maneira mais fácil de criar um vetor em R é concatenar (ligar) os 10 valores, usando a função concatenar c(), vista acima:\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\nn.filhos\n\n [1] 4 5 3 2 2 1 2 1 3 2\n\n\nComo os vetores são conjuntos indexados, pode-se dizer que cada valor dentro de um vetor tem uma posição. Essa posição é dada pela ordem em que os elementos foram colocados no momento em que o vetor foi criado. Isso nos permite acessar individualmente cada valor de um vetor (4).\nPara acessar um determinado valor, basta colocar a posição do mesmo entre colchetes [ ]. Se há interesse em conhecer o número de filhos da quinta família, procede-se da seguinte forma:\n\nn.filhos[5]\n\n[1] 2\n\n\nSe houver tentativa de acessar um valor inexixtente, o R retorna NA.\n\nn.filhos[11]\n\n[1] NA\n\n\nSe houver necessidade de excluir um dos elementos, basta colocar entre colchetes a posição do mesmo com sinal negativo. Por exemplo, para excluir o valor correspondente a sexta família, usa-se:\n\nn.filhos[-6]\n\n[1] 4 5 3 2 2 2 1 3 2\n\n\nObserva-se que o valor 1 foi excluído da série de elementos.\nQuando são colocados elementos em um vetor que pertençam a classes diferentes, o R promove o que se denomina de coerção, pois o vetor pode ter apenas uma classe de objeto. Dessa forma, as classes mais fortes reprimem as mais fracas. Por exemplo, sempre que for misturado números e texto em um vetor, os números serão considerados como texto:\n\nvetor &lt;- c(12, 15, 4, 6, \"A\", \"D\")\nvetor\n\n[1] \"12\" \"15\" \"4\"  \"6\"  \"A\"  \"D\" \n\n\nVeja que, agora, todos os elementos do vetor passaram a ser textos e, porisso, estão entre aspas.\n\n4.10.1 Tipos de vetores\nDado um vetor, pode-se determinar seu tipo com typeof(), ou verificar se é um tipo específico com uma das funções: is.character(), ’is.double(),is.integer(),is.logical( )`.\n\nn.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2)\ntypeof(n.filhos)\n\n[1] \"double\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\n\nAs expressões do tipo character devem aparecer entre aspas duplas ou simples. Os números no R são geralmente tratados como objetos numéricos (números reais de dupla precisão). Mesmo números inteiros são tratados como numéricos. Para fazer um número inteiro ser tratado como objeto inteiro, deve-se utilizar a letra L após o número.\nOs valores lógicos (ou booleanos) são TRUE ou FALSE. T ou F também são aceitos.\n\nn.filhos &lt;- c(4L, 5L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L)\ntypeof(n.filhos)\n\n[1] \"integer\"\n\nis.numeric(n.filhos)\n\n[1] TRUE\n\nis.double(n.filhos)\n\n[1] FALSE\n\n\n\nnomes &lt;- c('Maria', 'João', 'Manuel', 'Petronio', 'José')\ntypeof(nomes)\n\n[1] \"character\"\n\nis.numeric(nomes)\n\n[1] FALSE\n\nis.double(nomes)\n\n[1] FALSE\n\n\n\naltura &lt;- c(1.60, 1.78, 1.55, 1.67, 1.69)\ntypeof(altura)\n\n[1] \"double\"\n\nis.numeric(altura)\n\n[1] TRUE\n\nis.double(altura)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-dataframes",
    "href": "04-ambienteR.html#sec-dataframes",
    "title": "4  Ambiente do R",
    "section": "4.11 Dataframes",
    "text": "4.11 Dataframes\nDataframes são objetos de dados genéricos do R em formato tabular, onde os dados são organizados de maneira lógica em linha-e-coluna semelhante ao de uma planilha do Excel. O dataframe é uma estrutura bidimensional. Estas dimensões podem ser encontradas com a função dim(). Os dataframes podem ser formados com objetos criados previamente, desde que tenham o mesmo comprimento (5). Abaixo serão criadas algumas variáveis, todas relacionadas ao nascimento de 15 bebês:\n\nid &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\npesoRN &lt;- c (3340,3345,3750,3650,3220,4070,3380,3970,3060,3180,  \n             2865,2815,3245,2051,2630)  \ncompRN &lt;- c (50,48,52,48,50,51,50,51,47,47,47,49,51,50,44)\nsexo &lt;- c (2,2,2,1,1,1,2,1,1,1,2,2,1,1,2)\ntipoParto &lt;- c (1,1,2,1,2,2,1,2,1,1,1,2,1,1,1)\nidadeMae &lt;- c (40,19,26,19,32,24,27,20,21,19,23,36,21,23,23) \n\nTem-se um grupo de variáveis isoladas. Seria útil reuni-las em um só objeto. Pode-se fazer isso, usando a função data.frame(). Este novo objeto receberá o nome de dadosNeonatos.\n\ndadosNeonatos &lt;- data.frame (id,\n                             pesoRN, \n                             compRN, \n                             sexo, \n                             tipoParto, \n                             idadeMae)\n\nAo ser executado o comando retornará um novo objeto da classe data.frame:\n\nclass (dadosNeonatos)\n\n[1] \"data.frame\"\n\n\nHavendo necessidade de acrescentar outra variável no dataframe dadosNeonatos, por exemplo, os dados da ida ou não dos recém-nascidos para a UTI. Para isso, será atribuido a um vetor, contendo a situação dos 15 recém-nascidos, o nome de utiNeo e para relacioná-lo a uma coluna do dataframe dadosNeonatos, será usado o símbolo $, como mostrado abaixo 5:\n\ndadosNeonatos$utiNeo &lt;- c (2,2,2,2,1,2,1,2,2,2,2,1,2,2,2)\n\nPara observar a modificação realizada, pode-se usar a função str() do R base 6, digitando no R Script:\n\nstr (dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : num  2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: num  1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : num  2 2 2 2 1 2 1 2 2 2 ...\n\n\nNa saida da função, verifica-se que o dataframe contém 15 linhas e 6 colunas e que todas as variáveis estão como variáveis numéricas , mas as variáveis sexo, tipoParto são variáveis categóricas, bem como a variável utiNeo, acrescentada depois. Há necessidade de fazer uma transformação dessas variáveis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#sec-fatores",
    "href": "04-ambienteR.html#sec-fatores",
    "title": "4  Ambiente do R",
    "section": "4.12 Fatores",
    "text": "4.12 Fatores\nOs fatores são usados para trabalhar com variáveis categóricas. São variáveis usadas para categorizar e armazenar os dados, tendo um número limitado de valores diferentes. Um fator armazena os dados como um vetor de valores inteiros. O fator em R também é conhecido como uma variável categórica que armazena valores de dados de string e inteiros como níveis. O fator é usado principalmente em modelagem estatística e análise exploratória de dados com R (6).\n\n4.12.1 Criando fatores\nNo data frame dadosNeonatos, criado anteriormente, contém três variáveis (sexo, tipoParto e utiNeo) que estão como variáveis numéricas. É possível, desta forma, realizar operações aritméticas com elas. Isto, obviamente, seria um absurdo. Assim, é necessário transformá-las em fatores. Para isso, pode ser usada a função factor(), nativa do R. Os principais argumentos desta função são:\n\nx \\(\\to\\) vetor numérico\nlevels \\(\\to\\) vetor opcional dos valores que x pode assumir\nlabels \\(\\to\\) vetor de caracteres dos rótulos para os níveis, na mesma ordem\nordered \\(\\to\\) vetor lógico (TRUE ou FALSE). Se TRUE, os níveis dos fatores são assumidos como ordenados\n\nNo exemplo, as variáveis não têm uma ordem lógica, então, o argumento ordered não é necessário.\n\ndadosNeonatos$utiNeo &lt;- factor (dadosNeonatos$utiNeo,\n                                levels = c(1,2), \n                                labels = c('sim','não'))\ndadosNeonatos$tipoParto &lt;- factor(dadosNeonatos$tipoParto, \n                                   levels = c(1,2),\n                                   labels = c(\"normal\",\"cesareo\"))\ndadosNeonatos$sexo &lt;- factor (dadosNeonatos$sexo, \n                               levels = c(1,2), \n                               labels = c(\"M\",\"F\")) \n\nApós a transformação, executa-se novamente a função str() para ver como ficou a estrutura do dataframe:\n\nstr(dadosNeonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num  3340 3345 3750 3650 3220 ...\n $ compRN   : num  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : Factor w/ 2 levels \"M\",\"F\": 2 2 2 1 1 1 2 1 1 1 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 2 1 2 2 1 2 1 1 ...\n $ idadeMae : num  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 1 2 2 2 ...\n\n\nAgora, as três varáveis passaram a ser fatores e as outras mantiveram-se numéricas.\nDesta forma, é possível trabalhar com ela fazendo, por exemplo, uma contagem da frequência do tipo de parto, usando a função table():\n\ntable(dadosNeonatos$tipoParto)\n\n\n normal cesareo \n     10       5 \n\n\nOu seja, aproximadamente 70% dos partos desta amostra são normais.\n\n\n4.12.2 Salvando o dataframe criado\nO dataframe, criado e modificado anteriormente, pode ser salvo para uso posterior no diretório de trabalho.\nA função save() realiza esta ação, usando como argumentos o dataframe a ser salvo e o nome do arquivo (file =) entre aspas. Por convenção, esta função salva com a extensão .RData que deve ser digitada, pois o R não a adiciona automaticamente.\n\nsave(dadosNeonatos, file = \"dadosNeonatos.RData\")\n\nEste comando colocará o arquivo no diretório de trabalho em uso. Portanto, se o objetivo é salvar em outro local, deve ser informado qual o novo diretório.\nPara carregar o objeto salvo anteriormente com o comando save(), usa-se a função load(). Se o arquivo a ser lido não estiver no diretório de trabalho da sessão, há necessidade de especificar o caminho até o arquivo:\n\nload(\"dadosNeonatos.RData\")\n\nOu, indicando o diretório onde está o arquivo:\n\nload(\"C:/Users/petro/Dropbox/Estatistica/Meus_Livros/Bioestatistica_R/Book/dadosNeonatos.RData\")\n\nÉ possível salvar em outro tipo de extensão como Excel (.xlsx), Valores Separados por Vírgula (.csv), etc. O procedimento é o mesmo, mudando a função. Para salvar em uma extensão .xlsx,utiliza-se a função write_xlsx () do pacote writexl (7):\n\nwritexl::write_xlsx(dadosNeonatos, \"dadosNeonatos.xlsx\")\n\nPara salvar com a extensão .csv, usar a função write.csv() ou write.csv2() que faz parte do pacote utils, incluido no R base. A primeira função, usa \".\" para a separação dos decimais e \",\" para separar as variáveis; a segunda função usa \",\" para os decimais e \";\" para separar as variáveis, convenção do Excel para algumas localidades, como o Brasil (8). Portanto, uma maneira de salvar o arquivo é:\n\nwrite.csv2 (dadosNeonatos, \"dadosNeonatos.csv\")\n\n\n\n\n\n1. Chang W. Cookbook for R. Cookbook for R. http://www.cookbook-r.com; 2021. \n\n\n2. Verzani J. Using R for introductory statistics. Chapman; Hall/CRC; 2004. \n\n\n3. Rinker TW, Kurkiewicz D. pacman: Package Management for R [Internet]. Buffalo, New York; 2018. Disponível em: http://github.com/trinker/pacman\n\n\n4. Damiani A, Milz B, Lente C, al et. Ciência de Dados em R [Internet]. R6 Consultoria; 2015. Disponível em: https://livro.curso-r.com/index.html\n\n\n5. Zuur AF, Ieno EN, Meesters EH. Getting Data into R. Em: A Beginner’s Guide to R. Springer; 2009. p. 29–56. \n\n\n6. Wickham H, Grolemund G. 15 Factors|R for data science [Internet]. Welcome | R for Data Science. O’Reilly; 2017. Disponível em: https://r4ds.had.co.nz/factors.html\n\n\n7. Ooms J. writexl: Export Data Frames to Excel ’xlsx’ Format [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=writexl\n\n\n8. Team RC. write.table: Data Output/CSV files [Internet]. DataCamp; 2022. Disponível em: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "04-ambienteR.html#footnotes",
    "href": "04-ambienteR.html#footnotes",
    "title": "4  Ambiente do R",
    "section": "",
    "text": "Do inglês: Integrated Development Environment↩︎\nA instalação para Mac OS X pode ser facilmente obtida em busca do Google. Depois de instalado, o uso do RStudio não difere do Windows↩︎\nVersão disponível em 10/06/2025↩︎\nFoi usado o nome area sem acentuação, mas poderia ser qualquer nome.↩︎\nA variável criada, utiNeo, possui dois níveis: 1 = sim; 2 = não, referente se o bebê foi ou não para a UTI.↩︎\na função mostra a estrutura do dataframe↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ambiente do *R*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html",
    "href": "05-manipulandoDados.html",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "5.1 Importando dados de outros softwares\nÉ possível inserir dados diretamente no R Script, como mostrado na Seção 4.11. Entretanto, se o conjunto de dados for muito extenso, torna-se complicado. Desta forma, é melhor importar os dados de outro software, como o Excel, SPSS, etc. A recomendação é que se construa o banco de dados, por exemplo, no Excel, e, depois, exporte o arquivo em um formato que o R reconheça – .xlsx, .csv, .sav.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "href": "05-manipulandoDados.html#importando-dados-de-outros-softwares",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "5.1.1 Importando dados de um arquivo CSV\nO formato CSV significa Comma Separated Values, ou seja, é um arquivo de valores separados por vírgula. Esse formato de armazenamento é simples e agrupa informações de arquivos de texto em planilhas. É possível gerar um arquivo .csv, a partir de uma planilha do Excel, usando o menu salvar como e escolher CSV.\nAs funções read.csv() e read.csv2(), incluídas no R base, podem ser utilizadas para importar arquivos CSV. Existe uma pequena diferença entre elas. Dois argumentos dessas funções têm padrão diferentes em cada uma. São eles: sep (separador de colunas) e dec (separador de decimais). Na read.csv(), o padrão é sep = ”,” e dec = ”.” e em read.csv2() o padrão é sep = “;” e dec = ”,”. Portanto, quando se importa um arquivo .csv, é importante saber qual a sua estrutura. Verificar se os decimais estão separados por ponto ou por vírgula e se as colunas (variáveis), por vírgula ou ponto e vírgula. Para ver isso, basta abrir o arquivo em um bloco de notas (por exemplo, Bloco de Notas do Windows, Notepad ++).\nQuando se usa o read.csv() há necessidade de informar o separador e o decimal, pois senão ele usará o padrão inglês e o arquivo não será lido. Já com read.csv2(), que usa o padrão brasileiro, não há necessidade de informar ao R qual o separador de colunas e nem o separador dos decimais.\nAlém disso, é necessário saber em que diretório do computador está o arquivo para informar ao comando. Recomenda-se colocar o arquivo na pasta do diretório de trabalho, pois assim basta apenas colocar o nome do arquivo na função de leitura dos dados. Caso contrário, tem-se que se usar todo o caminho (path).\nComo exemplo, será importado o arquivo dadosNeonatos.csv que se encontra no diretório de trabalho do autor, salvo anteriormente. Para obter o arquivo, clique aqui e salve em seu diretório de trabalho.\nA estrutura deste arquivo mostra que as colunas estão separadas por ponto-e-virgula e, portanto, a leitura dos dados será feita com a função read.csv2() e, como o arquivo está no diretório de trabalho, não há necessidade de informar o diretório completo. Os dados serão colocados em um objeto de nome neonatos 1:\n\nneonatos &lt;- read.csv2(\"./dados/dadosNeonatos.csv\")\n\nUse a função str() para visualizar o conjunto de dados:2\n\nstr(neonatos)\n\n'data.frame':   15 obs. of  7 variables:\n $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : int  3340 3345 3750 3650 3220 4070 3380 3970 3060 3180 ...\n $ compRN   : int  50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr  \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr  \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : int  40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr  \"não\" \"não\" \"não\" \"não\" ...\n\n\nRecentemente, foi desenvolvido o pacote readr, incluído no conjunto de pacotes tidyverse (1), para lidar rapidamente com a leitura de grandes arquivos. O pacote fornece substituições para funções como read.csv(). As funções read_csv() e read_csv2() oferecidas pelo readr são análogas às do R base. Entretanto, são muito mais rápidas e fornecem mais recursos, como um método compacto para especificar tipos de coluna. Além disso, produzem tibbles (ver adiante, Seção 5.2) que são mais reproduzíveis, pois as funções básicas do R herdam alguns comportamentos do sistema operacional e das variáveis de ambiente, portanto, o código de importação que funciona no seu computador pode não funcionar no de outra pessoa. Para usar a função é necessário instalar e ativar o pacote readr. A função read_csv2() será utilizada para criar um outro objeto de nome recemNascidos, mas o conjunto de dados a ser ativado é o mesmo (dadosNeonatos):\n\n library(readr)\n recemNascidos &lt;- read_csv2(\"dados/dadosNeonatos.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 15 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): sexo, tipoParto, utiNeo\ndbl (4): id, pesoRN, compRN, idadeMae\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQuando você executa read_csv2(), ele imprime uma especificação de coluna que fornece o nome e o tipo de cada coluna.\nNovamente, a função str() mostrará a estrutura do arquivo 3:\n\nstr(recemNascidos)\n\nspc_tbl_ [15 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   pesoRN = col_double(),\n  ..   compRN = col_double(),\n  ..   sexo = col_character(),\n  ..   tipoParto = col_character(),\n  ..   idadeMae = col_double(),\n  ..   utiNeo = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n5.1.2 Importando um arquivo do Excel\nO pacote readxl, pertencente ao conjunto de pacotes do tidyverse, facilita a obtenção de dados do Excel para o R, através da função read_excel(). esta função tem o argumento sheet = , que deve ser usado indicando o número ou o nome da planilha, colocado entre aspas. Este argumento é importante se houver mais de uma planilha, caso contrário, ele é opcional. Para saber os outros argumentos da função, colque o cursor dentro da função e aperte a tecla Tab (Figura 5.1). Isto abrirá um menu com os argumentos:\n\n\n\n\n\n\n\n\nFigura 5.1: Argumentos da função para importar arquivos xlsx\n\n\n\n\n\nSerá feita a leitura dos mesmos dados, usados na leitura de dados csv, apenas o arquivo agora está no formato .xlsx. Para obter o arquivo, siga os mesmos passos, usados anteriormente. Clique aqui e salve em seu diretório de trabalho.\nOs dados serão atribuídos a um objeto com outro nome (recemNatos):\n\nrecemNatos &lt;- readxl::read_excel(\"dados/dadosNeonatos.xlsx\")\nstr(recemNatos)\n\ntibble [15 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:15] 3340 3345 3750 3650 3220 ...\n $ compRN   : num [1:15] 50 48 52 48 50 51 50 51 47 47 ...\n $ sexo     : chr [1:15] \"F\" \"F\" \"F\" \"M\" ...\n $ tipoParto: chr [1:15] \"normal\" \"normal\" \"cesareo\" \"normal\" ...\n $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ...\n $ utiNeo   : chr [1:15] \"não\" \"não\" \"não\" \"não\" ...\n\n\nNa Figura 5.1, o duplo dois pontos (::) precedido do nome do pacote, no caso readxl, especifica a procedência da função usada. Nesta situação, não há necessidade de usar a função library() para carregar o pacote já instalado em um diretório (biblioteca) previamente.\n\n\n5.1.3 Importando arquivos com o RStudio\nO RStudio permite importar arquivos sem a necessidade de digitar comandos, que, para alguns podem ser tediosos.\nNa tela inicial do RStudio, à direita, na parte superior, clique na aba Environment e em Import Dataset. Esta ação abre um menu que permite importar arquivos .csv, Excel, SPSS, etc.\nPor exemplo, para importar o arquivo dadosNeonatos.xlsx, clicar em From Excel... Abre uma janela com uma caixa de diálogo. Clicar no botão Browse..., localizado em cima à direita, para buscar o arquivo dadosNeonatos.xlsx. Assim que o arquivo for aberto, ele mostra uma preview do arquivo e, em baixo, à direita mostra uma preview do código (Figura 5.2)), igual ao digitado anteriormente, que cria um objeto denominado dadosNeonatos, nome do objeto escolhido pelo R, mas pode ser modificado na janela, à esquerda, Import Option em Name, onde pode-se digitar qualquer nome. Após encerrar as escolhas, clicar em Import. É um caminho diferente para fazer o mesmo. Este é um dos fascínios do R!\n\n\n\n\n\n\n\n\nFigura 5.2: Importando arquivos do excel com o RStudio.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-tibble",
    "href": "05-manipulandoDados.html#sec-tibble",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.2 Tibble",
    "text": "5.2 Tibble\nA maneira mais comum de armazenar dados no R é usar data.frames ou tibble.\nTibble é um novo tipo de dataframe. É como se fosse um dataframe mais moderno. Ele mantém muitos recursos importantes do data frame original, mas remove muitos dos recursos desatualizados.\nOs tibbles são outro recurso incrível adicionado ao R por Hadley Wickham, através do tidyverse, conjunto de pacotes que formam um conjunto básico de funções que facilitam a manipulação e representação gráfica dos dados (1). Para saber mais sobre tibble, veja vignette(‘tibbles’), em `help´.\nA maioria dos pacotes do R usa dataframes tradicionais, entretanto é possível transformá-los para tibble, usando a função as_tibble(), incluída no pacote tidyr (2). O único propósito deste pacote é simplificar o processo de criação de tidy data(dados organizados).\nO conceito de tidy data, introduzido por Wickman (3), se refere à estrutura dos dados organizados de maneira que cada linha é uma observação, cada coluna representa variáveis e cada entrada nas células do dataframe são os valores. A transformação de um dataframe tradicional em um tibble, é um procedimento rescomendável, em função da maior flexibilidade destes.\nComo exemplo deste procedimento, será usado o famoso conjunto de dados da flor iris (4) que fornece as medidas em centímetros das variáveis comprimento e largura da sepala e comprimento e largura da pétala, repectivamente, para 50 flores de cada uma das 3 espécies de íris (Iris setosa, versicolor e virginica). Este conjunto de dados encontra-se no pacote datasets no R base. Para visualizar os dados, será usado a função str(), também do R base, que mostra a estrutura interna de um objeto:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nObserva-se que é um conjunto de dados da classe data.frame, contendo 150 observações de 5 variáveis (colunas). Fazendo a coerção para um tibble, tem-se:\n\nlibrary(tidyr)\nas_tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nVerifica-se que não houve grandes mudanças, apenas o conjunto de dados está estruturalmente mais organizado, mais flexível.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#sec-mater",
    "href": "05-manipulandoDados.html#sec-mater",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.3 Pacote dplyr",
    "text": "5.3 Pacote dplyr\nO pacote dpylr é comumente usado para limpar e trabalhar com dados (5). No nível mais básico, as funções do pacote referem-se a “verbos” de manipulação de dados, como select, filter, mutate, arrange, summarize, entre outros, que permitem encadear várias etapas em algumas linhas de código, como será visto adiante.\nO pacote dplyr é adequado para trabalhar com um único conjunto de dados, bem como para obter resultados complexos em grandes conjuntos de dados. As funções dplyr são processadas mais rápido do que as funções R base.\nPara trabalhar na manipulação dos dados serão usados alguns pacotes, já mencionados anteriormente, readxl e dplyr, e o conjunto de dados dadosMater.xlsx. Para obter estes dados, clique aqui e faça o download para o seu diretório de trabalho, como orientado anteriormente.\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAnexando pacote: 'dplyr'\n\n\nOs seguintes objetos são mascarados por 'package:stats':\n\n    filter, lag\n\n\nOs seguintes objetos são mascarados por 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nmater &lt;- read_excel(\"dados/dadosMater.xlsx\")\n\nA função read_excel() carrega o arquivo e o atribui a um objeto , arbitrariamente, denominado de mater4.\n\nas_tibble(mater)\n\n# A tibble: 1,368 × 30\n      id idadeMae altura  peso ganhoPeso anosEst   cor eCivil renda  fumo\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1       42   1.65  69.9       3.9       3     2      1  1.45     2\n 2     2       29   1.66  78        16.5      11     1      2  2.41     2\n 3     3       19   1.72  81         5         9     2      1  1.93     2\n 4     4       31   1.55  74        43         5     2      2  1.45     2\n 5     5       34   1.6   60        15         7     2      2  0.48     2\n 6     6       29   1.5   60        11.4       8     2      2  0.96     1\n 7     7       30   1.54  75.5      10.5       4     1      2  1.2      1\n 8     8       34   1.63  61         9         6     1      2  2.41     2\n 9     9       17   1.68  57        15        10     1      2  2.17     2\n10    10       32   1.5   70        11.4       1     2      2  0.72     2\n# ℹ 1,358 more rows\n# ℹ 20 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;dbl&gt;, para &lt;dbl&gt;,\n#   droga &lt;dbl&gt;, ig &lt;dbl&gt;, tipoParto &lt;dbl&gt;, pesoPla &lt;dbl&gt;, sexo &lt;dbl&gt;,\n#   pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;,\n#   utiNeo &lt;dbl&gt;, obito &lt;dbl&gt;, hiv &lt;dbl&gt;, sifilis &lt;dbl&gt;, rubeola &lt;dbl&gt;,\n#   toxo &lt;dbl&gt;, infCong &lt;dbl&gt;\n\n\nPor padrão, a função retorna as dez primeiras linhas. Além disso, colunas que não couberem na largura da tela serão omitidas. Também são apresentadas a dimensão da tabela e as classes de cada coluna. Observa-se que ele tem 1368 linhas (observações) e 30 colunas (variáveis). Além disso, verifica-se que todas as variáveis estão como numéricas (dbl) e, certamente, algumas, dependendo do objetivo na análise, precisarão ser transformadas.\nO significado de cada uma das variáveis do arquivo dadosMater.xlsx 5 é mostrado abaixo.\n\nid \\(\\to\\) identificação do participante\n\nidadeMae \\(\\to\\) idade da parturiente em anos\n\naltura \\(\\to\\) altura da parturiente em metros\n\npeso \\(\\to\\) peso da parturiente em kg\n\nganhoPeso \\(\\to\\) aumento de peso durante a gestação\n\nanosEst \\(\\to\\) anos de estudo completos\n\ncor \\(\\to\\) cor declarada pela parturiente: 1 = branca; 2 = não branca\n\neCivil \\(\\to\\) estado civil: 1 = solteira; 2 = casada ou companheira\n\nrenda \\(\\to\\) renda familiar em salários minimos\n\nfumo \\(\\to\\) tabagismo: 1 = sim; 2 = não\n\nquantFumo \\(\\to\\) quantidade de cigarros fumados diariamente\n\nprenatal \\(\\to\\) realizou pelo menos 6 consultas no pré-natal? 1 = sim; 2 = não\npara \\(\\to\\) número de filhos paridos\n\ndroga \\(\\to\\) drogadição? 1 = sim; 2 = não\n\nig \\(\\to\\) idade gestacional em semanas\n\ntipoParto \\(\\to\\) tipo de parto: 1 = normal; 2 = cesareana\n\npesoPla \\(\\to\\) peso da placenta em gramas\nsexo \\(\\to\\) sexo do recém-nascido (RN): 1 = masc; 2 = fem\n\npesoRN \\(\\to\\) peso do RN em gramas\n\ncompRN \\(\\to\\) comprimento do RN em cm\n\npcRN \\(\\to\\) perímetro cefálico dorecém-nascido em cm\n\napgar1 \\(\\to\\) escore de Apgar no primeiro minuto\n\napgar5 \\(\\to\\) escore de Apgar no quinto minuto\n\nutiNeo \\(\\to\\) RN necessitou de terapia intesiva? 1 = sim; 2 = não\n\nobito \\(\\to\\) obito no período neonatal? 1 = sim; 2 = não\n\nhiv \\(\\to\\) parturiente portadora de HIV? 1 = sim; 2 = não\n\nsifilis \\(\\to\\) parturiente portadora de sífilis? 1 = sim; 2 = não\n\nrubeola \\(\\to\\) parturiente portadora de rubéola? 1 = sim; 2 = não\n\ntoxo \\(\\longrightarrow\\) parturiente portadora de toxoplasmose? 1 = sim; 2 = não\n\ninfCong \\(\\to\\) parturiente portadora de alguma infecção congênita? 1 = sim; 2 = não\n\n\n5.3.1 Função select()\nA função select () é usada para escolher quais colunas (variáveis) entrarão na análise. Ela recebe os nomes das colunas como argumentos e cria um novo banco de dados usando as colunas selecionadas. A função select () pode ser combinada com outras funções, como filter ().\nPor exemplo, um novo banco de dados será criado (mater1), contendo as mesmas 1368 linhas, mas apenas com as variáveis idadeMae, altura, peso, anosEst, renda, ig, fumo, pesoRN, sexo. Consulte a ajuda (?select()) para obter maiores informações em relação aos argumentos da função:\n\nmater1 &lt;- select(mater, idadeMae, altura, peso, anosEst, renda, ig, tipoParto, fumo, pesoRN, sexo)\n\nPara visualizar este novo banco de dados, pode-se usar a função str():\n\nstr(mater1)\n\ntibble [1,368 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: num [1:1368] 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n\n\nComo mostrado anteriormente, muitas variáveis numéricas do mater, na realidade, são fatores e necessitam de modificação. Entretanto, das selecionadas, para constituir o novo banco de dados, apenas tipoParto, fumo e sexo necessitam serem transformadas para fator:\n\nmater1$tipoParto &lt;- factor(mater1$tipoParto, \n                           levels = c(1,2),\n                           labels = c(\"normal\",\"cesareo\"))\n\nmater1$fumo &lt;- factor (mater1$fumo,\n                       levels = c(1,2), \n                       labels = c('sim','não'))\n\nmater1$sexo &lt;- factor (mater1$sexo, \n                       levels = c(1,2), \n                       labels = c(\"masc\",\"fem\"))\n\nUsando, de novo, a função str(), é possível observar a transformação:\n\nstr(mater1)\n\ntibble [1,368 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nSe houver necessidade de se excluir alguma variável (coluna), basta colocar o sinal de subtração (-) antes do nome da variável:\n\nmater2 &lt;- select(mater1, -altura)\n\n\nstr(mater2)\n\ntibble [1,368 × 9] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n5.3.2 Função filter()\nA função filter() é usada para criar um subconjunto de dados que obedeçam determinadas condições lógicas: & (e), | (ou) e ! (não). Por exemplo:\n\ny & !x \\(\\to\\) seleciona y e não x\nx & !y \\(\\to\\) seleciona x e não y\nx | !x \\(\\to\\) seleciona x ou y\nx & !x \\(\\to\\) seleciona x e y\n\nUm recém-nascido é dito a termo quando a duração da gestação é igual a 37 a 42 semanas incompletas. Se quisermos extrair do banco de dados mater1 os recém-nascidos a termo (mater3), pode-se usar a função filter():\n\nmater3 &lt;- filter (mater1, ig&gt;=37 & ig&lt;42)\n\nPara exibir o resultado, execute a função str():\n\nstr(mater3)\n\ntibble [1,085 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1085] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:1085] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:1085] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:1085] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:1085] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:1085] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:1085] 3285 3100 3100 2800 3270 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nObserve que, agora, o conjunto de dados mater3 tem 1085 linhas, número de recém-nascidos a termo do banco de dados original mater (1368). Logo, os recém nascidos a termo correspondem a 79.3% dos nascimentos, nesta maternidade.\nOutro exemplo\nPara selecionar apenas os meninos, nascidos a termo, codificados como \"masc\", procede-se da seguinte maneira6:\n\nmeninos &lt;- filter (mater3, sexo == 'masc')\n\n\nstr(meninos)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:592] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:592] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:592] 3285 3100 3100 2800 3270 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nO banco de dados meninos é constituídos por 592 meninos. Isto representa 43.3% dos nascimentos.\nUma outra maneira de se fazer a mesma coisa, é usar a função grepl(), dentro da função filter (). Ela é usada para pesquisar a correspondência de padrões. No código a seguir, pesquisa-se os registros em que a variável sexo contém “fem”, correspondentes às meninas.\n\nmeninas &lt;- filter (mater3, grepl(\"fem\", sexo))\n\n\nstr(meninas)\n\ntibble [493 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:493] 17 30 27 28 17 21 28 19 24 43 ...\n $ altura   : num [1:493] 1.65 1.6 1.53 1.4 1.55 1.52 1.58 1.55 1.72 1.6 ...\n $ peso     : num [1:493] 60 54 43.5 60 78 52 50 60.5 60 53 ...\n $ anosEst  : num [1:493] 7 5 11 8 10 11 11 8 8 4 ...\n $ renda    : num [1:493] 1.92 1.92 1.93 2.17 4.82 0.96 4.82 1.69 0.96 1.92 ...\n $ ig       : num [1:493] 38 39 39 38 40 38 37 38 40 39 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 2 1 1 2 1 1 1 1 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 1 1 1 2 ...\n $ pesoRN   : num [1:493] 3165 3150 2980 3095 3020 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n5.3.3 Função mutate()\nEsta função tem a finalidade de computar ou anexar uma ou mais colunas (variáveis) novas.\nO Índice de Massa Corporal (IMC) é igual a\n\\[IMC = \\frac {peso}{altura ^2}\\].\nO peso deve ser expreso em kg e a altura em metros. Para acrescentar este indicador no banco de dados mater1 , se fará uso da função mutate(), nomeando a nova variável de imc:\nSerá acrescentado a variável imc, no banco de dados mater1, usando a função mutate(), nomenando essa variável de imc:\n\nmater1 &lt;- mutate(mater1, imc = peso/altura^2)\nstr (mater1)\n\ntibble [1,368 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura   : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso     : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst  : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ renda    : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ...\n $ ig       : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 2 2 1 1 2 1 2 1 1 2 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 1 2 2 2 ...\n $ pesoRN   : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 2 2 2 2 2 2 2 2 2 2 ...\n $ imc      : num [1:1368] 25.7 28.3 27.4 30.8 23.4 ...\n\n\n\n\n5.3.4 Função sample_n()\nFunção usada para selecionar de forma aleatória linhas de um dataframe. Acostume-se a usar a ajuda (?sample_n) para obter informações das funções. Os seus argumentos básicos são:\n\ntbl \\(\\to\\) dataframe\nsize \\(\\to\\) número de linhas para selecionar\nreplace \\(\\to\\) amostra com ou sem reposição?. Padrão = FALSE\n\nUma mostra de 20 neonatos do banco de dados meninos, pode ser selecionada, usando a função sample_n:\n\nmeninos1 &lt;- sample_n(meninos, 20)\n\nAssim, selecionou-se uma amostra de 20 meninos do banco de dados de recém-nascidos a termo. Pode-se, fazer um resumo da variável peso do recém-nascido (meninos1$pesoRN) para ver como ela se comporta, usando a função summary():\n\nsummary(meninos1$pesoRN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2590    3139    3368    3343    3458    4400 \n\n\nÉ importante mencionar que toda vez que a função sample_n() for executada, ela irá gerar uma amostra aleatória diferente. Em consequência não se deve esperar, por exemplo, que a média dos pesos dos recém-nascidos de amostras diferentes sejam iguais.\n\nmeninos2 &lt;- sample_n(meninos, 20)\nsummary(meninos2$pesoRN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2285    2881    3148    3171    3375    4050 \n\n\nNa Seção 9.2.2 (Distribuições Amostrais), este assunto voltará à cena.\nA função sample_n() está sendo eliminada, pois será substituída por slice_sample() do conjunto de funções que acompanham a função slice().\n\n\n5.3.5 Função slice()\nEsta função é usada para selecionar um subconjunto linhas com base em seus locais inteiros. Permite selecionar, remover e duplicar linhas. Para os exemplos, será usado o conjunto de dados meninos, criado acima. Os argumentos básicos da função slice() são\n\n.data \\(\\to\\) dataframe\n.by \\(\\to\\) seleciona por grupo\nn, prop \\(\\to\\) fornecer n, o número de linhas, ou prop, a proporção de linhas a serem selecionadas.\n\nExemplpos:\n\n# Selecionando a linha 10\nslice(.data = meninos, 10)\n\n# A tibble: 1 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       14   1.51  56.7       6  1.92    39 normal    não     3200 masc \n\n# Selecionando varias linhas, por exemplo, de 1 a 5\nslice(.data = meninos, 1:5)\n\n# A tibble: 5 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       28   1.5   48.5       6  3.13    37 normal    não     3285 masc \n2       31   1.55  65         5  0.72    37 cesareo   não     3100 masc \n3       27   1.6   60         8  2.41    37 cesareo   sim     3100 masc \n4       28   1.58  47         8  1.69    38 normal    não     2800 masc \n5       18   1.76  65.5       7  1.93    39 normal    sim     3270 masc \n\n\nÉ possível também selecionar linhas de acordo com determinado grupo. Como grupo, será usada a variável fumo que tem dois níveis (sim e não). Será selecionada uma linha de cada grupo:\n\nslice(.data = meninos, .by = fumo, 1)\n\n# A tibble: 2 × 10\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       28    1.5  48.5       6  3.13    37 normal    não     3285 masc \n2       27    1.6  60         8  2.41    37 cesareo   sim     3100 masc \n\n\nPara separar por grupo , é possível usar a função group_by(), incluída no pacote dplyr. A melhor solução, neste caso, para aplicar diversas funções de manipulação em um dataframe é aplicar o operador pipe: %&gt;%. No final desta seção, será discutido com mais detalhes este operador.\n\n meninos %&gt;% \n   group_by(fumo) %&gt;% \n   slice (1)\n\n# A tibble: 2 × 10\n# Groups:   fumo [2]\n  idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n1       27    1.6  60         8  2.41    37 cesareo   sim     3100 masc \n2       28    1.5  48.5       6  3.13    37 normal    não     3285 masc \n\n\nA saída desse código é a mesma vista anteriormente. A vantagem é termos escrito o código na ordem em que as funções são aplicadas. Portanto, é um código mais legível.\n\n5.3.5.1 Funções auxiliares da função slice()\nA função slice() é acompanhada por várias funções auxiliares para casos de uso comuns:\n\nslice_head() e slice_tail() selecionam a primeira ou a última linha;\nslice_sample() seleciona linhas aleatoriamente, substitui a sample_n();\nslice_min() e slice_max() selecionam linhas com valores mais altos ou mais baixos de uma variável.\n\nPor exemplo, para selecionar uma amostra aleatória de 20 meninos do conjunto de dados meninos, pode-se usar a função slice_sample():\n\n meninos3 &lt;- slice_sample(.data = meninos, n = 20)\n str (meninos3)\n\ntibble [20 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:20] 20 31 19 19 34 27 18 26 25 39 ...\n $ altura   : num [1:20] 1.68 1.55 1.67 1.68 1.67 1.65 1.58 1.63 1.54 1.61 ...\n $ peso     : num [1:20] 59 53 45 75 73 60 70 61 54 70 ...\n $ anosEst  : num [1:20] 14 3 8 11 4 7 11 7 3 8 ...\n $ renda    : num [1:20] 2.41 1.92 0.92 1.69 1.92 1.45 1.2 3.86 1.45 1.92 ...\n $ ig       : num [1:20] 39 40 37 40 40 39 39 39 37 37 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 1 1 1 1 1 1 2 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 2 1 2 2 2 2 ...\n $ pesoRN   : num [1:20] 3075 3135 2745 4080 3847 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nComo se observa no código, o argumento n da função deve ser nomeado explicitamente (n = 20). É possível também usar o argumento prop, colocando a proporção de linhas que se deseja selecionar. Se o objetivo é selecionar 10% da amostra, coloca-se o argumento como prop = 0.10. Como o dataframe meninos contém 592 casos, serão selecionados 59. Além disso, a função permite selecionar por grupos com o argumento by =.\n\nmeninos4 &lt;- slice_sample(.data = meninos, prop = 0.10)\nstr (meninos4)\n\ntibble [59 × 10] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:59] 29 20 30 22 31 26 19 22 23 27 ...\n $ altura   : num [1:59] 1.71 1.47 1.53 1.65 1.57 1.63 1.68 1.55 1.55 1.66 ...\n $ peso     : num [1:59] 87 48 57 65 41 61 58 46 63.5 66 ...\n $ anosEst  : num [1:59] 10 9 8 7 11 7 10 6 8 8 ...\n $ renda    : num [1:59] 1.92 0.96 1.69 1.93 2.41 3.86 4.82 1.92 1.93 2.24 ...\n $ ig       : num [1:59] 41 40 40 37 39 39 39 39 39 40 ...\n $ tipoParto: Factor w/ 2 levels \"normal\",\"cesareo\": 1 1 2 1 1 1 2 1 2 1 ...\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 2 2 1 2 2 2 2 2 ...\n $ pesoRN   : num [1:59] 4660 2340 3275 2405 2700 ...\n $ sexo     : Factor w/ 2 levels \"masc\",\"fem\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nPara maiores informações em relação a estas funções consulte a ajuda (?slice()).\n\n\n\n5.3.6 Função arrange()\nOrdena as linhas pelos valores de uma coluna de forma ascendente ou descentente.\nVoltando a amostra meninos1, será colocado em ordem crescente a variável pesoRN:\n\narrange(meninos1, pesoRN)\n\n# A tibble: 20 × 10\n   idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1       19   1.53  53        11 10.7     40 normal    não     2590 masc \n 2       27   1.5   74        10  0.96    39 normal    não     2960 masc \n 3       35   1.58  74        11  1.93    39 cesareo   não     2965 masc \n 4       22   1.54  55         7  4.82    40 normal    não     3025 masc \n 5       21   1.65  60        10  2.17    38 normal    não     3105 masc \n 6       20   1.67  69        10  1.93    40 cesareo   sim     3150 masc \n 7       34   1.55  53         7  1.69    38 cesareo   sim     3170 masc \n 8       24   1.56  60         8  1.3     40 cesareo   sim     3230 masc \n 9       22   1.6   60         5  1.69    38 normal    sim     3270 masc \n10       25   1.55  70        11  2.89    41 cesareo   não     3355 masc \n11       18   1.55  51         8  0.92    39 normal    não     3380 masc \n12       16   1.47  49         8  1.92    39 cesareo   não     3385 masc \n13       32   1.55  70         4  1.22    39 normal    sim     3410 masc \n14       33   1.44  47.2       2  1.92    39 cesareo   sim     3415 masc \n15       22   1.6   70         5  3.86    39 normal    não     3440 masc \n16       41   1.51  99         3  0.72    37 cesareo   sim     3510 masc \n17       21   1.55  62         8  4.82    39 normal    não     3560 masc \n18       19   1.63  51        11  1.92    39 normal    não     3710 masc \n19       23   1.73  58        10  3.33    39 normal    não     3830 masc \n20       17   1.47  65        10  1.69    38 cesareo   não     4400 masc \n\n\nPara a ordem decrescente, colocar a função desc(), dentro da função arrange()\n\narrange(meninos1, desc(pesoRN))\n\n# A tibble: 20 × 10\n   idadeMae altura  peso anosEst renda    ig tipoParto fumo  pesoRN sexo \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1       17   1.47  65        10  1.69    38 cesareo   não     4400 masc \n 2       23   1.73  58        10  3.33    39 normal    não     3830 masc \n 3       19   1.63  51        11  1.92    39 normal    não     3710 masc \n 4       21   1.55  62         8  4.82    39 normal    não     3560 masc \n 5       41   1.51  99         3  0.72    37 cesareo   sim     3510 masc \n 6       22   1.6   70         5  3.86    39 normal    não     3440 masc \n 7       33   1.44  47.2       2  1.92    39 cesareo   sim     3415 masc \n 8       32   1.55  70         4  1.22    39 normal    sim     3410 masc \n 9       16   1.47  49         8  1.92    39 cesareo   não     3385 masc \n10       18   1.55  51         8  0.92    39 normal    não     3380 masc \n11       25   1.55  70        11  2.89    41 cesareo   não     3355 masc \n12       22   1.6   60         5  1.69    38 normal    sim     3270 masc \n13       24   1.56  60         8  1.3     40 cesareo   sim     3230 masc \n14       34   1.55  53         7  1.69    38 cesareo   sim     3170 masc \n15       20   1.67  69        10  1.93    40 cesareo   sim     3150 masc \n16       21   1.65  60        10  2.17    38 normal    não     3105 masc \n17       22   1.54  55         7  4.82    40 normal    não     3025 masc \n18       35   1.58  74        11  1.93    39 cesareo   não     2965 masc \n19       27   1.5   74        10  0.96    39 normal    não     2960 masc \n20       19   1.53  53        11 10.7     40 normal    não     2590 masc \n\n\n\n\n5.3.7 Função count()\nPermite contar rapidamente os valores únicos de uma ou mais variáveis. Esta função tem os seguintes argumentos:\n\nx \\(\\to\\) dataframe\nwt \\(\\to\\) pode ser NULL (padrão) ou uma variável\nsort \\(\\to\\) padrão = FALSE; se TRUE, mostrará os maiores grupos no topo\nname \\(\\to\\) O nome da nova coluna na saída; padrão = NULL\n\nQuando o argumento name é omitido, a função retorna n como nome padrão.\nUsando o dataframe mater1, a função count() irá contar o número de parturientes fumantes, variável dicotômica fumo:\n\ncount(mater1, fumo)\n\n# A tibble: 2 × 2\n  fumo      n\n  &lt;fct&gt; &lt;int&gt;\n1 sim     301\n2 não    1067\n\n\n\n\n5.3.8 Operador pipe %&gt;%\nO operador pipe %&gt;% pode ser usado para inserir um valor ou um objeto no primeiro argumento de uma função. Ele pode ser acionado digitando %&gt;% ou usando o atalho ctrl + shift + M. Em vez de passar o argumento para a função separadamente, é possível escrever o valor ou objeto e, em seguida, usar o pipe para convertê-lo como o argumento da função na mesma linha. Funciona como se o pipe jogasse o objeto dentro da função seguinte.\nVários comando foram utilizados, manipulando o banco de dados mater. Alguns procedimentos, serão mostrados, usando, agora, o operador pipe.\nEm primeiro lugar, serão selecionadas algumas colunas do dataframe mater; adicionada a variável imc; selecionado os recém-nascidos a termo do sexo masculino, que no banco de dados mater está codificado como 1. Tudo em um só comando!\n\nmeusDados &lt;- mater %&gt;% \n  select(idadeMae, altura, peso, anosEst, renda, \n         ig, tipoParto, fumo, pesoRN, sexo) %&gt;% \n  mutate(imc = peso/altura^2) %&gt;% \n  filter (ig&gt;=37 & ig&lt;42, sexo == 1)\n\n\nstr(meusDados)\n\ntibble [592 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ...\n $ altura   : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ peso     : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ...\n $ anosEst  : num [1:592] 6 5 8 8 7 11 6 5 9 6 ...\n $ renda    : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ...\n $ ig       : num [1:592] 37 37 37 38 39 39 39 39 39 39 ...\n $ tipoParto: num [1:592] 1 2 2 1 1 2 2 1 1 1 ...\n $ fumo     : num [1:592] 2 2 1 2 1 1 2 2 2 2 ...\n $ pesoRN   : num [1:592] 3285 3100 3100 2800 3270 ...\n $ sexo     : num [1:592] 1 1 1 1 1 1 1 1 1 1 ...\n $ imc      : num [1:592] 21.6 27.1 23.4 18.8 21.1 ...\n\n\nObserve que o dataframe mater aparece apenas no início e, como ele é um argumento das outras funções, ele é transferido, automaticamente, não havendo necessidade de escrever dentro na função.\nNo final, retornará um novo dataframe que foi colocado em um objeto, denominado meuDados, o qual contém informações de todos os 592 meninos, nascidos a termo e de suas mães.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#manipulação-de-datas",
    "href": "05-manipulandoDados.html#manipulação-de-datas",
    "title": "5  Manipulando os dados no RStudio",
    "section": "5.4 Manipulação de datas",
    "text": "5.4 Manipulação de datas\nOriginalmente, todos os que trabalham com o R queixavam-se de como era frustrante trabalhar com datas. Era um processo que causava grande perda de tempo nas análises. O pacote lubridate (6) foi criado para simplificar ao máximo a leitura de datas e extração de informações das mesmas.\nQuando o lubridate é carregado aparece uma mensagem, avisando que alguns nomes de funções também estão contidas no pacote base do R.\n\nlibrary(lubridate)\n\n\nAnexando pacote: 'lubridate'\n\n\nOs seguintes objetos são mascarados por 'package:base':\n\n    date, intersect, setdiff, union\n\n\nPara evitar confusões e verificar que as funções corretas estão sendo usadas, usa-se o duplo dois pontos (::) antes do nome da função, precedido do nome do pacote, por exemplo: lubridate::date().\nPara obter a data atual ou a data-hora, você pode usar as funções today()7 ou now()8:\n\ntoday() \n\n[1] \"2025-06-22\"\n\nnow()\n\n[1] \"2025-06-22 20:28:10 -03\"\n\n\n\n5.4.1 Convertendo strings ou caractere para data\nPara converter string ou caracteres em datas, basta executar funções específicas adequadas aos dados. Elas determinam automaticamente o formato quando você especifica a ordem do componente. Para usá-los, identifique a ordem em que o ano, o mês e o dia aparecem em suas datas e, em seguida, organize “y”, “m” e “d” na mesma ordem. Isso lhe dá o nome da função do lubridate que analisará a data. Por exemplo, suponhamos a data de 25/12/2022:\n\nnatal &lt;- \"25/12/2022\"\nnatal\n\n[1] \"25/12/2022\"\n\n\nAparentemente, o R aceitou a informação como uma data. Entretanto, se for verificada a classe do objeto, tem-se:\n\nclass(natal)\n\n[1] \"character\"\n\n\nEstando como caractere, esta data não poderá ser usada em operações com datas, pois necessitaria estar como uma classe date. Para converte-la em data, usa-se a função dmy():\n\nnatal &lt;- dmy(natal)\nnatal\n\n[1] \"2022-12-25\"\n\nclass(natal)\n\n[1] \"Date\"\n\n\nDessa forma, a data, agora está sendo reconhecida pelo R como date. É sempre importante verificar a classe da data.\nÀs vezes, as datas escritas estão com o mês abreviado, como 25/dez/2022. O procedimento é o mesmo\n\nminha.data &lt;- \"25/dez/2022\"\nclass (minha.data)\n\n[1] \"character\"\n\n\n\nminha.data &lt;- dmy(minha.data)\nclass (minha.data)\n\n[1] \"Date\"\n\n\nSe além da data, houver necessidade de especificar o horário, basta usar dmy_h(), dmy_hm() e dmy_hms(). No padrão americano, pode ser usado ymd().\nO lubridate traz diversas funções para extrair os componentes de um objeto da classe date.\n\nsecond() \\(\\rightarrow\\) extrai os segundos.\nminute() \\(\\rightarrow\\) extrai os minutos.\nhour() \\(\\rightarrow\\) extrai a hora.\nwday() \\(\\rightarrow\\) extrai o dia da semana.\nmday() \\(\\rightarrow\\) extrai o dia do mês.\nmonth() \\(\\rightarrow\\) extrai o mês.\nyear() \\(\\rightarrow\\) extrai o ano.\n\nPor exemplo, usando a data de nascimento (dn) de um dos netos do autor:\n\ndn &lt;- dmy(\"06/06/2018\")\nyear(dn)\n\n[1] 2018\n\n\nPara acrescentar um horário ao objeto data de nascimento (dn)^[UTC = Coordinated Universal Time}:\n\nhour(dn) &lt;- 18\ndn\n\n[1] \"2018-06-06 18:00:00 UTC\"\n\n\n\n\n5.4.2 Juntando componentes de datas\nPara juntar componentes de datas e horas, pode-se utilizar as funções make_date() e make_datetime(). Em muitos arquivos, os componentes da data estão em colunas diferentes e há necessidade de juntá-los em uma única coluna para compor a data:\n\nfelix &lt;- make_date(year = 2018, month = 06, day = 06)\nfelix\n\n[1] \"2018-06-06\"\n\n\nPara juntar ano, mês, dia, hora e minuto:\n\nminha.data &lt;- make_datetime(year = 2018, \n                            month = 06, \n                            day = 06, \n                            hour = 18 ,\n                            min = 00, \n                            sec = 15)\nminha.data\n\n[1] \"2018-06-06 18:00:15 UTC\"\n\n\n\n\n5.4.3 Extraindo componentes de datas\nQuando temos objetos do tipo POSIXt9 podemos extrair componentes ou elementos deles. Para isso são usadas algumas funções específicas do pacote lubridate como mostrado a seguir.\n\ndata &lt;- now()\n\nyear(data)            # Extrai o ano\n\n[1] 2025\n\nmonth(data)           # Extrai o mês\n\n[1] 6\n\nweek(data)            # Extrai a semana\n\n[1] 25\n\nday(data)             # Extrai o dia\n\n[1] 22\n\nminute(data)          # Extrai o minuto\n\n[1] 28\n\nsecond(data)          # Extrai o segundo\n\n[1] 10.48674\n\n\nPara verificar o número de dias tem em um determinado mês, usa-se a função days_in_month():\n\n data1 &lt;- dmy(\"25/02/2000\")\n days_in_month(data1)          \n\nFeb \n 29 \n\n\n\n\n5.4.4 Operações com datas\nO pacote lubridate possui funções de duração e de período para manipular as datas. As funções de duração calculam o número de segundos em um determinado num determinado número de dias. As funções de duração não levam em consideração anos bissextos e horário de verão, enquanto as funções de período consideram esses fatores.\n\nddays (1)           # Número de segundos em 1 dia\n\n[1] \"86400s (~1 days)\"\n\ndhours (1)          # Número de segundos em 1 hora\n\n[1] \"3600s (~1 hours)\"\n\ndminutes (1)        # Número de segundos em 1 minuto\n\n[1] \"60s (~1 minutes)\"\n\ndays (5)            # Cria um período de 5 dias\n\n[1] \"5d 0H 0M 0S\"\n\nweeks (5)           # Cria um período de 5 semanas\n\n[1] \"35d 0H 0M 0S\"\n\n\nSuponha-se que haja necessidade de saber em qual dia cairá após acrescentarmos 5 semanas à data1 (25/02/2000), criada acima:\n\ndata1 + weeks (5)           \n\n[1] \"2000-03-31\"\n\n\nAdicionando 1 ano à data1 (25/02/2000) com uma função de duração, tem-se:\n\ndata1 + dyears (1)           \n\n[1] \"2001-02-24 06:00:00 UTC\"\n\n\nSe for adicionado um ano à mesma data, mas agora com uma função de período, tem-se:\n\ndata1 + years (1)           \n\n[1] \"2001-02-25\"\n\n\nUm intervalo de tempo pode ser obtido a partir de uma data inicial e uma data final. Suponha que uma gestante tenha como data da sua última menstruação 04/10/2022 e o bebê tenha nascido em 30/06/2023. Qual a idade gestacional em dias? A sintaxe para calcular um intervalo é dada pela subtração das duas datas:\n\ndata.inicial &lt;- dmy(\"04/10/2022\")\ndata.final &lt;- dmy(\"30/06/2023\")\nidade_gesta &lt;- data.final - data.inicial\nidade_gesta\n\nTime difference of 269 days\n\n\nOu seja a gestação durou 269 dias, constituindo-se em um parto a termo, entre 37 (259 dias) e 42 semanas (294 dias).\nPara mais informações sobre o lubridate, consulte a ajuda do pacote ou o capítulo 16 do livro R for Data Science, Hadley Wickman e Garrett Grolemund, 2017 [https://r4ds.had.co.nz/index.html] .\n\n\n\n\n1. Wickham H, Averick M, Bryan J, Chang W, et al. Welcome to the Tidyverse. Journal of Open Source Software. 2019;4(43):1686. \n\n\n2. Wickham H, Girlich M. tidyr: Tidy Messy Data [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=tidyr\n\n\n3. Wickham H. Tidy Data. Journal of Statistical Software. 2014;59(10):11–23. \n\n\n4. Fisher RA. The use of multiple measurements in taxonomic problems. Annals of eugenics. 1936;7(2):179–88. \n\n\n5. Wickham H, François R, Henry L, Müller K, et al. dplyr: A grammar of data manipulation. R package version 04. 2015;3:156. \n\n\n6. Grolemund G, Wickham H. Dates and Times Made Easy with lubridate. Journal of Statistical Software [Internet]. 2011;40(3):1–25. Disponível em: https://www.jstatsoft.org/v40/i03/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "05-manipulandoDados.html#footnotes",
    "href": "05-manipulandoDados.html#footnotes",
    "title": "5  Manipulando os dados no RStudio",
    "section": "",
    "text": "A mudança do nome do dataframe de dadosNeonatos para neonatos é desnecessária. Foi realizada apenas por questões didáticas.↩︎\nObserve, na saída, que a variável utiNeo aparece palavras com acentuação (“não”). Às vezes, ao abrir o arquivo com a função read.csv2(), pode acontecer de esta palavra aparecer, por exemplo, como: “n3o”. Louco, não é? Se ocorrer isso, use, após o nome do arquivo e separado por vírgula, o argumento fileEncoding = “latin1”. Dessa forma, o erro será corrigido.↩︎\nDa mesma maneira, como acontece com a função read.csv2(), a função equivalente do readr pode retornar erro na leitura de palavras com acento. Para corrigir isso, usa-se o argumento locale (encoding = \"latin1\")↩︎\nATENÇÃO: Volta-se a insistir, o comando para carregar o conjunto de dados somente funciona, sem colocar o caminho (path) completo, se tudo está sendo realizado no diretório de trabalho.↩︎\nConjunto de dados coletados na maternidade-escola do Hospital Geral de Caxias do Sul↩︎\nLembrar que o sinal de igualdade, no R, é duplo \\(=\\)↩︎\nEquivalente ao Sys.Date()que acessa a data do sistema operacional.↩︎\nEquivalente ao Sys.time() que acessa a data e hora do sistema operacional.↩︎\nPOSIXt é uma classe de objetos do R que representa datas e horas. POSIXt significa Portable Operating System Interface for Unix Time, que é um padrão para medir o tempo em segundos desde 1 de janeiro de 1970. Existem duas formas internas de implementar POSIXt: POSIXct e POSIXlt. POSIXct armazena os segundos desde a época UNIX e POSIXlt armazena uma lista de dia, mês, ano, hora, minuto, segundo, etc.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipulando os dados no *RStudio*</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html",
    "href": "06-descrevendoDados.html",
    "title": "6  Descrevendo os dados",
    "section": "",
    "text": "6.1 Pacotes necessários neste capítulo\nNos relatórios ou artigos científicos, a comunicação dos resultados é feita através da combinação de medidas resumidoras e visualização dos dados por meio de tabelas e gráficos.\nPara trabalhar neste capítulo, serão necessários os seguintes pacotes.\npacman::p_load(dplyr,\n               DescTools,\n               flextable,\n               ggplot2, \n               ggpubr, \n               ggsci, \n               grDevices, \n               Hmisc, \n               kableExtra, \n               knitr, \n               plotrix, \n               readxl, \n               scales)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#dados-brutos",
    "href": "06-descrevendoDados.html#dados-brutos",
    "title": "6  Descrevendo os dados",
    "section": "6.2 Dados brutos",
    "text": "6.2 Dados brutos\nHabitualmente, costuma-se armazenar os dados em bancos de dados (dataframes ou tibbles). Entretanto, eles estão registrados de forma aleatória e não classificada. Ao se visualizar um dataframe, é difícil responder perguntas em relação a qualquer variável, principalmente, em grandes banco de dados. Eles se constituem uma lista, um rol de valores colocados na ordem em que foram obtidos. Parecem um jogo de quebra cabeça antes de serem organizados e resumidos (Figura 6.1)! São denominados de dados brutos ou, também, de dados não agrupados.\n\n\n\n\n\n\n\n\nFigura 6.1: Quebra-cabeça da Torre de Babel (pintura de Pieter Bruegel, 1563)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#medidas-resumidoras",
    "href": "06-descrevendoDados.html#medidas-resumidoras",
    "title": "6  Descrevendo os dados",
    "section": "6.3 Medidas resumidoras",
    "text": "6.3 Medidas resumidoras\n\n6.3.1 Dados usados nesta seção\nPara a demonstração prática será usado um conjunto de dados que é uma amostra aleatória de 15 recém-nascidos do banco de dados dadosMater.xlsx (ver Seção 5.3), extraída com a função slice_sample() do pacote dplyr. Cada vez que este comando for reproduzido, retornará uma nova série de 15 valores diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada. Após extrair a amostra, serão selecionadas as variáveis usadas nesta seção. A amostra será atribuída a um objeto denominado, dados:\n\nset.seed(1234) \ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;= 37 & ig&lt;42) %&gt;% \n  select(idadeMae, anosEst, pesoRN, apgar1) %&gt;% \n  slice_sample(n=15)\nstr(dados) \n\ntibble [15 × 4] (S3: tbl_df/tbl/data.frame)\n $ idadeMae: num [1:15] 23 26 18 25 35 19 20 23 26 35 ...\n $ anosEst : num [1:15] 5 9 7 10 11 9 10 11 4 5 ...\n $ pesoRN  : num [1:15] 3190 3715 2555 3795 2970 ...\n $ apgar1  : num [1:15] 9 7 9 6 8 8 8 8 9 9 ...\n\n\nApós a manipulação dos dados, tem-se um tibble de 15 linhas e quatro colunas.\n\n\n6.3.2 Introdução\nSempre que se está diante de um novo conjunto de dados para analisar, uma das primeiras tarefas é encontrar maneiras de resumir os dados de forma compacta e fácil de entender. Este processo se constitui na estatística descritiva que compreende métodos de tabulação, gráficos e resumo dos dados. Nesta seção, serão verificadas as medidas de resumo dos dados de duas maneiras:\n\nPrimeiro, um valor em torno do qual os dados têm uma tendência para se reunir ou se agrupar, denominado de medida sumária de localização ou medida de tendência central.\nEm segundo lugar, um valor que mede o grau em que os dados se dispersam, denominado de medida de dispersão ou variabilidade\n\n\n\n6.3.3 Medidas de tendência central\n\n6.3.3.1 Média\nA média ( \\(\\overline{x}\\) ) é a mais usada medida de tendência central para representar um valor típico dentro de um conjunto de números. O conceito mais comum é a média aritmética, que se calcula somando todos os valores do conjunto e dividindo pelo número total de elementos. A média é mais adequada para medidas numéricas simétricas, pois ela é sensível aos valores extremos (outliers).\n\\[\n\\overline{x}= \\frac{\\sum(x_1 + x_2 + x_3 + ... + x_n)}{n}\n\\]\nO R base possui uma função para o cálculo da média, mean(), apresentada na Seção 4.8, onde foi mostrado os seus argumentos. Se a variável analisada contiver algum valor ausente (missing), deve-se usar o argumento na.rm = TRUE, para removê-los, pois, caso contrário, a função retorna um resultado como NA (Not Available). Para evitar transtornos, recomenta-se usar sempre o argumento.\n\nmean (dados$pesoRN, na.rm = TRUE)\n\n[1] 3307.667\n\n\nPara reduzir o número de dígitos decimais, na saída do resultado, pode-se colocar a função mean(), dentro da função round()1, atribuindo o resultado da função a um objeto, por exemplo media.\n\nmedia &lt;- round(mean (dados$pesoRN, na.rm = TRUE), 1)\nprint(media)\n\n[1] 3307.7\n\n\nOu, usar a função round(), separadamente:\n\nmedia &lt;- mean (dados$pesoRN, na.rm = TRUE)\nround(media, 1)\n\n[1] 3307.7\n\n\n\n\n6.3.3.2 Mediana\nA mediana (Md) representa o valor central em uma série ordenada de valores. Assim, metade dos valores será igual ou menor que o valor mediano e a outra metade igual ou maior do que ele. Para encontrar a média:\n\nOrdene o conjunto de dados, por exemplo, a variável dados$pesoRN:\n\n\nvalores_ordenados &lt;- sort(dados$pesoRN)\nprint(valores_ordenados)\n\n [1] 2285 2555 2965 2970 3185 3190 3220 3500 3570 3585 3645 3710 3715 3725 3795\n\n\n\nSe o número de valores no conjunto de dados for ímpar, a mediana é o valor do meio. No exemplo, tem-se 15 valores, onde o valor do meio é o 7º valor e, portanto, a mediana é igual a 3220 g.\nSe o número de valores no conjunto de dados for par, a mediana é a média dos dois valores do meio. Apenas como exemplo, para se obter um número par de valores, será eliminado o 15º valor da variável dados$pesoRN:\n\n\nvalores_ordenados_pares &lt;- valores_ordenados [-15]\nprint(valores_ordenados_pares)\n\n [1] 2285 2555 2965 2970 3185 3190 3220 3500 3570 3585 3645 3710 3715 3725\n\n\nAssim, a mediana será igual a média do 7º e 8º e igual a 3360 g.\nÉ bem fácil de se calcular a mediana quando o número de observações é pequeno. Entretanto, quando se tem um número grande de valores seria extremamente tedioso esse calculo. O R facilita esse trabalho, fornecendo a função median().\nAgora, será usada a variável apgar1 do arquivo dadosMater.xlsx. Como o Apgar é um escore, a medida resumidora mais adequada, realmente, é a mediana.\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;= 37 & ig&lt;42) %&gt;% \n  select(idadeMae, anosEst, pesoRN, apgar1)\nmedian (mater$apgar1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\n6.3.3.3 Moda\nModa (Mo) é o valor que ocorre com maior frequência em um conjunto de dados. O R não possui uma função nativa e direta para calcular a moda como tem para a média (mean()) e a mediana (median()). Isso acontece porque a moda pode não ser única em um conjunto de dados (podem existir múltiplos valores com a mesma frequência máxima) ou pode nem existir (se todos os valores ocorrerem apenas uma vez).Tem o menor nível de sofisticação. No entanto, pode-se facilmente criar uma função própria para calcular a moda ou usar pacotes que oferecem essa funcionalidade, como o DescTools que oferece uma função chamada Mode(). Aqui estão algumas maneiras de calcular a moda em R:\nFunção personalizada\n\nmoda &lt;- function(v) {\n  freq_tab &lt;- table(v)\n  max_freq &lt;- max(freq_tab)\n  moda &lt;- names(freq_tab[freq_tab == max_freq])\n  return(moda)\n}\n\nEsta função moda()é constituída por:\n\ntable(v): Cria uma tabela de frequência dos valores v.\nmax(freq_tab): Encontra a frequência máxima.\nfreq_table[freq_table == max_freq]: Seleciona as entradas da tabela de frequência que são iguais à frequência máxima.\nnames(…): Obtém os nomes (os valores originais) dessas entradas, que são as modas.\n\nUsando a função criada, a moda da variável dados$apgar1 é igual a:\n\nmoda (dados$apgar1) \n\n[1] \"8\"\n\n\nA função moda() pode ser salva em seu diretório de trabalho, na pasta das suas funções próprias. Quando necessário ela pode ser acessada, como foi visto na Seção 4.8.1).\nFunção Mode() do pacote DescTools\n\nmoda &lt;- Mode(dados$apgar1)\nprint(moda)\n\n[1] 8\nattr(,\"freq\")\n[1] 7\n\n\nA saída mostra que a média é igual a 8 e que frequência deste valor entre os 15 valores é 7.\n\n\n6.3.3.4 Quantil\nUma medida de localização bastante utilizada são os quantis que são pontos estabelecidos em intervalos regulares que dividem a amostra em subconjuntos iguais. Se estes subconjuntos são em número de 100, são denominados de percentis; se são em número de 10, são os decis e em número de 4, são os quartis. A função nativa no R para obter o quantil é quantile().\nPara determinar os três quartis do peso dos recém-nascidos (dados$pexoRN), usa-se:\n\nquantile (dados$pesoRN, c (0.25, 0.50, 0.75))\n\n   25%    50%    75% \n3077.5 3500.0 3677.5 \n\n\nObserve que o percentil 50º é igual a mediana. O percentil 75º é o ponto do conjunto de dados onde 75% dos recém-nascidos têm um peso inferior a 3677.5g e 25% está acima deste valor.\n\n\n6.3.3.5 Média aparada\nAs médias aparadas são estimadores robustos da tendência central. Para calcular uma média aparada, é removida uma quantidade predeterminada de observações em cada lado de uma distribuição e realizada a média das observações restantes. Um exemplo de média aparada é a própria mediana.\nA base R tem como calcular a média aparada acrescentando o argumento trim =, proporção a ser aparada. Se for aparado 20%, usa-se trim = 0.2. isto significa que serão removidos 20% dos dados dos dois extremos. No caso da amostra de 15 recém-nascidos, serão removidos três valores mais baixos e três valores mais altos, passando a mostra a ter 9 valores, e a média aparada será a média destes 9 valores.\nO comando para obter a média aparada é:\n\nround(mean (dados$pesoRN, na.rm = TRUE, trim = 0.20), 1)\n\n[1] 3397.2\n\n\n\n\n\n6.3.4 Medidas de Dispersão\n\n6.3.4.1 Amplitude\nA amplitude de um grupo de medições é definida como a diferença entre a maior observação e a menor.\nNo conjunto de dados dos pesos dos recém-nascidos, a amplitude pode ser obtida, no R, com a função range(), que retorna o valor mínimo e o máximo.\n\nrange (dados$pesoRN, na.rm = TRUE)\n\n[1] 2285 3795\n\n\n\n\n6.3.4.2 Intervalo Interquartil\nA intervalo interquartil (IIQ), também conhecido como amplitude interquartil (AIQ) é uma forma de média aparada. É simplesmente a diferença entre o terceiro e o primeiro quartil, ou seja, a diferença entre o percentil 75 e o percentil 25. Considere a variável escolaridade (dados$anosEst), anos de estudos completos.\nOs percentis 25 e 75 são obtidos, usando a função quantile(), vista acima, ou com a função summary() , que retorna os valores mínimo, primeiro quartil, mediana, média, terceiro quartil e máximo.\n\nquantile (dados$anosEst, c(0.25,0.75))\n\n 25%  75% \n 6.5 11.0 \n\n\n\nsummary(dados$anosEst)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.000   6.500   9.000   8.467  11.000  11.000 \n\n\nPortanto, o IIQ está entre 6,5 a 11 anos de estudo ou, 11 – 6,5 = 4,5 anos de estudos completos. Em outras palavras, 50% das mulheres desta amostra têm de 6 a 8 anos de estudo.\nO R possui uma função específica para calcular o intervalo interquartil, denominada IQR() e incluída no R base. Ela possui os seguintes argumentos:\nx \\(\\to\\) Representa o vetor numérico;\nna.rm \\(\\to\\) Este assume um valor lógico, TRUE ou FALSE, indicando se os valores ausentes devem ser removidos ou não;\ntype \\(\\to\\) Representa um número inteiro selecionando um dos muitos algoritmos de quantil. Este é um parâmetro opcional.\n\nIQR(dados$anosEst, na.rm = TRUE)\n\n[1] 4.5\n\n\n\n\n6.3.4.3 Variância e Desvio Padrão\nA variância e o desvio padrão fornecem uma indicação de quão aglomerados em torno da média os dados de uma amostra estão. Estes tipos de medidas representam desvios (erros)da média. Quando se verifica o desvio de cada valor (x) em relação à média \\(\\overline{x}\\), os desvios positivos se anulam com os negativos, resultando em uma soma igual a zero.\nA consequência deste fato é que não é possível resumir os desvios numa única medida de variabilidade. Para se chegar a uma medida de variabilidade há necessidade de se eliminar os sinais, antes de somar todos os desvios em relação à média.\nUma maneira de se fazer isso é elevar todas as diferenças ao quadrado. Assim, se obtém o desvio em relação à média elevado ao quadrado. A soma destes valores é denominada de Soma dos Quadrados (SQ) dos Desvios ou Soma dos Erros ao Quadrado. Se o interesse é apenas saber o erro ou desvio médio, divide-se por n (tamanho da amostra). No entanto, em geral o interesse se concentra em usar o desvio ou erro na amostra para estimar o erro na população. Dessa maneira, divide-se a Soma dos Quadrados por \\(n-1\\). Essa medida é conhecida como variância (\\(s^2\\)). O divisor, \\(n – 1\\), é denominado de graus de liberdade (gl) associados à variância.\nOs graus de liberdade representam o número de desvios que estão livres para variar. É um conceito de difícil explicação, mas é possível compreendê-lo, usando a seguinte explicação:\n\n Suponha uma maternidade há 50 anos atrás, quando não havia alojamento conjunto. Nessa época era comum os recém-nascidos normais ficarem em um berçário. A cada horário de amamentação eles eram levados para os quartos de suas mães para mamar. Posteriormente, eram trazidos para o berçário e colocados nos berços até a próxima mamada. Suponha que, em um determinado momento, havia 15 bebês e que, no berçário, existiam 15 berços (postos) para colocá-los durante o intervalo das mamadas. Quando o primeiro recém-nascido chega, a enfermeira poderá escolher qualquer um dos berços para o colocar. Depois, quando o próximo recém-nascido chegar, ela terá 14 opções de escolha, pois um dos berços está ocupado. Ainda existe uma boa liberdade de escolha. No entanto, à medida que os recém-nascidos forem sendo trazidos para o berçário, chegará a um ponto em que 14 berços estarão ocupados. Agora, a enfermeira não terá liberdade de escolha, pois só resta um berço. Nesse exemplo existem 14 graus de liberdade. Para o último recém-nascido não houve liberdade de escolha (1). Portanto, os graus de liberdade são iguais ao tamanho da amostra menos um (\\(n-1\\)).\n\nA variância é a razão entre a soma dos quadrados e os graus de liberdade (observações realizadas menos um).\n\\[\ns^2= \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\n\\]\nNo R existem embutidas as funções sd() e var()que facilmente calculam essas medidas de dispersão.\nUsando a variável dados$pesoRN, tem-se:\n\nvar(dados$pesoRN, na.rm =TRUE)\n\n[1] 208310.2\n\n\nO desvio padrão é a raiz quadrada da variância: \\(s = \\sqrt var\\)\n\nsqrt (var(dados$pesoRN))\n\n[1] 456.4102\n\n\nOu, usando a função sd() e arredondando para 1 dígito decimal:\n\nround(sd (dados$pesoRN, na.rm = TRUE), 1)\n\n[1] 456.4\n\n\nA variância e desvio padrão são medidas de variabilidade e revelam quão bem a média representa os dados. Informa se ela está funcionando bem como modelo. Pequenos desvios padrão mostram que existe pouca variabilidade nos dados, que eles se aproximam da média. Quando existe um grande desvio padrão, a média não é muito precisa para representar os dados.\nO desvio padrão, além de medir a precisão com que a média representa os dados, também informa sobre o formato dos dados e por isso é uma medida de dispersão. Em uma amostra onde desvio padrão é pequeno, os dados se agrupam próximo a média e o formato da distribuição fica mais pontiagudo (curva em azul, Figura 6.2). Nesse caso a média representa bem os dados. Em outra amostra, com a mesma média anterior, mas com os dados mais dispersos entorno da média, o desvio padrão é maior e o formato da distribuição fica achatado (curva verde, na Figura Figura 6.2). Nesse caso a média não é uma boa representação dos dados.\n\n\n\n\n\n\n\n\nFigura 6.2: Dispersão dos dados em torno da média.\n\n\n\n\n\n\n\n6.3.4.4 Coeficiente de Variação\nO desvio padrão por si só tem limitações. Um desvio padrão igual a 2 pode ser considerado pequeno para um conjunto de valores cuja média é 100. Entretanto, se a média for 5, ele se torna muito grande. Além disso, o desvio padrão por ser expresso na mesma unidade dos dados, não permite aplicá-lo na comparação de dois ou mais conjunto de dados que têm unidades diferentes. Para eliminar essas limitações, é possível caracterizar a dispersão ou variabilidade dos dados em termos relativos, usando uma medida denominada Coeficiente de Variação (CV), também conhecido como como Desvio Padrão Relativo ou Coeficiente de Variação de Pearson. É expresso, em geral como uma porcentagem, sendo definido como a razão do desvio padrão pela média:\n\\[\nCV = \\frac{s}{\\overline{x}}\n\\]\nMultiplicando o valor da equação por 100 tem-se o CV percentual. O R não possui uma função específica para calcular o CV.\nFoi criada uma função específica para isso,já multiplicada por 100.\n\ncoef_var &lt;- function (valores) {\n  (sd(valores, na.rm=TRUE) / mean(valores, na.rm=TRUE))*100}\n\nPortanto, o CV da variável dados$pesoRN é igual a:\n\nround (coef_var (dados$pesoRN),1)\n\n[1] 13.8\n\n\nUsdando outra variável do banco de dados, por exemplo, dados$idadeMae, o CV será igual a:\n\nround(coef_var (dados$idadeMae), 1)\n\n[1] 21.4\n\n\nO peso do recem-nascido tem um CV = 13.8 % e a idade materna um CV = 21.4 %, mostrando que esta tem uma maior variabilidade. Quanto menor o desvio padrão, menor o CV e, consequentemente, menor a variabilidade. Um CV \\(\\ge\\) 50%, sugere que a variável tem uma distribuição assimétrica.\n\n\n\n6.3.5 Escolha da medida resumidora\nA seleção da medida de tendência central mais adequada depende de vários fatores, incluindo a natureza dos dados e do propósito da sumarização.\nO tipo da variável tem substancial influência na escolha da medida de tendência central a ser usada. A moda é mais apropriada para dados nominais e seu uso com variáveis ordinais resulta em uma perda no poder em termos de informação que se poderia obter dos dados.\nA mediana é mais adequada para variáveis ordinais, embora possa ser usada para variáveis contínuas, especialmente quando a distribuição dos dados é assimétrica. A mediana não deveria ser usada com dados nominais porque os postos assumidos não podem ser obtidos com dados de nível nominal.\nFinalmente, a média somente deve ser usada com dados contínuos simétricos, se houver assimetria a mediana deve ser preferida.\nAs medidas de dispersão devem estar associadas a uma medida de tendência central. Elas caracterizam a variabilidade dos dados na amostra. Com dados ordinais usar a amplitude ou o intervalo interquartil. O desvio padrão não é apropriado em dados ordinais devido à natureza não numérica destes.\nCom os dados numéricos deve-se usar o desvio padrão, que utiliza toda a informação nos dados, ou o intervalo interquartil (IIQ). Quando os dados forem simétricos, usar a média acompanhada do desvio padrão, caso contrário, usar a mediana e o IIQ. Não misturar e combinar medidas (2).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#sec-tabelas",
    "href": "06-descrevendoDados.html#sec-tabelas",
    "title": "6  Descrevendo os dados",
    "section": "6.4 Tabelas",
    "text": "6.4 Tabelas\nA apresentação tabular dos dados é a apresentação das informações por meio de tabelas. Uma tabela é uma forma eficiente de mostrar os dados levantados, facilitando a sua compreensão e interpretação. No R existem muitas maneira de criar tabelas.\n\n6.4.1 Dados usados nesta seção\nPara mostrar como construir uma tabela, será feita novamente a leitura do conjunto de dados dadosMater.xlsx. Como visto, este conjunto de dados contém uma grande quantidade colunas e, para tornar mais fácil a análise, serão selecionadas aquelas utilizadas nesta seção.\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n   select(idadeMae, altura, peso, anosEst, fumo, \n           para, ig, sexo, pesoRN, compRN, utiNeo)\n\nstr(mater)\n\ntibble [1,368 × 11] (S3: tbl_df/tbl/data.frame)\n $ idadeMae: num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura  : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso    : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo    : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para    : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig      : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo    : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN  : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN  : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo  : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n\n\n\n\n6.4.2 Tabelas de Frequência\n\n6.4.2.1 Tabela de frequência para dados categóricos\nUma maneira concisa que permite observar a variável e extrair informação sobre o seu comportamento, é a utilização de uma tabela de frequência. A tabela de frequência deve ser simples, clara e objetiva, ou seja, não deve ter um volume muito grande de informações. Deve ser autoexplicativa, não deve haver necessidade de ler o texto para entendê-la.\nA tabela de frequência agrupa os dados por categorias ou classes, contabilizando o número ocorrências em cada categoria. O número de observações em uma determinada classe recebe o nome de frequência absoluta (f). Além da frequência absoluta, costuma aparecer a frequência relativa (fr) que representa a proporção da classe em relação ao número total de observações (n), calculada por \\(fr = \\frac{f}{n}\\), a frequência percentual (fp), obtida pela multiplicação da frequência relativa por 100 e a frequência acumulada, que é a soma de todas as classes até a classe atual, podendo ser frequência acumulada absoluta (F), frequência acumulada relativa (Fr) ou frequência acumulada percentual (Fp).\nEm uma tabela, os dados são apresentados em colunas verticais indicadoras e linhas horizontais. Nas linhas aparecem as categorias e nas colunas as frequências, constituindo o corpo da tabela. O cabeçalho indica a natureza do conteúdo de cada coluna. No cruzamento das colunas e das linhas, tem-se as caselas ou casas.\nExistem algumas recomendações na construção de uma tabela de frequência (3):\n\nDeve ter um título na parte superior que responda as perguntas: “o que? quando? onde?” relativas ao fato estudado;\n\nDeve ter um rodapé, na parte inferior da tabela, onde se coloca notas necessárias e a fonte dos dados;\n\nAs colunas externas da tabela devem ser abertas, o emprego de linhas verticais para a separação das colunas no corpo da tabela é opcional;\n\nNa parte superior e inferior, as tabelas devem, ser fechadas por linhas horizontais;\n\nNenhuma casela deve ficar vazia, apresentando um número ou um símbolo. Se não se dispuser do dado, colocar reticências … e a presença de um X representa que o dado foi omitido para evitar a identificação.\n\nSe os dados forem nominais, a ordenação das categorias é arbitrária, costuma-se colocar em primeiro lugar a maior frequência (Tabela 6.1) , colocando-os em categorias ordenadas (4).\n\n\n\n\nTabela 6.1: Distribuição de frequência de drogadição em parturientes\n\n\n\nDrogaffrfpFpNenhuma9040.95595.595.5Medicamentos230.0242.497.9Álcool170.0181.899.7Crack20.0020.299.9Cocaína10.0010.1100Total9471.000100.0FONTE:Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\n6.4.2.1.1 Construção da tabela de frequência\nPara demonstrar como construir uma tabela de frequência, será usada uma variável categórica que não existe no conjunto de dados mater. Esta variável vai ser criada, categorizando a variável numérica idadeMae (idade da parturiente) em três categoria, classicamente, usadas: menores de 20 anos (adolescentes), 20 a 35 anos e maiores de 35 anos. No conjunto mater, a variável idadeMae tem como idade mínima 13 anos e idade máxima 46 anos. A nova variável receberá o nome de categIdade. Para realizar este trabalho de transformação da variável numérica em categórica, será usada a função mutate() do pacote dplyr (veja Seção 5.3.3). Esta função tem vários argumentos:\n\nmater &lt;- mater %&gt;%\n  mutate(categIdade = case_when(\n    idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n    idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n    idadeMae &gt; 35 ~ \"&gt; 35 anos\")) %&gt;%\n  mutate(categIdade = factor(categIdade, levels = c(\"&lt; 20 anos\", \"20 a 35 anos\", \"&gt; 35 anos\")))\n\nPara visualizar se a variável foi criada de maneira adequada, pode-se usar novamente a função str():\n\nstr(mater)\n\ntibble [1,368 × 12] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura    : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso      : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst   : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo      : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para      : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig        : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo      : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN    : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN    : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo    : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 3 2 1 2 2 2 2 2 1 2 ...\n\n\nPara se verificar como ficou a distribuição de frequência absoluta, constrói-se uma tabela, inicialmente com a função table():\n\nf_abs &lt;- table (mater$categIdade)\nf_abs\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n         219          992          157 \n\n\nAs frequências relativas podem ser obtidas com a função prop.table(), Esta função será usada dentro da função round() para arredondar os valores para 3 dígitos.\n\nf_rel &lt;- round(prop.table(f_abs), 3)\nf_rel\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n       0.160        0.725        0.115 \n\n\nMultiplicando por 100 a f_rel, tem-se a frequência percentual f_perc. De novo, a operação será colocada dentro da função round(), arredondando o resultado para dois dígitos.\n\nf_perc &lt;- round(f_rel*100, 2)\nf_perc\n\n\n   &lt; 20 anos 20 a 35 anos    &gt; 35 anos \n        16.0         72.5         11.5 \n\n\nPara construir uma tabela simples no R, pode-se proceder da seguinte maneira:\n\n# Criando as colunas das tabelas com o total de cada uma delas\nf_abs &lt;- c (f_abs, sum(f_abs))\nf_rel &lt;- c (f_rel, sum (f_rel))\nf_perc &lt;- c (f_perc, sum (f_perc))\n\n# Criando a tabela inicial com a concatenação das coluna, usando a função cbind()\ntab1 &lt;- cbind(f_abs,\n              f_rel ,\n              f_perc)\ntab1\n\n             f_abs f_rel f_perc\n&lt; 20 anos      219 0.160   16.0\n20 a 35 anos   992 0.725   72.5\n&gt; 35 anos      157 0.115   11.5\n              1368 1.000  100.0\n\n\nTransformando a tab1 em um dataframe, nomeando a linha 4 e renomeando as colunas para que tenham os nomes mencionados no início da seção:\n\ntab1 &lt;- as.data.frame(tab1)\nrow.names(tab1)[4] &lt;-  \"Total\"\ncolnames(tab1) &lt;- c(\"f\", \"fr\", \"fp (%)\")\ntab1\n\n                f    fr fp (%)\n&lt; 20 anos     219 0.160   16.0\n20 a 35 anos  992 0.725   72.5\n&gt; 35 anos     157 0.115   11.5\nTotal        1368 1.000  100.0\n\n\nEsta uma tabela simples que serve para visualizar as informações. Não serve para publicações. Para obter uma tabela simples, mas muito mais profissional (Tabela 6.2) pode ser utilizada a função flextable() do pacote flextable (5). Esta função possui um grande número de argumentos para personalizar a aparência das tabelas2:\n\ndf &lt;- data.frame(faixaEt = c(\"&lt; 20 anos\", \"20 a 35 anos\", \"&gt; 35 anos\", \"Total\"),\n                 f = c(219, 992, 157, 1368),\n                 fr = c(0.160, 0.725, 0.115, 1.000),\n                 fp = c(16.0, 72.5, 11.5, 100),\n                 Fp = c(16.0, 88.5, 100, ''))\n\nminha_tab &lt;- flextable(df) %&gt;%\n  set_header_labels(\n    faixaEt = \"Faixa Etária\",\n    f = \"f\",\n    fr = \"fr\",\n    fp = \"fp (%)\",\n    Fp = \"Fp (%)\") %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  width(j = \"faixaEt\", width = 1.5) %&gt;%\n  align(align = \"center\", part = \"header\") %&gt;%\n  hline(i = 3, part = \"body\", \n        border = fp_border_default(color = \"black\", width = 1.5)) %&gt;% \n  align(align = \"center\", part = \"body\") %&gt;%\n  align(j = 1, align = \"left\", part = \"body\") %&gt;%\n  bold(i = 4, part = \"body\") %&gt;%\n  bold(part = \"header\") %&gt;%\n  add_footer_lines(value = \"FONTE:Hospital Geral, Caxias do Sul, RS, 2008\") %&gt;%\n  align(align = \"right\", part = \"footer\") %&gt;% \n  fontsize(size = 9, part = \"footer\")\nminha_tab\n\n\n\nTabela 6.2: Distribuição das parturientes por faixa etária\n\n\n\nFaixa Etáriaffrfp (%)Fp (%)&lt; 20 anos2190.16016.01620 a 35 anos9920.72572.588.5&gt; 35 anos1570.11511.5100Total1,3681.000100.0FONTE:Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\n\n\n6.4.2.2 Tabela de frequência para dados numéricos\nComo fazer a distribuição de frequência de uma variável contínua sem um critério pré-determinado para as classes?\nComo exemplo, será usado, agora, o IMC pré-gestacional das parturientes do banco de dados dadosMater.xlsx). Esta variável ainda não existe no banco de dados, tem-se apenas o peso e a altura e, portanto, com estes dados, ela pode ser criada:\n\nmater$imc &lt;- round(mater$peso/mater$altura^2, 1)\nstr(mater)\n\ntibble [1,368 × 13] (S3: tbl_df/tbl/data.frame)\n $ idadeMae  : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ...\n $ altura    : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ...\n $ peso      : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ...\n $ anosEst   : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ...\n $ fumo      : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ...\n $ para      : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ...\n $ ig        : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ...\n $ sexo      : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ...\n $ pesoRN    : num [1:1368] 1035 2300 1580 1840 2475 ...\n $ compRN    : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ...\n $ utiNeo    : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ...\n $ categIdade: Factor w/ 3 levels \"&lt; 20 anos\",\"20 a 35 anos\",..: 3 2 1 2 2 2 2 2 1 2 ...\n $ imc       : num [1:1368] 25.7 28.3 27.4 30.8 23.4 26.7 31.8 23 20.2 31.1 ...\n\n\nA variável imc foi criada de forma adequada. Após, isso, para verificar a sua distribuição, segue-se os seguintes passos:\n\nEstabelecimento do número de classes (k):\n\nAntes, as classes foram estabelecidas de acordo com algum critério. Em geral, quando não há um padrão pré-determinado, o número de classes é estabelecido de acordo com o tamanho da amostra. Este número pode ser escolhido lembrando-se das oscilações que ocorrem nos dados e do interesse do pesquisador em mostrar seus dados. Não existe uma regra totalmente eficiente para determinar o número de classes. É importante ter bom senso, de maneira que seja possível ver como os valores se distribuem.\nPara a maioria dos dados, é recomendado e 8 a 20 classes, isto é, 8 \\(\\le\\) k \\(\\le\\) 20. Com poucas classes, perde-se precisão e, com muitas classes, a tabela torna-se muito extensa. Baseado na regra de Sturges , é sugerido usar a recomendação da Figura 6.3 (6).\n\n\n\n\n\n\n\n\nFigura 6.3: Número de classes baseado em Sturges\n\n\n\n\n\nPara a variável imc, como existem 1368 observações, deve-se usar ao redor de 10 classes. Executando a função nclass.Sturges (), abaixo, o número de classes é igual a:\n\nk &lt;- nclass.Sturges (mater$imc)\nk\n\n[1] 12\n\n\n\nAmplitude e limites das classes:\n\nA classe possui um limite inferior e um limite superior. O importante é que os limites dos intervalos sejam mutuamente exclusivos, isto é, cada valor deve ser representado em um único intervalo. Além disso, os intervalos devem ser exaustivos, isto é, devem conter todos os valores possíveis entre o valor mínimo e o máximo. O recomendado é que as classes sejam homogêneas, ou seja, tenham a mesma amplitude. A amplitude dos valores pode ser obtida com a função range():\n\namplitude &lt;- range(mater$imc) \namplitude \n\n[1] 11.8 48.7\n\n\nUsando esta amplitude dos dados, é possível ter a largura (amplitude) das classes (h), usando a diferença entre o mínimo e máximo e divdindo pelo número de classes (k):\n\nh &lt;- round(diff(amplitude)/k, 0)\nh\n\n[1] 3\n\n\nA fórmula é apenas a diferença absoluta dos limites inferior e superior dividida pelo número de classes, arredondado com o a função round ().\nA partir desses dados, é possível construir as classes. A primeira classe será o valor mínimo de 11,8 e máximo 14,8 (11,8 + 3) exclusive; a segunda classe será 14,8 até 17,8 (14,8 + 3) e assim por diante.\n\nConstrução da tabela:\n\nPode-se construir a tabela, usando a função mutate() e dentro desta a função cut()e dentro dela a função seq(limite inferior, limite superior, length.out = k + 1). A função cut() tem vários argumentos:\n\nx \\(\\to\\) vetor numérico\nbreaks \\(\\to\\) vetor numérico de dois ou mais pontos de corte exclusivos ou um único número (maior ou igual a 2) dando o número de intervalos nos quais x deve ser subdividido\nlabels \\(\\to\\) rótulos para os níveis das categorias resultante. Por padrão, os rótulos são construídos usando a notação de intervalo (a,b] - aberto à esquerda e fechado à direita.\ninclude.lowest \\(\\to\\) valor lógico, se o menor valor (ou o maior, se right = TRUE) será incluido. Padrão = include.lowest=TRUE.\nright \\(\\to\\) valor lógico indicando se o intervalo deve ser fechado à direita e aberto a esquerda. Padrão = right = TRUE.\nordered_result \\(\\to\\) valor lógico indicando se o resultado deve ser um fator ordenado.\n\n\nmater &lt;- mater %&gt;%\n  mutate(categImc = cut(\n    imc,\n    breaks = seq(11.8, 48.7, length.out = 13), # 12 intervalos definidos\n    include.lowest = TRUE,\n    ordered_result = TRUE))\n\ntable(mater$categImc)\n\n\n[11.8,14.9]   (14.9,18]     (18,21]   (21,24.1] (24.1,27.2] (27.2,30.3] \n          2          46         258         480         237         176 \n(30.3,33.3] (33.3,36.4] (36.4,39.5] (39.5,42.6] (42.6,45.6] (45.6,48.7] \n         87          39          22          12           5           4 \n\n\nPreste atenção! Estes comandos que vão gerar a tabela têm o argumento right = TRUE (padrão). Neste caso, os símbolos aparecem como (] (na tabela) e significa que o limite inferior da classe foi excluído (aberto à esquerda) e o superior foi incluído (fechado à direita). Aqui, também foi introduzido o argumento include.lowest  = TRUE para incluir o valor mínimo dos dados (11,8), e a representação gráfica fica [].\nOlhando a saída do objeto categImc, ela parece pouco esclarecedora e, no caso do IMC, talvez fosse melhor usar outro critério. Como por exemplo o que define o estado nutricional no 1° trimestre de gestação e classifica as gestantes em:\n\nbaixo peso (IMC \\(&lt;\\) 18,5 kg/\\(m^2\\)),\n\npeso adequado (18,5 \\(\\le\\) IMC \\(\\le\\) 24,9 kg/\\(m^2\\)),\n\nsobrepeso (25,0 \\(\\le\\) IMC \\(\\le\\) 29,9 kg/\\(m^2\\)) e\n\nobesidade (IMC \\(\\ge\\) 30 kg/\\(m^2\\)).\n\nAssim, é recomendado um ganho de peso total adequado de 12,5 kg a 18 kg para as gestantes classificadas como baixo peso; de 11,5 kg a 16,0 kg para as classificadas como peso adequado; de 7,0 a 11,5 kg nas classificadas com sobrepeso; e de 5,0 a 9,0 kg nas obesas (7).\n\nmater &lt;- mater %&gt;% \n  mutate (estNutri = case_when(\n    imc &lt; 18.5 ~ \"Baixo Peso\",\n    imc &gt;= 18.5 & imc &lt; 25 ~ \"Peso adequado\", \n    imc &gt;= 25 & imc &lt; 30 ~ \"Sobrepeso\",\n    imc &gt;= 30 ~ \"Obesidade\")) %&gt;%\n  mutate(estNutri = factor(estNutri, \n                           levels = c(\"Baixo Peso\", \"Peso adequado\", \n                                      \"Sobrepeso\", \"Obesidade\")))\n\nIsto cria uma nova variável estNutri (estado nutricional), no conjunto de dados mater, com 4 níveis (Baixo Peso, Peso Adequado, Sobrepeso e Obesidade). Desta forma, pode-se construir uma tabela que melhor define este grupo de mulheres quanto ao estado nutricional.\n\nf.abs &lt;- table (mater$estNutri)\nf.rel &lt;- round(prop.table(f.abs), 3)\nf.perc &lt;- round(f.rel*100, 2)\n\nf.abs &lt;- c (f.abs, sum(f.abs))\nf.rel &lt;- c (f.rel, sum (f.rel))\nf.perc &lt;- c (f.perc, sum (f.perc))\n\ntab2 &lt;- cbind(f.abs,\n              f.rel ,\n              f.perc)\n\ntab2 &lt;- as.data.frame(tab2)\nrow.names(tab2)[5] &lt;-  \"Total\"\ncolnames(tab2) &lt;- c(\"f\", \"fr\", \"fp\")\ntab2\n\n                 f    fr    fp\nBaixo Peso      67 0.049   4.9\nPeso adequado  791 0.578  57.8\nSobrepeso      335 0.245  24.5\nObesidade      175 0.128  12.8\nTotal         1368 1.000 100.0\n\n\nColocando em um formato mais científico, tem-se uma tabela ( Tabela 6.3)) bem mais elegante sobre o estado nutricional pré-gestacional:\n\ndf &lt;- data.frame(estNutri = c(\"Baixo Peso\", \"Peso adequado\", \"Sobrepeso\", \"Obesidade\",\"Total\"),\n                 f = c(67, 799, 327, 175, 1368),\n                 fr = c(0.049, 0.584, 0.239, 0.128, 1.000),\n                 fp = c(4.9, 58.4, 23.9, 12.8, 100),\n                 Fp = c(4.9, 63.3, 87.2, 100, ''))\n\nminha_tab &lt;- flextable(df) %&gt;%\n  autofit() %&gt;%\n  set_header_labels(\n    estNutri = \"Estado Nutricional\",\n    f = \"f\",\n    fr = \"fr\",\n    fp = \"fp (%)\",\n    Fp = \"Fp (%)\") %&gt;%\n  theme_booktabs() %&gt;%\n  width(j = \"estNutri\", width = 1.5) %&gt;%\n  width(j = 2:5, width = 1) %&gt;%\n  align(align = \"center\", part = \"header\") %&gt;%\n  align(align = \"center\", part = \"body\") %&gt;%\n  align(j = 1, align = \"left\", part = \"body\") %&gt;%\n  bold(i = 5, part = \"body\") %&gt;%\n  bold(part = \"header\") %&gt;%\n  hline(i = 4, part = \"body\", \n        border = fp_border_default(color = \"black\", width = 1.5)) %&gt;% \n  add_footer_lines(value = \"FONTE:Hospital Geral, Caxias do Sul, RS, 2008\") %&gt;%\n  align(align = \"right\", part = \"footer\") %&gt;% \n  fontsize(size = 9, part = \"footer\")\n\nminha_tab\n\n\n\nTabela 6.3: Estado nutricional pré-gestacional das parturientes\n\n\n\nEstado Nutricionalffrfp (%)Fp (%)Baixo Peso670.0494.94.9Peso adequado7990.58458.463.3Sobrepeso3270.23923.987.2Obesidade1750.12812.8100Total1,3681.000100.0FONTE:Hospital Geral, Caxias do Sul, RS, 2008\n\n\n\n\n\n\n\n\n6.4.3 Tabelas de contingência\nAs tabelas de contingência, também chamadas tabelas cruzadas, são bastante usadas em estatísticas epidemiológicas para resumir a relação entre duas ou mais variáveis categóricas.\nUma tabela de contingência é um tipo especial de tabela de distribuição de frequência, onde duas variáveis são mostradas simultaneamente. Por exemplo, um pesquisador pode estar interessado em saber se o hábito de fumar na gestação aumenta o risco de o recém-nascido precisar de cuidados intensivos.\nExistem duas variáveis fumo (fumo na gestação) e utiNeo (necessidade de cuidados intensivos neonatais) no banco de dados dadosMater.xlsx. Cada uma dessas variáveis tem duas alternativas, sim e não, por isso a tabela de cruzamento é denominada tabela de contingência 2 x 2. No arquivo, estão registradas como variáveis numéricas , 1 e 2, e devem ser transformadas para fatores (1 = sim e 2 = não)3, usando a função factor().\n\nmater$fumo &lt;- factor (mater$fumo, \n                      ordered = TRUE, \n                      levels = c (1,2), \n                      labels = c (\"sim\", \"não\"))\nmater$utiNeo &lt;- factor (mater$utiNeo, \n                        ordered = TRUE, \n                        levels = c (1,2), \n                        labels = c (\"sim\", \"não\"))\n\nBasta agora, usar a função with() junto com a função table(variável da linha, variável das colunas). Por convenção, costuma-se colocar a variável explicativa ou explanatória nas linhas (fumo) e o desfecho nas colunas (utiNeo):\n\ntabFumo &lt;- with(data = mater, table(fumo, utiNeo))\ntabFumo\n\n     utiNeo\nfumo  sim não\n  sim  71 230\n  não 204 863\n\n\nPara ter a soma das margens, usar a função addmargins (tabela, margin = c (1,2), FUN = sum) do pacote stats, incluído na instalação básica do R. A função adiciona a soma das linhas (1) e das colunas (2) às margens da tabela (tabFumo).\n\naddmargins (tabFumo, margin = c(1,2), FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: fumo\n2: utiNeo\n\n\n     utiNeo\nfumo   sim  não  sum\n  sim   71  230  301\n  não  204  863 1067\n  sum  275 1093 1368\n\n\nObservando a tabela de contingência, verifica-se que a proporção entre as gestantes fumantes de internação na UTI neonatal foi 71/301 = 0,236 e entre as não fumantes foi de 204/1067 = 0,191. Para verificar se esta diferença ocorreu por acaso ou ela é significativa, há necessidade de se realizar um teste de hipótese, o qui-quadrado, que será visto na Seção 16.2.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#gráficos",
    "href": "06-descrevendoDados.html#gráficos",
    "title": "6  Descrevendo os dados",
    "section": "6.5 Gráficos",
    "text": "6.5 Gráficos\nPara descrever os dados e visualizar o que está acontecendo, recomenda-se utilizar um gráfico adequado. O que é adequado depende principalmente do tipo de dados, bem como das características particulares do que se quer explorar. Além disso, um gráfico em um relatório sempre é um fator de “impacto”. Ou seja, pode ter um efeito positivo no leitor ou fazê-lo abandonar a leitura. Finalmente, um gráfico de frequência pode ser utilizado para ilustrar, explicar uma situação complexa onde palavras ou uma tabela podem ser confusos, extensos ou de outro modo insuficiente. Por outro lado, deve-se evitar usar gráficos onde poucas palavras expressam claramente o que se quer mostrar. Aconselha-se que, ao analisar os dados, é importante inspecioná-los como se fossem uma imagem, uma fotografia, ver como eles se parecem, qual o seu aspecto, e só então pensar em interpretar os aspectos vitais da estatística (8).\nO R básico fornece uma grande variedade de funções para visualizar dados, elas de uma maneira relativamente simples permitem a construção de gráficos que facilitam a interpretação tanto de variáveis categórica como contínuas. Para gráficos mais sofisticados existe um pacote denominado ggplot2 (9). Este pacote é uma ferramenta extremamente versátil. É um pouco mais complexo e exige mais tempo para dominá-lo, mas, uma vez que se aprenda a sua lógica, oferece uma estrutura extremamente flexível para exibir os dados . Inicialmente, serão usadas as funções do R básico e,posteriormente, será feita uma introdução ao ggplot2 (Seção 6.6).\n\n6.5.1 Gráfico de setores\nTambém conhecido como gráfico de pizza. Cada segmento (fatia) do gráfico de pizza deve ser proporcional à frequência da categoria que representa. A desvantagem do gráfico de pizza é que ele só pode representar uma variável, portanto, há necessidade de um gráfico separado para cada variável que se deseja representar. Além disso, um gráfico de pizza pode perder clareza se ele é usado para representar mais do que quatro ou cinco categorias. Na maioria das vezes, em um artigo ou relatório científico não há necessidade de se usar este tipo de gráfico. As tabelas são muito melhores. Segundo Edward Tufte, professor emérito de estatística, design gráfico e economia política na Universidade de Yale, o único gráfico pior do que um gráfico de pizza são vários deles (10)! Ele é usado mais no mundo dos negócios. Como regra, evite usar gráfico de pizza!\nEm uma consulta, entre estudantes de Medicina, foi perguntado a sua opinião em relação a este tipo de gráfico. A pergunta feita foi: “O que você sente ao ver um gráfico de pizza em um artigo científico?” As alternativas para a resposta eram quatro (ódio, irritação, indiferença, amor). O resultado do inquérito está na Tabela 6.4.\n\n\n\n\nTabela 6.4: Sentimento dos alunos do curso de Medicina em relação ao gráfico de pizza\n\n\n\nSentimentoffrfp (%)Fp (%)Odeiam60.151515Não gostam120.303045Indiferentes140.353580Amam80.2020100Total401.00100FONTE: Universidade de Caxias do Sul, 2012\n\n\n\n\n\nNo R base, pacote graphics, existe a função pie()para obter um gráfico de setores simples. Esta função usa os seguintes argumentos basicos, consulte a ajuda do R para outras informações:\n\nx \\(\\to\\) vetor numérico não negativo\n\nlabels \\(\\to\\) caracteres que fornecem nomes para as fatias. Para rótulos vazios ou NA (após coerção para caractere), nenhum rótulo ou linha indicadora é desenhada\n\nradius \\(\\to\\) A pizza é desenhada centralizada em um quadrado cujos lados variam de -1 a +1. Se os caracteres que rotulam as fatias forem longos, pode ser necessário usar um raio menor. O padrão é 0,8.\n\ndensity \\(\\to\\) Densidade das linhas de sombreamento, em linhas por polegada. O padrão é NULL significa que nenhuma linha de sombreamento é desenhada. Valores não positivos de densidade também inibem o desenho de linhas sombreadas\ncol \\(\\to\\) Vetor de cores a ser usado no preenchimento ou sombreamento das fatias. Se estiver faltando, um conjunto de 6 cores pastel é usado\n\nOs valores da coluna de frequência absoluta (f) da Tabela 6.4 serão usados como o argumento x. Ele informa a área (proporção de cada fatia). Os rótulos das fatias são escritos com a função concatenar c().\n\npie(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"))\n\n\n\n\n\n\n\nFigura 6.4: Gráfico de Pizza: Opinião dos estudantes de Medicina.\n\n\n\n\n\nAs cores que aparecem na Figura 6.4 foram escolhidas pelo R, usando o seu padrão. Entretanto, elas podem ser customizadas, especificando-as pelo nome colocado entre parênteses. Por exemplo, col = \"red\" e se for mais de uma cor usar a função concatenar, col = c(“gray58“, “yellow4”, “cyan”, “tomato”). As cores também podem se denotadas pelo sistemas RGB ou hexadecimal. A sigla RGB representa as cores primárias em inglês (Red, Green, Blue). O código hexadecimal da cor branca é #FFFFFFF, da gray58 é #949494, da yellow4 é #999900, da cyan é #00FFFFe da tomato é #FF6347 (Figura 6.5).\n\npie(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"),\n    col = c(\"gray58\", \"yellow4\", \"cyan\", \"tomato\"))\n\n\n\n\n\n\n\nFigura 6.5: Figura anterior com cores personalizadas.\n\n\n\n\n\nAs cores parecem “espetaculosas”, mas o objetivo foi de criticar os gráficos tipo pizza. Se o leitor quiser insistir no seu uso e com um gráfico em 3D (Figura 6.6), pode-se instalar o pacote plotrix (11) e carregar a função pie3D(). Os argumentos são praticamente os mesmos do gráfico simples. Acrescenta-se radius = 0.9 que muda o raio da pizza e explode = 0.1 que determina o afastamento das fatias (0, as mantém juntas). Além disso, como o gráfico exibe rótulos com textos muito grandes, usa-se o argumento labelcex = 1. Como qualquer função nova, basta clicar na tecla Tab, dentro da mesma, que aparece um menu com as alternativas de argumentos.\n\nlibrary (plotrix)\n\npie3D(x = c(6, 12, 14, 8),\n    labels = c(\"Odeiam\", \"Não gostam\", \"Indiferentes\", \"Amam\"),\n    radius = 0.9,\n    explode = 0.1,\n    col = c(\"gray58\", \"yellow4\", \"cyan\", \"tomato\"),\n    labelcex = 1)\n\n\n\n\n\n\n\nFigura 6.6: Gráfico de Pizza: Opinião dos estudantes de Medicina.\n\n\n\n\n\n\n\n6.5.2 Gráfico de barras\nOs gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas (12).\nAssim como o gráfico de setores, o gráfico de barras é utilizado para representar a frequência absoluta ou percentual de diferentes categorias. As barras são proporcionais as frequências. A forma mais simples de solicitar um gráfico de barra no R é digitar a função barplot() do pacote básico. Esta função é específica para desenhar gráficos de barras horizontais e verticais e usa os seguintes argumentos:\n\nheight \\(\\to\\) um vetor ou matriz de valores que descreve as barras que constituem o gráfico;\n\nwidth \\(\\to\\) especifica largura das barras, com padrão de 1, opcional;\n\nspace \\(\\to\\) a quantidade de espaço (como uma fração da largura média da barra) restante antes de cada barra. Pode ser fornecido como um único número ou um número por barra;\n\nbeside \\(\\to\\) argumento lógico para especificar se colunas devem ser mostradas lado a lado;\n\ncol \\(\\to\\) cores das barras componentes das barras, por padrão é usado grey (cinza);\n\nborder \\(\\to\\) cor das bordas das barras;\n\n… \\(\\to\\) outros argumentos. Consulte a ajuda do R.\n\nPara a construção do gráfico de barras simples da Figura 6.7, será utilizada a variável categIdade, anteriormente criada (Seção 6.4.2.1.1), a partir do conjunto de dados dadosMater.xlsx.\n\nbarplot(table(mater$categIdade))\n\n\n\n\n\n\n\nFigura 6.7: Gráfico de barra simples.\n\n\n\n\n\nObservando a Figura 6.7, verifica-se que não existem rótulos nos eixos x e y e o eixo y tem um tamanho inferior a barra mais alta. Estes e outros problemas podem ser resolvidos modificando-se ou acrescentando outros argumentos na função barplot(). Existem vários argumentos e para conhece-los melhor pesquise no Help do RStudio. Em um gráfico de barra simples são suficientes as seguintes modificações que irão resultar na Figura 6.8:\n\nPara corrigir a amplitude do eixo y, existe o argumento ylim = c(lim inf, lim sup). Na Tabela 6.2, observa-se que a frequência máxima é de 992, assim estende-se até 1000, bem próximo da frequência da categoria, acrescentando ylim = c (0,1000), separado por vírgulas de outros argumentos.\nPara os rótulos se utiliza os argumentos ylab = (“Frequência”) e xlab = (“Faixa Etária”). Também, pode ser incluído um título no gráfico com o argumento main = “Título”. Observe que os títulos estão entre aspas.\nPara modificar o tamanho das letras dos eixos x e y, que estão pouco visíveis, existe o argumento cex.lab = 1, que é o padrão. Para aumentar em 30%, por exemplo, usar cex.lab = 1.3. Os nomes tem padrão cex.names = 1, para modificar pode-se usar 1.3, 1.5, etc. Se nada for modificado, o R imprime o padrão.\nPara a cor das barras, use o argumento col = (“cor”). Escolha a cor entre as 657 opções, ou deixe o padrão cinza (grey). O argumento col.axis = “cor” controla a cor dos valores dos eixos (veja também Seção 6.6.3).\nPara modificar a borda das barras que por padrão é preta, usar o argumento border = “cor”. Se não desejar a borda, basta colocar 0 (zero), no lugar da cor.\nPara colocar as barras na posição horizontal, pode ser utilizado o argumento horiz = TRUE. Lembrar de inverter as barras. Ou seja, a variável x passa a ser y e vice-versa.\nO argumento las = 1 faz o o texto do eixo y ficar horizontal\nA função box(bty = \"L\"), colocada após é opcional e faz os eixos se encontraren em 0.\n\n\nbarplot(table(mater$categIdade), \n        ylim = c (0,1000), \n        col= \"tomato\", \n        border = \"black\", \n        ylab= \"Frequência absoluta\", \n        xlab = \"Faixa etária\", \n        cex.lab = 1.2,\n        las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.8: Gráfico de barra simples modificado.\n\n\n\n\n\nPara que as barras fiquem horizontais como na Figura 6.9, usa-se o argumento horiz=TRUE:\n\nbarplot(table(mater$categIdade), \n        xlim = c (0,1000), \n        col= \"tomato\", \n        border = \"black\", \n        ylab= \"Faixa Etária\", \n        xlab = \"Frequência absoluta\", \n        cex.lab = 1.2, \n        horiz=TRUE)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.9: Gráfico com barras horizontais.\n\n\n\n\n\nAlém das modificações realizadas, pode-se fazer outras para tornar o gráfico mais informativo . Por exemplo, colocar as frequência de cada barra no topo das mesmas (Figura 6.10):\n\n1º Passo: Criar um gráfico de barras , colocando-o em um objeto qualquer, por exemplo, x 4, que conterá o eixo X do centro de cada uma das barras. Para verificar isso, basta executar o objeto x;\n2º Passo: colocar a tabela table(mater$idadeCateg) como um objeto y 5, onde estarão as frequências absolutas);\n3º Passo: usar a função text() para colocar os valores.Consulte o Help para maiores detalhes desta função.\n\n\ny &lt;- table(mater$categIdade)\n\nx &lt;- barplot(y, \n             ylim = c (0,1000), \n             col= \"springgreen\", \n             border = \"black\", \n             ylab = \"Frequência absoluta\", \n             xlab = \"Faixa etária\", \n             cex.lab = 1.2,\n             las = 1)\nbox(bty = \"L\")\n\ntext (x, y, labels = as.character(y), adj = c(0.5, 2), col = \"black\")\n\n\n\n\n\n\n\nFigura 6.10: Gráfico de barra simples com frequências no topo.\n\n\n\n\n\n\n6.5.2.1 Gráfico de barras empilhadas\nPara este tipo de apresentação são utilizados, praticamente, os mesmos argumentos vistos para gerar um gráfico de barra simples. Como existem duas variáveis, há necessidade de avisar ao R como elas devem aparecer. Para isso, entra o argumento beside = FALSE, que informa que as barras não estarão uma ao lado da outra e sim empilhadas (Figura 6.11). O padrão é as barras ficarem uma ao lado da outra.\nAcrescenta-se uma legenda com a função legend() na parte superior esquerda (topleft). O argumento bty = \"n\" informa que será removido o quadro ao redor da legenda e fill = c(\"dimgrey\", \"salmon\") são as cores das barras.\nAs duas variáveis a serem visualizadas são o hábito tabagista entre as puérperas de acordo com a idade. No conjunto de dados dadosMater.xlsx, o hábito tabagista está registrado na variável fumo, vista quando se estudou tabelas de contingência. Aqui se construirá uma tabela 3 x 2, tabFumo2:\n\ntabFumo2 &lt;- table(mater$fumo, mater$categIdade)\n\nbarplot(tabFumo2,\n        beside = FALSE,\n        ylim = c(0, 1000),\n        xlab=\"Faixa Etária\", \n        ylab = \"Frequência\", \n        col = c (\"dimgrey\", \"cadetblue1\"),  \n        cex.lab = 1, \n        cex.axis = 1, \n        cex.names = 1,\n        las = 1)\nbox(bty = \"L\")\nlegend (\"topleft\",\n        legend = c(\"Fumantes\", \"Não Fumantes\"), \n        fill = c(\"dimgrey\", \"cadetblue1\"), \n        bty=\"n\", \n        cex = 1)\n\n\n\n\n\n\n\nFigura 6.11: Gráfico de barras empilhadas.\n\n\n\n\n\n\n\n6.5.2.2 Gráfico de barras lado a lado\nÉ igual a anterior, apenas com o argumento beside = TRUE (Figura 6.12).\n\nbarplot(tabFumo2,\n        beside = TRUE,\n        ylim = c(0, 1000),\n        xlab=\"Faixa Etária\", \n        ylab = \"Frequência\", \n        col = c (\"dimgrey\", \"cadetblue1\"),  \n        cex.lab = 1, \n        cex.axis = 1, \n        cex.names = 1,\n        las = 1)\nbox(bty = \"L\")\nlegend (\"topleft\",\n        legend = c(\"Fumantes\", \"Não Fumantes\"), \n        fill = c(\"dimgrey\", \"cadetblue1\"), \n        bty=\"n\", \n        cex = 1)\n\n\n\n\n\n\n\nFigura 6.12: Gráfico de barras lado a lado\n\n\n\n\n\n\n\n6.5.2.3 Gráfico de barras para uma variável discreta\nA variável mater$para, número de filhos anteriores ao atual, é uma variável numérica discreta e, para representá-la, o mais adequado é usar um gráfico de barras simples (Figura 6.13).\n\ntab_filhos&lt;- table (mater$para) \n\nbarplot (tab_filhos, \n         col = \"tomato\", \n         xlab=\"Número de filhos anteriores ao atual\", \n         ylab = \"Frequência\",\n         ylim = c(0, 500),\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.13: Gráfico de barras para uma variável discreta\n\n\n\n\n\n\n\n\n6.5.3 Gráfico de barra de erro\nO gráfico de barra de erro é um tipo de gráfico barra acrescido de uma medida de dispersão: desvio padrão, intervalos de confiança ou erro padrão. As barras de erro dão uma ideia geral de quão precisa é uma medição ou, inversamente, quão longe o valor observado está do valor verdadeiro.\nContinuando a usar o arquivo dadosMater.xlsx, será selecionada uma amostra de recém-nascidos a termo, definido pela OMS como o nascido de 37 semanas completas a 42 semanas incompletas (259 a 293 dias). A partir destes dados, será construido um gráfico de barra de erro dos recém-nascidos do sexo masculino e feminino.\nInicialmente, deve ser instalado e carregado o pacote Hmisc (13), necessário para fornecer a função errbar() que irá construir o gráfico de de barra de erro.\nA seguir, serão filtrados do conjunto de dados em uso, mater, os recém-nascidos a termo. O conjunto resultante será atribuído a um objeto denominado rnt e , usando o operador pipe %&gt;% será usada a função summarise() e group_by() provenientes do pacote dplyr, para calcular as medidas resumidoras, de acordo com o sexo. Como a variável sexo encontra-se como numérica, será transformada em fator:\n\nmater$sexo &lt;- factor(mater$sexo,\n                      labels = c('masc', 'fem'))\n \n rnt &lt;- mater %&gt;% \n   filter(ig &gt;= 37 & ig &lt; 42) %&gt;% \n   group_by(sexo) %&gt;% \n   summarise(n = n(),\n             media = mean(pesoRN, na.rm = T),\n             dp = sd(pesoRN, na.rm = T),\n             l_inf = media - 1.96*dp,\n             l_sup = media + 1.96*dp)\n\n rnt\n\n# A tibble: 2 × 6\n  sexo      n media    dp l_inf l_sup\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 masc    592 3274.  458. 2376. 4172.\n2 fem     493 3147.  458. 2250. 4044.\n\n\nO próximo passo é criar um objeto, denominado barras, que irá receber as médias dos pesos dos recém-nascidos masculinos e femininos, que representam a altura das barras. Este objeto servirá de base para a construção de um gráfico de barras que será recebido por outro objeto, bp. Finalmente, coloca-se os limites inferiores e superiores para cada sexo, usando os valores calculados pela função summarise() que junto com o objeto bp constituem-se de argumentos da função errbar() (Figura 6.14). Veja maiores detalhes na ajuda do R (?errbar).\n\nbarras &lt;- c(rnt$media[1], rnt$media[2])\n\n bp &lt;- barplot(barras,\n               ylim=c(0,4200), \n               ylab = \"Peso do Recém-nascido (g)\",\n               cex.lab = 1.2,\n               cex.axis = 0.8,\n               cex.names = 1,\n               space = c(0,0.5),\n               names.arg=c(\"Meninos\", \"Meninas\"), \n               col = c(\"lightblue\", \" pink2\"),\n               las = 1)\n box(bty = \"L\")\n\n lim_inf &lt;- c(rnt$l_inf[1], rnt$l_inf[2])\n lim_sup &lt;- c(rnt$l_sup[1], rnt$l_sup[2])\n\n errbar(bp, barras, lim_inf, lim_sup, add = TRUE, xlab = NULL)\n\n\n\n\n\n\n\nFigura 6.14: Gráfico de barras de erro\n\n\n\n\n\n\n\n6.5.4 Histograma\nO histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos.\nNo histograma, as frequências observadas são representadas por intervalos de classes de ocorrência que estão no eixo x e a altura das barras, representando a frequência de cada intervalo, no eixo y. A área de cada barra é proporcional à porcentagem de observações de cada intervalo.\nO R base possui uma função, denominada de hist() que constroi o histograma e possui vários argumentos:\n\nx \\(\\to\\) um vetor numérico usado na construção do histograma\n\nbreaks \\(\\to\\) especifica o número de barras\nfreq \\(\\to\\) lógico; se TRUE (padrão), o histograma é uma representação de frequências; se FALSE, densidades de probabilidade, densidade de componentes, são plotados\ncol \\(\\to\\) cor a ser usada para preencher as barras. O padrão de NULL produz barras não preenchidas\nborder \\(\\to\\) cor da borda ao redor das barras. O padrão é usar a cor de primeiro plano padrão\nmain, xlab, ylab \\(\\to\\) rótulo do título, do eixo x e do eixo y. Para remover o rótulousar NULL.\nxlim, ylim \\(\\to\\) limites do eixo x e do eixo y.\n\n\n6.5.4.1 Histograma Simples\nSerá usada a variável altura, incluída da arquivo mater (veja início da Seção 6.4), para a construção do histograma, executando:.\n\nhist(mater$altura)\n\n\n\n\n\n\n\nFigura 6.15: Histograma básico\n\n\n\n\n\nNo histograma da Figura 6.15, observam-se alguns problemas que devem ser melhorados para tornar a sua aparência mais elegante.\n\nO rótulo dos eixo x está com o nome da variável e do eixo y está em inglês;\nO título do histograma está em inglês e repete o eixo x. Pode ser removido.\nO eixo y tem um limite superior menor do que a barra mais alta;\nO gráfico está na cor cinza, que conforme o interesse pode ser modificada;\nO número de barras pode ser modificado com o argumento breaks. Existe uma função no R que permite calcular o número de intervalos, usando a regra de Sturges (nclass.Sturges()). Entretanto, na maioria das vezes, é o objetivo do estudo quem determina o número de barras e, também, porque nem sempre o R obedece ao argumento.\n\nÉ importante saber o limite inferior e superior da variável, para construir o eixo x. Pode-se fazer isso, com as funções min() e max():\n\nmin(mater$altura, na.rm = TRUE)\n\n[1] 1.4\n\nmax(mater$altura, na.rm = TRUE)\n\n[1] 1.85\n\n\nO número de classes é igual a:\n\nnclass.Sturges(mater$altura)\n\n[1] 12\n\n\nAcrescentado argumentos, modifica-se o aspecto do histograma (Figura 6.16):\n\nhist(mater$altura,\n     breaks = 12,\n     ylim = c (0, 450),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Frequência\", \n     xlab = \"Altura da gestante (metros)\",\n     col = \"tomato\",\n     las = 1)\nbox(bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.16: Histograma modificado\n\n\n\n\n\nObserve que o formato do histograma é igual ao anterior, mudando a cor das barras, o limite do eixo y e os rótulos dos eixos. O R não modificou o número de barras. Ou seja, não obedeceu à modificação do argumento breaks = 12. A função escolheu o que achou mais adequado!\n\n\n6.5.4.2 Histograma com curva normal sobreposta\nEventualmente, para melhor comparar a distribuição dos dados, é interessante incluir uma curva normal sobreposta que servirá de indicador (Figura 6.17). A distribuição normal será discutida mais adiante (Seção 7.7).\nOs passos para colocar a curva normal sobreposta são:\n\nConstruir um histograma de densidade, que é a proporção de todas as observações que se enquadram dentro do intervalo. Na função hist(), modificar o argumento para freq = FALSE.\nAdicionar uma curva normal ao histograma, usando a função curve(). Calcular antes a média e o desvio padrão da variável mater$altura.\n\n\nmu &lt;- mean(mater$altura, na.rm =TRUE)\ndp &lt;- sd(mater$altura, na.rm = TRUE)\n\nhist(mater$altura,\n     ylim = c (0, 6),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Densidade\", \n     xlab = \"Altura da gestante (metros)\",\n     col =\"steelblue\",\n     freq = FALSE,            \n     border = \"white\")\nbox (bty = \"L\")\n\ncurve (dnorm (x, \n              mean=mu, \n              sd=dp), \n       col=\"red\", \n       lty=1,\n       lwd=2,\n       add=TRUE)\n\n\n\n\n\n\n\nFigura 6.17: Histograma com curva normal sobreposta\n\n\n\n\n\n\n\n6.5.4.3 Componentes do Histograma\nPara verificar a lista de componentes de um histograma , há necessidade de colocar o histograma da ?fig-histh em um objeto, no exemplo, denominado de h:\n\nh &lt;- hist(mater$altura,\n          breaks = 8,\n          ylim = c (0, 450),\n          xlim = c (1.4, 1.9),\n          main= NULL, \n          ylab = \"Frequência\", \n          xlab = \"Altura da gestante (metros)\",\n          col =\"tomato\",\n          freq = TRUE,           \n          border = \"white\")\n      box (bty = \"L\")\n\n\n\n\nHistograma da altura da gestante\n\n\n\nh\n\n$breaks\n [1] 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85\n\n$counts\n[1]  18  87 304 406 334 151  50  16   2\n\n$density\n[1] 0.26315789 1.27192982 4.44444444 5.93567251 4.88304094 2.20760234 0.73099415\n[8] 0.23391813 0.02923977\n\n$mids\n[1] 1.425 1.475 1.525 1.575 1.625 1.675 1.725 1.775 1.825\n\n$xname\n[1] \"mater$altura\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nUma das utilidades dos componentes, é construir um histograma com os valores correspondentes as barras sobrepostos ao gráfico (Figura 6.18).\n\nhist(mater$altura,\n     breaks = 8,\n     ylim = c (0, 450),\n     xlim = c (1.4, 1.9),\n     main= NULL, \n     ylab = \"Frequência\", \n     xlab = \"Altura da gestante (metros)\",\n     col = \"salmon\")\nbox (bty = \"L\")\n\ntext (h$mids, h$counts, labels = h$counts, adj= c(0.5, -0.5))\n\n\n\n\n\n\n\nFigura 6.18: Histograma com frequência sobreposta\n\n\n\n\n\nNote que as informações deste gráfico são as mesmas de uma tabela de frequência construída com os mesmos dados. Maneiras diferentes de informar uma distribuição de frequência (veja a Seção 6.4.2).\n\n\n\n6.5.5 Boxplot\nO boxplot descreve a distribuição de uma variável contínua exibindo o resumo de cinco números: mínimo, 1º quartil (percentil 25), mediana (percentil 50), 3ª quartil (percentil 75) e máximo (Figura 6.19).\n\n\n\n\n\n\n\n\nFigura 6.19: Boxplot\n\n\n\n\n\n\n6.5.5.1 Boxplot a partir de um vetor\nPara construir um boxplot, serão usados os mesmos dados dos recém-nascidos a termo, filtrados do conjunto de dados dadosMater.xlsx, como realizado na seção da construção de um gráfico de barra de erro (Seção 6.5.3). Os dados obtidos, novamente serão atribuídos a um objeto de nome rnt. A variável usada para construir o boxplot será rnt$pesoRN.\nO R possui uma função no pacote básico denominada boxplot() que foi usada para construir o gráfico da Figura Figura 6.20. A função solicita vários argumentos que podem alterar a sua aparência e devem ser utilizados de acordo com necessidade:\n\nformula \\(\\to\\) este parâmetro é definido como um vetor ou uma fórmula (y ~ grupo);\ndata \\(\\to\\)este parâmetro define o conjunto de dados;\nnotch \\(\\to\\) parâmetro lógico. Se TRUE um entalhe será desenhado em cada lado da caixa, representando o intervalo de confiança para a mediana. Se os entalhes de dois boxplots não se sobrepuserem, indica uma “forte evidência” de que as duas medianas diferem;\nvarwidth \\(\\to\\) parâmetro lógico. Se for TRUE, as caixas serão desenhadas com larguras proporcionais às raízes quadradas do número de observações nos grupos;\nborder \\(\\to\\)um vetor opcional de cores para os contornos dos boxplots:\nmain \\(\\to\\) este parâmetro é o título do gráfico;\nxlab, ylab \\(\\to\\) rótulos dos eixos x e y ;\ncex \\(\\to\\) ver https://www.statology.org/r-plot-cex/;\nlas \\(\\to\\) altera orientação do rótulos do eixo. Valores aceitos 0 (paralelo ao eixo), 1 (horizontal), 2 (perpendicular) e 3 (vertical);\nnames \\(\\to\\) Este parâmetro são os rótulos dos grupos que serão mostrados em cada boxplot;\n… \\(\\to\\) Outros parâmetros (ver ajuda do R, digitando ?boxplot no Console)\n\n\nrnt &lt;- mater %&gt;% filter(ig &gt;= 37 & ig &lt; 42)\n\nrnt$sexo &lt;- factor(rnt$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\nboxplot (rnt$pesoRN)\n\n\n\n\n\n\n\nFigura 6.20: Boxplot simples\n\n\n\n\n\nEsse boxplot pode ser modificado (Figura 6.21), alterando alguns argumentos como colocação de um título no gráfico, e rótulos nos eixos e mudança na cor. Os argumento cex.lab, cex.axis e cex.names estabelecem o tamanho fontes. Por exemplo, para aumentar em 20%, usamos 1.2.\n\nboxplot (rnt$pesoRN, \n         col = \"lightblue2\", \n         main = \"RN a termo\", \n         ylab = \"Peso do Recém-nascido (g)\",\n         border = \"black\",\n         cex.lab = 1.2, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.21: Boxplot modificado\n\n\n\n\n\nInterpretação do boxplot\nO boxplot nos fornece uma análise visual da posição, dispersão, simetria, caudas e valores discrepantes (outliers) do conjunto de dados (Figura 6.19).\n\nPosição – Em relação à posição dos dados, observa-se a linha central do retângulo (a mediana ou segundo quartil).\nDispersão – A dispersão dos dados pode ser representada pelo intervalo interquartil (IIQ), tamanho da caixa, que é a diferença entre o terceiro quartil (3ºQ) e o primeiro quartil (1ºQ), ou ainda pela amplitude que é calculada da seguinte maneira: valor máximo – valor mínimo. Embora a amplitude seja de fácil entendimento, o intervalo interquartil é uma estatística mais robusta para medir variabilidade uma vez que não sofre influência de outliers.\nSimetria – Um conjunto de dados que tem uma distribuição simétrica, terá a linha da mediana no centro do retângulo. Quando a linha da mediana está próxima ao primeiro quartil, os dados são assimétricos positivos e quando a posição da linha da mediana é próxima ao terceiro quartil, os dados são assimétricos negativos. Vale lembrar que a mediana é a medida de tendência central mais indicada quando os dados possuem distribuição assimétrica, uma vez que a média aritmética é influenciada pelos valores extremos.\nCaudas – As linhas que vão do retângulo até aos outliers podem fornecer o comprimento das caudas da distribuição.\nValores atípicos (Outliers) – Os outliers indicam possíveis valores discrepantes. No boxplot, as observações são consideradas atípicas quando estão abaixo ou acima dos limites superior e inferior. O limite de detecção de outliers é construído utilizando o intervalo interquartil, dado pela distância entre o primeiro e o terceiro quartil. Sendo assim, os limites inferior e superior de detecção de outlier são dados por:\n\no Limite Inferior: 1ºQ – (1,5 * IIQ);\no Limite Superior: 3ºQ + (1,5 * IIQ). Tanto o limite superior como o inferior são representados por (º).\nos Valores extremos: são valores que estão acima ou abaixo de 3 vezes o IIQ e são representados por (*).\n\n\n\n\n6.5.5.2 Adicionando pontos ao boxplot\nQuando se observa um boxplot, verifica-se que os mesmos ocultam a distribuição subjacente dos dados. Para resolver este “problema”, pode-se adicionar pontos ao gráfico, usando a função stripchart(). Esta função permite criar um gráfico de dispersão unidimensional sobreposto ao boxplot (Figura 6.22). Os comandos para esta ação são:\n\nboxplot (rnt$pesoRN, \n          col = \"lightblue2\", \n          ylab = \"Peso do Recém-nascido (g)\", \n          border = \"black\",\n          cex.lab = 1.2, \n          cex.axis = 1, \n          cex.names = 1,\n          las = 1)\n\n stripchart(x= rnt$pesoRN, \n            method = \"jitter\", \n            col = \"tomato\",\n            cex = 0.5,\n            pch = 16,\n            vertical = TRUE, \n            add = TRUE)\n\n\n\n\n\n\n\nFigura 6.22: Boxplot com pontos de dispersão\n\n\n\n\n\nNeste exemplo, há uma grande sobreposição de pontos, pois a amostra é muito grande (n = 1085). Isto dificulta um pouco a visualização, mas ajuda a ver como a dispersão se comporta. Você também pode personalizar o símbolo (pontos) para criar o gráfico, a largura da linha e sua cor com os argumentos pch, lwd e col, respectivamente. Alguns símbolos, como pch = 21 a 25 permitem que você modifique a cor de fundo do símbolo com o argumento bg. O argumento vertical = TRUE, coloca os pontos na vertical sobreposto ao boxplot, quando o argumento add = TRUE. O argumento cex = 0.5 é o tamanho dos pontos e method = \"jitter\", espalha os pontos para diminuir a sobreposição entre eles.\n\n\n6.5.5.3 Boxplot com intervalos de confiança para a mediana\nÉ possível representar os intervalos de confiança de 95% para a mediana em um boxplot (Figura 6.23), definindo o argumento notch como TRUE.\n\nboxplot (rnt$pesoRN, \n          col = \"lightblue2\", \n          ylab = \"Peso do Recém-nascido (g)\", \n          border = \"black\",\n          cex.lab = 1.2, \n          cex.axis = 1, \n          cex.names = 1,\n          las = 1,\n          notch = TRUE)\n\n\n\n\n\n\n\nFigura 6.23: Boxplot modificado\n\n\n\n\n\n\n\n6.5.5.4 Estatísticas do boxplot\nA função boxplot.stats() do pacote grDevices fornece as estatísticas do boxplot, facilitando a interpretação do mesmo, de modo semelhante ao visto para o histograma.\n\nboxplot.stats (rnt$pesoRN)\n\n$stats\n[1] 2051 2920 3215 3505 4380\n\n$n\n[1] 1085\n\n$conf\n[1] 3186.939 3243.061\n\n$out\n [1] 1440 1980 1795 1810 4400 4950 4535 4670 1425 4410 4660 1715 1895 4485 4390\n[16] 4445 4620 1785\n\n\nInterpretação das estatísticas\n* $stats = é o resumo dos 5 números: mínimo, percentil 25, mediana, percentil 75 e máximo;\n* $n = nº de obs;\n* $conf = limite inf/sup do entalhe se houver;\n* $out = são os outliers.\n\n\n6.5.5.5 Múltiplos boxplots\nOs boxplots são muito usados na comparação de grupos. A necessidade mais comum é ordenar as categorias de acordo com o aumento da mediana, mas isto é opcional. Permite identificar rapidamente qual grupo tem o maior valor e como as categorias são classificadas (Figura 6.24).\nSerá realizada uma comparação visual, usando boxplots, dos pesos dos recém-nascidos por sexo. As variáveis são rnt$pesoRN e rnt$sexo. Esta última está codificada como numérica 1 e 2, portanto há necessidade de ser transformada em fator:\n\nmater &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(idadeMae, altura, peso, anosEst, fumo, \n         para, ig, sexo, pesoRN, compRN, utiNeo) %&gt;% \n  mutate(categIdade = case_when(\n    idadeMae &lt; 20 ~ \"&lt; 20 anos\",\n    idadeMae &gt;= 20 & idadeMae &lt;= 35 ~ \"20 a 35 anos\",\n    idadeMae &gt; 35 ~ \"&gt; 35 anos\")) %&gt;%\n  mutate(categIdade = factor(categIdade, \n                             levels = c(\"&lt; 20 anos\", \"20 a 35 anos\",\n                                                    \"&gt; 35 anos\")))\n\nrnt &lt;- mater %&gt;% filter(ig &gt;= 37 & ig &lt; 42)\n\nrnt$sexo &lt;- factor(rnt$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink\"), \n         ylab = \"Peso do Recém-nascido (g)\", \n         xlab = \"Sexo\",\n         border = \"black\",\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.24: Múltiplos boxplots\n\n\n\n\n\nObserve que foi utilizado o argumento rnt$pesoRN ~ rnt$sexo (y ~ grupo) para obter os dois boxplots. Existe uma pequena diferença entre eles, as caixas são quase coincidentes. Foi suprimido o argumento (xlab = sexo) relativo ao rótulo do eixo x, pois seria redundante.\nPode-se fazer um entalhe (notch) que podem ser interpretados como um intervalo de confiança em torno dos valores medianos Figura 6.25). É calculado pela fórmula :\\(mediana \\pm 1.57\\times IIQ/\\sqrt{n}\\). No nosso exemplo, observe que o entalhe nos meninos está um pouco acima do das meninas..\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink\"), \n         ylab = \"Peso do Recém-nascido (g)\", \n         xlab = \"Sexo\",\n         border = \"black\",\n         cex.lab = 1, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1,\n         notch = TRUE)\n\n\n\n\n\n\n\nFigura 6.25: Boxplots com entalhes\n\n\n\n\n\n\n\n6.5.5.6 Boxplots horizontais\nPara criar um boxplot horizontal Figura 6.26), usamos o argumento horizontal = TRUE e invertemos os rotulos dos eixos x e y.\n\nboxplot (rnt$pesoRN ~ rnt$sexo, \n         col = c(\"lightblue2\", \"pink2\"), \n         xlab = \"Peso do Recém-nascido (g)\", \n         ylab = NULL,\n         horizontal = TRUE,\n         border = \"black\",\n         cex.lab = 1.2, \n         cex.axis = 1, \n         cex.names = 1,\n         las = 1)\n\n\n\n\n\n\n\nFigura 6.26: Boxplots horizontais\n\n\n\n\n\n\n\n\n6.5.6 Gráfico de Dispersão\nUm gráfico de dispersão (Scatterplot) exibe a relação entre duas variáveis numéricas (Figura 6.27). Cada ponto representa uma observação. Suas posições nos eixos x (horizontal) e y (vertical) representam os valores das duas variáveis.\nO R Base é uma boa opção para construir um gráfico de dispersão, usando a função plot(). Ambas as variáveis numéricas do banco de dados devem ser especificadas nos argumentos x e y.\nA função plot() é uma função genérica que pode ser facilmente editada com múltiplos argumentos envolvendo os eixos e caracteres plotados da mesma maneira que foi feita com os gráficos anteriores. Aqui, novamente, serão usados os dados incluídos no conjunto de dados rnt:\n\nplot (x = rnt$compRN,\n      y = rnt$pesoRN,\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      cex.axis = 0.8,\n      las = 1)\n\n\n\n\n\n\n\nFigura 6.27: Gráfico de dispersão\n\n\n\n\n\nEste mesmo gráfico pode ser obtido, usando uma fórmula y~x e acrescentando o argumento bty = \"L\" (Figura 6.28). Este argumento permite personalizar a caixa ao redor do gráfico.\n\no: caixa completa (parâmetro padrão),\nn: sem caixa\n7: superior + direita\nL: inferior + esquerda\nC: superior + esquerda + inferior\nU: esquerda + inferior + direita\n\n\nplot (pesoRN ~ compRN,\n      data = rnt,\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      cex.axis = 0.8,\n      las = 1,\n      bty = \"L\")\n\n\n\n\n\n\n\nFigura 6.28: Gráfico de dispersão\n\n\n\n\n\nComo em qualquer outro gráfico, este também pode ser melhorado em seu aspecto, tornando os pontos sólidos e coloridos. O argumento pch estabelece o tipo de pontos (Figura 6.29).\n\n\n\n\n\n\n\n\nFigura 6.29: Símbolo dos formatos\n\n\n\n\n\nNa Figura 6.27, como os pontos estão aglomerados, devido a quantidade, é possível tentar espalhá-los, usando a função jitter() na variável compRN (Figura 6.30). O argumento 10 é variável e significa o grau de espalhamento:\n\nplot (jitter(rnt$compRN,10),\n      rnt$pesoRN,\n      col = \"steelblue\",\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      las = 1,\n      bty = \"L\",\n      pch = 16,\n      cex = 1,\n      cex.lab = 1.1,\n      cex.axis = 0.8)\n\n\n\n\n\n\n\nFigura 6.30: Gráfico de dispersão com jitter\n\n\n\n\n\n\n6.5.6.1 Mapeamento dos pontos de acordo com uma variável categórica\nInicialmente, será criado um vetor para representar as cores, de acordo com o sexo (meninos = azul; meninas = vermelho). Usa-se a função unclass() para discriminar os sexos (Figura 6.31). Acrescenta-se uma legenda para ilustrar a separação.\n\ncores &lt;- c(\"dodgerblue3\", \"tomato\")\n\nplot(x = jitter(rnt$compRN, 10), \n  y = rnt$pesoRN,\n  bg = cores[unclass(rnt$sexo)],\n  ylab = \"Peso de Recém-nascido (g)\",\n  xlab = \"Comprimento do Recém-nascido (cm)\",\n  las = 1,\n  bty = \"L\",\n  cex = 1.5,\n  pch=21,\n  cex.lab = 1,\n  cex.axis = 0.8)\n\nlegend (legend = c(\"Meninos\", \"Meninas\"), \n        fill = cores, \n        bty=\"n\", \n        cex = 1,\n        \"topleft\")\n\n\n\n\n\n\n\nFigura 6.31: Mapeamento dos pontos de acordo com uma variável categórica\n\n\n\n\n\n\n\n6.5.6.2 Adição da reta de ajuste\nUma linha reta de ajuste dos dados (Figura 6.32) pode ser acrescentada usando a função abline (), associada a função lm (). Um modelo típico lm (linear model) tem o formato resposta (y) ~ preditor (x). Mais detalhes sobre o modelo de ajuste linear na Seção 15.3 (regressão linear).\n\n# Construção do gráfico de dispersão\nplot (jitter(rnt$compRN,10),\n      rnt$pesoRN,\n      col = \"gray40\",\n      bg = \"darkturquoise\",\n      ylab = \"Peso de Recém-nascido (g)\",\n      xlab = \"Comprimento do Recém-nascido (cm)\",\n      las = 1,\n      bty = \"L\",\n      pch = 21,\n      cex = 1.3,\n      cex.lab = 1,\n      cex.axis = 0.8)\n\n# Criação do modelo de ajuste\nmodelo &lt;- lm (rnt$pesoRN ~ rnt$compRN)\n\n# Adição da reta, usando o modelo\nabline (modelo, \n        col=\"red\", \n        lwd=2, \n        lty = 2)\n\n\n\n\n\n\n\nFigura 6.32: Gráfico de dispersão com reta de ajuste\n\n\n\n\n\nAo executar o modelo, se obtém os parâmetros para a construção da equação da regressão linear:\n\nsummary(modelo)\n\n\nCall:\nlm(formula = rnt$pesoRN ~ rnt$compRN)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1434.56  -218.40   -19.56   177.76  2097.87 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3416.451    215.821  -15.83   &lt;2e-16 ***\nrnt$compRN    137.674      4.475   30.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 337.7 on 1083 degrees of freedom\nMultiple R-squared:  0.4664,    Adjusted R-squared:  0.4659 \nF-statistic: 946.6 on 1 and 1083 DF,  p-value: &lt; 2.2e-16\n\n\nA equação de predição da regressão linear permite que, conhecendo o valor do comprimento, é possível prever o peso do recem-nascido:\n\\[\n\\hat{y} = b_{0}+ b_{1}\\times x\n\\]\nDesta forma, substituindo pelos valores contidos nas estimativas da tabela dos coeficientes do sumário do modelo, um bebê com 50 cm terá um peso de aproximadamente:\n\\[\n\\hat{y} = -3416.45 + 137.67\\times 50 = 3467.05\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#sec-ggplot2",
    "href": "06-descrevendoDados.html#sec-ggplot2",
    "title": "6  Descrevendo os dados",
    "section": "6.6 Introdução ao ggplot2",
    "text": "6.6 Introdução ao ggplot2\nO R tem vários sistemas para fazer gráficos e, na ,maioria das vezes, eles são suficientes. Entretanto, o surgimento do ggplot2 (14) trouxe a possibilidade de serem construídos gráficos mais elegantes e versáteis. Além disso, torna o processo mais rápido, baseado em uma sofisticada gramática (15).\nOs três componentes principais de cada gráfico do ggplot2são: dados, estética e geometria.\n\nDados (data),\nMapeamentos estéticos (aesthetic mappings) entre variáveis e propriedades visuais (como posição, cor, tamanho, forma e transparência), e\ngeom são as funções que criam camadas, pelo menos uma, que descrevem como renderizar cada observação.\n\n\n6.6.1 Principais gráficos usando ggplot2\n\n6.6.1.1 Gráfico de dispersão\nSerão usados os dados coletados em um ambulatório pediátrico relacionados à idade e ao comprimento de 40 crianças entre 18 e 36 meses (20 meninos e 20 meninas), incluídos no arquivo dadosReg.xlsx. Eles podem ser baixados aqui. Salve o arquivo no seu diretório de trabalho e carrege-o com a função read_excel() do pacote readxl:\n\ndados &lt;- readxl::read_excel(\"dados/dadosReg.xlsx\")\nstr(dados)\n\ntibble [40 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ...\n $ comp  : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ...\n $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ...\n $ sexo  : chr [1:40] \"masc\" \"fem\" \"masc\" \"fem\" ...\n\n\nPara introduzir a lógica do ggplot2, será construído um gráfico de dispersão. Os mapeamentos de dados e estéticos são fornecidos na função ggplot() e aes(). Em seguida, as camadas são adicionadas com sinal +. Esse é um padrão importante e, à medida que se aprende mais sobre o ggplot2, se construirá gráficos cada vez mais sofisticados adicionando mais tipos de componentes. A primeira camada do gráfico, a camada base (Figura 6.33)), é dada pela função ggplot(). Essa função recebe um dataframe ou tibble, no exemplo dados, onde serão acrescentadas outras camadas.\nExecutando apenas a função ggplot(), aparece o seguinte painel:\n\nggplot(data = dados) \n\n\n\n\n\n\n\nFigura 6.33: Camada base do ggplot2\n\n\n\n\n\nObserva-se um painel de cor cinza, vazio, apesar de todos os dados terem sido passados para função. Para que o gráfico seja esboçado, é necessário que as observações sejam mapeadas (estética) e as formas geométricas especificadas. Como será construído um gráfico de dispersão, que mostra a correlação entre duas variáveis (idade e comprimento), o código inicial é o seguinte:\n\nggplot(data = dados, \n       mapping = aes(x = idade, y = comp)) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.34: Gráfico de dispersão\n\n\n\n\n\nA estética, com a função aes(), foi adicionada na função ggplot() (Figura 6.34) e isto significa que ela será usada em todas as outras camadas que forem acrescentadas. Pode-se colocar a a estética dentro da função do geom específico e ,dessa forma, funcionárá apenas para ele.\nO mesmo resultado da Figura 6.34, pode ser obtido da seguinte maneira:\n\nggplot(data = dados) +\n  geom_point(mapping = aes(x = idade, y = comp))\n\nEm gráficos de dispersão, é útil acrescentar uma terceira camada, representando uma suavização dos dados com função geom_smooth(). Se você não estiver interessado no intervalo de confiança, desative-o com geom_smooth(se = FALSE). Um argumento importante para geom_smooth() é o method, que permite que você escolha qual tipo de modelo é usado para ajustar a curva. Como o gráfico de dispersão indica uma correlação linear, o melhor ajuste é dado pelo method = \"lm\" que se ajusta a um modelo linear, fornecendo a reta de melhor ajuste 6.\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 6.35: Gráfico de dispersão com reta de regressão\n\n\n\n\n\nO gráfico da Figura 6.35, mostra um ajuste dos pontos a uma reta, representando a correlação perfeita entre x ~ y. A distância dos pontos à reta é o erro (resíduo). A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada (veja também Seção 15.3. O gráfico mostra uma forte correlação: à medida que o idade aumenta, aumenta o comprimento da criança\nOutros atributos estéticos\nO estabelecimento das cores, formato e tamanho dos pontos pode ser realizado adicionando o argumento color = dentro da função aes(). Por exemplo:\n\nggplot(dados, aes(x = idade, y = comp, color = sexo)) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.36: Gráfico de dispersão com separação dos pontos por uma variável categórica (sexo)\n\n\n\n\n\nNa Figura 6.36, as cores, escolhidas pelo padrão do R, estão separando os sexos (masculino e feminino). A legenda, à direita identifica, as cores dos sexos.\nO que acontece se colocarmos a cor fora da função aes()?\n\nggplot(dados, aes(x = idade, y = comp), color = sexo) +\n  geom_point()\n\n\n\n\n\n\n\nFigura 6.37: Gráfico de dispersão com a cor sendo especificada fora da aes().\n\n\n\n\n\nNão acontece nada! O R ignora o código e retorna um gráfico com as sua cor padrão, a preta (Figura 6.37).\nPara mudar a cor dos pontos de acordo com a nossa escolha (Figura 6.38), é necessário informar a cor no geom (color = \"cor\").\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(color = \"tomato\")\n\n\n\n\n\n\n\nFigura 6.38: Gráfico de dispersão com a cor dos pontos por escolha pessoal\n\n\n\n\n\nPara aumentar o tamanho dos pontos usa-se o argumento size =7 no geom; o formato é modificado, da mesma maneira, com o argumento shape =8. O tamanho do ponto pode também ser modificado de acordo com uma variável, como feito com o argumento color = sexo (Figura 6.37), colocando o argumento dentro da função aes(), no ggplot().\nO código a seguir, modifica o tamanho do ponto e o seu formato de acordo com o sexo.\n\nggplot(dados, \n       aes(x = idade, y = comp, shape = sexo)) + \n  geom_point(color = \"tomato\", size = 3)\n\n\n\n\n\n\n\nFigura 6.39: Gráfico de dispersão com pontos aumentados e com formatos diferentes de acordo com o sexo.\n\n\n\n\n\nA saída (Figura 6.39) mostra um triangulo e um ponto, correspondendo, respectivamente, ao sexo masculino e feminino com tamanhos duas vezes maior do que o padrão.\nSerá criado outro exemplo (Figura 6.40) com o argumento fill = sexo para permitir a visualização dos sexos com preenchimento em cores diferentes, conforme o padrão do ggplot2. Observe que foi utilizado o argumento dentro da estética do ggplot(). O argumento color=“black” será colocado no geom_point(), para colocar uma borda preta ao redor de todos os pontos. O argumento shape = 21 corresponde a um formato de pontos vazios.\n\nggplot(dados, aes(x = idade, y = comp, fill = sexo)) +\n  geom_point(color = \"black\", size = 5, shape = 21) \n\n\n\n\n\n\n\nFigura 6.40: Gráfico de dispersão semelhante ao da Figura 5, apenas com pontos maiores.\n\n\n\n\n\nFacetamento\nOutra técnica para exibir variáveis categóricas adicionais em um gráfico é o facetamento. O facetamento cria gráficos dividindo os dados em subconjuntos e exibindo o mesmo gráfico para cada subconjunto (Figura 6.41) . Para facetar um gráfico, basta adicionar uma especificação de facetamento com a função facet_wrap(), que recebe o nome de uma variável precedido pelo sinal ~.\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(fill = \"tomato\", color = \"black\", size = 5, shape = 21) +\n  facet_wrap(~sexo)\n\n\n\n\n\n\n\nFigura 6.41: Facetamento: gráfico de dispersão por categorias.\n\n\n\n\n\nPara observar um padrão dominante, como feito acima (Figura 6.35), pode-se adicionar ao gráfico a função geom_smooth(method = \"lm\") para ajustar um modelo linear, para comparação 9:\n\nggplot(dados, aes(x = idade, y = comp)) +\n  geom_point(fill = \"tomato\", color = \"black\", size = 5, shape = 21) +\n  facet_wrap(~sexo) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 6.42: Reta de regressão com intervalo de confiança por categorias\n\n\n\n\n\nNeste caso (Figura 6.42), observa-se que as inclinações das retas são praticamente iguais, indicando que o sexo não modifica a correlação entre variáveis.\n\n\n6.6.1.2 Boxplot\nNa Seção 6.5.5 foi mostrado o objetivo de um boxplot, agora, será construído um boxplot, usando o geom_boxplot() para comparar o comprimento das crianças de acordo com o sexo (Figura 6.43):\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFigura 6.43: FIGURA 14: Boxplot padrão do ggplot2\n\n\n\n\n\nEste é um boxplot simples, mas facilmente, pode-se modificá-lo, tornando-o mais atraente 10. Por exemplo, controlando a cor das caixas (Figura 6.44) com o argumento color (ou colour) para modificar as bordas e fill para modificar o preenchimento das caixas. A adição do argumento alpha modifica a transparência de um geom. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes:\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot(colour = \"skyblue4\", \n               fill = \"skyblue\", \n               alpha = 0.3)\n\n\n\n\n\n\n\nFigura 6.44: Boxplots com as cores das caixas alteradas.\n\n\n\n\n\nA escolha da cor pode ser realizada entre as 657 opções que o R oferta, consultando Dealing with colors in ggplot2 ou Colors in R 11.\nO boxplot oculta a distribuição subjacente dos dados. Para resolver isto, pode-se adicionar pontos ao gráfico (Figura 6.45) usando o geom_jitter(). Para evitar que os pontos fiquem muito espalhados, dificultando a visualização das categorias, usa-se o argumento width 12. Um espalhamento de 10% (width = 0.10)para poucos pontos, como no exemplo, parece bom, para observar a distribuição dos mesmos.\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_boxplot(colour = \"skyblue4\", \n               fill = \"skyblue\", \n               alpha = 0.3) +\n  geom_jitter(width = 0.10)\n\n\n\n\n\n\n\nFigura 6.45: Boxplot com jitter.\n\n\n\n\n\nContinuando a alteração da aparência do boxplot, é posível modificar o seu formato para o formato clássico com “bigodes” terminando em “T” (Figura Figura 6.46) e não um traço simples. Para isso, cria-se uma camada de barra de erro, usando a função geom_errorbar(), antes de geom_boxblot(). Assim, como o boxplot passa ser a camada mais superficial, ele impede que se visualize a barra de erro na caixa, desde que ele seja opaco (remover ou zerar o argumento alpha). A função geom_errorbar() normalmente é usada para barras de erro. No entanto, aqui ela está sendo utilizada com stat = \"boxplot\" 13, o que significa que os cálculos de estatística do boxplot serão aplicados à barra de erro. O argumento width = 0.1 ajusta a largura das barras de erro, tornando-as mais estreitas.\n\nggplot(dados, aes(x = sexo, y =  comp)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot(colour = \"black\", \n               fill = \"chartreuse\") \n\n\n\n\n\n\n\nFigura 6.46: Boxplot no ggplot2 com bigodes finalizando em T.\n\n\n\n\n\n\n\n6.6.1.3 Gráfico de Violino (Violin plot)\nOs gráficos de violino permitem visualizar a distribuição de uma variável numérica para um ou vários grupos. No ggplot2, são construidos com o geom_violin()e, com frequência, substituem os boxplots.\nCada “violino” representa uma variável de agrupamento. A forma representa a estimativa de densidade da variável: quanto mais pontos de dados em um intervalo específico, mais largo será o violino para esse intervalo. É muito parecido com um boxplot, mas permite uma compreensão mais profunda da distribuição.\nO gráfico de violino é uma técnica poderosa de visualização de dados, pois permite comparar a classificação de vários grupos e sua distribuição. São particularmente adequados quando a quantidade de dados é grande e é impossível mostrar observações individuais. Para conjuntos de dados pequenos, um boxplot com jitter é provavelmente uma opção melhor, pois realmente mostra todas as informações.\nAqui, será usado o conjunto de dados dadosFumo.xlsx que mostra a distribuição dos pesos dos recém-nascidos a termo de acordo com o tabagismo materno (não fumante, fumante leve. fumante moderada, fumante pesada). Ele pode ser obtido aqui, para ser salvo em seu diretório de trabalho.\n\ndados_fumo &lt;- readxl::read_excel(\"dados/dadosFumo3.xlsx\")\nstr(dados_fumo)\n\ntibble [310 × 3] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN   : num [1:310] 3055 3190 4350 2740 2270 ...\n $ quantFumo: chr [1:310] \"fumante_leve\" \"fumante_leve\" \"fumante_leve\" \"fumante_leve\" ...\n\n\nPara construir o gráfico de violino, serão usados os argumentos trim = FALSE, para não aparar caudas, e draw_quantiles = c(0.25, 0.5, 0.75), para traçar os quartis (Figura 6.47). No final, a função theme() será colocada para evitar que a legenda das categorias apareça, uma vez que ela é explicita no gráfico.\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN,         \n                       fill=quantFumo)) + \n  geom_violin(trim = FALSE,\n              draw_quantiles = c(0.25, 0.5, 0.75)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.47: Gráfico de violino com os quartis.\n\n\n\n\n\nUma alteração interessante que pode ser feita no gráfico de violino é colocar um boxplot (Figura 6.48), dentro do mesmo, faz o efeito do argumento draw_quantiles, usado na Figura 6.47. Facilita a interpretação e, na opinião do autor, é mais elegante. O argumento width = 0.5, na função geom_boxplot(), estabelece a largura do boxplot.\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN, fill=quantFumo)) + \n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.48: Gráfico de violino com boxplot.\n\n\n\n\n\nPara obter uma versão horizontal da Figura 6.48, chama-se a função coord_flip() 14 que permite inverter os eixos X e Y e, assim, tornar a interpretação mais intuitiva, mais amigável (Figura 6.49).\n\nggplot(dados_fumo, aes(x=quantFumo, y=pesoRN, fill=quantFumo)) + \n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.5) +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.49: Gráfico de violino horizontal.\n\n\n\n\n\nPara interpretar um gráfico de violino, observar o seguinte:\n\nForma do violino, observando a largura em diferentes pontos para entender onde os dados se concentram.\nA linha mediana e a caixa do boxplot associado indicam a mediana e o intervalo interquartil, respectivamente.\nSe o violino é simétrico em torno da mediana, a distribuição dos dados é aproximadamente simétrica.\nSe a parte superior do violino é mais larga, os dados podem ser assimétricos, inclinados para valores maiores.\nEm múltiplas categorias, pode-se comparar rapidamente as distribuições. Diferentes formas e larguras entre as categorias fornecem uma visão clara das variações entre elas.\n\n\n\n6.6.1.4 Histograma\nComo visto na Seção 6.5.4, o histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e da dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos.\nO geom_histogram() é a geometria para a construção de um histograma. Aqui, há necessidade apenas do eixo x, pois existe uma única variável. A execução do comando retorna a distribuição dessa variável.\nPara construir um histograma, será usada a variável pesoRN do banco de dados usado nos gráficos de violino (dadosFumo.xlsx). O histograma (Figura 6.50) pode ser obtido por qualquer um dos códigos abaixo:\n\nggplot(dados_fumo) + \n  geom_histogram(aes(x = pesoRN))\n\nOu\n\nggplot(dados_fumo, aes(x = pesoRN)) + \n  geom_histogram()\n\n\n\n\n\n\n\nFigura 6.50: Histograma padrão.\n\n\n\n\n\nA aparência deste histograma não está boa. A sua mensagem depende dessa aparência. O histograma recebe uma variável numérica e a divide em vários “compartimentos”, os intervalos, representados pelas barras. A escolha do tamanho (amplitude) do intervalo é de extrema importância para a aparência do histograma.\nO geom_histogram() tem um argumento, denominado binwidth que permite alterar a amplitude do intervalo. O binwidth é um intervalo e sua unidade é igual a da variável que se está histogramando. No exemplo, foi usado o peso do recém-nascido (g). Se quisermos um intervalo de 100 em 100 gramas, o binwidth = 100. Uma outra maneira, é usar bins que agrupa em intervalos de mesmo tamanho. Se estabelecermos bins = 15, o geom_histogram() dividirá em 15 intervalos iguais.\nJunto com a alteração dos intervalos, vamos modificar a cor de preenchimento (fill) e bordas (colour) das barras (Figura 6.51).\n\nggplot(dados_fumo, aes(x = pesoRN)) + \n  geom_histogram(binwidth = 100,\n                 fill = \"chartreuse\",\n                 colour = \"darkgreen\")\n\n\n\n\n\n\n\nFigura 6.51: Histograma modificado com o argumento binwidth = 100 e com preenchimento e bordas das barras customizadas.\n\n\n\n\n\nCom frequência se observa um histograma com curva normal sobreposta (Figura 6.52) para facilitar a comparação dos dados com a distribuição normal. Isso pode ser conseguido com um código que usa função stat_function() para a construção da curva normal, baseada nos dados (média e desvio padrão da variável pesoRN) e a função after_stat(density), colocada na estética do histograma no eixo Y, para substituir a frequência pela densidade de probabilidade. O restante do código somente estabelece que a linha da curva será tracejada (linetype = “dashed”), de cor vermelha (color = “red”) e com tamanho 1 (linewidth = 1).\n\nggplot(dados_fumo) + \n  geom_histogram(aes(x = pesoRN, \n                     y = after_stat(density)),\n                 binwidth = 100,\n                 fill = \"chartreuse\",\n                 colour = \"darkgreen\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(dados_fumo$pesoRN),\n                            sd = sd(dados_fumo$pesoRN)),\n                linetype = \"dashed\",\n                linewidth = 1,\n                color = \"red\")\n\n\n\n\n\n\n\nFigura 6.52: Histograma com curva normal sobreposta.\n\n\n\n\n\nO gráfico da Figura 6.52, mostra que os pesos dos recém-nascidos se ajustam bem à curva normal.\n\n\n6.6.1.5 Gráfico de barras\nO gráfico de barras é uma análogo do histograma, onde as barras , ao contrário deste, são separadas. Os gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas.\nA função geom_bar() permite delinear um gráfico de barras (Figura 6.53). O exemplo será construído com a variável quantFumo do arquivo dadosFumo.xlsx.\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count))))\n\n\n\n\n\n\n\nFigura 6.53: Gráfico de barras padrão do ggplot2.\n\n\n\n\n\nAs cores de preenchimento das barras podem ser alteradas, de acordo com a variável categórica (Figura 6.54). As cores serão estabelecidas de acordo com o padrão do ggplot2:\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo))\n\n\n\n\n\n\n\nFigura 6.54: Gráfico de barras com as cores das barras estabelecidas pelo ggplot2.\n\n\n\n\n\nO gráfico retorna uma legenda, mostrando o que representa cada cor. Ela é desnecessária porque já está explicito, no eixo X, o que cada barra representa. Portanto, vamos remover a legenda (Figura 6.55)com a função theme(legend.position = \"none\"):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.55: Gráfico de barras anterior sem legenda.\n\n\n\n\n\nEssas cores do padrão do ggplot2 podem ser modificadas com a função scale_fill_manual()que permite a customização das cores (Figura 6.56):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.56: Gráfico de barras com cores customizadas.\n\n\n\n\n\n\n\n\n6.6.2 Modificação dos eixos do gráfico\n\n6.6.2.1 Rótulos nos eixos\nObserve que no gráfico da Figura 6.56, o eixo y contém um rótulo que é a fórmula para o cálculo da proporção e o eixo x, o nome da variável quantFumo que também é pouco esclarecedor. Os rótulos dos eixos são muito importantes em um gráfico, pois a sua leitura deve explicar o que o gráfico está apresentando. Nessa figura, o gráfico está mostrando a proporção da intensidade de tabagismo em um grupo de gestantes. Isto deve estar claro no gráfico. Para modificar o rótulo dos eixos, utiliza-se, com frequência, as funções xlab() e ylab() (Figura 6.57).\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.57: Gráfico de barras com os rótulos dos eixos alterados.\n\n\n\n\n\nO mesmo trabalho de alteração dos rótulos pode ser feito com a função labs() que faz , retornando um gráfico igual ao da Figura 6.57:\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  labs (y = \"Proporção por categoria\",\n        x = \"Tabagismo Materno\") +\n  theme(legend.position = \"none\")\n\n\n\n6.6.2.2 Mudando o nome e a ordem dos rótulos no eixo x\nA Figura 6.57 ficou melhor, mas ainda tem problemas. O rótulo de cada barra está como cada nível está escrito no banco de dados, por exemplo, a palavra não está sem acentuação. Isto pode ser corrigido, usando a função scale_x_discrete() com os argumentos limits = (coloca os níveis na ordem desejada) e labels = (coloca os novos nomes15 na ordem estabelecida pelo argumento limits =):\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.58: Gráfico de barras com nome e ordem dos rótulos alterados.\n\n\n\n\n\nObserve que a barra azul das não fumantes foi trocada de posição, passando a ser a primeira, para colocar a intensidade do tabagismo em ordem crescente (Figura 6.58).\n\n\n6.6.2.3 Título e subtítulo do gráfico\nNem sempre necessários, o título, o subtítulo ou uma nota de rodapé podem ser adicionados ao gráfico através da função labs(), usada anteriormente para colocar rótulos nos eixos X e Y, mas que também tem argumentos para colocar título, subtítulo e nota de rodapé (caption).\nAgora, será desenhado um gráfico com boxplots que ilustrem a influência do tabagismo materno sobre o peso do recém-nascido. Será usado o mesmo banco de dados dadosFumo.xlsx. Além de usar todos os argumento da função labs(), se repetirá o que foi feito na construção do gráfico da Figura 6.58, alterando os nomes e a posição das não fumantes. O gráfico (Figura 6.59) vai ser atribuído a um objeto denominado bxp:\n\nbxp &lt;- ggplot(dados_fumo, aes(x = quantFumo, \n                              y = pesoRN,\n                              fill = quantFumo)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.1) +\n  geom_boxplot() + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  stat_summary(fun = \"mean\", \n               colour = \"red\", \n               size = 3, \n               geom = \"point\") +\n  labs(title = \"Tabagismo Materno e Peso do Recém Nascido\", \n       subtitle = \"Maternidade do Hospital Geral de Caxias do Sul, 2010\",\n       x = \"Tabagismo Materno\",\n       y = \"Peso dos Recém-Nascidos (g)\",\n       caption = \"O ponto vermelho é a média de cada grupo\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                    labels = c(\"Não fumante\", \"Leve\", \n                               \"Moderado\", \"Pesado\")) +\n  theme(legend.position = \"none\")\n\nprint(bxp)\n\n\n\n\n\n\n\nFigura 6.59: Boxplots mostrando o impacto do tabagismo materno no peso do recém-nascido\n\n\n\n\n\nO gráfico da Figura 6.59 está com bom aspecto, mas alguém poderia dizer que gostaria de alterar o tamanho da fonte do título e do subtítulo, mudando inclusive a cor. É exagero? Poder ser, mas é possível! Para isso, existe, no ggplot2, uma função denominada theme() com múltiplos argumentos que exercem uma grande quantidade ações. Sugere-se consultar a ajuda (?theme())para maiores informações. Para realizar essas alterações, vamos começar com o objeto bxp 16 e adicionar os códigos com a função theme() para alterar a cor do título e subtítulo (Figura 6.60):\n\nbxp +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\"))\n\n\n\n\n\n\n\nFigura 6.60: Gráfico anterior com pequenas alterações na fonte e cores do título e subtítulo\n\n\n\n\n\n\n\n6.6.2.4 Modificação dos limites dos eixos\nO pacote ggplot2 possui uma família de funções scale_ para modificar as propriedades referentes às escalas do gráfico. Como é possível ter escalas de números, categorias, cores, datas, entre outras, é disponibilizada uma função específica para cada tipo de escala. Cada tipo fundamental é manipulado por uma das três funções construtoras de escala: continuous_scale(), discrete_scale() e binned_scale().\nNo gráfico da Figura 6.60, os pesos dos RNs estão dispostos em uma escala que varia a cada 1000 g. Para modificar esses limites, pode-se usar a função scale_y_continuous() para ter intervalos de 500 g:\n\nbxp +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\")) +\n  scale_y_continuous(breaks = seq(1500, 5000, 500)) \n\n\n\n\n\n\n\nFigura 6.61: Gráfico anterior com modificação da escala do eixo Y\n\n\n\n\n\nAgora, na Figura 6.61, o eixo Y contém intervalos de 500 g. Este mesmo grafico já é um exemplo de modificação do eixo X com a função scale_x_discrete() para mudar a ordem dos níveis originais, realizado na construção do gráfico da Figura 6.59.\n\n\n6.6.2.5 Modificação da expansão\nVoltando ao gráfico da Figura 6.58, observe que abaixo do valor 0 (zero) existe uma expansão. Isto, visualmente, é desagradável (pelo menos para o autor). Para que as barras tenham início exatamente no 0 (zero), pode-se empregar a função scale_y_continuous() com o argumento expand = expansion (add = c(0,50)), significando que não se expande nada abaixo do 0 e se adiciona 50 unidades para cima, criando uma margem superior (Figura 6.62).\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  ylab(\"Proporção por categoria\") +\n  xlab(\"Tabagismo Materno\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.62: Gráfico de barra sem a expansão abaixo de 0 (zero)\n\n\n\n\n\n\n\n6.6.2.6 Proporção ou percentagem nos eixos\nNa Figura 6.62, a unidade do eixo Y encontra-se como um proporção y = after_stat(count/sum(count). É possível modificar para porcentagem (Figura 6.63), empregando a função percent_format() do pacote scales. O código é praticamente igual, apenas acrescentar o argumento labels = percent_format(accuracy = 0.1, decimal.mark = “,”) dentro da função scale_y_continuous().\n\nlibrary(scales)\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count)),\n               fill = quantFumo)) + \n  scale_fill_manual(values = c(\"lightsalmon1\", \"lightsalmon3\",  \n                               \"lightsalmon4\", \"lightblue\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05)),\n                      labels = percent_format (accuracy = 0.1,\n                                               decimal.mark = \",\")) +\n  scale_x_discrete(limits = c(\"fumante_leve\", \"fumante_moderada\", \n                              \"fumante_pesada\", \"nao_fumante\"),\n                   labels = c(\"Leve\", \"Moderado\",\n                              \"Pesado\", \"Não fumante\")) +\n  labs(x = \"Tabagismo Materno na Gestação\",\n       y = \"Proporção em cada categoria\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.63: Gráfico de barra com eixo Y em porcentagem\n\n\n\n\n\n\n\n\n6.6.3 Modificação das cores\nNa Seção 6.6.1.2, foi introduzido o uso de cores no R. Agora, apesar deste tema praticamente não ter limites, serão mostrados alguns princípios do manuseio das cores no ggplot2.\nNo gráfico de barras da Figura 6.63, as cores foram selecionadas com a função scale_fill_manual(). A escolha das cores pode ser feita especificando o seu nome em inglês. Essa escolha é pessoal. O R possui 657 cores integradas que permitem uma gama ampla de opções. Uma outra maneira de especificar as cores, é usar o sistema RGB ou hexadecimal. O código hexadecimal da cor branca é #FFFFFFF, da “gray58” é #949494, da “yellow4” é #999900, etc. Opcionalmente, a cor pode ser transparente, usando o formato “#RRGGBBAA”. Alpha refere-se à transparência de um geom. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes. Alpha também pode ser modificada por meio da estética de colour ou fill se qualquer uma das estéticas fornecer valores de cor usando uma especificação RGB.\nUsando este mesmo gráfico, iremos mudar a cor, usando a cor cinza-claro para o preenchimento das barras (fill = “gray70”) e vermelho-escuro para bordas das barras (color = “darkred”), colocados fora da aes(), pois se forem colocadas dentro, as cores ficarão diferentes dentro do padrão do ggplot2 (Figura 6.64).\n\nlibrary(scales)\n\nggplot(data = dados_fumo) +\n  geom_bar(aes(x = quantFumo, \n               y = after_stat(count/sum(count))),\n               fill = \"gray70\",\n               color = \"darkred\") + \n  labs(x = \"Tabagismo Materno na Gestação\",\n       y = \"Percentagem em cada categoria\") +\n  scale_x_discrete(limits = c(\"nao_fumante\", \n                              \"fumante_leve\", \n                              \"fumante_moderada\", \n                              \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05)),\n                      labels = percent_format(accuracy = 0.1,  \n                                              decimal.mark = \",\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.64: Gráfico de barras com cores modificadas fora da estética do geom.\n\n\n\n\n\n\n6.6.3.1 Cores de acordo com uma determinada paleta\nO ggsci é um pacote que oferece uma coleção de paletas de alta qualidade inspiradas em cores usadas em revistas científicas, bibliotecas de visualização de dados, filmes de ficção científica e programas de TV. As paletas de cores no ggsci estão disponíveis como escalas ggplot2. Para todas usa-se as seguintes funções: scale_color_palname() e scale_fill_palname(). Por exemplo, para a paleta do Lancet, usa-se para o preenchimento: scale_fill_lancet() . O pacote ggsci deve ser instalado e carregado para usar estas paletas.\nComo base, será usado o código que gerou o gráfico da Figura 6.61 com alterações, usando a paleta do periódico Lancet (Figura 6.65).\n\nlibrary(ggsci)\n\nbxp1 &lt;- ggplot(dados_fumo, \n               aes(x = quantFumo, y = pesoRN, fill = quantFumo)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.1) +\ngeom_boxplot() + \n  scale_fill_lancet() +\n  stat_summary(fun = \"mean\", colour = \"white\", size = 2, geom = \"point\") +\n  labs(title = \"Tabagismo Materno e Peso do Recém Nascido\", \n       subtitle = \"Maternidade do Hospital Geral de Caxias do Sul, 2010\",\n       x = \"Tabagismo Materno\",\n       y = \"Peso dos Recém-Nascidos (g)\",\n       caption = \"O ponto branco é a média de cada grupo\") +\n  theme(plot.title = element_text(size = 14,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 12,\n                                     face = \"bold\",\n                                     color = \"darkgreen\")) +\n  scale_x_discrete(limits = c(\"nao_fumante\", \"fumante_leve\", \n                              \"fumante_moderada\", \"fumante_pesada\"),\n                   labels = c(\"Não fumante\", \"Leve\", \n                              \"Moderado\", \"Pesado\")) +\n  scale_y_continuous(breaks = seq(1500, 5000, 500)) +\n  theme(legend.position = \"none\")\nprint(bxp1)\n\n\n\n\n\n\n\nFigura 6.65: Gráfico com paleta de cores do Lancet.\n\n\n\n\n\n\n\n\n6.6.4 Mudança dos temas\nO tema padrão do ggplot2 tem uma aparência acinzentada que pode ser modificada pela definição de outro tema integrado, como o theme_bw(), que é uma variação de theme_grey(), que usa um fundo branco e linhas finas de grade cinza . Outro tema interessante é o theme_classic() que é um tema de aparência clássica, com linhas dos eixos X e Y e sem linhas de grade . Para ver outras possibilidades acesse Completes themes - ggplt2.\nSerá repetido o boxplot da Figura 6.65, adicionando o theme_bw() (Figura 6.66) e, após, o theme_classic()(Figura 6.67).\n\nbxp1 +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.66: Gráfico da Figura 36 com o theme_bw().\n\n\n\n\n\n\nbxp1 +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 6.67: Gráfico da Figura 36 com o theme_classic().\n\n\n\n\n\n\n\n6.6.5 Estudo de caso: Gráfico de barra de erro\nUm gráfico de barra de erro (Figura 6.68) é uma ferramenta visual que mostra a variabilidade de dados em um ponto específico (veja também Seção 6.5.3). Ele consiste em pontos ou barras que representam as médias (ou outras estatísticas) de um conjunto de dados, com linhas verticais (ou horizontais) que indicam o intervalo de confiança, o desvio padrão ou o erro padrão da média. Essas linhas verticais são conhecidas como “barras de erro”. Usado para comparar as médias de diferentes grupos, mostrando a variabilidade dentro de cada grupo. É visto com frequência em pesquisas científicas e publicações para apresentar os resultados experimentais com suas respectivas variabilidades.\n\n6.6.5.1 Caso\nPara visualizar a influência do sexo e do tabagismo materno no peso do recém-nascido, será usado um gráfico de barras de erro, onde a representação das colunas (barras) e as barras de erro com intervalo de confiança de 95%, calculado usando média ± margem de erro, onde a margem de erro = 1.96 × erro padrão.\n\n\n6.6.5.2 Dados\n\nset.seed(12)\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;%   filter(ig &gt; 37 & ig &lt;= 42) %&gt;% \n  select(id, fumo, sexo, pesoRN) %&gt;% \n  slice_sample(n = 100)\n\nExploração e transformação dos dados\n\ndados$sexo &lt;- factor(dados$sexo, \n                     levels = c(1, 2),\n                     labels = c(\"Masculino\", \"Feminino\"))\n\ndados$fumo &lt;- factor(dados$fumo,\n                     levels = c(1, 2),\n                     labels = c(\"fumante\", \"nao_fumante\"))\n\nresumo &lt;- dados %&gt;% \n  group_by(sexo, fumo) %&gt;% \n  summarise(n = n(),\n            media = mean(pesoRN, na.rm = TRUE),\n            dp = sd(pesoRN, na.rm = TRUE),\n            me = 1.96 * dp/sqrt(n),\n            .groups = 'drop')\nprint(resumo)\n\n# A tibble: 4 × 6\n  sexo      fumo            n media    dp    me\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Masculino fumante        11 3258.  544.  322.\n2 Masculino nao_fumante    47 3217.  409.  117.\n3 Feminino  fumante         6 2920   384.  307.\n4 Feminino  nao_fumante    36 3200.  413.  135.\n\n\n\n\n6.6.5.3 Gráfico de barra de erro\n\nggplot(resumo, \n       aes(x=sexo, y=media, fill=fumo)) + \n  geom_bar(stat=\"identity\", color=\"black\", \n           position=position_dodge(0.9)) +\n  geom_point(position=position_dodge(0.9)) +\n  geom_errorbar(aes(ymin=media, ymax=media+me), width=0.2,\n                position=position_dodge(.9)) +\n  labs(x=\"Sexo\", \n       y = \"Peso do Neonato(g)\",\n       fill = \"Tabagismo\") +\n  coord_cartesian(ylim = c(0, 3600),\n                  expand = TRUE) +\n  scale_fill_manual(values = c(\"nao_fumante\" = \"darkslategray1\", \n                               \"fumante\" = \"gray80\"),\n                    labels = c(\"nao_fumante\" = \"Não fumante\", \n                               \"fumante\" = \"Fumante\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +\n  theme_classic()\n\n\n\n\n\n\n\nFigura 6.68: Gráfico de barra de erro.\n\n\n\n\n\n\n\n\n\n1. Field A, Miles J, Field Z. Everithing you ever wanted to know about statistics (well, sort of). Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 38. \n\n\n2. Bowers D. First things first-the nature of data. Em: Medical Statistics from Scratch. Second Edition. John Wiley; Sons; 2008. p. 3–13. \n\n\n3. Arango HG. Organização dos dados em tabelas. Em: Bioestatística: teórica e computacional. 3ª edição. Guanabara Koogan; 2009. p. 32–57. \n\n\n4. Oliveira Filho PF de. Tabelas. Em: Epidemiologia e Bioestatística-Fundamentos para a Leitura Crítica. 2ª edição. Editora Rubio; 2022. p. 9–12. \n\n\n5. Gohel D, Skintzos P. flextable: Functions for Tabular Reporting [Internet]. 2024. Disponível em: https://CRAN.R-project.org/package=flextable\n\n\n6. Arango HG. Números de classes e Intervalo de Classes. Em: Bioestatística teórica e computacional. Terceira edição. Guanabara Koogan, RJ; 2009. p. 35–40. \n\n\n7. Rasmussen KM, Yaktine AL, et al. Weight gain during pregnancy: reexamining the guidelines. 2009; \n\n\n8. Field A, Miles J, Field Z. Exploring data with graphs. Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 117. \n\n\n9. Wickham H. Getting Started with ggplot2. Em: ggplot2. Second edition. Springer; 2016. p. 11–31. \n\n\n10. Tufte ER. Aesthetics and Technique in Data Graphical Design. Em: The Visual Display of Quantitative Information. Second edition. Graphics Press; 2001. p. 178. \n\n\n11. Lemon J, Bolker B, Oom S, et al. Package «plotrix». Vienna: R Development Core Team. 2015; \n\n\n12. Kabacoff RI. Basic graphs. Em: R in Action: Data analysis and graphics with R. Manning Publications Co.; 2011. p. 120–4. \n\n\n13. Harrell FE, Dupont C. Hmisc: Harrell Miscellaneous [Internet]. R package version. 2022. Disponível em: https://cran.r-project.org/web/packages/Hmisc/index.html\n\n\n14. Wickham H. ggplot2: Elegant Graphics for Data Analysis [Internet]. Springer-Verlag New York; 2016. Disponível em: https://ggplot2.tidyverse.org\n\n\n15. Wickham H. A layered grammar of graphics. Journal of Computational and Graphical Statistics. 2010;19(1):3–28.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "06-descrevendoDados.html#footnotes",
    "href": "06-descrevendoDados.html#footnotes",
    "title": "6  Descrevendo os dados",
    "section": "",
    "text": "Usa-se esta sintaxe: round(x, digits = 0), onde x é o numero que se quer arredondar e digits é número de casas decimais. O padrão é 0, ou seja, arredonda para o inteiro mais próximo.↩︎\nPara maiores detalhes: https://ardata-fr.github.io/flextable-book/index.html ou consulte a ajuda digitando no Console ?flextable() ou help(flextable).↩︎\nPoderiam ser transformados em fatores sem trocar os rótulos e manter os números 1 e 2, como se fossem palavras. O autor prefere usar nomes.↩︎\nFoi escolhido o nome x, porque lembra o eixo X, onde estão as barras↩︎\npara lembrar o eixo Y↩︎\nOutros métodos: “loess”, “gam”, “rlm”. Para mais informações, consulte a ajuda ?loess, ?gam ou ?rlm↩︎\npadrão = 1.5↩︎\nhttp://www.sthda.com/english/wiki/ggplot2-point-shapes↩︎\nObserve que não foi usado o argumento se = FALSE. Por isso, aparece o intervalo de confiança de 95%.↩︎\nÉ claro, que este aspecto tem uma conotação pessoal. Em publicações, consulte as normas do periódico↩︎\nVeja mais detalhes na Seção 6.6.3↩︎\nO padrão é width = 0.40. Usando width = 0, teremos uma distribuição dos pontos em uma mesma posição, como o gráfico inicial do Boxplot↩︎\nO padrão é stat = \"identity\", o que significa que os valores das barras de erro devem ser fornecidos diretamente no conjunto de dados, sem cálculos adicionais↩︎\nEsta função pode ser utilizada para outros gráficos, consulte a ajuda↩︎\nPodemos aproveitar aqui para trocar os nomes ou , simplesmente, corrigir acentuação que, às vezes, não foi colocada nos níveis↩︎\nEste procedimento de atribuir a um objeto os comandos que constroem um gráfico, facilita a digitação, pois não é necessário repetir os códigos da Figura @ref(fig:ggbxp4)↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Descrevendo os dados</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html",
    "href": "07-probabilidades.html",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "7.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr, readxl)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#introdução",
    "href": "07-probabilidades.html#introdução",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.2 Introdução",
    "text": "7.2 Introdução\nA teoria das probabilidades é a base sobre a qual a estatística é desenvolvida. Os jogos de azar deram um grande impulso ao conhecimento da moderna teoria das probabilidades, principalmente, pelo trabalho de Blaise Pascal (1623-1662), em parceria com Pierre de Fermat (1601-1665). Eles foram estimulados por um escritor francês e matemático amador, Antoine Gombaud (1607-1684), conhecido como Chevalier de Méré, que era muito interessado em jogos de azar (1).\nA Teoria das probabilidades permite que seja possível modelar populações, experimentos ou qualquer situação que possa ser considerada aleatória. Estes modelos possibilitam fazer inferência sobre populações a partir da observação de uma amostra dessa população. Ao usar apenas uma parte da população, inevitavelmente, é cometido um erro, o erro amostral. Este erro amostral pode ser dimensionado pela teoria das probabilidades.\nExistem duas interpretações alternativas de probabilidades: a frequentista e a bayesiana (2). Neste livro, será discutida, basicamente, a definição de probabilidade frequentista. O processo bayesiano de formulação de um modelo probabilístico faz uso do conhecimento subjetivo, estabelecendo uma especificação a priori, combinado com a informação objetiva ou empírica. A teoria bayesiana é a estrutura integradora dessas duas fontes de informação, derivando como resultado a distribuição a posteriori dos parâmetros de interesse. Na Seção 18.2, sobre análise de testes diagnósticos, serão abordados alguns aspectos relacionados à teoria bayesiana em medicina.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#processo-aleatório",
    "href": "07-probabilidades.html#processo-aleatório",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.3 Processo aleatório",
    "text": "7.3 Processo aleatório\nUm processo ou experimento é dito aleatório quando em uma situação se sabe quais os resultados que podem acontecer, mas não se sabe qual resultado particular irá acontecer. Por exemplo, quando uma moeda é lançada, se conhece que a probabilidade de o desfecho cara ocorrer é de 50%, mas se desconhece o que irá ocorrer até que a moeda esteja no chão.\nO número de caras que podem surgir em vários lançamentos da moeda é chamado de variável aleatória, ou seja, uma variável que pode assumir mais de um valor com determinadas probabilidades (3). Da mesma forma, um dado lançado pode mostrar seis faces, numeradas de um a seis, com igual probabilidade de 16,7%. Portanto, quando a probabilidade é associada a todos os conjuntos de valores possíveis de uma variável, diz-se que ela é aleatória. O conjunto de todos os possíveis resultados de um experimento aleatório é denominado espaço amostral.\nNa área da saúde, trabalha-se com uma infinidade de variáveis aleatórias, por exemplo, o número de filhos de uma mulher, o número de mortos diários em uma epidemia, o número de vacinados em uma campanha, etc. Essas variáveis são variáveis aleatórias discretas, pois apenas permitem ser quantificadas por processo de contagem. Por outro lado, o peso ou a altura de uma mulher são ditos variáveis aleatórias contínuas, pois podem assumir qualquer valor real entre uma medida e outra, dependendo da precisão do aparelho usado.\nEm geral, variáveis aleatórias são representadas por letras maiúsculas, como X, Y e Z e sua a probabilidade, por exemplo, pode ser denotada por: \\(P(X)\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#definição-frequentista",
    "href": "07-probabilidades.html#definição-frequentista",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.4 Definição frequentista",
    "text": "7.4 Definição frequentista\nA probabilidade se relaciona a eventos futuros ou que ainda não ocorreram, desta forma a probabilidade pode ser entendida como uma medida de incerteza em relação ao evento. A probabilidade de um evento ocorrer, em determinadas circunstâncias, pode ser definida como a proporção de vezes que o evento é observado quando o experimento é repetido um número infinitamente grande de vezes (2). Pode-se dizer que a visão frequentista define a probabilidade como uma frequência de longo prazo.\nA chamada Lei dos Grandes Números diz que à medida que múltiplas observações são coletadas, a proporção observada de ocorrências de um determinado desfecho, após n ensaios, converge para a probabilidade real P desse desfecho. Ou seja, quanto mais vezes for repetido uma experiência, a melhor estimativa de probabilidade tende a ocorrer. Suponha que seja lançada uma moeda honesta repetidas vezes. Por definição, essa é uma moeda que tem \\(P(cara)=0,5\\). O que se observaria? O autor fez 20 lançamentos seguidos com uma mesma moeda e obteve o seguinte resultado, onde 1 = cara (Figura 7.1):\n\n\n\n\n\n\n\n\nFigura 7.1: 20 lançamentos seguidos de uma moeda\n\n\n\n\n\nNeste caso, 10 (50%) desses lançamentos deram cara. Agora, suponha que foram feitos registros do número de caras (\\(n_1\\)) dos primeiros lançamentos (N) e calculadas as proporções de caras (\\(n_1⁄N\\)) todas as vezes. O resultado está na ?fig-propmoeda.\n\n\n\n\n\nProporção em 20 lançamentos de moeda\n\n\n\n\nObserva-se, nessa sequência, que a proporção de caras flutua muito, variando de 0,17 a 0,75. Se o número de lançamentos for aumentando tem-se a sensação de que a proporção se aproxima da “correta”. Por exemplo, com 100 jogadas, obteve-se 53 caras (0,53); com 150 jogadas, 79 (0,53) e com 200 jogadas, 111 (0,56). Quando N se aproximar do infinito (\\(N \\to\\infty\\)) a proporção de caras convergirá para 0,50. A definição frequentista de probabilidade segue essa definição. Ninguém consegue um número infinito de lançamentos de moedas, mas um computador pode simular milhares de lançamentos. A Figura 7.2 mostra o que acontece com a proporção \\(n_1⁄N\\) à medida que N aumenta em lançamentos de moedas. As simulações foram repetidas 4 vezes somente para ter certeza de que o que aconteceu não foi obra do acaso.\n\n\n\n\n\n\n\n\nFigura 7.2: Proporção à medida que N aumenta em lançamentos de moedas\n\n\n\n\n\nEmbora nenhuma das simulações tenha realmente terminado com um valor exato de 0,5, elas se aproximaram, oscilando muito pouco em torno desse valor.\n\n7.4.1 Aplicando a visão frequentista no dia a dia\nA definição frequentista também pode ser aplicada no cotidiano. Utilizando a altura de 1368 mulheres, uma medida numérica contínua, incluída no conjunto de dados dadosMater.xlsx (veja Seção 5.3). Essas alturas serão selecionadas e colocadas em um objeto, denominado dados.\n\n dados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nUsando a função summary(), será feito um resumo da variável altura:\n\nsummary(dados$altura)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.400   1.550   1.600   1.598   1.650   1.850 \n\n\nA mediana da altura das gestantes é 1.6 m. Em um longo conjunto de sorteios, a probabilidade de uma mulher ter altura acima devalor é 50%. O percentil 75 (3º quartil) é igual a 1.65 m, a probabilidade de estar acima deste valor, portanto, é 25%. É possível encontrar a probabilidade de a altura estar acima, abaixo ou entre quaisquer valores. Quando se faz a mensuração de uma variável contínua, fica-se limitado ao método usado. Portanto, quando se diz que uma mulher tem 160 cm, significa dizer que está entre 159,5 e 160,5 cm, dependendo da precisão do instrumento de medição. Dessa maneira, o interesse está na probabilidade de a variável aleatória assumir valores entre certos limites.\nA probabilidade de encontrar um valor exatamente igual à média (159.8) cm é quase igual a zero. Como se verá a seguir, isto pode ser verificado, no R, com bastante facilidade,calculando a distância que esta medida está da média em número de desvios padrão (escore Z):\n\nZ &lt;- (1.60 - mean(dados$altura))/sd(dados$altura)\nZ\n\n[1] 0.03103551\n\n\nObserve que o valor de 1,60 m está muito próximo da média e isto é um indicativo de que essa variável tem uma distribuição praticamente simétrica. Sabendo a distância, em números de desvios padrão, que 1,60 m está da média, qual a probabilidade de encontrar, na maternidade do HGCS 1, uma parturiente que tenha exatamente esta altura?\nPara responder a essa pergunta, será usada a função pnorm() (veja adiante na Seção 7.7.2) que utiliza o escore Z, a média e o desvio padrão para encontrar essa proporção que, multiplicada por 100, fornece a percentagem.\n\n p &lt;- pnorm (Z, mean(dados$altura),sd(dados $altura))\n p\n\n[1] 7.387473e-127\n\n\nO R por padrão retorna números grandes como notação científica. O resultado dessa operação é um número tão grande que para escrevê-lo sem este tipo de notação, seriam necessários 127 dígitos decimais. O resultado não caberia em apenas uma linha. Ficaria assim, suprimindo a notação científica 2:\n\n options(scipen =999)\n p\n\n[1] 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000007387473\n\n options(scipen = 0)\n\nOu seja, um número tão próximo de zero que poderia muito bem ser zero!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#propriedades-das-probabilidades",
    "href": "07-probabilidades.html#propriedades-das-probabilidades",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.5 Propriedades das probabilidades",
    "text": "7.5 Propriedades das probabilidades\nAs seguintes propriedades simples decorrem da definição de probabilidade.\nSendo E um evento aleatório, a \\(P[E]\\) está entre 0 e 1, ou seja \\(0\\le P[E]\\le 1\\). Quando o evento certamente não ocorre, a probabilidade é 0, quando sempre ocorre a probabilidade é 1. Quando a probabilidade for igual a 0,50 tem-se máxima incerteza.\n\nRegra de adição (regra do “ou”)\n\nDois eventos A e B são mutuamente exclusivos, ou seja, quando A acontece, B não pode acontecer. Então, a probabilidade de que um ou outro aconteça é a soma de suas probabilidades. Por exemplo, um dado lançado pode mostrar um ou dois, mas não ambos. A probabilidade de mostrar um ou dois é igual a \\(1/6 + 1/6 = 1/3\\).\n\\[\nP[A ou B]=P[A]+P[B]\n\\]\nSe A e B não são mutuamente exclusivos, ou seja, quando A acontece pode também ocorrer B. Por exemplo, o nascimento de uma menina pode ser concomitante com o fato de ser branca.\n\\[\nP[A ou B]=P[A]+P[B]-P[A \\space e \\space B]\n\\]\n\nRegra de multiplicação (regra do “e”)\n\nSuponha que dois eventos (A e B) sejam independentes, ou seja, saber que um aconteceu não nos diz nada sobre se o outro aconteceu. Então, a probabilidade de que ambos aconteçam é o produto de suas probabilidades. Por exemplo, suponha que jogamos duas moedas. Uma moeda não influencia a outra, portanto os resultados dos dois lançamentos são independentes e a probabilidade de ocorrerem duas caras é 050 × 0,50 = 0,25.\n\\[\nP[A \\quad e\\quad B]=P[A]×P[B]\n\\]\nSe os eventos são dependentes, a probabilidade que ambos aconteçam é igual a:\n\\[\nP[A \\quad e \\quad B]=P[A]×P[B \\rvert A]\n\\] Com essas propriedades simples e outras mais complexas, é possível construir algumas ferramentas matemáticas extremamente poderosas, mas isso não faz parte do objetivo deste livro e não se entrará em detalhes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-de-probabilidades",
    "href": "07-probabilidades.html#distribuição-de-probabilidades",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.6 Distribuição de Probabilidades",
    "text": "7.6 Distribuição de Probabilidades\nUm conjunto de eventos que são mutuamente excludentes e que inclui todos os eventos que podem acontecer, é chamado de exaustivo. A soma de suas probabilidades é 1. O conjunto dessas probabilidades constitui uma distribuição de probabilidade.\nExistem diversos modelos probabilísticos que procuram descrever vários tipos de variáveis aleatórias discretas ou contínuas. Estas distribuições também são chamadas de modelos probabilísticos estocásticos que são definidas por duas funções matemáticas: a função de probabilidade (fp) para variáveis discretas, que atribui a cada valor a sua probabilidade de ocorrência (P(X=x)) e função densidade de probabilidade (fdp) para variáveis contínuas.\nA função de probabilidade é a função que atribui probabilidades a cada um dos possíveis valores da variável aleatória discreta, usando, em geral, as frequências relativas, apresentadas em uma tabela de frequência. O modelo de Bernoulli ou Binomial e o modelo de Poisson são exemplos de modelo probabilístico de variáveis discretas.\nA função densidade de probabilidade é a função que atribui probabilidade a qualquer intervalo de número reais, ou seja, um conjunto de valores não enumerável (infinito). Não é possível atribuir probabilidades para um determinado valor, é possível apenas para um intervalo. Por exemplo, o peso dos recém-nascidos. Para atribuir probabilidade a intervalos de valores é utilizada uma função e as probabilidades são representadas por áreas. Existem diversos modelos contínuos de probabilidade, mas o mais importante deles, é o modelo normal, também conhecido como modelo gaussiano.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#sec-normal",
    "href": "07-probabilidades.html#sec-normal",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.7 Distribuição Normal",
    "text": "7.7 Distribuição Normal\nO modelo probabilístico normal ou gaussiano é extremamente importante em estatística, pois serve como um fundamento para técnicas de inferência. Variáveis como os pesos dos recém-nascidos a termo, as alturas das mulheres adultas, a renda familiar em reais e muitas outras variáveis, na natureza, se ajustam ao modelo da distribuição normal.\nO modelo de distribuição normal sempre descreve uma curva simétrica, unimodal e em forma de sino (Figura 7.3).\n\n\n\n\n\n\n\n\nFigura 7.3: Curva normal.\n\n\n\n\n\nUma distribuição normal é descrita por meio de dois parâmetros: a média da distribuição \\(\\mu\\) e o desvio padrão da distribuição \\(\\sigma\\). Em função dessa informação, observe como a distribuição normal funciona se esses parâmetros forem alterados.\nComo é fácil prever, alterar a média desloca a curva de sino para a esquerda ou para a direita, enquanto a alteração do desvio padrão estende ou achata a curva, ou seja, muda a dispersão da distribuição.\nA Figura 7.4, mostra a distribuição normal com média 0 e desvio padrão 1, na curva à direita, a distribuição normal com média 1.5 e desvio padrão 1. Sobrepondo-se à curva da esquerda observa-se uma curva mais achatada (verde) que tem média 0 e desvio padrão 1.5. Observa-se, como mencionado, que modificando os parâmetros da curva, altera-se a posição ou o formato da mesma.\n\n\n\n\n\n\n\n\nFigura 7.4: Curvas normais com modificação dos parâmetros.\n\n\n\n\n\n\n7.7.1 Características da distribuição normal\nA curva normal apresenta as seguintes características:\n\nA média e o desvio padrão descrevem exatamente uma distribuição normal, eles são chamados de parâmetros da distribuição. Se uma distribuição normal tem média \\(\\mu\\) e desvio padrão \\(\\sigma\\), pode-se escrever a distribuição como \\(N (\\mu,\\sigma)\\). As três distribuições dos gráficos da Figura 7.4 podem ser escritas como:\n\nCurva azul \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1)\\)\nCurva verde \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1.5)\\)\nCurva vermelha \\(\\to\\) \\(N(\\mu = 1.5,\\sigma = 1)\\)\n\nNa distribuição normal, a média, a mediana e a moda coincidem.\nA curva normal é simétrica em torno da média (\\(\\mu\\)).\nAs extremidades da curva, em ambos os lados da média, se estendem cada vez mais próximas do eixo x (abscissa) sem jamais tocá-lo. É assintótica.\nOs pontos de inflexão da curva são \\(\\mu - \\sigma\\) e \\(\\mu + \\sigma\\).\nA área total sob a curva é 1 ou 100%.\n\n\n\n7.7.2 Distribuição normal padronizada\nCada variável aleatória contínua tem a sua média e seu desvio padrão e, portanto, a sua curva normal correspondente.\nPara facilitar a comparação entre variáveis, foi criado o conceito de curva normal padronizada, que é uma curva normal com média 0 e desvio padrão 1. A distribuição normal padrão também pode ser chamada de distribuição normal centrada ou reduzida.\nPara calcular probabilidades associadas a distribuição normal, costuma-se converter a variável aleatória original X, em unidades reduzidas ou padronizadas, denominadas de escore Z ou escore padrão. Essa transformação é realizada pela equação que indica o número de desvios padrão envolvidos no afastamento do valor x em relação à média da população:\n\\[\nZ =\\frac{x-\\mu}{\\sigma}\n\\] onde:\n\nZ \\(\\to\\) escore Z\n\nx \\(\\to\\) valor qualquer da variável aleatória X\n\n\\(\\mu\\) \\(\\to\\) média da variável X\n\n\\(\\sigma\\) \\(\\to\\) desvio padrão da variável X\n\nQualquer distribuição de uma variável aleatória normal pode ser padronizada, usando o escore Z. Isto permite que se calcule a probabilidade de se encontrar determinados intervalos de valores (4).\nComo exemplo, se retornará à altura das mulheres. É, praticamente, impossível saber o valor da média populacional, por isso. costuma-se usar a média aritmética como um estimador da média populacional. Dessa forma, a variável dados$altura poderá utilizada com estimativa da média populacional. Em primeiro lugar, se construirá um tibble de nome resumo:\n\n resumo &lt;- dados %&gt;% \n   dplyr::summarise(n = n(),\n                    media = mean(altura, na.rm = TRUE),\n                    dp = sd(altura, na.rm = TRUE),\n                    min = min(altura, na.rm = TRUE),\n                    max = max(altura, na.rm = TRUE))\n resumo\n\n# A tibble: 1 × 5\n      n media     dp   min   max\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1368  1.60 0.0655   1.4  1.85\n\n\nAssim, pode-se verificar quantos desvios padrão uma mulher, pertencente a essa amostra, com 1,725m está afastada da média:\n\nZ &lt;- (1.725 - resumo$media)/resumo$dp\nround(Z, 2)\n\n[1] 1.94\n\n\nEsta mulher está distante praticamente 2 desvios padrão acima da média da sua população. Portanto, ela é considerada alta. Por que?\nPara responder a essa pergunta, há necessidade de calcular a probabilidade de encontrar uma mulher com esta altura, nesta população. Primeiro, calcula-se a probabilidade de encontrar uma mulher com esta altura, nesta amostra. No R, existem as funções dnorm(), pnorm() e qnorm(), que permitem calcular a densidade de probabilidade, a distribuição cumulativa e a função quantílica da distribuição normal para um conjunto de valores. Além dessas, há a função rnorm() que permite obter observações aleatórias que seguem uma distribuição normal@jain_2022norm.\n\n7.7.2.1 Função pnorm()\nA função pnorm() fornece a Função de Distribuição Cumulativa (CDF) da distribuição Normal, que é a probabilidade de que a variável X contenha um valor menor ou igual a x.\nArgumentos:\n\nq \\(\\rightarrow\\) vetor de quantis\n\nmean \\(\\rightarrow\\) média\n\nsd \\(\\rightarrow\\) desvio padrão\n\nlower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são \\(P(X\\le x)\\), caso contrário \\(P(X &gt; x)\\)\n\nSe for usado \\(mean = 0\\) e \\(sd = 1\\), o valor de q = Z, caso contrário, toma-se os valores da média, o desvio padrão da população e o valor de x. Com esta função, é possível responder a pergunta feita anteriormente em relação a probabilidade de encontrar uma mulher com mais de 1,725m, equivalente a 1.94 desvios padrão acima da média, em uma população com média = 1.5979678 e desvio padrão = 0.0654787.\n\np &lt;- pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\np\n\n[1] 0.02618654\n\n\nOu, usando os valores:\n\npnorm(1.725, mean = resumo$media, sd = resumo$dp, lower.tail = FALSE)\n\n[1] 0.02618654\n\n\nObserva-se que, nesta amostra, apenas 2.6% das mulheres têm acima de 1,725m, razão de ser considerada uma mulher alta. Ou seja, é pouco provável encontrar mulheres acima dessa altura, nesta amostra.\nPara representar graficamente essa pequena probabilidade, será construída uma curva com essa pequena área sombreada, colorida em vermelho. Para isso, será feito uso de uma função própria, denominada normal_area(). Ela pode ser obtida aqui para ser baixada em seu diretório de trabalho para uso posterior 3.\nA Figura 7.5 representa com clareza esta pequena probabilidade. Foi usada a função text() para escrever o valor da probabilidade.\n\nsource(\"dados/normal_area.R\")\nnormal_area(media = 0, dp = 1, linf = 1.94, lsup = 3, cor = \"tomato\", lwd = 2 )\ntext(2.6, 0.05, \"2.6%\")\n\n\n\n\n\n\n\nFigura 7.5: Probabilidade de encontrar mulheres com mais de 1,725m\n\n\n\n\n\n\n\n7.7.2.2 Função qnorm()\nA função qnorm() permite encontrar o quantil q para qualquer probabilidade p. Portanto, a função qnorm é o inverso da função pnorm().\nArgumentos:\n\np \\(\\to\\) vetor de probabilidades\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nNo exemplo anterior, a probabilidade de se encontrar mulheres, na maternidade, com mais de 1,725m foi de 2.6%. Poderia ser calculado com a função qnorm() qual o escore Z correspondente:\n\nqnorm(p, mean = 0, sd = 1, lower.tail = FALSE)\n\n[1] 1.940054\n\n\n\n\n7.7.2.3 Função dnorm()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição normal dada uma certa variável aleatória X, uma média populacional \\(\\mu\\) e o desvio padrão populacional \\(\\sigma\\).\nArgumentos:\n\nx \\(\\to\\) vetor de quantis\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nEmbora x represente a variável independente da pdf para a distribuição normal, também é útil pensar em x como um escore Z. Por exemplo, a densidade de probabilidade quando x = 0 é igual:\n\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\nPara se construir uma curva de densidade de probabilidades normal ( \\(X \\sim N(μ=0,σ=1)\\)), basta aplicar a função dnorm() a uma sequência contínua de escores Z. O vetor de escores Z é obtido com a função seq(), como mostrado a seguir:\n\nescores_z &lt;- seq(-3,3, by = 0.05)\nescores_z\n\n  [1] -3.00 -2.95 -2.90 -2.85 -2.80 -2.75 -2.70 -2.65 -2.60 -2.55 -2.50 -2.45\n [13] -2.40 -2.35 -2.30 -2.25 -2.20 -2.15 -2.10 -2.05 -2.00 -1.95 -1.90 -1.85\n [25] -1.80 -1.75 -1.70 -1.65 -1.60 -1.55 -1.50 -1.45 -1.40 -1.35 -1.30 -1.25\n [37] -1.20 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65\n [49] -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05\n [61]  0.00  0.05  0.10  0.15  0.20  0.25  0.30  0.35  0.40  0.45  0.50  0.55\n [73]  0.60  0.65  0.70  0.75  0.80  0.85  0.90  0.95  1.00  1.05  1.10  1.15\n [85]  1.20  1.25  1.30  1.35  1.40  1.45  1.50  1.55  1.60  1.65  1.70  1.75\n [97]  1.80  1.85  1.90  1.95  2.00  2.05  2.10  2.15  2.20  2.25  2.30  2.35\n[109]  2.40  2.45  2.50  2.55  2.60  2.65  2.70  2.75  2.80  2.85  2.90  2.95\n[121]  3.00\n\n\nAgora, usando a função dnorm(), será construído conjunto de valores de densidade de probabilidade correspondentes aos escores Z obtidos anteriormente:\n\nvalores_d &lt;- dnorm(escores_z, mean = 0, sd = 1)\n\nEstes valores serão plotados para construir a curva normal (Figura 7.6):\n\nplot(valores_d,\n     type = \"l\",                          \n     lwd = 2,                             \n     col = \"steelblue\",                  \n     xaxt = \"n\",                          \n     ylab = \"Densidade de Probabilidade\",\n     xlab = \"Escores Z\")\n\n# Rótulos do eixo x\naxis(side = 1, at = which(valores_d == dnorm(0)), labels = c(0))\naxis(side = 1, at=which(valores_d == dnorm(1)), labels=c(-1, 1))\naxis(side = 1, at=which(valores_d == dnorm(2)), labels=c(-2, 2))\naxis(side = 1, at=which(valores_d == dnorm(3)), labels=c(-3, 3))\n\n\n\n\n\n\n\nFigura 7.6: Função densidade de probabilidade.\n\n\n\n\n\nOs argumentos básicos a serem informados da função axis() são: side=, at= e labels=. Esses argumentos determinam qual eixo será preenchido, qual a posição dos valores no eixo e a sequência de valores a ser preenchida, respectivamente. O argumento side= recebe valores que vão de 1 a 4: 1 = eixo inferior, 2 = eixo lateral esquerdo, 3 = eixo superior, 4= eixo lateral direito. Ou seja, partindo do eixo inferior (eixo x), os valores aumentam até 4 seguindo o sentindo horário para os quatros lados do gráfico. No exemplo, foi modificado o eixo x, logo side = 1. O argumento at = estabelece os pontos (densidades de probabilidade) do eixo x que receberão os rótulos, especificados no argumento label =.\nComo se pode ver, dnorm() fornece a “altura” do pdf da distribuição normal em qualquer escore Z que se forneça como argumento.\n\n\n7.7.2.4 Função rnorm()\nA função rnorm() gera n números aleatórios com distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\sigma\\).\nArgumentos:\n\nn \\(\\to\\) número de observações a serem geradas\n\nmean \\(\\to\\) média\n\nsd \\(\\to\\) desvio padrão\n\nCom esta função é possível, por exemplo, gerar 10 observações de uma distribuição normal:\n\nrnorm(10)\n\n [1]  1.04799258  0.26005291 -0.48737566  0.89902656  1.09604145 -0.23007241\n [7]  0.41746484 -0.67704354 -0.64862717 -0.01283553\n\n\nNo entanto, deve-se notar que, se uma “semente” (seed) não for especificada, a saída não será reproduzível, ou seja, cada vez que o comando for executado, retornará um novo conjunto de observações:\n\nrnorm(10)\n\n [1] -0.90766866  0.61247960 -0.26114451  0.62023715  0.62275968  0.73178017\n [7] -0.33884301 -0.04844878 -0.92625678  0.05941792\n\n\nCada vez que este comando for reproduzido, retornará uma nova série de 10 números diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada, no exemplo, pela função rnorm(). O valor do número (“semente”) não é importante, é apenas um identificador. Para ilustrar, será construído dois conjuntos de 10 números que serão recebidos pelos objetos x e y. Para gerar o conjunto de números x, será usado o número 123 como “semente”. A “semente” funciona como uma espécie de marca. Para o y não será usado a função set.seed():\n\nn &lt;- 10\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny &lt;- rnorm(n)\ny\n\n [1]  1.2240818  0.3598138  0.4007715  0.1106827 -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n\nComparando os conjuntos com a função identical() do R base, observa-se que os conjuntos são diferentes:\n\nidentical(x, y)\n\n[1] FALSE\n\n\nAgora, repetindo os mesmos comandos, mas usando antes a mesma “semente”, observa-se que os conjuntos são idênticos.\n\nset.seed (123)\nx &lt;- rnorm (n)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nset.seed (123)\ny &lt;- rnorm(n)\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\nidentical(x, y)\n\n[1] TRUE\n\n\nOutros usos da função rnorm():\nA função rnorm()será usada para gerar três vetores diferentes de números aleatórios de uma distribuição normal.\n\nset.seed(1234)\nn10 &lt;- rnorm(10, mean = 0, sd = 1)\nn100 &lt;- rnorm(100, mean = 0, sd = 1)\nn10000 &lt;-  rnorm(10000, mean = 0, sd = 1)\n\nEmn sequência, serão construídos histogramas (Figura 7.7), onde se pode observar que, aumentando o número de observações, tem-se gráficos que irão progressivamente se aproximando da verdadeira função de densidade normal.\nA função par(mfrow(1,3)) coloca os gráficos gerados em uma mesma linha e em três colunas. No final, se repete a função, restaurando as configurações basais de plotagem (uma linha e uma coluna).\n\n# Este comando coloca os gráficos em uma mesma linha, o argumento mfrow(c(1,3)) diz ao R para construir uma linha e três colunas:\npar(mfrow=c(1,3))\n\n# Histogramas\nhist(n10, breaks = 5, main = \"n =10\", ylab = \"Frequência\")\nhist(n100, breaks = 20, main = \"n =100\", ylab = \"Frequência\")\nhist(n10000, breaks = 50, main = \"n =10000\", ylab = \"Frequência\")\n\n# Restaura as configurações basais de plotagem\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\nFigura 7.7: Histogramas construídos com amostras geradas pela função rnorm.\n\n\n\n\n\nObserve também que à medida que n aumenta, a distribuição dos dados caracteriza-se como uma distribuição normal. Na Seção 9.2.2, este assunto voltará à cena.\n\n\n\n7.7.3 Regra Empírica 68-95-99.7\nA regra empírica diz que, se uma população de um conjunto de dados tem uma distribuição normal com média 0 e desvio padrão 1 (\\(X\\sim N(\\mu=0,\\sigma=1)\\)) pode-se afirmar que aproximadamente, 68%, 95% e 99,7% dos valores encontram-se, respectivamente, dentro de \\(\\pm\\) 1, 2 e 3 desvio padrão acima e abaixo média.\nEssa regra pode ser usada para descrever uma população e ajudar a decidir se uma amostra de dados veio de uma distribuição normal. Se uma amostra é grande o suficiente e a observação do histograma tem um formato parecido com um sino, é possível verificar se os dados seguem as especificações 68-95-99,7%. Se sim, é razoável concluir que os dados vieram de uma distribuição normal.\nExemplo:\nCom a função rnorm(), será gerado um vetor de 1000 números e um histograma com curva normal sobreposta:\n\n\n\n\n\n\n\n\nFigura 7.8: Histograma com curva de densidade de probabilidade sobreposta\n\n\n\n\n\nNo histograma (Figura 7.8), a probabilidade entre os escores Z - 1 e + 1 (entre as duas linhas tracejadas A e B) é igual a aproximadamente 68%. Pode-se calcular isto facilmente, usando a função pnorm():\nProbabilidade abaixo de z = 1, abaixo do ponto B:\n\nB &lt;- pnorm (1, 0, 1)\nB &lt;- round(B, 3)*100\nB\n\n[1] 84.1\n\n\nProbabilidade abaixo de z = -1, abaixo do ponto A:\n\n A &lt;- pnorm (-1, 0, 1)\n A &lt;- round(A, 3)*100\n A\n\n[1] 15.9\n\n\nLogo , a área abaixo da curva entre A e B é igual a:\n\n prob &lt;- B - A\n prob\n\n[1] 68.2\n\n\n\n\n7.7.4 Calculando probabilidades em uma distribuição normal\nComo visto na Seção 7.4.1, a variável dados$altura tem uma distribuição praticamente simétrica. Usando esses dados (\\(X\\sim N(\\mu=1,598,\\sigma=0,065)\\)), pode-se calcular probabilidades, dadas pela área sob a curva.\nExemplo 1: Qual a probabilidade de se encontrar mulheres com altura entre 1,47 e 1,73 m?\n\n# Dados\n mu &lt;- 1.598\n sigma &lt;- 0.065\n x1 &lt;- 1.47\n x2 &lt;- 1.73\n \n # Solução\n z1 &lt;-  (x1 - mu)/sigma\n z2 &lt;-  (x2 - mu)/sigma\n\n p1 &lt;- pnorm(x1, mu, sigma)\n p2 &lt;- pnorm(x2, mu, sigma)\n \n p2 - p1\n\n[1] 0.9543975\n\n\n\n\n\n\n\n\n\n\nFigura 7.9: Probabilidade de alturas entre 1,47 e 1,73m.\n\n\n\n\n\nA probabilidade de alturas entre 1,47 m e 1,73m é igual a 95,4% (Figura 7.9).\nExemplo 2: Os dados de uma pesquisa mostram informações sobre o tempo de cirurgia para reconstrução do ligamento cruzado anterior (LCA). A distribuição de probabilidades se a justa à normal com o tempo médio de cirurgia de 129 minutos com um desvio padrão de 14 minutos.\n\nQual a probabilidade de uma cirurgia de reconstrução do LCA requerer um tempo menor do que 100 minutos?\n\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 100\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.01915938\n\n\n\n\n\n\n\n\n\n\nFigura 7.10: Probabilidade do tempo de cirurgia de LCA menor ou igual a 100 minutos.\n\n\n\n\n\nDe acordo com a distribuição, 1,92%% das cirurgias irão demandar quantidade de tempo menor do que 100 minutos (Figura 7.10).\n\nSe uma cirurgia demorar 160 minutos, o que se conclui em relação a essa informação?\n\n\n# Dados\n mu &lt;- 129\n sigma &lt;- 14\n x &lt;- 160\n\n # Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = FALSE)\n p\n\n[1] 0.01340457\n\n\n\n\n\n\n\n\n\n\nFigura 7.11: Probabilidade do tempo de cirurgia de LCA maior ou igual a 160 minutos.\n\n\n\n\n\nDe acordo com a distribuição, 1,34% das cirurgias irão demandar quantidade de tempo \\(\\ge 160\\) minutos (Figura 7.11). Ou seja, é uma probabilidade muito pequena!\nExemplo 3: Suponha-se que em uma determinada ilha hipotética existam duas populações etnicamente diferentes onde as mulheres têm as seguintes medidas de altura: população 1 tem μ = 160 cm e σ = 6,6 cm e a população 2 tem μ = 140 cm e σ = 6,6 cm. As alturas de ambas as populações têm distribuição normal. Essas duas populações têm o mesmo aspecto físico, podendo ser distinguidas apenas geneticamente.\n\nQual a probabilidade de uma mulher com 150 cm pertencer a população 1?\n\n\n# Dados\n mu &lt;- 160\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n\n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.06486702\n\n\n\n\n\n\n\n\n\n\nFigura 7.12: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 160 cmm.\n\n\n\n\n\nNa população 1, apenas 6.5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.12). Em outras palavras, existe pouca probabilidade dessa mulher pertence à população 1.\n\nQual a probabilidade de uma mulher com 150 cm pertencer a população 2?\n\n\n# Dados\n mu &lt;- 140\n sigma &lt;- 6.6\n x &lt;- 150\n\n# Solução\n z &lt;-  (x - mu)/sigma\n \n p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE)\n p\n\n[1] 0.935133\n\n\n\n\n\n\n\n\n\n\nFigura 7.13: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 140 cmm.\n\n\n\n\n\nNa população 2, 93,5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.13). Concluindo, ela pode pertencer a qualquer uma das populações. Pode ser uma mulher alta da população 2 ou uma “baixinha” da população 1!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-binomial",
    "href": "07-probabilidades.html#distribuição-binomial",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.8 Distribuição Binomial",
    "text": "7.8 Distribuição Binomial\nA distribuição normal padrão é apenas um dos exemplos de distribuição de probabilidade. Uma boa parte das situações se ajustam a ela. Entretanto, diversas situações reais muitas vezes se aproximam de outras distribuições estocásticas definidas por algumas hipóteses. Daí a importância de se conhecer e manipular algumas destas distribuições. Entre elas, a distribuição binomial.\nQuando um experimento aleatório resulta em um de dois, mutuamente exclusivos, desfechos, tais como vivo/morto, positivo/negativo, sim/não, masculino/feminino é denominado de Ensaio de Bernoulli. Recebeu esta denominação em homenagem ao matemático suíço, Jacob Bernoulli (1654-1705), considerado fundador do cálculo e da teoria da probabilidade (5).\nA distribuição de frequências que descreve as proporções de um ensaio de Bernoulli, chama-se Distribuição Binomial. A probabilidade binomial dá a probabilidade de determinado desfecho ocorrer em determinado número de ensaios independentes. Uma sequência de ensaios de Bernoulli forma um Processo de Bernoulli.\nA distribuição binomial é importante para variáveis discretas. Existem poucas condições que precisam ser atendidas para distribuição binomial:\n\nCada ensaio resulta em um de dois desfechos, mutuamente exclusivos, denominados, arbitrariamente, de sucesso e fracasso;\n\nA probabilidade de sucesso é fixa, igual a p, constante em cada ensaio, e a probabilidade de fracasso é igual a 1 – p;\nO número de repetições n em um ensaio é fixo.\n\nOs ensaios são independentes\n\nA distribuição binomial é na verdade uma família de distribuições, cujos membros são definidos pelos valores de n e p (parâmetros da distribuição binomial).\nA probabilidade de sucesso 4, em uma distribuição binomial, é dada pela fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nonde n = ensaios, x = sucessos, p = probabilidade de um sucesso e C representa o número possível de combinações em um ensaio.\nO número de combinações, C de x sucessos entre n repetições podem ser computado pela fórmula:\n\\[\nC = \\frac{n!}{x!(n - x)!}\n\\]\nou, no R, com a função choose (n, x).\nO modelo de distribuição binomial trata de encontrar a probabilidade de sucesso de um evento que tem apenas dois resultados possíveis em uma série de experimentos. Usando dados de uma distribuição binomial, é possível calcular os valores esperados de uma variável aleatória conforme ela passa por tentativas independentes. Em outras palavras, é possível prever o número exato de caras ou coroas que se deve esperar ao jogar uma moeda um certo número de vezes.\nTambém, pode-se usar a probabilidade binomial cumulativa para encontrar a probabilidade de obter um determinado intervalo de resultados. Por exemplo, saber a probabilidade do nascimento de até três meninos em 10 nascimentos consecutivos quando a probabilidade de nascer um menino é 0,50.\nO R tem quatro funções embutidas para gerar distribuição binomial. Ela são descritas a seguir.\n\n7.8.1 Funções da distribuição binomial\n\n7.8.1.1 Função pbinom()\nEsta função retorna o valor da função de densidade cumulativa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob).\nArgumentos:\n\nq \\(\\to\\) vetor de quantis\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, qual é a probabilidade de nascer até três meninos em cinco nascimentos, sabendo que a probabiliade de nascer um menino é igual a 0.50?\n\npbinom (3, 5, 0.50)\n\n[1] 0.8125\n\n\nIsso corresponde a soma das probabilidades de nascer nenhum menino, um menino, dois meninos e três meninos (Figura 7.14). Isto é calculado pela equação \\(P(X = x)\\), vista anteriormente.\nColocando no R:\n\nn = 5\np = 0.50\nx &lt;- 0:5\n# Probabilidades de meninos \nFx &lt;- (factorial(n)/(factorial(x)*factorial(n-x)))* p^x *(1-p)^(n-x)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n[1] 0.8125\n\n\n\n\n\n\n\n\nFigura 7.14: Distribuição binomial, mostrando a P (x &lt; 4) com n = 5 e p = 0.50\n\n\n\n\n\n\n\n7.8.1.2 Função qbinom()\nEsta função retorna o valor da função de densidade cumulativa inversa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). Com o uso desta função, podemos descobrir o quantil da distribuição binomial.\nArgumentos:\n\np \\(\\to\\) probabilidade ou vetor de probabilidades\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nlower.tail \\(\\to\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\)\n\nPor exemplo, quantos meninos nascerão em 5 partos com 81.25% de probabilidade cumulativa?\n\nqbinom (0.8125, size = 5, prob = 0.50)\n\n[1] 3\n\n\n\n\n7.8.1.3 Função rbinom()\nA função rbinom() permite extrair n observações aleatórias de uma distribuição binomial. Os argumentos da função são descritos abaixo:\nArgumentos:\n\nn \\(\\to\\) número de observações aleatórias a ser gerado\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nPara fazer uma simulação de 1000 amostras, aleatoriamente, de tamanho 5 e com probabilidade de nascer menino igual a 0,50, usa-se 5:\n\nset.seed(23)\nmenino &lt;- rbinom(n = 1000, size = 5, prob = 0.5)\n\nCada amostra de n = 5 exibe o número de meninos nascidos. Pode-se fazer a média que representa o valor esperado do número de sucessos (nascimento de menino, no exemplo) em um conjunto de ensaios independentes:\n\nmean(menino)\n\n[1] 2.515\n\n\nQuanto maior o número de variáveis aleatória criadas, mais próximo a média do número de sucessos estará do número esperado de sucessos que é igual ao número de sucessos vezes a probabilidade de sucesso em cada ensaio (5 x 0,50 = 2,5).\nEstranho, não é? Dois meninos e meio, em média por ensaio! É, a média é assim, uma estimativa, expectativa matemática! Não é real…\n\n\n7.8.1.4 Função dbinom()\nEssa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição binomial dada uma determinada variável aleatória X, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). A função tem a seguinte sintaxe:\nArgumentos:\n\nx \\(\\to\\) vetor de números\n\nsize \\(\\to\\) numero de ensaios\n\nprob \\(\\to\\) probabilidade de sucesso em cada ensaio\n\nA função é usada para encontrar a probabilidade de um determinado valor para dados que seguem a distribuição binomial, ou seja, encontra \\(P(X=x)\\), probabilidade de x sucessos em tentativas de tamanho (size) n quando a probabilidade (p) de sucesso é prob. Obtém o mesmo resultado da fórmula:\n\\[\nP(X = x)= C \\times p^x \\times (1 - p)^{n-x}\n\\]\nPor exemplo, no nascimento de uma criança, as duas possibilidades, menino ou menina, são mutuamente excludentes e esses são os únicos eventos que podem acontecer. A probabilidade de nascimento de menino, como visto, é 0,50, qual seria a probabilidade de nascerem 4 meninos em 5 partos consecutivos não gemelares (Figura 7.15)?\n\ndbinom(4, size = 5, prob = 0.50)\n\n[1] 0.15625\n\n\nAs probabilidades de nascerem meninos em 5 nascimentos são:\n\nFx &lt;- dbinom(0:5, 5, 0.50)\nFx\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n\n\n\n\n\n\n\n\n\nFigura 7.15: Distribuição binomial para P (x = 4) com n = 5 e p = 0,50\n\n\n\n\n\n\n\n\n7.8.2 Média e desvio padrão da distribuição binomial\nQuando o número de repetições é grande, geralmente há necessidade de resumir as probabilidades. A distribuição binomial pode ser descrita por sua média e variância.\nA média é o valor médio da variável aleatória em um longo número de repetições. É também chamada de valor esperado ou expectativa. A expectativa de uma variável aleatória X, geralmente, é denotada por \\(E(X)\\) e obtida pela multiplicação do número de ensaios independentes (n) pela probabilidade (p) de sucesso em cada ensaio:\n\\[\n\\mu = E(X) = n \\times p\n\\]\nPortanto, a expectativa (esperança) de nascimento de meninos em 5 partos é \\(E(X)=5 \\times 0,50 = 2,5\\), como visto na função rbinom(). Observe que o valor esperado de uma variável aleatória discreta não tem um valor que a variável aleatória pode realmente assumir.\nPor exemplo, para o número médio de meninos em um parto, ou não se tem menino ou se tem 1 menino, cada uma possibilidade com probabilidade de 0,50 e o valor esperado é (0 × 0,50) + (1 × 0,50) = 0,50. O número de meninos deve ser 0 ou 1, mas o valor esperado é a metade, a média que se obteria no longo prazo.\nA variância de uma variável aleatória discreta X é igual a\n\\[\n\\sigma^2=var(X) = n\\times p \\times (1-p)\n\\]\nConsequentemente, o desvio padrão é igual a\n\\[\n\\sigma = \\sqrt{var(X)} = \\sqrt{n\\times p \\times (1-p)}\n\\]\nPara o exemplo de 5 nascimentos, a média foi de 2,5 meninos e o desvio padrão\n\\[\n\\sigma =\\sqrt{5\\times 0.50 \\times (1-0.50)}=\\sqrt{2.5 \\times 0.50}= 1.12\n\\]\nPortanto, se espera que ocorram em média 2,5 (\\(\\sigma\\) = 1,12) nascimentos de meninos em 5 partos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#distribuição-de-poisson",
    "href": "07-probabilidades.html#distribuição-de-poisson",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "7.9 Distribuição de Poisson",
    "text": "7.9 Distribuição de Poisson\nA distribuição de Poisson é utilizada para descrever a probabilidade do número de ocorrências em um intervalo contínuo (de tempo ou espaço). No caso da distribuição binomial, a variável de interesse é o número de sucessos em um intervalo discreto (n ensaios de Bernoulli).\nA unidade de medida (tempo ou espaço) é uma variável contínua, mas a variável aleatória, o número de ocorrências, é discreta. Esta distribuição segue as mesmas premissas da distribuição binomial:\n\nas tentativas são independentes;\na variável aleatória é o número de eventos em cada amostra;\na probabilidade é constante em cada intervalo\n\nEla é utilizada para modelar eventos discretos que ocorrem com pouca frequência no tempo ou espaço, por isso é algumas vezes denominada de distribuição de eventos raros. Pode-se usar a distribuição de Poisson como uma aproximação da distribuição Binomial quando n, o número de tentativas, for grande e p ou (1 – p) for pequeno (eventos raros).\nUm bom princípio básico é usar a distribuição de Poisson quando \\(n \\ge 20\\) e \\(n \\times p\\) ou \\(n \\times (1- p)\\) &lt; 5% (6). Nessas condições, a probabilidade que uma variável aleatória X adote um valor x é\n\\[\nP(X = x) = \\frac {e^{-\\lambda} \\times \\lambda^x}{x!}\n\\]\nonde \\(\\lambda\\) (lambda) representa o número de ocorrências de um evento em um intervalo de tempo e é conhecida como parâmetro da distribuição de Poisson e é igual em média a \\(n \\times p\\).\nNo R, essa probabilidade é dada pela função dpois(x, lambda).\nExemplo:\nSuponha que a probabilidade de uma puérpera ter infecção congênita (rubéola) seja igual a 0,0009. Qual seria a probabilidade, em uma população de 6000 gestantes, de que 5 estejam infectadas?\n\np &lt;- 0.0009\nx &lt;- 5\nn &lt;- 6000\nlambda &lt;- n * p\nP &lt;- dpois(x, lambda)\nround (P, 3)\n\n[1] 0.173\n\n\nPortanto, a probabilidade de se encontrar 5 mulheres com infecção congênita é de aproximadamente 17%.\n\n\n\n\n1. Debnath L, Basu K. A short history of probability theory and its applications. International Journal of Mathematical Education in Science and Technology. 2015;46(1):13–39. \n\n\n2. Menezes RX de. Introdução à Probabilidade. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 151–87. \n\n\n3. Pagano M, Kimberly G. Theoretical Probability Distributions. Em: Principles of Biostatistics. Second Edition. CRC Press; 2000. p. 162. \n\n\n4. Gonzalez JCS. Normal distribution in R [Internet]. R CODER. 2021. Disponível em: https://r-coder.com/\n\n\n5. Robertson E, O’Connor J. Jacob (Jacques) Bernoulli [Internet]. Maths History. School of Mathematics; Statistics, University of St Andrews; 2022. Disponível em: https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/\n\n\n6. Fisher LD, Van Belle G. Poisson Random Variables. Em: Biostatistics: A Methodology for the Health Sciences. New York, NY: John Wiley & Sons; 1993. p. 211–8.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "07-probabilidades.html#footnotes",
    "href": "07-probabilidades.html#footnotes",
    "title": "7  Introdução à Teoria das Probabilidades",
    "section": "",
    "text": "Hospital Geral de Caxias do Sul, Hospital de Ensino da Universidade de Caxias do Sul, RS↩︎\nPara remover a notação científica, usar a função options (scipen = 999) e, para desfazer essa ação, trocar o 999 por 0.↩︎\nVeja na Seção Seção 4.8.1.↩︎\nSucesso, aqui, não está no sentido de vitória, êxito, triunfo, glória e sim com a conotação de obter o desfecho esperado. Por exemplo, se uma moeda é lançada e se espera obter cara, sucesso significa um resultado igual a cara.↩︎\nDeve ser especificado uma “semente” (seed) antes de executar a função, senão será obtido um conjunto diferente de observações aleatórias a cada execução. Teste para verificar↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introdução à Teoria das Probabilidades</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html",
    "href": "08-assimetria_curtose.html",
    "title": "8  Assimetria e Curtose",
    "section": "",
    "text": "8.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools,\n               dplyr, \n               e1071, \n               flextable,\n               ggplot2, \n               ggpubr, \n               grDevices, \n               moments, \n               readxl, \n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#dados",
    "href": "08-assimetria_curtose.html#dados",
    "title": "8  Assimetria e Curtose",
    "section": "8.2 Dados",
    "text": "8.2 Dados\nSerá usada a mesma variável altura de 1368 mulheres do conjunto de dadosdadosMater.xlsx, já mostrado anteriormente (Seção 7.4.1).\n\n8.2.1 Exploração dos dados\nO resumo dos dados pode ser realizado, usando a função summarise() do pacote dplyr. A moda será calculada usando com função Mode() do pacote DescTools.\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\nresumo &lt;- dados %&gt;% \n  summarise(n = n(),\n            media = mean(altura, na.rm = TRUE),\n            dp = sd(altura, na.rm = TRUE),\n            mediana = median(altura, na.rm = TRUE),\n            moda = Mode(altura),\n            Q1 = quantile (altura, 0.25),\n            Q3 = quantile (altura, 0.75),\n            CV = dp/media)\nresumo\n\n# A tibble: 1 × 8\n      n media     dp mediana  moda    Q1    Q3     CV\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1368  1.60 0.0655     1.6   1.6  1.55  1.65 0.0410\n\n\nPara a exploração visual dos dados, será construído um histograma com um boxplot sobreposto (Figura 8.1). A função layout() tem o formato layout(mat) onde mat é um objeto da classe matriz que permite dividir a janela de plotagem em áreas com tamanhos personalizados. Abaixo, cria-se uma matriz com uma coluna e duas linhas com uma relação de 1:8 entre as linhas. A função par() é utilizada para alterar as margens. Para mais detalhes acesse aqui.\n\n# Estruturação do layout do gráfico\nlayout(matrix(c(1,2), nrow = 2 , ncol = 1, \n              byrow = TRUE), heights = c(1, 8))\n\n# Boxplot\npar (mar=c (0, 4.3, 1.1, 2))\nboxplot (dados$altura, \n         horizontal = TRUE, \n         ylim = c (1.4, 1.9), \n         xaxt = \"n\", \n         col = \"lightblue\", \n         frame = FALSE)\n\n#Histograma\npar (mar=c (4, 4.3, 1.1, 2))\nhist (dados$altura, \n      breaks=15,\n      col = \"lightblue\",\n      border = \"black\",\n      main = \"\",\n      xlab = \"Altura (m)\",\n      ylab = \"Frequência\",\n      xlim = c(1.4,1.9),\n      las = 1)\nbox(bty = \"L\")\n# Restauração do padrão\npar (mar = c(5, 4, 4, 2) + 0.1)\n\n\n\n\n\n\n\nFigura 8.1: Histograma da altura das gestantes com boxplot sobreposto.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#assimetria",
    "href": "08-assimetria_curtose.html#assimetria",
    "title": "8  Assimetria e Curtose",
    "section": "8.3 Assimetria",
    "text": "8.3 Assimetria\nA assimetria analisa a proximidade ou o afastamento de um conjunto de dados quantitativos em relação à distribuição normal. Mede o grau de afastamento de uma distribuição em relação a um eixo central (geralmente a média).\nQuando a curva é simétrica, a média, a mediana e a moda coincidem, num mesmo ponto, havendo um perfeito equilíbrio na distribuição. Quando o equilíbrio não acontece, isto é, a média, a mediana e a moda recaem em pontos diferentes da distribuição esta será assimétrica; enviesada a direita ou esquerda. podendo-se caracterizar como curvas assimétricas à direita ou à esquerda. Quando a distribuição é assimétrica à esquerda ou assimetria negativa, a cauda da curva localiza-se à esquerda, desviando a média para este lado (Figura 8.2). Na assimetria positiva, ocorre o contrário, a cauda está localizada à direita e da mesma forma a média (1).\n\n\n\n\n\n\n\n\nFigura 8.2: Assimetria\n\n\n\n\n\n\n8.3.1 Avaliação da assimetria\nO R dispões de diversas maneiras para o cálculo do coeficiente de assimetria. O coeficiente de assimetria é um método numérico estatístico para medir a assimetria da distribuição ou conjunto de dados. Ele fala sobre a posição da maioria dos valores de dados na distribuição em torno do valor central.\n\n8.3.1.1 Cálculo do coeficiente de assimetria\nVárias medidas de coeficientes de assimetria amostrais foram propostas. O coeficiente de assimetria pode ser calculado no R, usando a função skewness() do pacote e1071 (2). Esta função usa os seguintes argumentos:\n\nx \\(\\to\\) vetor numérico que contém os valores\nna.rm \\(\\to\\) um valor lógico que indica se os valores NA devem ser eliminados antes que o cálculo prossiga.\ntype \\(\\to\\) número inteiro entre 1 e 3 selecionando um dos algoritmos para calcular assimetria detalhados abaixo.\n\nOs três tipos são os seguintes:\n\nTipo 1, g1 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_1=\\frac{m_3}{m_2^\\frac{3}{2}}\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 3, tem-se:\n\\[\nm_3=\\frac{\\sum(x_i - \\overline{x})^3}{n}\n\\] Para r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m3 &lt;- (sum((dados$altura - (mean(dados$altura)))^3))/resumo$n\n m3\n\n[1] 5.081924e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g1 no R, chega-se ao resultado:\n\n g1 &lt;- m3/(m2)^(3/2)\n g1\n\n[1] 0.1812196\n\n\nUsando a função skewness() do pacote e1071, chega-se ao mesmo resultado:\n\ne1071::skewness(dados$altura, type = 1)\n\n[1] 0.1812196\n\n\n\nTipo 2, G1 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_1=\\frac{g_1 \\sqrt{n(n-1)}}{n-2}\n\\]\nColocando os dados na fórmula na linguagem do R, tem-se:\n\n G1 &lt;- (g1*sqrt((resumo$n*(resumo$n-1))))/(resumo$n-2)\n G1\n\n[1] 0.1814186\n\n\nCalculando com a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 2)\n\n[1] 0.1814186\n\n\n\nTipo 3, b1 \\(\\to\\) É o padrão da função skewness() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_1= \\frac {m_3}{s^3}\n\\]\nonde s é o desvio padrão da amostra. Na linguagem R, tem-se:\n\n b1 &lt;- m3/(resumo$dp)^3\n b1\n\n[1] 0.1810209\n\n\nUsando a função skewness() do pacote e1071:\n\ne1071::skewness(dados$altura, type = 3)\n\n[1] 0.1810209\n\n\nPara amostras grandes, há muito pouca diferença entre as várias medidas (3). Todas as três medidas de assimetria são imparciais sob normalidade.\nInterpretação do coeficiente de assimetria\nQuando a \\(assimetria = 0\\), tem-se uma distribuição simétrica e a média, a mediana e a moda coincidem; quando a \\({assimetria} &lt; {0}\\), \\({média} &lt; {mediana} &lt; {moda}\\), a distribuição tem assimetria negativa e quando a \\({assimetria} &gt; {0}\\), \\({média} &gt; {mediana} &gt; {moda}\\), a distribuição tem assimetria positiva.\nA Tabela 8.1 sugere uma forma de interpretar o coeficiente de assimetria (4).\n\n\n\n\nTabela 8.1: Interpretação do Coeficiente de Assimetria\n\n\n\nCoeficiente de assimetriaAssimetria-1 a +1leve-1 a -2 e +1 a +2moderada-2 a -3 e +2 a +3importante&lt; -3 ou &gt; +3grave\n\n\n\n\n\nObservando o formato da distribuição no histograma e no boxplot, na Figura 8.1, e no resultado do coeficiente de assimetria, conclui-se que a variável altura tem uma assimetria positiva leve, não preocupante. É possível aceitar essa variável como praticamente simétrica.\n\n\n8.3.1.2 Avaliação da assimetria com o gráfico QQ\nOutra ferramenta gráfica que permite avaliar a simetria dos dados é o gráfico QQ (gráfico quantil-quantil). Ele permite observar se a distribuição se ajusta a distribuição normal. O gráfico QQ é um gráfico de dispersão que compara os quantis 1 da amostra com os quantis teóricos de uma distribuição de referência. Se os pontos do gráfico QQ formarem uma reta, isso indica que os dados têm a mesma distribuição da referência. Se os pontos se afastarem da reta, isso indica que os dados têm uma distribuição diferente da referência. Para construir um gráfico QQ, pode-se usar a função ggqqplot()do pacote ggpubr. Ele apresenta uma linha de referência, acompanhada de uma area sombreada, correspondente ao Intervalo de Confiança de 95% (veja o Capítulo 10):\n\nggqqplot(data = dados, \n         x = \"altura\",\n         conf.int = TRUE,\n         shape = 19,\n         xlab = \"Quantis teóricos\",\n         ylab = \"Altura (m)\",\n         color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 8.3: Gráfico QQ\n\n\n\n\n\nA Figura 8.3 exibe que a linha formada pelos pontos, praticamente, formam uma linha reta. É mais uma informação mostrando que os dados têm uma distribuição simétrica aceitável.\n\n\n8.3.1.3 Pesquisa de valores atípicos\nOs valores atípicos atraem as caudas da dispersão aumentando a possibilidade de assimetria. No boxplot da Figura 8.1, verifica-se a presença de outliers que devem ser avaliados.\nPara examinar os outliers, as estatísticas do boxplot são úteis, pois mostram a quantidade e os respectivos valores. A função boxplot.stats() do pacote grDevices, entregam as estatísticas dos 5 números (min, P25, mediana, P75 e max), o total de observações, o limite inferior e superior do intervalo de confiança de 95% e os valores atípicos (outliers)::\n\nboxplot.stats(dados$altura)\n\n$stats\n[1] 1.42 1.55 1.60 1.65 1.78\n\n$n\n[1] 1368\n\n$conf\n[1] 1.595728 1.604272\n\n$out\n[1] 1.40 1.82 1.80 1.40 1.40 1.85 1.80\n\n\nOutra maneira de identificar os outliers é através da função indentify_outliers() do pacote rstatix:\n\n dados %&gt;% \n   rstatix::identify_outliers(altura)\n\n# A tibble: 7 × 3\n  altura is.outlier is.extreme\n   &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1   1.4  TRUE       FALSE     \n2   1.82 TRUE       FALSE     \n3   1.8  TRUE       FALSE     \n4   1.4  TRUE       FALSE     \n5   1.4  TRUE       FALSE     \n6   1.85 TRUE       FALSE     \n7   1.8  TRUE       FALSE     \n\n\nAmbas as funções identificaram 7 valores atípicos (acima ou abaixo 1,5 vezes o intervalo interquartil), mas, como mostra a função identify_outliers, eles exercem pouca influência, pois não são extremos, ou seja, acima de três vezes o intervalo interquartil.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#curtose",
    "href": "08-assimetria_curtose.html#curtose",
    "title": "8  Assimetria e Curtose",
    "section": "8.4 Curtose",
    "text": "8.4 Curtose\nÉ o grau de achatamento de uma distribuição, em relação a distribuição normal. A curtose indica como o pico e as caudas de uma distribuição diferem da distribuição normal. A assimetria mede essencialmente a simetria da distribuição, enquanto a curtose determina o peso das caudas da distribuição. Portanto, é uma medida dos tamanhos combinados das duas caudas; mede a quantidade de probabilidade nas caudas. A curtose pode ser de três tipos (Figura 8.4):\n\nMesocúrtica \\(\\to\\) quando a distribuição é normal;\nLeptocúrtica \\(\\to\\) quando a distribuição é mais pontiaguda e concentrada que a normal, mostrando caudas pesadas em ambos os lados;\nPlaticúrtica \\(\\to\\) quando a distribuição é mais achatada e dispersa que a normal, com caudas planas.\n\nUma curtose em excesso é uma medida que compara a curtose de uma distribuição com a curtose de uma distribuição normal. A curtose de uma distribuição normal é igual a 3. Portanto, o excesso de curtose é determinado subtraindo 3 da curtose:\n\\[\nExcesso \\space de \\space curtose = curtose - 3\n\\]\nA distribuição normal tem uma curtose de zero e é chamada de mesocúrtica. Uma distribuição com curtose maior que zero (ou três) é mais alta e concentrada que a normal, mostrando caudas pesadas em ambos os lados, e é chamada de leptocúrtica. Uma distribuição com curtose menor que zero é mais achatada e dispersa que a normal, com caudas planas, e é chamada de platicúrtica.\nOs dados que seguem uma distribuição mesocúrtica mostram um excesso de curtose de zero ou próximo de zero. Isso significa que se os dados seguem uma distribuição normal, eles seguem uma distribuição mesocúrtica. A distribuição leptocúrtica mostra caudas pesadas em ambos os lados, indicando grandes valores discrepantes. Uma distribuição leptocúrtica manifesta uma curtose excessiva positiva. Uma distribuição platicúrtica mostra uma curtose excessiva negativa, revela uma distribuição com cauda plana.\n\n\n\n\n\n\n\n\nFigura 8.4: Assimetria\n\n\n\n\n\n\n8.4.1 Avaliação da curtose\n\n8.4.1.1 Cálculo do coeficiente de curtose\nO coeficiente de curtose pode ser calculado no R usando a função kurtosis() do pacote e1071. Esta função usa os mesmos argumentos da função skewness(), vista acima. Calcula três tipos de coeficientes:\n\nTipo 1, g2 \\(\\to\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula:\n\n\\[\ng_2=\\frac{m_4}{m_2^2} - 3\n\\]\nonde os momentos amostrais para amostras de tamanho n são dados por:\n\\[\nm_r=\\frac{\\sum(x_i - \\overline{x})^r}{n}\n\\]\nPara o momento central amostral de ordem r = 4, tem-se:\n\\[\nm_4=\\frac{\\sum(x_i - \\overline{x})^4}{n}\n\\]\nPara r = 2,\n\\[\nm_2=\\frac{\\sum(x_i - \\overline{x})^2}{n}\n\\]\nUsando o resumo dos dados:\n\n m4 &lt;- (sum((dados$altura - (mean(dados$altura)))^4))/resumo$n\n m4\n\n[1] 5.734699e-05\n\n m2 &lt;- (sum((dados$altura - (mean(dados$altura)))^2))/resumo$n\n m2\n\n[1] 0.004284321\n\n\nColocando os dados na fórmula do g2 no R, chega-se ao resultado:\n\ng2 &lt;- (m4/(m2)^2)-3\ng2\n\n[1] 0.1242567\n\n\nUsando a função do pacote e1071, chega-se ao mesmo resultado:\n\n e1071::kurtosis(dados$altura, type = 1)\n\n[1] 0.1242567\n\n\n\nTipo 2, G2 \\(\\to\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula:\n\n\\[\nG_2=\\left (\\left (n + 1 \\right )g_2 + 6 \\right )\\frac{\\left (n - 1 \\right)}{\\left ( \\left(n-2 \\right)\\left (n-3 \\right) \\right )}\n\\] Colocando os dados na fórmula na linguagem do R, tem-se:\n\n G2 &lt;- ((resumo$n+1)*g2 + 6)*(resumo$n-1)/((resumo$n-2)*(resumo$n-3))\n G2\n\n[1] 0.1291109\n\n\nCom a função kurtosis() do pacote e1071:\n\n e1071::kurtosis(dados$altura, type = 2)\n\n[1] 0.1291109\n\n\n\nTipo 3, b2 \\(\\to\\) É o padrão da função kurtosis() do pacote e1071. Usa-se a seguinte fórmula para o cálculo:\n\n\\[\nb_2=\\frac{m_4}{s^4}-3\n\\] onde s é o desvio padrão da amostra.\nNa linguagem R, tem-se:\n\n b2 &lt;- m4/(resumo$dp)^4 - 3\n b2\n\n[1] 0.1196907\n\n\nCom a função kurtosis():\n\ne1071::kurtosis(dados$altura, type = 3)\n\n[1] 0.1196907\n\n\nNovamente, para amostras grandes, há muito pouca diferença entre as várias medidas, principalmente entre G2 e b2 (3).\n\n\n8.4.1.2 Interpretação do coeficiente de curtose\nOs coeficientes calculados pela função do pacote e1071 retornam um resultado equivalente ao excesso de curtose. A curva normal tem um excesso de curtose próximo a zero e a curva é dita mesocúrtica. Se o coeficiente for positivo, os dados são leptocúrticos e se for negativo, os dados são platicúrticos. O resultado do exemplo aponta para uma distribuição leptocúrtica, pois existe um pequeno excesso de curtose (g2 = 0.1242567). Os valores que contribuem para a curtose são aqueles fora da região do pico, ou seja, ou outliers. A curva mesocúrtica tem um coeficiente de 3. Portanto, os valores calculados anteriormente referem-se ao excesso de curtose. O resultado da g2 = 0,1242567 pode ser escrito como b2 = 3,1242567. Daí o termo excesso de curtose.\nA função kurtosis() do pacote moments retorna um resultado ao redor de 3, para o coeficiente tipo 1. Para chegar ao mesmo resultado do coeficiente tipo 1 da função do pacote e1071, deve-se subtrair 3 do resultado.\n\nmoments::kurtosis(dados$altura)\n\n[1] 3.124257",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#exercício",
    "href": "08-assimetria_curtose.html#exercício",
    "title": "8  Assimetria e Curtose",
    "section": "8.5 Exercício",
    "text": "8.5 Exercício\n\nCriar um conjunto de dados com distribuição normal com média 0 e desvio padrão 1 e n = 10000 que será atribuído ao um objeto denominado meusDados.\n\n\nset.seed(1234)\nmeusDados &lt;- rnorm(100000, mean = 0, sd = 1)\n\n\nConstrua um histograma (Figura 8.5) com curva normal sobreposta:\n\n\nggplot() +\n  geom_histogram(aes(x = meusDados,\n                     y =after_stat(density)), \n                 bins = 20,\n                 fill='tomato',\n                 col=alpha('gray40',0.5)) + \n  geom_function(fun=dnorm,\n                args=list(mean=0,sd=1), \n                col='dodgerblue4',\n                lwd=1,\n                lty=2) + \n  labs(x='X',    \n       y='Densidade de probabilidade')+\n  scale_x_continuous(limits = c(-3, 3),\n                      n.breaks = 6) +\n  theme_bw() \n\n\n\n\n\n\n\nFigura 8.5: Histograma com curva normal\n\n\n\n\n\n\nObserve a skewness e a kurtosis\n\n\ne1071::skewness(meusDados)\n\n[1] 0.008609517\n\ne1071::kurtosis(meusDados)\n\n[1] -0.003450388\n\n\nComo era de se esperar, usando a rnorm(), a distribuição é um exemplo de distribuição normal, \\(skewness \\approx 0\\) e \\(kurtosis \\approx 0\\). Observe que a cada vez que os comandos forem executados, os resultados serão discretamente diferentes. Para evitar isso, deve-se usar set.seed(), veja a seção Seção 7.7.2. Faça o teste!\n\n\n\n\n1. Peat J, Barton B. Descriptive statistics. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 24–51. \n\n\n2. Meyer D, Dimitriadou E, Hornik K, Weingessel A, Leisch F, Chang C-C, et al. Package «e1071». The R Journal. 2019;1–67. \n\n\n3. Joanes D, Gill C. Comparing Measures of Sample Skewness and Kurtosis. Journal of the Royal Statistical Society. 1998;47(1):183–9. \n\n\n4. George D, Mallery P. Descriptive Statistics. Em: IBM SPSS Statistics 26 Step by Step: A Simple Guide and Reference. New York, NY: Taylor & Francis Group; 2020. p. 114–20.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "08-assimetria_curtose.html#footnotes",
    "href": "08-assimetria_curtose.html#footnotes",
    "title": "8  Assimetria e Curtose",
    "section": "",
    "text": "Sobre os quantis, veja na Seção 6.3.3.4.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assimetria e Curtose</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html",
    "href": "09-distAmostrais.html",
    "title": "9  Distribuições Amostrais",
    "section": "",
    "text": "9.1 Pacotes necessários para este capítulo\npacman::p_load(dplyr, \n               e1071, \n               ggplot2, \n               ggpubr, \n               knitr, \n               readxl)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#distribuições-populacional-e-amostral",
    "href": "09-distAmostrais.html#distribuições-populacional-e-amostral",
    "title": "9  Distribuições Amostrais",
    "section": "9.2 Distribuições populacional e amostral",
    "text": "9.2 Distribuições populacional e amostral\nMétricas como a média, a mediana e o desvio padrão são medidas numéricas de resumo. Quando calculadas a partir de dados de uma amostra são denominadas estatísticas amostrais. Por outro lado, as mesmas medidas numéricas de resumo calculadas para dados populacionais são chamadas de parâmetros populacionais.\nUm parâmetro populacional é sempre uma constante, enquanto uma estatística de amostra é sempre uma variável aleatória. Como cada variável aleatória deve possuir uma distribuição de probabilidade, cada estatística de amostra possui uma distribuição de probabilidade. A distribuição de probabilidade de uma estatística de amostra é mais comumente chamada de distribuição amostral. Os conceitos abordados neste capítulo são a base da estatística inferencial.\n\n9.2.1 Distribuição populacional\nA distribuição populacional é a distribuição de probabilidade derivada das informações sobre todos os elementos de uma população.\nPara fins de raciocínio didático, o conjunto de dados de 1368 observações de puérperas e recém-nascidos da Maternidade-escola do Hospital Geral de Caxias do Sul, RS, será considerado uma população. O gráfico da Figura 8.1, da Seção 8.2.1, mostra a distribuição da altura das puérperas dessa ‘população’. Os parâmetros (\\(\\mu\\) e \\(\\sigma\\)) dessa “população” são:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select(altura)\n\n media = mean(dados$altura, na.rm =TRUE)\n round(media, 3)\n\n[1] 1.598\n\n dp = sd(dados$altura, na.rm =TRUE)\n round(dp, 3)\n\n[1] 0.065\n\n\n\n\n9.2.2 Distribuição amostral\nConforme mencionado no início deste capítulo, o valor de um parâmetro da população é sempre constante. Por exemplo, para qualquer conjunto de dados populacionais, há apenas um valor para a média populacional, \\(\\mu\\).\nNo entanto, não se pode dizer o mesmo sobre a média amostral. Amostras diferentes do mesmo tamanho, retiradas da mesma população, produzem valores diferentes da média amostral, \\(\\bar{x}\\). O valor da média amostral, para qualquer amostra, dependerá dos elementos incluídos nessa amostra. Em decorrência, a média amostral é uma variável aleatória. Portanto, como outras variáveis aleatórias, a média amostral possui uma distribuição de probabilidade, que é mais comumente chamada de distribuição amostral da média.\nOutras estatísticas de amostra, como mediana, moda e desvio padrão, também possuem distribuições amostrais. Em geral, a distribuição de probabilidades de uma amostra é denominada de distribuição amostral.\nUsar a variável altura das puérperas da Maternidade do HGCS como a população de interesse é apenas uma estratégia didática. Raramente, na vida real, é possível obter dados da população inteira. Reunir essa informação costuma ser muito custoso ou impossível. Por essa razão, a prática é selecionar apenas uma amostra da população e a usar para compreender as suas características.\nA função slice_sample() do pacote dplyrextrairá uma amostra 1 de n = 30 da população. As funções mean() e sd() calcularão a média e o desvio padrão, repectivamente:\n\nset.seed(234)\namostra1 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia1 &lt;- mean(amostra1$altura, na.rm =TRUE)\ndp1 &lt;-  sd(amostra1$altura, na.rm =TRUE)\nprint(c(media1, dp1))\n\n[1] 1.59266667 0.06073875\n\n\nSe este processo for repetido várias vezes, a cada amostra aleatória 2, serão gerados médias e desvios padrão diferentes.\n\nset.seed(236)\namostra2 &lt;- dados %&gt;% \n  dplyr::slice_sample(n = 30)\n\nmedia2 &lt;- mean(amostra2$altura, na.rm =TRUE)\ndp2 &lt;-  sd(amostra2$altura, na.rm =TRUE)\nprint(c(media2, dp2))\n\n[1] 1.60633333 0.06960397\n\n\nÀ medida que o número de amostras possíveis forem aumentando, elas constituem uma distribuição cuja média, média das médias, \\(\\bar{x}_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\). Essa distribuição, no caso da média, recebe o nome de distribuição amostral das médias.\nAgora, para exemplificar este conceito, serão geradas 5000 amostras e calculada a média de cada uma das amostras de n = 30 que constituirão a distribuição, mostrada no gráfico da Figura 9.1.\n\n# extraindo 5000 amostras\namostras5000 &lt;- rep (0, 5000)\nfor (i in 1:5000) {\n  amostra &lt;- dados %&gt;% dplyr::slice_sample (n = 30) \n  amostras5000 [i] &lt;- mean(amostra$altura)\n}\n\nMedia e desvio padrão das 5000 amostras:\n\nmu &lt;- round (mean (amostras5000), digits = 3)\nsigma &lt;- round (sd (amostras5000), digits = 3)\nprint(c(mu, sigma))\n\n[1] 1.598 0.012\n\n\n\n\n\n\n\n\n\n\nFigura 9.1: Distribuição amostral das médias de 5000 amostras de n = 30\n\n\n\n\n\nSe a média, \\(\\bar{x}_{\\bar{x}}\\), dessas 5000 amostras de n = 30, for comparada com a média populacional, \\(\\mu\\), observa-se que até 3 dígitos decimais não há uma diferença. Entretanto, o desvio padrão é bem menor (0.012) que o da população (0.065).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "href": "09-distAmostrais.html#erros-amostrais-e-não-amostrais",
    "title": "9  Distribuições Amostrais",
    "section": "9.3 Erros amostrais e não amostrais",
    "text": "9.3 Erros amostrais e não amostrais\nAmostras diferentes selecionadas da mesma população darão resultados diferentes porque contêm elementos diferentes. Isso é evidente nas medias das amostra1 e amostra2, 1.593m e 1.606m, respectivamente, comparadas com a média da população igual a 1.598m .\n\nerro1 &lt;- abs(mean(amostra1$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nerro2 &lt;- abs(mean(amostra2$altura, na.rm =TRUE) - mean(dados$altura, na.rm =TRUE))\nprint(c(erro1, erro2), digits = 2)\n\n[1] 0.0053 0.0084\n\n\nSe outras amostras forem extraídas, o resultado obtido de qualquer amostra geralmente será diferente do resultado obtido da população correspondente. A diferença entre o valor de uma estatística amostral obtida de uma amostra e o valor do parâmetro populacional correspondente, é chamado de erro amostral. Observe que essa diferença representa o erro amostral apenas se a amostra for aleatória e não houver nenhum erro não amostral. Caso contrário, apenas uma parte dessa diferença será devido ao erro amostral.\n\\[\nerro \\quad amostral = \\bar{x}_{i} - \\mu  \n\\]\nÉ importante lembrar que o erro amostral ocorre devido ao acaso. Não é possível evitar o erro amostral. É possível limitar o seu valor através da seleção de uma amostra adequada. Os erros que ocorrem por outros motivos, como erros cometidos durante a coleta, registro e tabulação dos dados, são chamados de erros não amostrais. Esses erros ocorrem, em geral, por causa de erros humanos e não por acaso.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#média-e-desvio-padrão-da-média",
    "href": "09-distAmostrais.html#média-e-desvio-padrão-da-média",
    "title": "9  Distribuições Amostrais",
    "section": "9.4 Média e desvio padrão da média",
    "text": "9.4 Média e desvio padrão da média\nA média e o desvio padrão calculados para a distribuição amostral da média são chamados de média (\\(\\mu_{\\bar{x}}\\)) e desvio padrão (\\(\\sigma_{\\bar{x}}\\)) da média. Na verdade, a média e o desvio padrão da média são, respectivamente, a média e o desvio padrão das médias de todas as amostras do mesmo tamanho selecionadas de uma população. O desvio padrão da média é, comumente, chamado de erro padrão da média (\\(\\sigma_{\\bar{x}}\\)).\nA média amostral, \\(\\bar{x}\\), é chamada de estimador da média da população, \\(\\mu\\). Quando o valor esperado (ou média) de uma estatística amostral é igual ao valor do parâmetro populacional correspondente, essa estatística amostral é considerada um estimador não enviesado, consistente.\nPara a média amostral \\(\\bar{x}\\), \\(\\mu_{\\bar{x}} = \\mu\\). Logo, \\(\\bar{x}\\), é um estimador imparcial de \\(\\mu\\). Esta é uma propriedade muito importante que um estimador deve possuir. No entanto, o desvio padrão da média, \\(\\sigma_{\\bar{x}}\\), não é igual ao desvio padrão, \\(\\sigma\\), da distribuição populacional (a menos que n = 1). O desvio padrão da média amostral é igual ao desvio padrão da população dividido pela raiz quadrada do tamanho amostral:\n\\[\n\\sigma_{\\bar{x}} = \\frac {\\sigma}{\\sqrt{n}}\n\\]\nA dispersão da distribuição amostral da média é menor do que dispersão da distribuição populacional correspondente, como mostrado acima. Em outras palavras, \\(\\sigma_{\\bar{x}} &lt; \\sigma\\). Isso é visível na fórmula do \\(\\sigma_{\\bar{x}}\\) . Quando n é maior que 1, o que geralmente é verdadeiro, o denominador em \\(\\frac {\\sigma}{\\sqrt{n}}\\) é maior que 1. Desta forma, \\(\\sigma_{\\bar{x}}\\) é menor que \\(\\sigma\\). O desvio padrão da distribuição amostral da média diminui à medida que o tamanho amostral aumenta.\nSempre que o n for grande, em geral &gt; 30 (1), pode ser assumido que a distribuição será uma curva normal e que o desvio padrão da amostra (s) é um estimador não enviesado do desvio padrão populacional (\\(\\sigma\\)). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\):\n\\[\nEP_{\\bar{x}} = \\frac {s}{\\sqrt{n}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#teorema-do-limite-central",
    "href": "09-distAmostrais.html#teorema-do-limite-central",
    "title": "9  Distribuições Amostrais",
    "section": "9.5 Teorema do Limite Central",
    "text": "9.5 Teorema do Limite Central\nNa maioria das vezes, a população da qual as amostras são extraídas não é normalmente distribuída. Em tais casos, a forma da distribuição amostral de X é inferida de um teorema muito importante chamado teorema do limite central. De acordo com este teorema para um grande tamanho de amostra (&gt; 30), a distribuição amostral da média é aproximadamente normal, independentemente da forma da distribuição da população (1). Esta aproximação tornar-se-á mais acurada à medida que aumenta o tamanho amostral:\n\na média da distribuição amostral, \\(\\mu_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\);\ndesvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}\\), é igual a \\(\\frac {\\sigma}{\\sqrt{n}}\\);\no erro padrão da média, \\(\\sigma_{\\bar{x}}\\), é sempre menor (Figura 9.2) que o desvio padrão populacional, \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\nFigura 9.2: Erro padrão versus desvio padrão.\n\n\n\n\n\nAgora, será tomado como exemplo a variável renda, do conjunto de dados dadosMater.xlsx, que representa a renda familiar em salários mínimos (sm). Como foi feito anteriormente, suponha que essa variável seja a “população” de estudo. Ela tem as seguintes medidas resumidoras e de assimetria:\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\")\nresumo &lt;- dados %&gt;% \n  select (renda) %&gt;% \n  dplyr::summarise (media.sm = mean (dados$renda, na.rm = TRUE),\n                    dp.sm = sd(dados$renda, na.rm = TRUE),\n                    mediana.sm = median(dados$renda, na.rm = TRUE),\n                    assimetria = e1071::skewness(dados$renda),\n                    curtose = e1071::kurtosis(dados$renda))\nresumo\n\n# A tibble: 1 × 5\n  media.sm dp.sm mediana.sm assimetria curtose\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1     2.22  1.23       1.92       2.22    8.21\n\n\nO desvio padrão é grande em relação à média, com um coeficiente de variação de 55.1% e uma mediana &lt; média. Estas métricas junto com os coeficientes de assimetria e curtose apontam para a assimetria positiva da variável renda. O gráfico da Figura 9.3 confirma esta afirmação:\n\n\n\n\n\n\n\n\nFigura 9.3: Distribuição assimétrica positiva\n\n\n\n\n\nOs valores da média e do desvio padrão calculados para a distribuição de probabilidade dessa população fornecem os valores dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Esses valores são \\(\\mu\\) =2.22sm 3 e \\(\\sigma\\) =1.23sm.\nSe extrairmos múltiplas amostras dessa população, observa-se a modificação do formato da distribuição à medida que aumenta o tamanho amostral, se aproximando progressivamente do modelo normal, com um número grande de amostras.\nExtração de múltiplas amostras(1000)\n\namostras1000 &lt;- rep (0, 1000)\nfor (i in 1:1000) {\n  amostra.sm &lt;- sample (dados$renda, 30) \n  amostras1000 [i] &lt;- mean(amostra.sm)\n}\n\nMedia e desvio padrão das 1000 amostras\n\nmu &lt;- round (mean (amostras1000), digits = 3)\nsigma &lt;- round (sd (amostras1000), digits = 3)\nmd &lt;- round (median(amostras1000), digits = 3)\nprint(c(mu, sigma, md))\n\n[1] 2.231 0.236 2.224\n\n\nAssimetria e curtose\n\nb1 &lt;- e1071::skewness(amostras1000)\nb2 &lt;- e1071::kurtosis(amostras1000)\nprint(c(b1, b2))\n\n[1] 0.4357889 0.2241975\n\n\n\n\n\n\n\n\n\n\nFigura 9.4: Distribuição praticamente normal\n\n\n\n\n\nOu seja, extraindo-se 1000 amostras de n = 30 e calculando as mesmas métricas anteriores, observa-se que, embora a distribuição populacional original seja assimétrica, a distribuição amostral da média se aproxima bastante da distribuição gaussiana (Figura 9.4).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#sec-popamostra",
    "href": "09-distAmostrais.html#sec-popamostra",
    "title": "9  Distribuições Amostrais",
    "section": "9.6 Proporções populacional e amostral",
    "text": "9.6 Proporções populacional e amostral\nO conceito de proporção é o mesmo que o conceito de frequência relativa e o conceito de probabilidade de sucesso em um experimento binomial, discutidos anteriormente, na distribuição binomial.\nA frequência relativa de uma categoria ou classe dá a proporção da amostra ou população que pertence a essa categoria ou classe. Da mesma forma, a probabilidade de sucesso em um experimento binomial representa a proporção da amostra ou população que possui uma determinada característica.\nA proporção populacional, representada por p, é obtida considerando a razão entre o número de elementos em uma população com uma característica específica e o número total de elementos na população. A proporção amostral, denotada por \\(\\hat{p}\\) (pronuncia-se p-chapéu), fornece uma proporção semelhante para uma amostra.\n\\[\np = \\frac{X}{N} \\quad e \\quad \\hat{p}= \\frac{x}{n}\n\\] onde,\n\nN \\(\\to\\) número total de elementos em uma população\nn \\(\\to\\) número total de elementos em uma amostra\nX \\(\\to\\) número de elementos na população que possui determinada característica\nx \\(\\to\\) número de elementos na amostra que possui determinada característica\n\nComo no caso da média, a diferença entre a proporção amostral e a proporção populacional correspondente, determina o erro amostral, assumindo que a amostra é aleatória e nenhum erro não amostral foi cometido. Ou seja,\n\\[\nerro \\quad amostral = \\hat{p} - p\n\\]\nA distribuição amostral de uma proporção é a distribuição das proporções de todas as amostras possíveis de tamanho n retiradas de uma população. De acordo com o Teorema Central do Limite: * Considerando m o número de vezes que o processo de repetição das amostras de tamanho n, a média das proporções, quando m $m$, tende para a verdadeira proporção populacional; * A distribuição amostral das proporções segue aproximadamente uma distribuição normal.\nAssim,\n\\[E(\\hat{p})=\\mu_\\hat{p}\\]\n\\[Var(\\hat{p})=\\sigma^2_\\hat{p}=\\frac{\\hat{p}(1-\\hat{p})}{n}\\] Logo,\n\\[E(\\hat{p})=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nDessa forma, a distribuição amostral de \\(\\hat{p}\\) será:\n\\[\n\\hat{p} \\sim N(\\hat{p}, \\frac{\\hat{p}(1-\\hat{p})}{n})\n\\]\nQuando não conhecemos a proporção populacional p, pode-se usar \\(\\hat{p}\\) como estimativa dessa proporção, desde que as seguintes condições sejam satisfeitas:\n\n\\(n \\times p ≥5\\)\n\\(n \\times(1-p) ≥5\\)\n\nDessa forma, pode-se calcular probabilidades aproximadas por uma distribuição normal com média \\(μ = n \\times p\\) e \\(σ = \\sqrt{(n×p(1-p))}\\) (veja também Seção 7.8.2).\nConsiderando a “população” que está sendo usada neste capítulo, o conjunto de dados dadosMater.xlsx, será verificado a proporção de mulheres fumantes. Inicialmente, a variável fumo, que está como variável numérica, será transformada em fator, pois, na realidade, é categórica:\n\ndados$fumo &lt;- factor (dados$fumo, \n                      levels = c (1,2), \n                      label = c (\"sim\", \"não\"))\n\nA proporção de fumantes, frequência relativa (fr) é:\n\nfumo &lt;- with(dados, table(fumo))\nfr.fumo &lt;- prop.table(fumo)\nfr.fumo\n\nfumo\n      sim       não \n0.2200292 0.7799708 \n\n\nA saída retorna que a proporção de fumantes entre as mulheres desse arquivo é 0.22. Esta será considerada a proporção p da ‘população’. Agora, imagine que esse resultado fosse desconhecido. Então, para saber a qual a proporção de fumantes dessa ‘população’, seria necessário extrair uma amostra adequada. Foi selecionada uma amostra de n = 100 da ‘população’ alvo:\n\n set.seed(134)\n amostra.fumo &lt;- dados %&gt;% dplyr::slice_sample(n = 100)\n\nUsando a amostra.fumo, calcula-se a proporção de fumantes:\n\n tabagismo &lt;- with(amostra.fumo, table(fumo))\n fr &lt;- prop.table(tabagismo)\n fp &lt;- fr*100\n\n tab.fumo &lt;- cbind(n = tabagismo,\n                   fr = round(fr, 2),\n                   fp = round(fp, 2))\n tab.fumo\n\n     n  fr fp\nsim 20 0.2 20\nnão 80 0.8 80\n\n\nA proporção de uma amostra é uma variável aleatória: varia de amostra para amostra de uma forma que não pode ser prevista com certeza. O Teorema Central do Limite se aplica em proporções. À medida que novas amostras forem extraídas, o valor da proporção amostral \\(\\hat{p}\\) se aproxima da proporção populacional p. Na “população” p = 0,22; na amostra de n = 100, \\(\\hat{p}\\) = 0.2. Para amostras grandes, a proporção amostral tem distribuição aproximadamente normal com as seguinte características mencionadas acima em relação a \\(\\mu_\\hat{p}\\) e \\(\\sigma_\\hat{p}\\).\nComo verificar se uma amostra é grande?\nUma amostra é grande se o intervalo\n\\[\n[\\hat{p}-3 \\times \\sigma_\\hat{p} , \\quad \\hat{p}-3 \\times \\sigma_\\hat{p}]\n\\]\nestiver totalmente dentro do intervalo [0,1].\nNa prática, p não é conhecido, portanto, \\(\\sigma_\\hat{p}\\) também não é. Nesse caso, para verificar se a amostra é suficientemente grande, substitui-se o valor de p pelo valor conhecido de \\(\\hat{p}\\). Isso significa verificar se o intervalo\n\\[\n\\hat{p}-3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\\quad \\hat{p}+3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nencontra-se totalmente dentro do intervalo [0,1].\nTransportando os dados da amostra de gestantes, para a fórmula e usando o R para o cálculo, tem-se:\n\np.chapeu &lt;- tab.fumo[1,2]\nn &lt;- tab.fumo[1,1] + tab.fumo[2,1]\n\nli &lt;- p.chapeu - 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nls &lt;- p.chapeu + 3*sqrt((p.chapeu*(1-p.chapeu))/n)\nprint(c(li, ls), digits = 3)\n\n[1] 0.08 0.32\n\n\nComo os limites ficam no intervalo [0, 1], chega-se à conclusão de que a amostra de n = 100 é aceitável para estimar a proporção populacional.\nComo exercício, verificar se uma amostra de n = 40 é aceitável.\n\n\n\n\n1. Pagano M, Gavreau K. The Central Limit Theorem. Em: Principles of Biostatistics. Second Edition. Pacific Grove, CA: Duxbury; 2000. p. 197–8.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "09-distAmostrais.html#footnotes",
    "href": "09-distAmostrais.html#footnotes",
    "title": "9  Distribuições Amostrais",
    "section": "",
    "text": "Para que a cada nova amostragem retorne o mesmo conjunto de dados, é usado a função set.seed()(veja Seção 7.7.2.4)↩︎\nObserve que se modificou o número da “semente”↩︎\nSalários mínimos↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribuições Amostrais</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html",
    "href": "10-estimacao.html",
    "title": "10  Estimação",
    "section": "",
    "text": "10.1 Pacotes necessários neste capítulo\npacman::p_load(DescTools, \n               dplyr,\n               ggplot2, \n               flextable,\n               knitr,\n               readxl, \n               Rmisc, \n               tidyr)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#sec-dadoscap10",
    "href": "10-estimacao.html#sec-dadoscap10",
    "title": "10  Estimação",
    "section": "10.2 Dados",
    "text": "10.2 Dados\nOs dados deste capítulo são os mesmo usados no Capítulo 9, incluídos no arquivo dadosMater.xlsx, considerando apenas os recém-nascidos a termo e a variável altura e pesoRN:\n\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(altura, pesoRN)\nstr(dados)\n\ntibble [1,085 × 2] (S3: tbl_df/tbl/data.frame)\n $ altura: num [1:1085] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ...\n $ pesoRN: num [1:1085] 3285 3100 3100 2800 3270 ...",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#introdução",
    "href": "10-estimacao.html#introdução",
    "title": "10  Estimação",
    "section": "10.3 Introdução",
    "text": "10.3 Introdução\nA estatística inferencial é a parte da estatística que usa os resultados da amostra para tomar decisões e tirar conclusões sobre a população de onde a amostra foi retirada. A estimação e o teste de hipóteses, tomados em conjunto, constituem a inferência estatística.\nEstimação é um procedimento pelo qual um valor ou valores numéricos são atribuídos a um parâmetro populacional com base nas informações de uma amostra. Na estatística inferencial, \\(\\mu\\) é chamada de média populacional e p é chamada de proporção populacional. Existem muitos outros parâmetros populacionais, como mediana, moda, variância e desvio padrão, como observado na Seção 9.2.2.\nSe houvesse possibilidade de realizar um censo (pesquisa incluindo toda a população de interesse), não haveria necessidade dos procedimentos de estimação. Seria equivalente ao que ocorre em uma eleição, basta contar os votos, para declarar os vencedores da eleição. No entanto, em saúde, realizar censo é um procedimento caro, demorado ou virtualmente impossível. Portanto, geralmente é utilizada uma amostra da população e calculada o valor das estatísticas da amostra apropriada. Baseado nessas estatísticas, é atribuído valores ao parâmetro.\nA estatística usada para estimar um parâmetro é chamada de estimador. Assim, a média da amostra, \\(\\bar{x}\\), é um estimador da média da população, \\(\\mu\\); e a proporção da amostra, \\(\\hat{p}\\), é um estimador da proporção da população, p. Estimativa é um valor que a função estimador assume.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "href": "10-estimacao.html#estimativa-pontual-e-intervalo-de-confiança",
    "title": "10  Estimação",
    "section": "10.4 Estimativa Pontual e Intervalo de Confiança",
    "text": "10.4 Estimativa Pontual e Intervalo de Confiança\nA partir do dataframe dados, serão calculados a média e o desvio padrão da variável pesoRN (peso dos recém-nascidos em g) que, para fins didáticos, serão considerados os parâmetros dessa “população”:\n\n mu &lt;- round(mean(dados$pesoRN, na.rm = TRUE))\n sigma &lt;- round(sd(dados$pesoRN, na.rm = TRUE))\n print(x = c(mu, sigma))\n\n[1] 3216  462\n\n\nA seguir, será extraída, dessa população, uma amostra de n = 30 1 e calculado os mesmas medidas resumidoras, que se constituirão nas estimativas da amostra:\n\nset.seed (1234)\namostra &lt;- dados %&gt;% slice_sample(n = 30)\n\n# Média amostral\nx_barra &lt;- round(mean(amostra$pesoRN, na.rm = TRUE))\n\n# Desvio padrão amostral  \ns &lt;- round(sd(amostra$pesoRN, na.rm = TRUE))\n\nprint(c(x_barra, s))\n\n[1] 3222  407\n\n\nO valor de 3222g é a média amostral, \\(\\bar{x}\\), usado como um estimativa da \\(\\mu\\), é denominado de estimativa pontual. Como já mencionado anteriormente, espera-se que cada amostra selecionada produza um valor diferente da estatística amostral. Assim, o valor atribuído a uma média populacional, \\(\\mu\\), com base em uma estimativa pontual depende de qual das amostras está sendo usada. Consequentemente, a estimativa pontual atribui um valor a \\(\\mu\\) que quase sempre difere da mesma.\nPara melhorar a precisão, usa-se uma estimativa de intervalo. Em vez de atribuir um único valor para o parâmetro populacional, é construído um intervalo, acrescentando ou subtraindo um valor, chamado de margem de erro, à estimativa pontual.\nEste procedimento é conhecido como estimação por intervalo e o intervalo construído, estabelecendo um limite inferior e um limite superior em torno da estimativa amostral, é denominado de intervalo de confiança. Desta forma, é possível afirmar que o intervalo de confiança, provavelmente, contém o parâmetro populacional correspondente (Figura 10.1).\n\n\n\n\n\n\n\n\nFigura 10.1: Intervalo de Confiança.\n\n\n\n\n\nA construção do intervalo de confiança depende da obtenção da margem de erro. Este processo necessita de dois fatores:\n\ndo desvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}=\\frac{\\sigma }{\\sqrt{n}}\\), que em decorrência do Teorema do Limite Central, pode ser escrito \\(EP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\\);\ndo nível de confiança (NC) atribuído ao intervalo.\n\nPrimeiro, quanto maior for o desvio padrão de \\(\\bar{x}\\), maior será a margem de erro subtraída e adicionada à estimativa pontual. Consequentemente, o intervalo de confiança se modifica de acordo com a margem de erro. Quanto maior a margem de erro mais amplo o intervalo de confiança.\nEm segundo lugar, a quantidade subtraída e adicionada à estimativa se modifica de acordo o nível de confiança. Para ter uma maior confiança, deve-se aumentar a margem de erro, de acordo com a probabilidade declarada. Quanto maior o nível de confiança, maior a probabilidade. O nível de confiança é mostrado como \\((1 - \\alpha) \\times 100\\)%, onde \\(\\alpha\\) é o nível de significância. Tradicionalmente, o valor de \\(\\alpha\\) é igual a 0,05, mas qualquer outro valor pode ser usado.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "href": "10-estimacao.html#estimação-da-média-populacional-sigma-conhecido",
    "title": "10  Estimação",
    "section": "10.5 Estimação da média populacional: \\(\\sigma\\) conhecido",
    "text": "10.5 Estimação da média populacional: \\(\\sigma\\) conhecido\nA margem de erro para a estimativa da média populacional, \\(\\mu\\), quando se conhece o desvio padrão populacional,\\(\\sigma\\), e \\(n \\ge 30\\) ou, mesmo que \\(n &lt; 30\\), mas a população de onde amostra foi selecionada tem distribuição normal, é a quantidade que é subtraída ou adicionada ao valor da média da amostra, \\(\\bar{x}\\), para obter o intervalo de confiança para \\(\\mu\\). Desta forma, a margem de erro é igual a:\n\\[\nmargem \\quad de\\quad erro\\quad(me)= z_{(1-\\frac{\\alpha}{2})} \\times \\sigma_{\\bar{x}}\n\\]\nOu,\n\\[\nme = z_{(1-\\frac{\\alpha}{2})} \\times \\frac{\\sigma }{\\sqrt{n}}\n\\]\nLogo, o intervalo de confiança para a média populacional, \\(\\mu\\), para um nível de confiança (1 - \\(\\alpha \\times 100\\))%, é igual a:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  me\n\\]\nSe objetivo é construir um intervalo de confiança de 95%, a última equação passa a ser:\n\\[\nIC_{(1-\\alpha)}(\\mu) \\rightarrow  \\bar{x} \\pm  z_{(0,975)} \\times me\n\\]\nOnde Z é o valor crítico para o nível de confiança escolhido, obtido da tabela de distribuição normal padrão, e me é a margem de erro (\\(z_{0,975} \\times erro \\quad padrao\\)). Um intervalo de confiança de 95% significa que a área total sob a curva normal entre dois pontos em torno da média populacional, \\(\\mu\\), é igual a 95%, ou 0,95. A área das caudas é \\(\\alpha\\), ou seja, cada cauda á igual a \\(\\frac{\\alpha}{2}\\) (Figura 10.2)).\n\n\n\n\n\n\n\n\nFigura 10.2: Intervalo de Confiança de 95%.\n\n\n\n\n\nPara encontrar o valor de Z para um nível de confiança de 95%, primeiro encontram-se as áreas à esquerda desses dois pontos, \\(z_1\\) e \\(z_2\\). Esses dois valores de Z serão iguais, mas com sinais opostos. A área total sob a curva é igual a 1. A área entre \\(z_1\\) e \\(z_2\\) é igual a \\(1 - \\alpha = 0,95\\).\nA área a esquerda de \\(z_1\\) é igual a 0,025 e a área a esquerda de \\(z_2\\) é igual a 1 – 0,025 = 0,975. No R, os valores \\(z_1\\) e \\(z_2\\) podem facilmente ser obtidos com a função qnorm():\n\nprint(c(qnorm(0.025),qnorm(0.975)), 3)\n\n[1] -1.96  1.96\n\n\nDessa maneira, para uma confiança de 95%, é usado um \\(Z = 1.96\\), onde:\n\\[\np(-1,96 \\le z \\le 1,96) = 0,95\n\\] Logo,\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\sigma_{\\bar{x}})\n\\] ou\n\\[\nIC_{95\\%}(\\mu) \\rightarrow  \\bar{x} \\pm  (1.96 \\times \\frac{\\sigma}{\\sqrt{n}})\n\\]\n\n10.5.1 Cálculo do intervalo de confiança com \\(\\sigma\\) conhecido\nUsando a média dos pesos dos recém-nascidos da amostra (n = 30), \\(\\bar{x}\\)= 3222 g, e o desvio padrão populacional conhecido, \\(\\sigma\\)= 462 g, tem-se que o intervalo de confiança de 95% (IC95%), para o peso dos recém-nascidos a termo na ‘população’ de onde esta amostra é proveniente:\nDados do exemplo para o cálculo\n\n n &lt;- 30\n x_barra &lt;- 3222\n sigma &lt;- 462\n\nCom 95% de confiança a margem de erro é igual a 1,96 vezes o erro padrão da média:\n\n n &lt;- 30\n me &lt;- 1.96 * sigma/sqrt(n)\n round(me,2)\n\n[1] 165.32\n\n\nBasta, agora, adicionar e subtrair a margem de erro da média:\n\n lim_inf &lt;- x_barra - me\n lim_sup &lt;- x_barra + me\n ic95 &lt;- c(lim_inf, lim_sup)\n round(ic95, 1)\n\n[1] 3056.7 3387.3\n\n\nAssim, tem-se uma confiança de 95% de que a verdadeira média, esteja incluída no intervalo. O nome para isso é intervalo de confiança de 95% para a média populacional.\n\n\n10.5.2 Função para calcular IC com \\(\\sigma\\) conhecido\nO cálculo manual é simples, mas enfadonho, nos tempos dos computadores. Em decorrência, como o R não tem uma função para encontrar os intervalos de confiança para a média de dados com distribuição normal quando o desvio padrão da população é conhecido, foi criada uma função para cumprir essa ação. Ela necessita dos seguintes argumentos:\n\nx \\(\\to\\) conjunto de números da amostra\ns \\(\\to\\) desvio padrão populacional\nnc \\(\\to\\) nível de confiança. Padrão: nc = 0.95\n\n\nIC_z &lt;- function (x, s, nc = 0.975)\n{\n  `%&gt;%` &lt;- dplyr::`%&gt;%`\n   n &lt;- length(x)\n   me &lt;- abs(qnorm((1-nc)/2))* sigma/sqrt(n)\n   df_out &lt;- data.frame( tamanho_amostral = n, \n                         media_amostral = mean(x), \n                         margem_erro = me,\n                         'IC limite inferior'=(mean(x) - me),\n                         'IC limite superior'=(mean(x) + me)) %&gt;%\n    tidyr::pivot_longer(names_to = \"Medidas\", values_to =\"valores\", 1:5 )\n  return(df_out)\n}\n\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.95)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           165.\n4 IC.limite.inferior   3056.\n5 IC.limite.superior   3387.\n\n\nEssa função pode ser salva no seu diretório e, quando necessária, pode ser ativada com a função source(), como visto na Seção 4.8.1. Com essa função fica fácil alterar o nível de confiança, por exemplo, para 99%. Isso mudará o Z crítico para:\n\n alpha &lt;- 0.01\n p &lt;- 1-(alpha/2)\n p\n\n[1] 0.995\n\n z_critico &lt;- qnorm(p)\n round(z_critico, 2)\n\n[1] 2.58\n\n\nCom a função IC_z():\n\nIC_z(x = amostra$pesoRN, s = sigma, nc = 0.995)\n\n# A tibble: 5 × 2\n  Medidas            valores\n  &lt;chr&gt;                &lt;dbl&gt;\n1 tamanho_amostral       30 \n2 media_amostral       3222.\n3 margem_erro           237.\n4 IC.limite.inferior   2985.\n5 IC.limite.superior   3458.\n\n\nObservando o IC95% e o IC99%, verifica-se que a amplitude do intervalo aumentou com o crescimento da confiança de 95% para 99%, porque houve um aumento na margem de erro (Figura 10.3).\n\n\n\n\n\n\n\n\nFigura 10.3: Comparação do IC95% e IC99%.\n\n\n\n\n\n\n\n10.5.3 Interpretação do intervalo de confiança\nSe fossem extraídas todas as possíveis amostras de n = 30 da população de recém-nascidos a termo e construído para cada uma delas um intervalo de confiança de 95% em torno de cada média amostral, espera-se que 95% desses intervalos incluirão a média populacional e 5% não incluirão.\nO IC95% informa sobre a precisão com que a média amostral estima a média populacional desconhecida 2.\nNa Figura 10.4, são mostradas 20 amostras diferentes de tamanho n = 30, dessa população. Junto aparecem os intervalos de confiança de 95% construídos em torno dessas amostras. Observa-se que apenas uma amostra (em vermelho) não inclui a média populacional (linha tracejada vertical em azul). Pode-se afirmar com 95% de confiança que se forem extraídas muitas amostras do mesmo tamanho de uma população e construído intervalos de confiança de 95% em torno das médias dessas amostras, 95% desses intervalos de confiança incluirão a média populacional.\n\n\n\n\n\n\n\n\nFigura 10.4: Intervalos de confiança de 95% que mostra 20 replicações simuladas de amostras de n = 30 do peso do recém-nascido. Apenas um intervalo (em vermelho) não inclui a média populacional (linha vertical azul).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "href": "10-estimacao.html#estimação-da-média-populacional-sigma-desconhecido",
    "title": "10  Estimação",
    "section": "10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido",
    "text": "10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido\nCom amostras pequenas, usar o modelo normal para construir intervalos de confiança, pode gerar um erro, pois os pressupostos do teorema do limite central não são respeitados. Quando o desvio padrão populacional, \\(\\sigma\\), é desconhecido e o tamanho amostral é pequeno (&lt; 30), a estimação da média populacional é feita usando a distribuição t.\n\n10.6.1 Distribuição t\nA distribuição t, desenvolvida por William Sealy Gosset, em 1908, é semelhante à distribuição normal. Como a curva de distribuição normal, a curva de distribuição t é unimodal, simétrica (em forma de sino) em torno da média e nunca encontra o eixo horizontal. A área total sob uma curva de distribuição t é 1 ou 100%. A curva da distribuição t é mais plana do que a curva de distribuição normal padrão. Em outras palavras, ela é mais achatada e mais espalhada. No entanto, conforme o tamanho da amostra aumenta, a distribuição t aproxima-se da distribuição normal padrão.\nO formato de uma curva de distribuição t particular depende do número de graus de liberdade. O número de graus de liberdade (gl) para uma distribuição t é igual ao tamanho da amostra menos um, ou seja, \\(gl=n-1\\), veja Seção 6.3.4.3.\nO número de graus de liberdade é o único parâmetro da distribuição t. Há uma diferente distribuição t para cada número de graus de liberdade, portanto, a distribuição t se constitui em uma família de distribuições (Figura 10.5).\n\n\n\n\n\n\n\n\nFigura 10.5: Curvas de distribuição t conforme o grau de liberdade comparadas à distribuição normal.\n\n\n\n\n\nDa mesma maneira que a distribuição normal padrão, a média da distribuição padrão t é 0. Entretanto, ao contrário da distribuição normal padrão, cujo desvio padrão é 1, o desvio padrão de uma distribuição t é \\(\\sqrt{\\frac{gl}{gl-2}}\\) , para gl &gt; 2, sempre é maior do que 1. Assim, o desvio padrão de uma distribuição t é maior do que o desvio padrão da distribuição normal padrão.\nOs valores de \\({t}_{crítico}\\) podem ser obtidos usando a função qt() que usa os seguintes argumentos:\n\np \\(\\to\\) probabilidade, igual a \\(1 - \\frac{\\alpha}{2}\\), considerando-se bicaudal e \\(1 - \\alpha\\) quando unicaudal;\ndf \\(\\to\\) graus de liberdade;\nlower.tail \\(\\to\\) lógico; se TRUE, informa a probabilidade da cauda inferior. O padrão é TRUE.\n\nAssim, o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é:\n\nalpha  &lt;-  0.05\np &lt;- 1 - (alpha/2)\ngl = 10\nt &lt;- qt(p = p, df = 10, lower.tail = TRUE)\nround(t, digits = 2)\n\n[1] 2.23\n\n\nA área compreendida entre \\(\\pm\\) 2.23$ é igual a 95% (Figura 10.6):\n\\[\np(-2,23\\le t\\le 2,23)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 10.6: Distribuição t com gl = 10, bilateral.\n\n\n\n\n\nQuando se considera apenas uma das caudas (unicaudal ou unilateral), o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é\n\nt1 &lt;- qt(p = 0.95, df = 10, lower.tail = TRUE)\nround(t1, digits = 2)\n\n[1] 1.81\n\n\nAssim, a área abaixo de 1.81 é igual a 95% (Figura 10.7).\n\\[\np(t\\le 1,81)=0,95\n\\]\n\n\n\n\n\n\n\n\nFigura 10.7: Distribuição t com gl = 10, unilateral.\n\n\n\n\n\n\n\n10.6.2 Cálculo do intervalo de confiança com \\(\\sigma\\) desconhecido\nSerão utilizados nesta seção, os dados da altura de mulheres, obtidos na Seção 10.2. Suponha-se que os parâmetros sejam desconhecidos. Para estimar esses parâmetros, selecionou-se uma amostra de n = 30 desse conjunto dados. Tomando essa amostra, calcula-se a sua média e o seu desvio padrão:\n\nset.seed(2345)\namostra1 &lt;- dados %&gt;%\n  slice_sample(n = 30)\n\nx_barra1 &lt;- mean(amostra1$altura, na.rm = TRUE)\ns1 &lt;- sd(amostra1$altura, na.rm = TRUE)\nprint(round(c(x_barra1, s1),3))\n\n[1] 1.577 0.062\n\n\nA maneira mais intuitiva de estimar a média da população com base na amostra, é, simplesmente, calcular a média e o desvio padrão. Entretanto, para uma maior precisão, é sempre importante calcular o intervalo de confiança.\n\n10.6.2.1 Cálculo manual do IC\nQuando o desvio padrão da população (\\(\\sigma\\)) não é conhecido, pode-se usar o seu estimador que é o desvio padrão da amostra (s), respeitando os pressupostos (1). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\).\n\\[\nEP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\n\\]\nO intervalo de confiança para a \\(\\mu\\) para um nível de confiança (NC) de \\((1 – \\alpha) \\times100\\)% é igual a:\n\\[\nIC_{NC}(\\mu)\\rightarrow x\\pm (t_{({1-\\frac{alpha}{2})} } \\times \\frac {s}{\\sqrt{n}})\n\\]\nQuando o tamanho amostral é grande, o valor de t se aproxima do valor de z, portanto, em situações em que não se conhece o desvio padrão populacional, não há muita diferença se houver uma aproximação de t para z (Tabela 10.1).\n\n\n\n\nTabela 10.1: Comparação dos valores z e t(gl)\n\n\n\nnglzt541.962.571091.962.2330291.962.0450491.962.01100991.961.982001991.961.975004991.961.961,0009991.961.96\n\n\n\n\n\nA amostra1 de n = 30, \\(\\overline x\\) = 1.577m e \\(s\\) = 0.062m. Essas estimativas servirão para o cálculo do intervalo de confiança, usando uma distribuição t bicaudal e um nível de significância \\(\\alpha = 0,05\\).\n\nn1 &lt;-  length(amostra1$altura)\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\n# Graus de liberdade  \ngl &lt;- n1 - 1\n# Valor t crítico  \ntc &lt;-  qt(p, gl, lower.tail = TRUE)\n# Erro padrão\nEP1 &lt;- round(s1/sqrt(n1),3)\nprint(round(c(tc, EP1),3))\n\n[1] 2.045 0.011\n\n\nCom esses dados, calcula-se o intervalo de confiança de 95%:\n\nme1 &lt;- tc*EP1\nlim_inf &lt;- x_barra1 - me1\nlim_sup &lt;- x_barra1 + me1\nic95 &lt;- c(lim_inf, lim_sup)\nround(ic95, 2)\n\n[1] 1.55 1.60\n\n\n\n\n10.6.2.2 Cálculo usando uma função do R\nO R possui algumas funções que calculam o intervalo de confiança para variáveis numéricas, baseadas na distribuição t. Entre elas, a função CI(), incluída no pacote Rmisc. Esta função tem dois argumentos:\n\nx ⟶ vetor de dados;\nci ⟶ intervalo de confiança a ser calculado\n\n\nIC95 &lt;- CI(amostra1$altura, ci = 0.95)\nround(IC95, 2)\n\nupper  mean lower \n 1.60  1.58  1.55",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "href": "10-estimacao.html#intervalo-de-confiança-para-uma-proporção-populacional",
    "title": "10  Estimação",
    "section": "10.7 Intervalo de Confiança para uma proporção populacional",
    "text": "10.7 Intervalo de Confiança para uma proporção populacional\n\n10.7.1 Dados para estimar a proporção populacional\nAqui, será utilizada uma amostra aleatória de n = 60 do conjunto de dados dadosMater.xlsx (sem filtro para os recém-nascidos a termo) para estimar a proporção de mulheres fumantes.\n\nset.seed(2346)\ndados &lt;- read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  select (fumo) %&gt;% \n  slice_sample(n = 60)\n\nstr(dados)\n\ntibble [60 × 1] (S3: tbl_df/tbl/data.frame)\n $ fumo: num [1:60] 2 2 1 2 2 2 2 2 2 1 ...\n\n\nA seleção mostra que temos 60 observações da variável fumo e que a mesma está classificada como numérica (num), 1 e 2. Onde 1 representa as mulheres fumantes. A variável é categórica e deve ser transformada para fator.\n\ndados$fumo &lt;- factor (dados$fumo, \n                      levels = c (1,2), \n                      label = c (\"fumante\", \"não fumante\"))\n\n\n\n10.7.2 Cálculo da estimativa pontual da proporção\nNessa amostra, a proporção de fumantes é:\n\ntab &lt;- table(dados$fumo)\ntab\n\n\n    fumante não fumante \n         14          46 \n\ntabFumo &lt;- round (prop.table (tab), 3)\ntabFumo\n\n\n    fumante não fumante \n      0.233       0.767 \n\n\n\n\n10.7.3 Cálculo do intervalo de confiança para a proporção\nCálculo manual com aproximação normal\n1ª etapa: verificar a premissa de que quando a proporção populacional é desconhecida a proporção pontual (\\(\\hat p\\)) e o seu complemento (\\(\\hat q = 1 - \\hat p\\)) multiplicados, cada um, por \\(n\\), devem ser maior do que 5.\n\nn &lt;- length(dados$fumo)\n(tabFumo) * n\n\n\n    fumante não fumante \n      13.98       46.02 \n\n\nComo se observa, ambos os valores são maiores do que 5.\n2ª Etapa: O intervalo pode ser estimado pela distribuição normal e é necessário calcular o z_crítico:\n\nalpha &lt;- 0.05\np &lt;-  1 - alpha/2\nzc &lt;- qnorm (p, mean = 0, sd = 1)\nround(zc, 2)\n\n[1] 1.96\n\n\n3ª Etapa: Cálculo do erro padrão da proporção (\\(\\sqrt \\frac {\\hat p \\times \\hat q}{n}\\)) e da margem de erro (veja também a Seção 9.6):\n\n# Extração da proporção amostral do tabFumo\nprop &lt;- tabFumo [1]\n# Cálculo do EP amostral\nEP &lt;- sqrt((prop * (1 - prop))/n)\n# Cálculo da margem de erro(me)\nme &lt;- zc * EP\n# dados necessários para o cálculo do IC95%\nprint(c(prop, me), digits = 3)\n\nfumante fumante \n  0.233   0.107 \n\n\n4ª Etapa: Intervalo de confiança\n\nic_prop &lt;- c((prop - me), (prop + me))\nround(ic_prop, 3)\n\nfumante fumante \n  0.126   0.340 \n\n\nCálculo usando uma função\nO chamado Intervalo de Confiança Exato corrigem as deficiências da aproximação normal. O R tem uma função para este cálculo: BinomCI() do pacote DescTools(2). É preferível usar o método de Clopper e Pearson que fornece o IC exato.\nOs argumentos da função BinomCI() são:\n\nx \\(\\to\\) é o número de desfechos, sucessos;\nn \\(\\to\\) é o tamanho da amostra, número de ensaios;\np \\(\\to\\) probabilidade, hipótese nula; se ignorada o padrão é 0,50;\nconf.level \\(\\to\\) nível de confiança, o padrão é 0.95;\nmethod \\(\\to\\) possui vários métodos para calcular intervalos de confiança para uma proporção binomial como: “clopper-pearson” (exact interval), “wilson”, “wald”, “agresti-coull”, “jeffreys”, “modified wilson”, “modified jeffreys”, “arcsine”, “logit”, “witting”, “pratt”. O método padrão é o de “wilson”. Qualquer outro método, há necessidade de solicitar;\nsides \\(\\to\\) hipótese alternativa padrão “two.sided” (bilateral), mas pode ser “right” ou “left” (unilateral a direita ou a esquerda, respectivamente).\n\n\nx &lt;-  tab[1]\nIC &lt;- BinomCI (x, \n               n, \n               conf.level = 0.95, \n               method = \"clopper-pearson\")\nround(IC, 3)\n\n       est lwr.ci upr.ci\n[1,] 0.233  0.134   0.36\n\n\nObserve que existe uma pequena diferença entre os valores da aproximação normal e o exato, com o método de “clopper-pearson”\n\n\n\n\n1. Motulsky H. The Theory of Confidence Intervals. Em: Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking. Second Edition. New York, NY: Oxford University Press; 2010. p. 96–102. \n\n\n2. Signorell A et al. DescTools: Tools for Descriptive Statistics [Internet]. 2022. Disponível em: https://cran.r-project.org/package=DescTools",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "10-estimacao.html#footnotes",
    "href": "10-estimacao.html#footnotes",
    "title": "10  Estimação",
    "section": "",
    "text": "Repetindo, é importante lembrar que toda vez que for extraída uma nova amostra, o resultado será um conjunto de números diferentes e, em consequência, a média será diferente. Por isso, se for importante repetir o mesmo resultado, deve-se usar a função set.seed(). Consulte a Seção 7.7.2.4.↩︎\nAnteriormente, mostrou-se a media populacional por uma questão didática. A regra é não se conhecer a média populacional, razão da importância do intervalo de confiança↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html",
    "href": "11-testeHipoteses.html",
    "title": "11  Teste de Hipóteses",
    "section": "",
    "text": "11.1 Pacotes necessários neste capítulo\npacman::p_load(dplyr,\n               lsr,\n               pwr,\n               readxl,\n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#sec-dadosth",
    "href": "11-testeHipoteses.html#sec-dadosth",
    "title": "11  Teste de Hipóteses",
    "section": "11.2 Dados do exemplo",
    "text": "11.2 Dados do exemplo\nConsidere o mesmo arquivo dadosMater.xlsx, usado várias vezes neste livro e disponível para consulta na Seção 5.3. Após a leitura do arquivo com a função read_excel() do pacote readxl, serão filtrados os recém-nascidos a termo (37 a 42 semanas de gestação) e selecionadas as varáveis sexo e pesoRN. Considerando esses dados como uma “população” para fins didáticos, será extraída uma amostra de 200 observações e atribuido o resultado ao objeto dados:\n\ndados &lt;- readxl::read_excel(\"dados/dadosMater.xlsx\") %&gt;% \n  dplyr::filter(ig&gt;=37 & ig&lt;42) %&gt;% \n  select(sexo, pesoRN) %&gt;%\n  slice_sample(n = 200)\n\n\n11.2.1 Exploração e transformação dos dados\nInicialmente, para ter uma visão da estrutura dos dados, usa-se:\n\nstr(dados)\n\ntibble [200 × 2] (S3: tbl_df/tbl/data.frame)\n $ sexo  : num [1:200] 1 1 1 1 1 1 2 2 1 2 ...\n $ pesoRN: num [1:200] 3270 2590 3340 3645 3175 ...\n\n\nA seguir, transformar a variável sexo em fator:\n\ndados$sexo &lt;- factor(dados$sexo,\n                   levels = c(1, 2),\n                   labels = c(\"masc\", \"fem\"))\n\nEste conjunto de dados fica restrito, portanto, a 200 casos, contendo duas variáveis sexo e pesoRN, necessárias neste capítulo e assim resumidas:\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr::summarise (n = n(),\n                    media = mean(pesoRN, na.rm = TRUE),\n                    dp = sd(pesoRN, na.rm = TRUE))\nresumo\n\n# A tibble: 2 × 4\n  sexo      n media    dp\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 masc    115 3254.  504.\n2 fem      85 3118.  485.\n\n\nEsta amostra de 115 meninos e 85 meninas, informa que os meninos têm, em média, 3254 g ao nascer e as meninas 3118 g. Esta diferença de peso entre os sexos pode ter ocorrido devido ao acaso. Portanto, há necessidade de realizar um teste de hipóteses para tomar uma decisão sobre o parâmetro populacional. Esta diferença é grande o suficiente para rejeitar a hipótese de igualdade entre os pesos e concluir que existe uma diferença real entre eles?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#introdução",
    "href": "11-testeHipoteses.html#introdução",
    "title": "11  Teste de Hipóteses",
    "section": "11.3 Introdução",
    "text": "11.3 Introdução\nNo capítulo anterior, foi discutido aspectos relacionados à estimação, que se constitui, junto com o teste de hipótese, em procedimentos básicos da estatística inferencial. Em um teste de hipóteses, testa-se uma teoria ou crença sobre um parâmetro populacional (1). Na maioria das vezes, como mencionado anteriormente, obtém-se informações a partir de uma amostra em função da impossibilidade ou dificuldade de se conseguir essas informações a partir da população. Portanto, extrapolar ou estender os resultados, obtidos de uma amostra, para a população, significa aceitá-los como representações adequadas da mesma.\nSabe-se que as estimativas amostrais diferem dos valores reais (populacionais) e o objetivo dos testes de hipóteses é estabelecer a probabilidade de essa diferença ser explicada pelo acaso. O teste de hipóteses fornece um sistema referencial para a tomada de decisão sobre a adequação ou não dos dados amostrais serem representativos de uma população. Este sistema referencial é a distribuição de probabilidade do evento observado (2).\nInicialmente é importante fazer uma distinção entre hipótese de pesquisa e hipótese estatística. Uma hipótese de pesquisa é uma afirmação que expressa a relação esperada entre as variáveis de um estudo científico. Ela é baseada em uma pergunta de pesquisa e serve para orientar a coleta e análise dos dados. Uma hipótese de pesquisa pode ser confirmada ou refutada pelos resultados do estudo. Um exemplo de hipótese de pesquisa é: “O tabagismo durante a gestação interfere sobre o peso dos conceptos”. Uma hipótese de pesquisa corresponde àquilo que se quer acreditar sobre o mundo. Uma hipótese estatística é uma afirmação relacionada aos parâmetros de uma população. Baseia-se em uma hipótese de pesquisa e serve para testar a validade da mesma usando técnicas estatísticas. Uma hipótese estatística pode ser aceita ou rejeitada com um certo nível de confiança. A hipótese estatística deve ter uma relação clara com as hipóteses de pesquisa Por exemplo: “A média de peso dos recém-nascidos de mães fumantes é menor do que o das não fumantes”; “A média de peso dos recém-nascidos masculinos é igual ao peso dos recém-nascidos femininos”, ou ainda, “A média de peso dos recém-nascidos masculinos é diferente do peso dos recém-nascidos femininos”. Todos esses exemplos são legítimos de uma hipótese estatística porque são afirmações sobre um parâmetro populacional e estão significativamente relacionados à hipótese de pesquisa.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#hipótese-nula-e-alternativa",
    "href": "11-testeHipoteses.html#hipótese-nula-e-alternativa",
    "title": "11  Teste de Hipóteses",
    "section": "11.4 Hipótese nula e alternativa",
    "text": "11.4 Hipótese nula e alternativa\nEm função da hipótese de pesquisa, mencionada anteriormente, foram gerados os dados do exemplo. A hipótese de pesquisa corresponde ao que se quer acreditar, “o sexo interfere no peso dos neonatos”. Para refutar ou não essa afirmação constrói-se um teste de hipótese para verificar se ela é compatível ou não com os dados disponíveis (3).\nNo teste de hipóteses (TH), existem dois tipos de hipóteses, definidas como:\nHipótese nula(\\(H_{0}\\)): hipótese que afirma a não existência de diferença entre os grupos e, portanto, a diferença observada é atribuível ao acaso. É a hipótese a ser testada, aquela que se busca afastar, demonstrando que é, provavelmente 1, falsa, não válida. É denotada como:\n\\[\nH_{0}: \\mu_{1}= \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2}=0\n\\]\nHipótese alternativa (\\(H_{1} \\quad ou \\quad H_{a}\\)): é a hipótese contrária, como o nome diz, alternativa à \\(H_{0}\\). Representa a posição de uma nova perspectiva, a conclusão que será apoiada se \\(H_{0}\\) for rejeitada. Ela supõe que realmente exista uma diferença entre os grupos. É a hipótese que o pesquisador pretende comprovar. É denotada, em geral, simplesmente como havendo uma diferença entre os grupos, sem indicar uma direção, hipótese bilateral ou bicaudal:\n\\[\nH_{1}: \\mu_{1} \\neq  \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} \\neq  0\n\\]\nOu, se houver uma suspeita, através de um conhecimento prévio, apontar uma direção para a diferença, ou seja, usar uma hipótese unilateral ou monocaudal. Neste caso existe duas possibilidade:\n\nUnilateral à direita:\n\n\\[\nH_{1}: \\mu_{1} &gt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &gt; 0\n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\le \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\le 0\n\\] 2)\n\nUnilateral à esquerda:\n\n\\[\nH_{1}: \\mu_{1} &lt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &lt; 0   \n\\]\nConsequentemente,\n\\[\nH_{0}: \\mu_{1} \\ge \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\ge 0\n\\]\nA \\(H_{0}\\) e \\(H_{1}\\) são opostas e mutuamente exclusivas. No teste de hipótese calcula-se a probabilidade de obter os resultados encontrados caso não haja efeito na população, ou seja, caso a \\(H_{0}\\) seja verdadeira. Portanto, o TH é um teste de significância para a \\(H_{0}\\).\n\n11.4.1 Exemplo\nVoltando à hipótese de pesquisa, usando os dados da Seção 11.2, as hipóteses estatísticas seriam escritas da seguinte maneira, considerando uma hipótese alternativa bilateral.\n\\[\nH_{0}: \\mu_{peso_{masc}} = \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}}=0\n\\]\n\\[\nH_{1}: \\mu_{peso_{masc}} \\neq \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}} \\neq 0\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "href": "11-testeHipoteses.html#escolha-do-teste-estatítico-e-regra-de-decisão",
    "title": "11  Teste de Hipóteses",
    "section": "11.5 Escolha do teste estatítico e regra de decisão",
    "text": "11.5 Escolha do teste estatítico e regra de decisão\n\n11.5.1 Teste estatístico\nUsa-se um teste estatístico para testar as hipóteses estabelecidas. Este depende do tipo de distribuição da variável, por exemplo, teste z, teste t, teste F, qui-quadrado (\\(\\chi^2\\)). Cada teste fornece um valor para dirigir a decisão de rejeitar ou não a hipótese nula. Essa decisão depende da magnitude do teste valor. O nome para esse indicador, calculado para orientar a escolha, é estatística de teste. Para fazer isso, há necessidade de determinar qual seria a distribuição amostral da estatística de teste se a hipótese nula fosse realmente verdadeira. Depois de analisar esse valor, decide-se se a hipótese nula está correta ou, caso contrário, ela é rejeitada em favor da alternativa.\nÉ fundamental lembrar que cada teste estatístico tem suas características e seus pressupostos que devem ser analisados para garantir a validade das estatísticas de teste. Para uma boa parte deles, por exemplo, deve-se verificar se os dados se ajustam à distribuição normal (normalidade), a igualdade das variâncias (homocedasticidade), independência entre os grupos, tipo de correlação, etc.\n\n\n11.5.2 Regra de decisão\nRealizado o teste estaístico, para rejeitar ou não rejeitar a \\(H_{0}\\), partindo do pressuposto de que ela é verdadeira, há necessidade de determinar uma regra de decisão que permita uma declaração fundamentada. Essa regra de decisão cria duas regiões, uma região de rejeição e uma região de não rejeição da \\(H_{0}\\), demarcadas por um valor crítico.\nEste valor de referência é determinado pelo nível de significância, \\(\\alpha\\), e deve ser explicitamente mencionado antes de se iniciar a pesquisa, pois é baseado nele que se fundamentam as conclusões da mesma. O nível de significância corresponde a probabilidade de rejeitar uma hipótese nula verdadeira. Quando a hipótese alternativa não tem uma direção definida, a área de rejeição, \\(\\alpha\\), é colocada nas duas caudas (Figura 11.1), superior), dividindo a probabilidade (\\(\\frac {\\alpha}{2}\\)); quando houver indicação prévia de um sentido, a área de rejeição ficará a direita (Figura 11.1), inferior) ou a esquerda dependendo da direção escolhida.\n\n\n\n\n\n\n\n\nFigura 11.1: Regiões bicaudais (acima) e monocaudal à direita (abaixo) de rejeição e não rejeição da hipótese\n\n\n\n\n\nQuais valores exatos da estatística de teste deve-se associar à hipótese nula e quais valores exatos devem ser associados à hipótese alternativa? Para encontrar a região de rejeição, deve-se levar em consideração:\n\nA estatística do teste deve ser muito grande ou muito pequena para que a hipótese nula seja rejeitada;\n\nDistribuição da variável de teste, que depende da distribuição da população em estudo e do tamanho da amostra;\n\nNível se significância adotado, em geral, usa-se um \\(\\alpha\\) = 0,05, o que equivale a dizer que a região de rejeição abrange 5% da distribuição.\n\nÉ importante entender bem este último ponto. A região de rejeição corresponde aos valores da estatística de teste para os quais se rejeita a hipótese nula e a distribuição amostral em questão descreve a probabilidade de obtermos um determinado valor da estatística de teste se a hipótese nula for efetivamente verdadeira. Agora, suponha-se que foi escolhido uma região de rejeição que cobre 10% da distribuição amostral e que a hipótese nula é realmente verdadeira. Qual seria a probabilidade de rejeitar incorretamente a hipótese nula? Obviamente, a resposta é 10%! E o teste usado teria um nível \\(\\alpha\\) = 0,10. Ou seja, se a hipótese nula é verdadeira e for rejeitada, foi cometido um erro.\n\n11.5.2.1 Erros de decisão\nComo se observa, ao se tomar uma decisão existe a possibilidade de se cometer erros. O primeiro erro é denominado de erro tipo I e ocorre quando, baseado na regra de decisão escolhida, uma hipótese nula verdadeira é rejeitada. Nesse caso, tem-se um resultado falso positivo. Há uma conclusão de que existe um efeito quando na verdade ele não existe. A probabilidade de cometer esse tipo de erro é \\(\\alpha\\), o mesmo usado como nível de significância no estabelecimento da regra de decisão.\n\\[\nP(rejeitar \\quad H_{0}|H_{0} \\quad verdadeira) = \\alpha\n\\]\nQual o valor de \\(\\alpha\\) que pode representar forte evidencia contra \\(H_{0}\\), reduzindo a possibilidade de erro tipo I?\nO valor de \\(\\alpha\\) escolhido, apesar de arbitrário, deve corresponder a importância do que se pretende demonstrar, quanto mais importante, menor deve ser o valor de \\(\\alpha\\). Nesses casos, não se quer rejeitar incorretamente \\(H_{0}\\) mais de 5% das vezes. Isso corresponde ao nível de significância mais usado de 0,05 (\\(\\alpha = 0,05\\)). Em algumas situações também são utilizados 0,01 e 0,10. Como mencionado, o valor de \\(\\alpha\\) deve ser escolhido antes de iniciar o estudo.\nExiste uma outra possibilidade de erro, denominado de erro tipo II, que ocorre quando a hipótese nula é realmente falsa, mas com base na regra de decisão escolhida, não se rejeita essa hipótese nula. Nesse caso, o resultado é um falso negativo; não se conseguiu encontrar um efeito que realmente existe. A probabilidade de cometer esse tipo de erro é chamada de \\(\\beta\\).\n\\[\nP(não \\quad rejeitar \\quad H_{0}|H_{0} \\quad falsa) = \\beta\n\\]\nNa construção de um teste de hipótese, o erro tipo II é considerado menos grave que o erro tipo I. Entretanto, ele é bastante importante. Tradicionalmente, adota-se o limite de 0,10 a 0,20 para o erro tipo II.\nNa Figura 11.2 estão resumidas as possíveis consequências na tomada de decisão em um teste de hipótese (4).\n\n\n\n\n\n\n\n\nFigura 11.2: Tomada de decisão e erros.\n\n\n\n\n\n\n\n\n11.5.3 Exemplo (continuação)\nContinuando com o exemplo da Seção 11.4.1, aceita-se que os pesos dos recém-nascidos de ambas as amostras tenham distribuição normal e que as variâncias são semelhantes. Apesar de o desvio padrão (\\(\\sigma\\)) da população-alvo ser conhecido (504, 485g), será suposto que ele é desconhecido 2. Portanto, o teste t de amostras independentes será o teste escolhido como o teste estatístico. A hipótese alternativa é bilateral e o \\(\\alpha\\) = 0,05.\nA distribuição t é dependente dos grau de liberdade, que para duas amostras independentes é igual \\(gl=n_1+n_2-2\\). Para os dados em uso, tem-se:\n\n n1 &lt;- resumo$n[1]\n n2 &lt;- resumo$n[2]\n gl &lt;- n1 + n2 - 2\n gl\n\n[1] 198\n\n\nPara o nível de significância escolhido, o valor crítico de t para gl = 198 e uma hipótese alternativa bilateral pode ser obtido da seguinte maneira:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round(qt(p, gl),3)\ntc\n\n[1] 1.972\n\n\nA partir do cálculo do valor crítico de t, podemos estabelecer a regra de decisão para as hipóteses estatísticas:\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\n|t_{calculado}| \\ge |t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\nO teste t pode ser calculado no R, usando a função t_teste() do pacote rstatix. Esta função usa, entre outros, os seguintes argumentos:\n\ndata \\(\\to\\) dataframe contendo as variáveis da formula;\nformula \\(\\to\\) uma fórmula da forma x ~ grupo onde x é uma variável numérica que fornece os valores dos dados e grupo é um fator;\npaired \\(\\to\\) lógico; indicando se o teste é pareado. Padrão é FALSE;\nvar.equal \\(\\to\\) lógico: se TRUE, uma variância combinada é usada; caso contrário, a aproximação de Welch dos graus de liberdade é usada\nalternative \\(\\to\\) two.sided (padrão) ou greater ou less.\n\n\nteste &lt;- rstatix::t_test(data = dados, \n                         formula = pesoRN~sexo, \n                         alternative = \"two.sided\",\n                         detailed = TRUE)\nteste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.    group1 group2    n1    n2 statistic      p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     136.     3254.     3118. pesoRN masc   fem      115    85      1.93 0.0556\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída do teste mostra uma estatística de teste 3 igual a 1.926. Esta é maior do que o t_crítico = 1.972, consequentemente, rejeita-se a hipótese nula e conclui-se, com uma confiança de 95%, que existe uma diferença estatisticamente significativa no peso dos recém-nascidos entre os sexos. Esta diferença é em média igual a 136 g (IC95%: -3, 275), \\(peso_{meninos} &gt; peso_{meninas}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#sec-valorp",
    "href": "11-testeHipoteses.html#sec-valorp",
    "title": "11  Teste de Hipóteses",
    "section": "11.6 Valor P do teste",
    "text": "11.6 Valor P do teste\nNas seções anteriores, foi discutido um procedimento onde se encontrou o valor de probabilidade tal que uma dada hipótese nula é rejeitada ou não é rejeitada, de acordo com o nível de significância, \\(\\alpha\\), fixado, pelo pesquisador, no início da pesquisa.\nEssa abordagem do valor de probabilidade, mais comumente chamada de abordagem do valor P, fornece esse valor. Uma vez realizada a pesquisa, o pesquisador calcula a probabilidade de obter um resultado tão ou mais extremo que o observado, uma vez que a hipótese nula é verdadeira. O valor P também é conhecido como nível descritivo do teste (5).\nO objetivo de um teste estatístico é transformar em probabilidade a magnitude do desvio verificado em relação ao valor esperado, fornecendo o valor P. A partir daí pode-se, também, definir a regra de decisão, usando esse valor P. Toma-se o valor predeterminado (em geral, 0,05) de \\(\\alpha\\) e, então, compara-se o valor P com \\(\\alpha\\) e toma-se a decisão. Usando essa abordagem, rejeita-se a \\(H_{0}\\) se o valor P &lt; \\(\\alpha\\) e não se rejeita se o valor P &gt; \\(\\alpha\\). Costuma-se dizer que se o valor P &lt; \\(\\alpha\\), o resultado é significativo e não significativo quando P &gt; \\(\\alpha\\).\nUma boa parte dos pesquisadores, principalmente no início da carreira, ficam empolgados pelo conhecimento do valor P. Entretanto, deve ser sempre lembrado que encontrar o valor P não é o único foco da pesquisa. O foco deve estar dirigido ao tamanho do efeito (effect size). O valor P obtido pelo teste estatístico, vai informar apenas sobre a probabilidade de se cometer erro ao rejeitar ou não rejeitar a hipóteses nula.\n\n11.6.1 Exemplo (continuação)\nO teste realizado, t_test(), fornece o valor P = 0.0556. Este valor é menor do que \\(\\alpha\\) e leva as mesmas conclusões da Seção 11.5.3.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#poder-do-teste",
    "href": "11-testeHipoteses.html#poder-do-teste",
    "title": "11  Teste de Hipóteses",
    "section": "11.7 Poder do teste",
    "text": "11.7 Poder do teste\nO poder do teste estatístico é a probabilidade de que um teste de hipótese rejeite corretamente a hipótese nula quando uma hipótese alternativa específica é verdadeira. É denotado comumente por \\(1 - \\beta\\) e representa a capacidade de um teste para detectar um efeito, se esse efeito realmente existir. O poder varia de 0 a 1 e, à medida que o poder do teste aumenta, a probabilidade \\(\\beta\\) de cometer um erro tipo II diminui.\n\\[\nPoder \\quad do \\quad teste = P(rejeitar \\quad H_{0}|H_{0} \\quad falsa)\n\\]\n\nNa Figura Figura 11.3, visualiza-se o poder em verde mais escuro. Em um teste de hipótese, o valor \\(\\alpha\\) sempre é estabelecido com antecedência, que geralmente é definido como 0,05, de modo que a taxa de erro do Tipo I é definida antes mesmo de se iniciar o teste. Em seguida, pode-se calcular o valor crítico mínimo necessário para rejeitar \\(H_0\\). É possível traçar uma linha da distribuição da hipótese nula até a distribuição da hipótese alternativa e separar a área sob a curva em duas partes. Se o valor t calculado cair à esquerda da linha tracejada, não se consegue rejeitar \\(H_0\\) quando \\(H_1\\) for verdadeira e é cometido um erro do Tipo II. Se o valor calculado cair à direita, rejeita-se \\(H_0\\) quando \\(H_1\\) é verdadeira e a decisão é correta. Portanto, a área à direita da curva é o poder.\n\n\n\n\n\n\n\n\nFigura 11.3: Nível de significância, probabilidade de erro tipo II, poder e nível de confiança em um teste de hipótese e a região de rejeição da hipótese nula (à direita da linha vertical tracejada).\n\n\n\n\n\nO poder do teste depende de vários fatores, como:\n\nO nível de significância do teste, que é a probabilidade de rejeitar a hipótese nula quando ela é verdadeira (erro tipo I).\nA magnitude do efeito, que é a diferença entre o valor real do parâmetro e o valor considerado na hipótese nula.\nA variabilidade da população, que é medida pelo desvio padrão ou pela variância dos dados.\nO tamanho da amostra, que é o número de observações coletadas para o teste.\n\nEm geral, pode-se dizer:\n\nquanto maior o nível de significância, maior o poder do teste;\n\nquanto maior a magnitude do efeito, maior o poder do teste;\n\nquanto menor a variabilidade da população, maior o poder do teste;\n\nquanto maior o tamanho da amostra, maior o poder do teste.\nExistem diferentes métodos para calcular o poder do teste, dependendo do tipo de teste e da distribuição dos dados. Por exemplo, para um teste de uma média com variância desconhecida, usa-se a distribuição t de Student com \\(n - 1\\) graus de liberdade. Para um teste de duas proporções, usa-se a distribuição normal aproximada. A análise de poder é uma ferramenta útil para planejar um estudo e determinar o tamanho da amostra necessário para obter um poder desejado. Ela também pode ser usada para avaliar a qualidade de um estudo realizado e verificar se o teste foi capaz de detectar um efeito relevante.\n\n\n11.7.1 Exemplo (continuação)\nO teste t retornou um resultado significativo, com valor de t = 1.926 &gt; 1.972, com P = 0.0556. Um resultado significativo não informa sobre a magnitude do efeito. Para isso, lançamos mão do teste d de Cohen que pode ser calculado, usando a função cohensD() do pacote lsr:\n\nd &lt;- lsr::cohensD (data = dados, formula = pesoRN ~ sexo)\nd\n\n[1] 0.2738968\n\n\nNa Seção 12.2.6.1, se entrará em maiores detalhes, por enquanto, será assumido que a magnitude do efeito é pequena.\nDe posse do valor do d de Cohen, é possível calcular, através da função pwr.t.test() do pacote pwr, o poder do teste estatístico. Os argumentos dessa função são:\n\nn \\(\\to\\) número de observações por amostra;\nd \\(\\to\\) magnitude do efeito, d de Cohen;\nsig.level \\(\\to\\) nível de significância (padrão = 0.05);\npower \\(\\to\\) poder do teste;\ntype \\(\\to\\) tipo de teste (one- , two- ou paired-samples);\nalternative \\(\\to\\) hipótese alternativa, deve ser “one-sided” ou “two-sided (padrão).\n\nO parâmetro que se quer calcular deve ser passado como NULL. Assim, o poder do teste estatístico do exemplo é:\n\npoder &lt;- pwr::pwr.t.test(n = 150,\n                         d = d,\n                         sig.level = 0.05, \n                         power = NULL,\n                         type = \"two.sample\",\n                         alternative = \"two.sided\")\npoder\n\n\n     Two-sample t test power calculation \n\n              n = 150\n              d = 0.2738968\n      sig.level = 0.05\n          power = 0.657049\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nA saída mostra que no lugar do NULL, aparece o poder do teste estatístico. Ou seja, o poder foi de 0.657 , consequentemente, como \\(\\beta = 1 – Poder\\), então, \\(\\beta\\) = 0.343.\nO poder geralmente é definido em 0,80 (ou 0,90). Isto significa que se existirem efeitos verdadeiros a serem encontrados em 100 estudos diferentes com 80% de poder, apenas 80 em 100 testes estatísticos irão realmente detectá-los. Se não for garantido poder suficiente, é possível que nenhum efeito seja detectado, por isso, deve-se calcular o tamanho amostral necessário, antes de iniciar qualquer estudo, para garantir o poder pretendido.\n\n\n\n\n1. Kelen GD, Brown CB, Ashton J. Statistical reasoning in clinical trials: hypothesis testing. Am J Emerg Med. 1988;1(1):52–61. \n\n\n2. Menezes RX de, Burattini MN. Testes de Hipótese e intervalos de Confiança. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n3. Guyatt G, Jaeschke R, Heddle N, et al. Basic statistics for clinicians: 1. Hypothesis testing. CMAJ: Canadian Medical Association Journal. 1995;152(1):27. \n\n\n4. Fletcher RH, Fletcher SW, Fletcher GS. Acaso. Em: Epidemiologia Clínica: Elementos Essenciais. Quinta Edição. Artmed Editora; 2014. p. 108–9. \n\n\n5. Menezes RX de, Burattini MN. Testes de Hipótese e intervalos de Confiança. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 225–41.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "11-testeHipoteses.html#footnotes",
    "href": "11-testeHipoteses.html#footnotes",
    "title": "11  Teste de Hipóteses",
    "section": "",
    "text": "Ter em mente que nunca se pode saber com total certeza se existe um efeito na população.↩︎\nEsta foi uma suposição inicial! Fingiu-se que os dados do arquivo dadosMater.xlsx como filtro para os recém-nascidos a termo é a “população”↩︎\nPara ver todas as estatísticas do teste, basta escrever teste$ e apertar a tecla TAB do teclado e surgirá um menu para escolha.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Teste de Hipóteses</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html",
    "href": "12-teste-t.html",
    "title": "12  Comparação entre duas médias",
    "section": "",
    "text": "12.1 Pacotes necessários para este capítulo\npacman::p_load(car, \n               effectsize, \n               flextable,\n               ggpubr, \n               ggsci, \n               knitr,\n               readxl, \n               rstatix, \n               tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#teste-t-para-amostras-independentes",
    "href": "12-teste-t.html#teste-t-para-amostras-independentes",
    "title": "12  Comparação entre duas médias",
    "section": "12.2 Teste t para amostras independentes",
    "text": "12.2 Teste t para amostras independentes\nO teste t de amostras independentes é usado para comparar duas médias de amostras de grupos não relacionados. Isso significa que há pessoas diferentes fornecendo escores para cada grupo. O objetivo desse teste é determinar se as amostras são diferentes uma da outra\n\n12.2.1 Dados usados nesta seção\nSuponha que. em uma determinada Universidade, tenham sido coletadas as notas de Bioestatística de uma turma de 40 alunos. Estes dados estão aqui. Salve o mesmo no seu diretório de trabalho para a leitura dos dados.\n\n12.2.1.1 Leitura dos dados\nPara a leitura dos dados, será usada a função read_excel() incluída no pacote readxl, que precisa ser instalado e carregado. Os dados serão recebidos por um objeto que será denominado de dados:\n\ndados &lt;- readxl::read_excel(\"dados/dadosNotas.xlsx\")\n\nPara visualizar os dados, pode-se usar a função str():\n\nstr(dados)\n\ntibble [40 × 2] (S3: tbl_df/tbl/data.frame)\n $ notas: num [1:40] 63.1 76.3 57.7 66.9 73.1 70.3 63.6 75.7 73.5 83 ...\n $ sexo : chr [1:40] \"F\" \"F\" \"F\" \"F\" ...\n\n\nObserva-se que existem 40 alunos, sendo 20 mulheres e 20 homens. A variável altura é uma variável numérica que corresponde a a nota centesimal e sexo é uma variável categórica, onde F são as mulheres e M os homens.\n\n\n12.2.1.2 Exploração e resumo dos dados\nInicialmente, a variável sexo será transformada em fator:\n\ndados$sexo &lt;- as.factor(dados$sexo)\n\nA seguir, calcular a média e o desvio padrão da variável notas de acordo com sexo, usando a função group_by () e summarise do pacote dplyr\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;% \n  dplyr:: summarise(n = n(),\n                    media = mean(notas, na.rm = TRUE),\n                    dp = sd(notas, na.rm = TRUE),\n                    mediana = median(notas, na.rm = TRUE),\n                    Q1 = quantile(notas,0.25, na.rm = TRUE),    \n                    Q3 = quantile(notas, 0.75, na.rm = TRUE),\n                    me = 1.96 * dp/sqrt(n)) \nresumo\n\n# A tibble: 2 × 8\n  sexo      n media    dp mediana    Q1    Q3    me\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 F        20  68.4  7.79    68.6  63.2  73.8  3.41\n2 M        20  59.9  7.34    59.2  54.3  63.5  3.22\n\n\nA saída informa que a média das notas das mulheres é 68.4, bem acima da notas dos homens, mostrando uma diferença de 8.5. Parece que o desempenho das mulheres em Bioestatística é melhor do que o dos homens!\nAlém do resumo numérico, é interessante construir um gráfico do tipo boxplot (Figura 12.1), usando o pacote ggplot2 (veja Seção 6.6) para observar a distribuição dos dados:\n\nggplot2::ggplot(data = dados, aes(x = sexo, \n                                  y = notas, \n                                  fill = sexo)) + \n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  labs (x = \"Sexo\", \n        y = \"Notas\") + \n  theme_bw() + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 12.1: Boxplot dos dados\n\n\n\n\n\nOs boxplot sugerem que as notas dos alunos diferem, de acordo o sexo.\n\n\n\n12.2.2 Definição das hipóteses estatísticas\nAs hipóteses comparam as médias dos dois grupos. Para um teste bicaudal, as hipóteses são escritas como:\n\\[\nH_{0}: \\mu_{F} = \\mu_{M}\n\\]\n\\[\nH_{1}: \\mu_{F} \\neq \\mu_{M}\n\\]\n\n\n12.2.3 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0.05. A distribuição t é dependente dos graus de liberdade, dados por:\nNo exemplo,\n\nn1 &lt;- resumo$n[1]\nn2 &lt;- resumo$n[2]\ngl &lt;- n1 + n2 - 2\ngl\n\n[1] 38\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl =38 para uma hipótese alternativa bicaudal é obtido com a função qt (p, df), onde \\(df = gl\\) e \\(p = 1 - \\alpha/2\\)\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\ntc &lt;- round (qt((1-alpha/2), gl), 3)\ntc\n\n[1] 2.024\n\n\nPortanto, se\n\\[\n|t_{calculado}| &lt; |t_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0}\n\\]\n\\[\nt_{calculado}| \\ge t_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\n\n\n12.2.4 Teste estatístico\nPara determinar se existe uma diferença estatisticamente significativa entre as médias das notas de dois grupos independentes, será usado o teste t para duas amostras independentes, também conhecido como teste t de Student, baseado na distribuição de mesmo nome.\n\n12.2.4.1 Lógica do teste t\nO teste t compara as médias de duas amostras independentes, usando o erro padrão como métrica da diferença entre essas médias. Quanto maior o valor de t , maior a probabilidade de que as amostras pertençam a grupos diferentes, ocorrendo nessas circunstâncias a rejeição da hipótese nula (1).\nCalcula-se o teste t com a seguinte equação:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{EP_{d}}\n\\]\nOnde \\(EP_d\\) é o erro padrão da diferença entre a médias \\(\\bar{x}_1 - \\bar{x}_2\\). Se a hipótese nula for verdadeira, as amostras foram retiradas da mesma população e, portanto, \\(\\mu_1 - \\mu_2 = 0\\). Assim, a equação fica:\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{EP_d}\n\\]\nO erro padrão da diferença \\(\\bar{x}_1 - \\bar{x}_2\\) é calculado de maneiras diferentes:\n\nSe a variâncias nos dois grupos forem iguais, usa-se:\n\n\\[\nEP_d = \\sqrt{s_o^2(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\nOnde \\(s_o^2\\) é a variância combinada ou conjugada que é, simplesmente, a média ponderada das variância dos grupos:\n\\[\ns_0^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{(n_1 -1)+ (n_2-1)}\n\\] Quando os grupos têm o mesmo tamanho (\\(n_1 = n_2\\)), \\(s_o^2\\) é simplesmente a média aritmética da variância dos grupos:\n\\[\ns_0^2 = \\frac {s_1^2 + s_2^2}{2}\n\\]\n\\[\nEP_d = \\sqrt{\\frac{2 s_o^2}{n}}\n\\]\n\nSe as variâncias dos dois grupos forem diferentes:\n\n\\[\nEP_d = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\n\\]\nEsta explicação da lógica e dedução da estatística de teste serve para uma melhor compreensão de como o teste funciona, mas para executar um teste t não há necessidade disso, basta saber como encaminhar ao R e como interpretar o resultado fornecido por ele.\n\n\n12.2.4.2 Pressupostos do teste t\nO teste t assume que:\n\nAs amostras são independentes;\nDeve haver distribuição normal. Entretanto, quando as amostras são grandes (teorema do limite central), isso não é muito importante;\nExista homocedasticidade, ou seja, as variâncias dos grupos devem ser iguais.\n\nViolar o pressuposto de número 3 tem importância se os tamanhos dos grupos forem diferentes. Se os grupos tiverem o mesmo tamanho e a amostra for grande, este pressuposto torna-se menos importante, não preocupando muito se essa hipótese foi violada (2). O pressuposto tem mais importância em grupos pequenos e desiguais. Existe um teste, denominado teste t de Welch que corrige essa violação. É possível portanto, esquecer esse pressuposto e fazer o teste de Welch sempre.\nAvaliação da normalidade\nUma boa parte dos procedimentos estatísticos são testes paramétricos 1 com base na distribuição normal. Ou seja, se assume que a distribuição dos dados segue o modelo da distribuição normal. Se essa suposição não for atendida, a lógica por trás do teste de hipóteses pode ser violada.\nPode-se verificar a normalidade de maneira visual, observando o comportamento dos dados através de gráficos como o histograma (Figura 12.2) e o gráfico Q-Q (Figura 12.3). É útil também sobrepor uma distribuição normal no histograma, para fins de comparação com a distribuição normal. Além disso, nos histogramas, pode-se observar como as duas populações se sobrepõem.\n\nmu1 &lt;- resumo$media[1]\ndp1 &lt;- resumo$dp[1]\nmu2 &lt;- resumo$media[2]\ndp2 &lt;- resumo$dp[2]\n\nmasc &lt;- dados %&gt;% filter (sexo == \"M\")\n\nfem &lt;- dados %&gt;% filter (sexo == \"F\")\n\nggplot(dados) +                       \n  geom_histogram(aes(x = notas, fill = sexo,\n                     y = after_stat(density)), \n                 col= \"white\", \n                 alpha = 0.5, \n                 bins = 15) +\n  stat_function(data = masc,\n                fun = dnorm,\n                color = \"red\",\n                lty = \"dashed\",\n                lwd = 1,\n                args = list(mean = mu1,\n                            sd = dp1)) +\n  stat_function(data = fem,\n                fun = dnorm,\n                color = \"darkred\",\n                lty = \"dashed\",\n                lwd = 1,\n                args = list(mean = mu2,\n                            sd = dp2)) +\n  labs(x=\"Notas\", y=\"Densidade\") +\n  scale_fill_manual(values = c(\"pink3\", \"lightblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\nFigura 12.2: Histogramas das notas dos alunos de acordo com o sexo\n\n\n\n\n\nO gráfico QQ (ou gráfico quantil-quantil) desenha a correlação entre uma determinada amostra e a distribuição normal. Uma linha de referência de 45 graus também é plotada. Um gráfico Q-Q é um gráfico de dispersão criado plotando dois conjuntos de quantis um contra o outro. Se ambos os conjuntos de quantis vierem da mesma distribuição, observa-se os pontos formando uma linha aproximadamente reta.\nSe os valores caírem na diagonal do gráfico, a variável é normalmente distribuída. Os desvios da diagonal mostram desvios da normalidade. Para desenhar um gráfico Q-Q pode ser usado a função ggqqplot ()2 do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%.\n\nggqqplot(dados, x = \"notas\", color = \"sexo\") +\n  labs(y = \"Notas\",\n       x = \"Quantis teóricos\")\n\n\n\n\n\n\n\nFigura 12.3: Gráficos Q-Q\n\n\n\n\n\nObservando os gráficos, verifica-se que a variável notas tem uma distribuição visualmente normal aceitável em ambas populações, pois o histograma se ajusta à curva normal e os gráficos Q-Q mostram que os dados seguem aproximadamente a linha diagonal.\nOutra maneira de analisar a normalidade é verificar se a distribuição como um todo se desvia de uma distribuição normal comparável. Para isso, usam-se testes estatísticos de normalidade. Os dois principais são o teste de Shapiro-Wilk e o teste de Kolmogorov-Smirnov (K-S).\nEsses testes comparam os dados da amostra com um conjunto de valores normalmente distribuídos com a mesma média e desvio padrão. Se o teste não for significativo (P &gt; 0,05), informa-se que a distribuição da amostra não é significativamente diferente de uma distribuição normal. Se, no entanto, o teste for significativo (P \\(\\le\\) 0,05), a distribuição em questão será significativamente diferente de uma distribuição normal.\nO método de Shapiro-Wilk é amplamente recomendado para teste de normalidade (3), (4), (5).\n\nsw &lt;- dados %&gt;% \n  dplyr::group_by(sexo) %&gt;%\n  rstatix::shapiro_test(notas)\nsw\n\n# A tibble: 2 × 4\n  sexo  variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 F     notas        0.973 0.812\n2 M     notas        0.972 0.802\n\n\nA saída mostra que ambos valores P do teste, 0.812 e 0.802, estão acima de 0,05, corroborando com a não rejeição da normalidade dos dados.\nHomogeneidade da Variância\nNa visualização da Figura 12.1, nos dois grupos de alunos, observa-se que há, entre os limites inferior e superior, uma dispersão das notas em torno da região central. Esta dispersão parece ser semelhante nos grupos. Isto sugere que haja homogeneidade das variâncias.\nPortanto, homogeneidade da variância é o pressuposto de que a dispersão das medidas é aproximadamente igual em diferentes grupos de casos, ou que a dispersão dos valores são aproximadamente iguais em pontos diferentes da variável preditora.\nAlém do aspecto visual, a homogeneidade da variância pode ser testada com o teste de Levene. Neste teste, a \\(H_{0}\\) é todas as variâncias são iguais. No R, a função que calcula o teste é leveneTest() do pacote car (6). Os argumentos são:\n\ny \\(\\to\\) variável de resposta para o método padrão ou um objeto lm ou fórmula. Se y for um objeto de modelo linear ou uma fórmula, as variáveis do lado direito do modelo devem ser todas fatores e devem ser completamente cruzadas;\ngroup \\(\\to\\) fator que define os grupos;\ncenter \\(\\to\\) O nome de uma função para calcular o centro de cada grupo; mean fornece o teste de Levene original; o padrão, median, fornece um teste mais robusto;\ndata \\(\\to\\) conjunto de dados para avaliar a formula.\n\n\nlevene &lt;- car::leveneTest(notas~sexo, \n                          center = mean, \n                          data = dados)\nlevene\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  1  0.4575 0.5029\n      38               \n\n\nA saída do teste de Levene retorna um valor P &gt; 0,05, confirma a impressão visual dos boxplots de que os grupos têm homogeneidade das variâncias, portanto a hipótese nula de igualdade das variâncias não pode ser rejeitada.\nUm outro teste que compara duas variância poderia ser usado. É o teste F que pode ser calculado com a função var.test() do pacote stats, incluído no R base. Seus argumentos pode ser consultados na ajuda do R.\n\nteste.Var &lt;- var.test(notas~sexo, alternative = \"two.sided\" , data = dados)\nteste.Var\n\n\n    F test to compare two variances\n\ndata:  notas by sexo\nF = 1.1274, num df = 19, denom df = 19, p-value = 0.7966\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4462295 2.8482624\nsample estimates:\nratio of variances \n          1.127377 \n\n\nA saída do teste permite uma conclusão igual ao teste de Levene, pois o valor P = 0.7966.\n\n\n12.2.4.3 Execução do teste t de Student\nOs pressupostos do teste não foram violados, portanto ele pode ser realizado com confiança. Será utilizado a função t_test() do pacote rstatix (7) para calcular o teste t para amostras independentes. Ele fornece uma estrutura compatível com operador pipe %&gt;% (pipe-friendly) para executar testes t de uma e duas amostras. Para consultar os argumentos, consulte a Seção 11.5.3 ou a ajuda do RStudio.\n\n teste &lt;- dados %&gt;% rstatix::t_test(formula = notas ~ sexo,\n                                    detailed = TRUE,\n                                    var.equal = TRUE)\n teste\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1 group2    n1    n2 statistic       p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     8.51      68.4      59.9 notas F      M         20    20      3.56 0.00102\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\nA saída retorna a estimativa da diferença média (8.515), as estimativas das médias dos grupos (arredondadas), a estatística do teste (3.5586328) o valor P (0.00102), graus de liberdade (38)3 e outras métricas.\nTambém é possível ver os resultados do teste t , usando o objeto teste que os recebeu. Por exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa da diferença entre as médias.\n\nIC95 &lt;- round(c(teste$conf.low, teste$conf.high),3)\nIC95\n\n[1]  3.671 13.359\n\n\n\n\n\n12.2.5 Conclusão\nComo \\(|t_{calculado}|\\) = 3.559 &gt; \\(|t_{0,05;58}|\\) = 2.024, rejeita-se \\(H_{0}\\). Observa-se que o valor P é muito pequeno (0.00102) e, portanto, a diferença observada nas médias dos dois grupos deve ser assumida como significativa.\nAssim, pode-se admitir que as médias das notas são diferentes, com probabilidade de erro extremamente pequena. A estimativa da diferença média (\\(\\mu_1 - \\mu_2\\)) é fornecida pelo intervalo de confiança de 95% (3.671, 13.359). Observe que o valor zero não está contido no intervalo e isto confirma a não significância estatística da diferença.\nConcluindo, as notas de Bioestatística das mulheres e as notas dos homens são diferentes, a diferença (\\(\\mu_1 - \\mu_2\\)) encontrada é estatisticamente significativa (t = 3.559, gl = 38, P = 0.00102), com uma confiança de 95%.\nEsta conclusão pode ser visualizada em um gráfico (Figura 12.4) que exibirá a saída do teste t:\n\nConstruir dois boxplots, usando o ggplot2 com cores do New England Journal of Medicine (NEJM), do pacote ggsci . Atribuir a um objeto bp:\n\n\nbp &lt;- ggplot(dados, aes(x=sexo, y=notas)) +\n    geom_errorbar(stat = \"boxplot\", width = 0.1)+\n    geom_boxplot(aes(fill = sexo),\n                 color = \"black\")+\n    scale_color_nejm() +\n    theme_bw() +\n    theme(legend.position=\"none\")\n\n\nAdicionar ao boxplot novos rótulos e os testes realizados:\n\n\n bp +\n   labs(x = \"Sexo\", \n        y = \"Notas\", \n        title = \"Notas de Bioestatística\",\n        subtitle = rstatix::get_test_label(stat.test = teste,\n                                           correction = \"none\",\n                                           detailed = TRUE,\n                                           type = \"expression\",\n                                           p.col = \"p\"))\n\n\n\n\n\n\n\nFigura 12.4: Boxplots comparando os dois grupos\n\n\n\n\n\n\n\n12.2.6 Tamanho do Efeito\nA significância estatística deve ter uma atenção relativa do pesquisador, pois ela apenas mede a probabilidade de rejeitar uma hipótese nula, uma vez que ela seja verdadeira. Ajudam a determinar, em uma pesquisa, a significância dos resultados encontrados em relação à hipótese nula, mas não informam nada em relação a magnitude do efeito. Por exemplo, mostra se determinado tratamento afeta as pessoas, mas não dizem quanto isso as afeta.\nO tamanho do efeito (effect size) é uma medida quantitativa da magnitude do efeito. Quanto maior o tamanho do efeito, mais forte é a relação entre duas variáveis. É possível observar o tamanho do efeito ao comparar dois grupos quaisquer para ver quão substancialmente diferentes eles são.\nNormalmente, em ensaios clínicos tem-se um grupo de tratamento e um grupo de controle. O grupo de tratamento é uma intervenção que se espera efetue um resultado específico. O valor do tamanho do efeito mostrará se a terapia teve um efeito pequeno, médio ou grande. Isso tem mais relevância do que simplesmente informar o tamanho do valor P.\n\n12.2.6.1 d de Cohen\nTambém conhecida como diferença média padronizada, o d de Cohen (8) (9) é uma medida adequada e bastante popular para encontrar a magnitude do efeito na comparação entre duas médias.\nPara calcular a diferença média padronizada se verifica a diferença entre as médias dos dois grupos e se divide pelo desvio padrão conjugado:\n\\[\nd = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_{o}}\n\\]\nOnde,\n\\[\ns_o =\\sqrt \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2 - 2}\n\\]\nVoltando ao exemplo das notas dos alunos de Bioestatística, o d de Cohen é calculado, usando a função cohensD() do pacote lsr que usa os seguintes argumentos:\n\nx \\(\\to\\) um vetor numérico de valores de dados, variável preditora;\ny \\(\\to\\) um vetor numérico de valores de dados, variável resposta;\nformula \\(\\to\\) Fórmula na forma variável resposta ~ grupo;\ndata \\(\\to\\) dataframe ou matriz;\nmethod \\(\\to\\) Qual versão da estatística d devemos calcular? Os valores possíveis são pooled(padrão), x.sd, y.sd, corrected, raw, paired e unequal.;\nmu \\(\\to\\) O valor “nulo” contra o qual o tamanho do efeito deve ser medido. Quase sempre é 0 (padrão); raramente especificado.\n\nAssim, o d de Cohen pode ser obtido da seguinte forma:\n\nd &lt;- lsr::cohensD (notas ~ sexo, data = dados)\nd\n\n[1] 1.125339\n\n\nBastante simples! Agora, como interpretar este resultado de d = 1,3 (arredondado)? Sua interpretação não é intuitiva, recomenda-se usar a Tabela 12.1 para interpretar (8).\n\n\n\n\nTabela 12.1: Tamanho do Efeito\n\n\n\nd de CohenInterpretação&lt; 0,2insignificante0,2 &lt; 0.5pequeno0.5 &lt; 0.8médio&gt;= 0,8 grande\n\n\n\n\n\nAssim, as notas dos alunos diferem significativamente (P &lt; 0,0001) de acordo com a sexo, sendo que as mulheres têm notas mais altas do que os homens e a magnitude dessa diferença é grande (d = 1.13).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#teste-t-para-grupos-pareados",
    "href": "12-teste-t.html#teste-t-para-grupos-pareados",
    "title": "12  Comparação entre duas médias",
    "section": "12.3 Teste t para grupos pareados",
    "text": "12.3 Teste t para grupos pareados\nUm teste t pareado é usado para estimar se as médias de duas medidas relacionadas são significativamente diferentes uma da outra. Esse teste é usado quando duas variáveis contínuas são relacionadas porque são coletadas do mesmo participante em momentos diferentes (antes e depois), de locais diferentes na mesma pessoa ao mesmo tempo ou de casos e seus controles correspondentes.\n\n12.3.1 Dados usados nesta seção\nO banco de dados é constituído por uma amostra de 15 escolares portadores de asma não controlada. Fizeram avaliação da sua função pulmonar no início do uso de um novo corticoide inalatório. Após 60 dias, repetiram a avaliação da função pulmonar. Para baixar o banco de dados, clique aqui. Faça o downloado para o seu diretório de trabalho.\n\n12.3.1.1 Leitura e transformação dos dados\nLeia o arquivo dadosPar.xlsx a partir do diretório de trabalho, usando a função read_excel() do pacote readxl. Atribuir os dados a um objeto com o nome dados.\n\ndados &lt;- readxl::read_excel(\"dados/dadosPar.xlsx\")\n\nA estrutura dos dados podem ser visualizada, usando a função str():\n\nstr(dados)\n\ntibble [15 × 3] (S3: tbl_df/tbl/data.frame)\n $ id   : num [1:15] 1 2 3 4 5 6 7 8 9 10 ...\n $ basal: num [1:15] 1.3 1.47 2.06 1.95 1.47 1.13 1.48 0.94 1.05 0.87 ...\n $ final: num [1:15] 1.53 1.63 2.35 2.7 2.01 1.53 1.66 1.59 1.5 1.61 ...\n\n\nO dataframe dados encontra-se no formato amplo (wide), ou seja, com as colunas basal e final colocadas lado a lado como se fossem duas variáveis distintas, quando, na realidade, constituem-se em apenas uma variável contendo as medidas de VEF1 (Volume Expiratório Forçado no primeiro segundo).\nA função pivot_longer() do pacote tidyr fará a transformação do formato amplo para o longo (long). Este processo não é obrigatório, mas será realizado para fins de treinamento. O novo banco de dados será atribuído ao objeto dadosL. A função pivot_longer() necessita dos seguintes argumentos:\n\ndados \\(\\to\\) dataframe a ser pivotado, tranformado;\ncols \\(\\to\\) colunas a serem transformadas no formato longo;\nnames_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos nomes das colunas de dados;\nvalues_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos valores das células;\n… \\(\\to\\) possui outros argumento. Ver ajuda.\n\n\ndadosL &lt;- dados %&gt;% \n  tidyr::pivot_longer(c(basal, final), \n                      names_to = \"momento\",\n                      values_to = \"medidas\")\nstr(dadosL)\n\ntibble [30 × 3] (S3: tbl_df/tbl/data.frame)\n $ id     : num [1:30] 1 1 2 2 3 3 4 4 5 5 ...\n $ momento: chr [1:30] \"basal\" \"final\" \"basal\" \"final\" ...\n $ medidas: num [1:30] 1.3 1.53 1.47 1.63 2.06 2.35 1.95 2.7 1.47 2.01 ...\n\n\n\n\n12.3.1.2 Medidas Resumidoras\nPara resumir as variáveis, serão usadas as funções group_by() e summarise() do pacote dplyr, aplicadas ao formato longo dadosL:\n\nresumo &lt;- dadosL %&gt;% \n  dplyr::group_by(momento) %&gt;% \n  dplyr::summarise(n = n (),\n                   media = mean(medidas, na.rm = TRUE),\n                   dp = sd (medidas, na.rm = TRUE),\n                   mediana = median (medidas, na.rm = TRUE),\n                   IIQ = IQR (medidas, na.rm =TRUE),\n                   ep = dp/sqrt(n),\n                   me = ep * qt(1 - (0.05/2), n - 1)) \nresumo\n\n# A tibble: 2 × 8\n  momento     n media    dp mediana   IIQ    ep    me\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 basal      15  1.31 0.427    1.26  0.48 0.110 0.236\n2 final      15  1.69 0.471    1.59  0.38 0.122 0.261\n\n\n\n\n12.3.1.3 Visualização dos dados\n\nGráficos\n\nApenas, por uma questão didática, serão apresentadas três maneiras de mostrar os dados visualmente. Podem ser usados qualquer um dos tipos a seguir, pois todos dão, praticamente, a mesma informação.\nGráfico de barra de erro\n\nresumo %&gt;% \n  ggplot2::ggplot(aes(x=momento, y=media, fill=momento)) + \n  geom_bar(stat=\"identity\", width = 0.4, color=\"black\") +\n  geom_point() +\n  geom_errorbar(aes(ymin=media, ymax=media+me), width=0.1,\n                position=position_dodge(.9)) +\n  labs(title=\"Avaliação de um corticoide inalatório\", \n       x=\"Momento\", y = \"Volume Forçado em 1 seg (L)\")+\n  theme_classic() +\n  scale_fill_manual(values=c(\"cyan4\",\"cyan3\")) +\n  scale_y_continuous (expand = expansion(add = c(0,0.05))) +                    \n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\nFigura 12.5: Gráfico de barra de erro comparando o grupo antes-e-depois\n\n\n\n\n\nNesse gráfico (Figura 12.5), a altura da barra representa a média do Volume Forçado em 1 seg (VEF1) nos diferentes momentos (basal e final). O erro corresponde a margem de erro (me) a partir do ponto (média), ou seja, é o intervalo de confiança de 95%. O limite inferior do IC95% foi suprimido.\nBoxplot\n\ndadosL %&gt;% \n  ggplot2::ggplot(aes(x = momento, y = medidas, fill = momento)) +\n  geom_errorbar(stat = \"boxplot\", width = 0.1) +\n  geom_boxplot (outlier.color = \"red\", \n                outlier.shape = 1,\n                outlier.size = 1) +\n  scale_fill_manual(values = c(\"cyan4\",\"cyan3\")) +\n  ylab(\"Volume Forçado em 1 seg (L)\") +\n  xlab(\"Momento\") +\n  stat_summary(fun = mean, \n               geom = \"point\", \n               shape = 19, size = 2,  color=\"red\") +\n  theme_bw() + \n  theme(text = element_text(size = 12)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 12.6: Boxplots comparando o grupo antes-e-depois\n\n\n\n\n\nA altura da caixa dos boxplots (Figura 12.6)) é o intervalo interquartil (IIQ) e corresponde a 50% dos dados. A linha que corta horizontalmente a caixa é a mediana. Os bigodes da caixa (whiskers) em suas extremidades são os limites inferior e superior dos dados, excluindo os valores atípicos (outliers), representado no boxplot final por um ponto vermelho, acima do limite superior. Os pontos em vermelho (dentro das caixas) representam as médias.\nGráfico de linha\n\nresumo %&gt;%\n  ggplot2::ggplot(aes(x=momento, y=media, group=1)) +\n  geom_line(linetype ='dashed') +\n  geom_errorbar(aes(ymin=media - me,\n                    ymax=media + me),\n                width=0.1,\n                linewidth = 1,\n                col = c(\"cyan4\",\"cyan3\")) +\n  geom_point(size = 3, color = c(\"cyan4\", \"cyan3\")) +\n  theme_classic()+\n  labs(x='Momento',\n       y='Volume Forçado em 1 seg (L)')\n\n\n\n\n\n\n\nFigura 12.7: Gráfico de linha comparando o grupo antes-e-depois\n\n\n\n\n\nEste gráfico de linha (Figura 12.7) com representação da margem de erro tem a mesma interpretação do gráfico de barra de erro. A escolha do tipo de gráfico depende da ênfase do autor sobre os dados.\n\n\n12.3.1.4 Criação de uma variável que represente a diferença entre as médias\nA diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados):\n\ndados$D &lt;- dados$basal - dados$final\nhead (dados)\n\n# A tibble: 6 × 4\n     id basal final     D\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  1.3   1.53 -0.23\n2     2  1.47  1.63 -0.16\n3     3  2.06  2.35 -0.29\n4     4  1.95  2.7  -0.75\n5     5  1.47  2.01 -0.54\n6     6  1.13  1.53 -0.4 \n\n\nAtenção, agora, o banco de dados apresenta uma nova variável D, pois o foco do teste t pareado é essa diferença entre as médias, basal e final, a média das diferenças.\nResumo da variável D\nAo resumo será atribuído ao nome sumario (sem acento):\n\nresumoD &lt;- dados %&gt;% \n  dplyr::summarise(media = mean (D),\n                   dp = sd (D),\n                   mediana = median (D),\n                   IIQ = IQR (D),\n                   min = min (D),\n                   max = max (D))\nresumoD\n\n# A tibble: 1 × 6\n   media    dp mediana   IIQ   min     max\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 -0.377 0.218   -0.38 0.285 -0.75 -0.0400\n\n\nExiste uma diferença de 0.38L entre o VEF1 basal e o final. A pergunta que se faz é: Esta diferença tem significância estatística? Os gráficos sugerem que sim!\n\n\n\n12.3.2 Definição das hipóteses estatísticas\nSerá usado um teste bicaudal. Se a intervenção não produz efeito, então:\n\\[\nH_0: \\mu_D = 0  \n\\] Se a intervenção produz efeito, então:\n\\[\nH_1: \\mu_D \\neq 0\n\\]\n\n\n12.3.3 Regra de decisão\nO nível significância, \\(\\alpha\\), escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição t que é dependente dos graus de liberdade. O número de graus de liberdade á igual ao número de observações menos 1, neste caso são o número de pares menos 1.\n\nn &lt;- length(dados$D)\ngl &lt;- n - 1\ngl\n\n[1] 14\n\n\nPara um \\(\\alpha = 0,05\\), o valor crítico de t para gl = 14 para uma hipótese alternativa bicaudal:\n\nalpha &lt;- 0.05\np &lt;- 1 - alpha/2\nround(qt(p, 14), 3)\n\n[1] 2.145\n\n\nPortanto, se\n\\[\n\\mid t_{calculado}\\mid &lt; \\mid t_{crítico}\\mid -&gt; não \\quad rejeitar \\quad H_{0} \\\\ \\mid t_{calculado}\\mid &gt; \\mid t_{crítico}\\mid -&gt; rejeitar \\quad H_{0}\n\\]\n\n\n12.3.4 Teste estatístico\n\n12.3.4.1 Lógica do teste\nA estatística do teste t dependente é a mesma do teste t independente r dada por:\n\\[\nT = \\frac{\\bar{D} - \\mu_{D}}{EP_{D}}\n\\]\nComo na equação do teste t para amostras independentes, sob a hipótese nula igual a zero, \\(\\mu_{D} = 0\\), assim, a equação fica:\n\\[\nT = \\frac{\\bar{D}}{EP_{D}}\n\\]\nA estimativa do erro padrão das diferenças é dada por:\n\\[\nEP_{D}=\\frac{s_{D}}{\\sqrt{n}}\n\\]\nO desvio padrão das diferenças, \\(s_{D}\\) , é dado por:\n\\[\ns_{D}=\\sqrt\\frac{\\Sigma(D_{i} - \\bar{D})^2}{n - 1}\n\\]\nOnde \\(D_{i}\\) são as diferença individuais (\\(x_1 - y_1, x_2 - y_2, ..., x_n - y_n\\)).\nDa mesma maneira que no teste t para grupos independentes, essa demonstração serve para uma melhor compreensão de como o teste funciona, mas para executar este teste t não há necessidade disso, basta saber como encaminhar ao R, como será visto adiante.\n\n\n12.3.4.2 Pressupostos do teste\nO teste t pareado assume que os seguintes pressupostos devem ser atendidos:\n\nOs dados devem ser dependentes;\nA variável desfecho deve estar em uma escala contínua;\nAs diferenças entre os pares devem ter distribuição normal.\n\nAo usar um teste t pareado, a variação entre os pares de medidas é a estatística mais importante e a variação entre os participantes, como no teste t de duas amostras independentes, é de pouco interesse, não havendo necessidade de se verificar se as variâncias dos grupos são iguais.\nPara testar o pressuposto de normalidade das diferenças, usa-se a variável criada da diferença entre os pares, D. Verifica-se a normalidade dessa variável com o teste Shapiro-Wilk, usando a função shapiro_test() do pacote rstatix, já usada no teste t de amostras independentes.\n\nshapiro &lt;- dados %&gt;% \n   rstatix::shapiro_test(D)\n shapiro\n\n# A tibble: 1 × 3\n  variable statistic     p\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 D            0.942 0.410\n\n\nO teste de Shapiro-Wilk retorna um valor P &gt; 0,05, mostrando que a variável D que não se pode rejeitar a hipóteses nula de sua normalidade.\nAlém disso, um gráfico Q-Q (Figura 12.8) pode ser usado para avaliar a normalidade, com a função ggqqplot() do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%\n\nggpubr::ggqqplot (dados$D, color = \"steelblue\") +\n  labs(y = \"Diferença Basal-Inicial\", \n       x = \"Quantis teóricos\") +\n  theme_bw() \n\n\n\n\n\n\n\nFigura 12.8: Gráfico Q-Q para avaliar a normalidade\n\n\n\n\n\nOs resultados do teste de Shapiro-Wilk e ográfico QQ, mostram que a \\(H_{0}\\) de normalidade da variável D não é rejeitada, apesar de haver uma pequena assimetria à esquerda que não impede o prosseguimento da análise.\n\n\n12.3.4.3 Execução do teste estatístico\nO cálculo do teste t pareado pode usar a mesma função do teste t para amostras independentes, t_test(), do pacote rstatix, mudando o argumento paired =FALSE(padrão) por paired =TRUE. Assim:\n\nteste_par &lt;- dadosL %&gt;% \n  rstatix:: t_test(formula = medidas ~ momento,\n                   paired = TRUE,\n                   detailed = TRUE) \nteste_par\n\n# A tibble: 1 × 13\n  estimate .y.     group1 group2    n1    n2 statistic         p    df conf.low\n*    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.377 medidas basal  final     15    15     -6.70 0.0000102    14   -0.497\n# ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObserve que foi usado o conjunto de dados de formato longo (dadosL) para usar a fórmula (x ~ grupo). Da mesma maneira do que o teste t para amostras independentes, é possível ver os resultados do teste t , usando o objeto teste_par que os recebeu.\nPor exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa de diferença (D) entre as médias\n\nIC95 &lt;- round(c(teste_par$conf.low, teste_par$conf.high),3)\nIC95\n\n[1] -0.497 -0.256\n\n\n\n\n\n12.3.5 Conclusão\nConclui-se que o VEF1 dos escolares asmáticos se modificou significativamente entre o início e após 60 dias do uso de um novo medicamento com uma confiança de 95%. A diferença (\\(\\mu_{basal} - \\mu_{final}\\)) encontrada é estatisticamente significativa (t = -6.6969, gl = 14, P = 1.02^{-5}), com uma confiança de 95%.\nObserve que o intervalo de confiança de 95% da diferença de -0.38 está todo abaixo de zero (-0.497, -0.256), confirmando a significância.\n\n\n12.3.6 Tamanho do Efeito\nO tamanho do efeito pode ser determinado, também, com o teste d de Cohen, usando a função cohensD() do pacote lsr:\n\nd_par &lt;- lsr::cohensD (dados$basal, dados$final)\nd_par\n\n[1] 0.8379499\n\n\nDessa forma, o uso do novo corticoide inalatório modificou significativamente o VEF1 dos escolares asmáticos com o uso de um novo corticoide inalatório (P = 1.02^{-5}), mostrando um aumento deste e que a magnitude dessa diferença é grande (d = 0.84).\nOs resultados podem ser apresentados usando um gráfico de linha (Figura 12.9)), aproveitando o resultado da função t_test().\n\nresumo %&gt;% \n    ggplot2::ggplot(aes(x=momento, y=media, group=1)) +\n    geom_line(linetype ='dashed') +\n    geom_errorbar(aes(ymin=media - me, \n                      ymax=media + me), \n                  width=0.1,\n                  size = 1,\n                  col = c(\"cyan4\",\"cyan3\")) +\n    geom_point(size = 2) +\n   labs(title=\"Avaliação do Uso de Corticosteroide Inalatório\",\n       subtitle = rstatix::get_test_label(stat.test = teste_par,\n                                          correction = \"none\",\n                                          detailed = TRUE,\n                                          type = \"expression\"),\n       x=\"Momento\", \n       y = \"Volume Expiratório Forçado em 1 seg (L)\",\n       caption = \"d Cohen = 0,84\")+\n   theme_bw() + \n   theme(legend.position=\"none\")\n\n\n\n\n\n\n\nFigura 12.9: Gráfico de linha comparando um grupo de escolares asmáticos antes e depois do uso de um corticosteroide inalatório\n\n\n\n\n\n\n\n\n\n1. Pagano M, Kimberly G. Comparison of Two Means. Em: Principles of Biostatistics. Second Edition. CRC Press; 2000. p. 262–72. \n\n\n2. Zimmerman DW. A note on preliminary tests of equality of variances. Br J Math Stat Psychol. 2004;57(1):173–81. \n\n\n3. Razali NM, Wah YB, et al. Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics. 2011;2(1):21–33. \n\n\n4. Ghasemi A, Zahediasl S. Normality tests for statistical analysis: a guide for non-statisticians. International journal of endocrinology and metabolism. 2012;10(2):486. \n\n\n5. Yap BW, Sim CH. Comparisons of various types of normality tests. Journal of Statistical Computation and Simulation. 2011;81(12):2141–55. \n\n\n6. Fox J, Weisberg S. An R Companion to Applied Regression [Internet]. Third. Thousand Oaks CA: Sage; 2019. Disponível em: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n7. Kassambara A. rstatix: Pipe-Friendly Framework for Basic Statistical Tests [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=rstatix\n\n\n8. Cohen J. Statistical power analysis for the behavioral sciences. 2nd Edition. Routledge; 1988. \n\n\n9. Lindenau JD, Guimaraes LSP. Calculating the Effect Size in SPSS. Revista HCPA [Internet]. 2012;32(3):363–81. Disponível em: https://seer.ufrgs.br/hcpa",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "12-teste-t.html#footnotes",
    "href": "12-teste-t.html#footnotes",
    "title": "12  Comparação entre duas médias",
    "section": "",
    "text": "Teste paramétricos são testes estatísticos que se baseiam nos padrões da distribuição populacional da variável em estudo, por exemplo, a distribuição normal é descrita por dois parâmetros – média e desvio padrão – que são suficientes para se conhecer as probabilidades. Os testes que não requerem a especificação da forma de distribuição da população, ou seja, têm distribuição livre, são denominados de não paramétricos.↩︎\nVeja também a Seção 8.3.1.2.↩︎\nSe as variâncias forem diferentes (var.equal = FALSE), o teste calcula os graus de liberdade pela fórmula de Welch, bem mais complicada.↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparação entre duas médias</span>"
    ]
  },
  {
    "objectID": "13-anova.html",
    "href": "13-anova.html",
    "title": "13  Análise de Variância",
    "section": "",
    "text": "13.1 Pacotes necessários para este capítulo\npacman::p_load(car,\n               dplyr,\n               effectsize,\n               emmeans,\n               fastGraph,\n               flextable,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               kableExtra,\n               knitr,\n               readxl,\n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#por-que-realizar-uma-anova",
    "href": "13-anova.html#por-que-realizar-uma-anova",
    "title": "13  Análise de Variância",
    "section": "13.2 Por que realizar uma ANOVA?",
    "text": "13.2 Por que realizar uma ANOVA?\nInicialmente, para analisar os grupos, se ficaria tentado a fazer comparações por pares usando um teste t de amostras independentes. Com existem quatro grupos, é possível compará-los realizando seis testes, grupo 1 versus grupo 2, grupo 1 versus grupo 3, grupo 1 versus grupo 4, grupo 2 versus grupo 3, grupo 2 versus grupo 4 e grupo 3 versus grupo 4. Se os dados têm k grupos são necessários \\(\\frac {k!}{2!(k-2)!}\\) testes.\nA probabilidade de um erro do tipo I não ocorrer para cada teste t é de 0,95 (isto é, 1 – 0,05), supondo um \\(\\alpha\\) = 0,05. Os três testes são independentes; portanto, a probabilidade de um erro do tipo I não ocorrer nos seis testes é de \\((0,95)^6 = 0,735\\). Dessa maneira, a probabilidade de ocorrer pelo menos um erro do tipo I nos seis testes t de duas amostras é de 1 – 0,735 ou 0,265 (26,5%), o que é mais alto do que o nível de significância definido de 0,05 (1).\nLogo, uma ANOVA de um fator é usada para verificar as diferenças entre vários grupos dentro de um fator, reduzindo assim o número de comparações em pares e a probabilidade de ocorrer um erro tipo I.\n\n13.2.1 Lógica do Modelo da ANOVA\nO procedimento de ANOVA é utilizado para testar a hipótese nula de que as médias de três 1 ou mais populações são as mesmas contra hipótese alternativa de que nem todas as médias são iguais.\nNa Seção 12.2.4.2, foram comparadas duas variâncias, usando um teste, denominado de teste F. Este teste, é uma razão entre duas variâncias e recebeu este nome em homenagem a Sir Ronald Aylmer Fisher (veja a Seção 1.2. A variância é uma medida de dispersão que mensura como os dados estão espalhados em torno da média (veja Seção 6.3.4.3 ) . Quanto maior o seu valor, maior a dispersão.\nConsidere a Figura 13.1, onde está representada a distribuição de uma variável X em três grupos independentes. Pode-se, claramente, distinguir observações provenientes dessas distribuições, pois a sobreposição delas é pequena. Cada uma dela se dispersa pouco em torno da média.\n\n\n\n\n\n\n\n\nFigura 13.1: Três distribuições diferentes\n\n\n\n\n\nAgora, observe o Figura 13.2, onde a distribuição da variável X é mostrada, mantendo as mesmas médias, mas com variâncias maiores. Isto torna claro que se o objetivo é distinguir observações provenientes desses grupos não basta avaliar suas médias, há necessidade de comparar a variação entre os grupos com a variação dentro de cada grupo (2).\n\n\n\n\n\n\n\n\nFigura 13.2: Distribuições com mesmas médias da figura anterior, mas variâncias maiores\n\n\n\n\n\nSe a variação entre os grupos for grande quando comparada à variação dentro de cada grupo, aumenta a probabilidade de reconhecer a proveniência das observações (Figura 13.1). Entretanto, se a variação entre os grupos for pequena comparada à variação dentro do grupo, torna difícil a distinção de observações provenientes dos grupos (Figura 13.2).\nPortanto, usar o teste F para determinar se as médias de grupo são iguais é apenas uma questão de incluir as variâncias corretas na razão. Na ANOVA com um fator, a estatística F é a razão dos estimadores das variâncias entre e dentro dos grupos.\n\\[\nF = \\frac{variância \\quad ENTRE \\quad os \\quad grupos}{variância \\quad DENTRO \\quad dos \\quad grupos}\n\\]\nQuando o valor de F fica próximo de 1, significa que as variâncias são muito próximas; quando F é significativamente maior do que 1, é possível distinguir os indivíduos de diferentes grupos. Ou seja, se o objetivo for mostrar que as médias são diferentes, será bom que a variância dentro dos grupos seja baixa. Pode-se pensar na variância dentro do grupo como o ruído que pode obscurecer a diferença entre os sons (as médias). No gráfico da Figura 13.1, o valor de F seria alto; no da Figura 13.2, seria baixo.\nComo saber se o valor de F é alto o suficiente? Um único valor F é difícil de interpretar sozinho. Há necessidade de colocá-lo em um contexto maior antes que seja possível interpretá-lo. Para fazer isso, usa-se a distribuição F para calcular as probabilidades.\n\n\n13.2.2 Distribuição F\nA razão entre a variabilidade entre os grupos e a variabilidade dentro do grupo segue uma distribuição F quando a hipótese nula é verdadeira. Quando se realiza uma ANOVA com um fator obtém-se um valor F. No entanto, se forem extraídas várias amostras aleatórias do mesmo tamanho da mesma população e fosse repetida a mesma análise, o resultado seriam muitos valores F diferentes, constituindo uma distribuição amostral, denominada de distribuição F.\nDessa forma, como a distribuição F assume que a hipótese nula é verdadeira, é possível colocar o resultado de qualquer valor F, resultante do teste de ANOVA, e determinar quão consistente ele é com a hipótese nula e calcular a probabilidade. A probabilidade que se quer calcular é a probabilidade de observar uma estatística F que é pelo menos tão alta quanto o valor que o estudo obteve. Essa probabilidade permite determinar quão comum ou raro é o valor F, sob a suposição de que a hipótese nula é verdadeira. Se a probabilidade for pequena o suficiente, pode-se concluir que dados são inconsistentes com a hipótese nula. Como já foi mostrado em outros momentos, essa probabilidade é o valor P (Seção 11.6).\nO formato de uma curva de distribuição F depende do número de graus de liberdade. No entanto, a distribuição F tem dois números de graus de liberdade: graus de liberdade para o numerador (variância entre) e graus de liberdade para o denominador (variância dentro). Esses dois graus de liberdade são os parâmetros da distribuição F. Cada combinação de graus de liberdade fornece uma curva de distribuição F diferente. As unidades de uma distribuição F são denotadas por F, que assume apenas valores positivos. Como as distribuições normal, t e qui-quadrado (veja Seção 16.2), a distribuição F é uma distribuição contínua. A forma de uma curva de distribuição F é inclinada para à direita, mas a assimetria diminui à medida que o número de graus de liberdade aumenta, conforme observado na Figura 13.3.\n\n\n\n\n\n\n\n\nFigura 13.3: Distribuições F.\n\n\n\n\n\n\n13.2.2.1 Funções do R para trabalhar com a distribuição F\nNo R, existem quatro funções principais para trabalhar com a distribuição F:\n\ndf(x, gl1,gl2) calcula a densidade de probabilidade da distribuição F no ponto x; df1 e df2 são os graus de liberdade do numerador e denominador, respectivamente 2 ;\npf(x, gl1, gl2) calcula a função de probabilidade acumulada da distribuição F no ponto x;\nqf(p, gl1, gl2) calcula o quantil da distribuição F correspondente a uma probabilidade p;\nrf(n, gl1, gl2) gera n valores aleatórios da distribuição F com os parâmetros gl1 e gl2.\n\nEssas funções são úteis para resolver problemas de probabilidade envolvendo a distribuição F. Por exemplo, se o objetivo é saber qual é a probabilidade de uma variável aleatória F com 10 e 20 graus de liberdade no numerador e no denominador, respectivamente, ser menor que 1, pode-se usar a função pf() da seguinte forma:\n\n x &lt;- 1\n p &lt;- pf(x, 10, 20,lower.tail = TRUE)\n round(p, 3)\n\n[1] 0.524\n\n\nOu seja, ao se observar a curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20), a probabilidade abaixo de x = 1 é igual a 52,4%.\nPara calcular a densidade de probabilidade quando x = 1, pode-se usar a função df():\n\n x &lt;- 1\n d &lt;- df(x, 10, 20)\n round(d, 3)\n\n[1] 0.714\n\n\nA saída da função df() corresponde à altura da curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20) quando x é igual a 1.\nPara encontrar o valor da distribuição F(10,20) que corresponde ao percentil 50%, ou seja, o valor que deixa 50% da área da curva à esquerda, usa-se a função qf() com lower.tail=TRUE. Assim:\n\n p &lt;- 0.50\n x &lt;- qf(p, 10, 20, lower.tail = TRUE)\n round(x,3) \n\n[1] 0.966\n\n\nPara representar, graficamente, esse resultado, foi construido o gráfico da Figura 13.4 com a função shadeDist() do pacote fastGraph (3) , verifica-se que a área sob a curva abaixo de 0,97 é igual a 50%. Consulte a ajuda do RStudio para maiores detalhes dos argumentos da função.\n\n\n\n\n\n\n\n\nFigura 13.4: Área da curva da distribuição F (10,20) abaixo de x = 0,97 é igual a 50%\n\n\n\n\n\nPara gerar 100.000 valores aleatórios da distribuição F com gl1=10 e gl2=20,será usada a função rf(). Em seguida, plota-se um histograma (Figura 13.5) com curva da distribuição F sobreposta (linha vermelha) e compara-se com a função de densidade de probabilidade da distribuição F (curva verde da Figura 13.3).\n\nx &lt;- rf(100000, df1 = 10, df2 = 20)\nhist(x, \n     breaks = 'Scott', \n     freq = FALSE, \n     xlim = c(0,3), \n     ylim = c(0,1),\n     ylab = \"Densidade\",     \n     xlab = '', \n     main = 'Histograma para uma distribuição F(10,20)', \n     cex.main=0.9)\n\ncurve(df(x, df1 = 10, df2 = 20), \n      from = 0, \n      to = 4, \n      n = 5000, \n      col= 'red', \n      lwd=2, add = T)\n\n\n\n\n\n\n\nFigura 13.5: Histograma com curva sobreposta de uma distribuição F (10,20)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#anova-de-um-fator",
    "href": "13-anova.html#anova-de-um-fator",
    "title": "13  Análise de Variância",
    "section": "13.3 ANOVA de um fator",
    "text": "13.3 ANOVA de um fator\nA análise de variância (ANOVA) de um fator, também conhecida como ANOVA de uma via, é uma extensão do teste t independente para comparar duas médias em uma situação em que há mais de dois grupos. Dito de outra forma, o teste t para uso com duas amostras independentes é um caso especial da análise de variância de uma via.\nA ANOVA de um fator compara o efeito de uma variável preditora (variável independente, fator) sobre uma variável contínua (desfecho). Por exemplo, verificar se a intensidade do tabagismo na gestação (não fumantes, fumantes leves, moderados ou pesados) afetam o peso dos recém-nascidos Figura 13.6.\n\n13.3.1 Dados do exemplo\nPara testar a hipótese de que a intensidade do tabagismo materno tem efeito sobre o peso do recém-nascido, foram selecionados aleatoriamente 200 recém-nascidos classificados em quatro grupos de n = 50 cada grupo, conforme a quantidade de cigarros fumados por dia por suas mães.\n\nGrupo 1: recém-nascidos de mães não fumantes;\nGrupo 2: recém-nascidos de mães que fumavam até 10 cigarros/dia – categorizado como tabagismo leve;\nGrupo 3: recém-nascidos de mães que fumavam de 11 a 19 cigarros/dia – categorizado como tabagismo moderado;\nGrupo 4: recém-nascidos de mães que fumavam \\(\\ge\\) 20 cigarros por dia – categorizado como tabagismo pesado.\n\nEstes dados estão no arquivo dadosFumo.xlsx. Para baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho.\n\n13.3.1.1 Leitura dos dados\nA leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função str().\n\ndados &lt;- readxl::read_excel(\"dados/dadosFumo.xlsx\")\nstr (dados)\n\ntibble [200 × 3] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:200] 1 2 3 4 5 6 7 8 9 10 ...\n $ pesoRN: num [1:200] 3458 2723 4125 2905 3608 ...\n $ fumo  : num [1:200] 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n13.3.1.2 Exploração e resumo dos dados\nComo a variável fumo encontra-se como uma variável numérica, será transformada em fator que é a sua verdadeira classe com 4 níveis.\n\ndados$fumo &lt;- factor (dados$fumo, \n                      ordered = TRUE,\n                      levels = c(1, 2, 3, 4),\n                      labels = c (\"não\", \n                                  \"leve\", \n                                  \"moderado\",\n                                  \"pesado\"))\n\nAs medidas resumidoras serão obtidas, usando as funções group_by () e summarise () do pacote dplyr.\n\nalpha = 0.05\nresumo &lt;- dados %&gt;%\n  dplyr::group_by(fumo) %&gt;%\n  dplyr::summarise(n = n(),\n                   media = mean(pesoRN, na.rm = TRUE),\n                   dp = sd (pesoRN, na.rm = TRUE),\n                   ep = dp/sqrt(n),\n                   me = qt ((1-alpha/2), n-1)*ep,\n                   IC_Inf = media - me,\n                   IC_sup = media + me)\nresumo\n\n# A tibble: 4 × 8\n  fumo         n media    dp    ep    me IC_Inf IC_sup\n  &lt;ord&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 não         50 3395.  405.  57.3  115.  3280.  3510.\n2 leve        50 3102.  431.  60.9  122.  2980.  3225.\n3 moderado    50 3151.  495.  70.0  141.  3011.  3292.\n4 pesado      50 2954.  443.  62.6  126.  2828.  3080.\n\n\n\n\n13.3.1.3 Visualização gráfica dos dados\nOs boxplots (Figura 13.6) são uma maneira interessante de visualizar os dados, principalmente com o pacote ggplot23 :\n\n\n\n\n\n\n\n\nFigura 13.6: Boxplots do impacto do tabagismo materno no peso ao nascer\n\n\n\n\n\nObserva-se que há uma tendência de o peso ao nascer diminuir à medida que quantidade de cigarros fumados aumenta. Entretanto, esta diferença pode ser pelo acaso.\n\n\n\n13.3.2 Definição das hipóteses estatísticas\nPara testar a igualdade entre as médias, \\(H_{0}: \\mu_{1} = \\mu_{2} =  \\mu_{3} =  \\mu_{4}\\), supondo homocedasticidade, isto é, as variâncias \\(σ_1^2=σ_2^2=σ_3^3=σ_4^2\\).\nA hipótese alternativa, \\(H_1\\), diz que, pelo menos, uma das médias é diferente das demais. Ela não é unilateral ou bilateral, é multifacetada porque permite qualquer relação que não seja todas as médias iguais. Por exemplo, a \\(H_1\\) inclui o caso em que \\(μ_1=μ_2=μ_3\\), mas \\(μ_4\\) tem um valor diferente.\n\n\n13.3.3 Definição da regra de decisão\nO nível significância, \\(\\alpha\\), geralmente escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição F. O número de graus de liberdade total \\((n – 1)\\) é dividido em dois componentes:\n\nGrau de liberdade do numerador (ENTRE) é dado por \\(gl_{E} = k - 1\\), onde k é o número de grupos.\nGrau de liberdade do denominador (DENTRO ou residual) é dado por \\(gl_{D} = n - k\\), onde, \\(n = \\sum n_{i}\\).\n\nNo exemplo, para um \\(\\alpha = 0,05\\), tem-se:\n\nalpha &lt;- 0.05\nk &lt;-  length(resumo$media)\nn &lt;- nrow(dados)\nglE &lt;-  k - 1\nglE\n\n[1] 3\n\nglD &lt;- n - k\nglD\n\n[1] 196\n\n\nCom esses dados, usando a a função qf()calcula-se o valor crítico de F (Figura 13.7) que é igual:\n\nFc &lt;- qf(1 - alpha, glE, glD)\nround(Fc, 2)\n\n[1] 2.65\n\n\nPortanto, se\n\\[\n|F_{calculado}| &lt; |F_{crítico}|  \\to não \\quad se \\quad rejeita \\quad H_{0} \\\\F_{calculado}| \\ge F_{crítico}| \\to rejeita-se \\quad H_{0}\n\\]\n\n\n\n\n\n\n\n\nFigura 13.7: Curva da Distribuição F 3,196 = 2,65\n\n\n\n\n\n\n\n13.3.4 Teste Estatístico\nA estatística de teste é obtida calculando duas estimativas da variância populacional, \\(\\sigma^2\\): a variância entre os grupos (\\(s_{E}^2\\)) e a variância dentro dos grupos (\\(s_{D}^2\\)).\nA variância entre os grupos também é chamada de quadrado médio entre os grupos (\\(QM_{E}\\)) e é igual a soma dos quadrados entre (\\(SQ_{E}\\)) ou do fator dividida pelos graus de liberdade entre:\n\\[\nQM_{E} = \\frac{SQ_{E}}{gl_{E}}\n\\]\nA variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual (\\(QM_{D}\\)) e é igual a soma dos quadrados dentro dividida pelos graus de liberdade dentro:\n\\[\nQM_{D} = \\frac {SQ_{D}}{gl_{D}}\n\\]\nA variância entre os grupos, \\(QM_{E}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação entre as médias das amostras extraídas de diferentes populações. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{E}\\) será baseado nos valores das médias dos pesos dos recém-nascidos nos quatro grupos diferentes. Se as médias de todas as populações em consideração forem iguais, as médias das respectivas amostras ainda serão diferentes, mas a variação entre elas deverá ser pequena e, consequentemente, espera-se que o valor do \\(QM_{E}\\) seja pequeno. No entanto, se as médias das populações consideradas não são todas iguais, espera-se que a variação entre as médias das respectivas amostras seja grande e, consequentemente, o valor de \\(QM_{E}\\) seja grande.\nA variância dentro das amostras, \\(QM_{D}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação dos dados de diferentes amostras. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{D}\\) será baseado nas médias individuais dos pesos dos recém-nascidos incluídos nas quatro amostras retiradas de quatro populações. O conceito de \\(QM_{D}\\) é semelhante ao conceito de desvio padrão conjugado ou agrupado, \\(s_{o}\\), para duas amostras.\nA estatística de teste é, como visto, a razão das variâncias entre e dentro do grupo. Dessa maneira,\n\\[\nF = \\frac {s_{E}^2}{s_{D}^2} = \\frac {\\frac {SQ_{E}}{gl_{E}}}{\\frac {SQ_{D}}{gl_{D}}} = \\frac {QM_{E}}{QM_{D}}\n\\]\n\n13.3.4.1 Avaliação dos pressupostos do teste\nAo realizar um teste de ANOVA de um fator deve-se assumir que:\n\nAs populações das quais as amostras são retiradas são normalmente distribuídas;\nAs populações das quais as amostras são retiradas têm a mesma variância (homocedasticidade);\nAmostras aleatórias e independentes;\nTodos os grupos devem ter tamanho amostral adequado. Grupos com menos de 10 participantes são problemáticos por reduzirem a precisão da média. Na prática, deve-se evitar menos de 30 participantes. A relação entre os grupos não deve ser maior do que 1:4 (4);\nNão devem existir valores atípicos (outliers);\nA mensuração dos dados deve ser em nível intervalar ou de razão.\n\nPortanto, antes iniciar com o teste de hipótese, verifica-se se as suposições mencionadas para o teste de hipótese ANOVA unidirecional foram atendidas. As amostras são amostras aleatórias e independentes. Isto já é um bom começo!\nAvaliação da normalidade\nVerifica-se a premissa de normalidade, usando o teste de Shapiro-Wilk para os múltiplos grupos e desenhando um gráfico de probabilidade normal (gráficos Q-Q) para cada grupo.\n\n dados %&gt;% \n  dplyr::group_by(fumo) %&gt;% \n  shapiro_test(pesoRN)\n\n# A tibble: 4 × 4\n  fumo     variable statistic     p\n  &lt;ord&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 não      pesoRN       0.976 0.385\n2 leve     pesoRN       0.979 0.499\n3 moderado pesoRN       0.985 0.776\n4 pesado   pesoRN       0.971 0.257\n\n\nPara o gráfico Q-Q (Figura 13.8), pode ser usado a função ggqqplot () do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%.\n\nggpubr::ggqqplot(dados, \n                 x=\"pesoRN\", \n                 facet.by = \"fumo\") +\n  labs(y = \"Peso ao nascer (g) (m)\",\n       x = \"Quantis teóricos\")\n\n\n\n\n\n\n\nFigura 13.8: Gráficos Q-Q\n\n\n\n\n\nO resultado do teste de Shapiro-Wilk entregou todos os resultados com valor P acima de 0.05 e os gráficos Q-Q, não são perfeitos, mas pode-se assumir que os dados para cada grupo caem aproximadamente em uma linha reta.\nAvaliação da homogeneidade das variâncias\nEm seguida, testa-se a suposição de que as variâncias são iguais, usando o Teste de Levene através da função leveneTest () do pacote car.\n\ncar::leveneTest(pesoRN~fumo, center = mean, data = dados)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value Pr(&gt;F)\ngroup   3  0.6306 0.5961\n      196               \n\n\nO teste de Levene exibe como resultado um valor P &gt; 0,05, mostrando que não é possível rejeitar a \\(H_0\\) de igualdade das variâncias.\nVerificação da presença de outliers\nPode-se aqui, além de verificar nos boxplots, usar a função by_group() do pacote dplyr junto com a função identify_outliers() do pacote rstatix :\n\ndados %&gt;% \n  dplyr::group_by(fumo) %&gt;% \n  rstatix::identify_outliers(pesoRN)\n\n# A tibble: 1 × 5\n  fumo      id pesoRN is.outlier is.extreme\n  &lt;ord&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 pesado   168  1611. TRUE       FALSE     \n\n\nComo mostrado nos boxplots, existe um valor atípico, ou seja, está abaixo de 1,5 IIQ. Entretanto, ele não é extremo (&gt; 3 IIQ).\nDa mesma maneira que no teste t, os pressupostos têm mais importância em grupos pequenos e desiguais. Para o exemplo em análise, os pressupostos foram verificados e pode-se assumir que os grupos são independentes e as médias têm distribuição normal e existe homocedasticidade, além disso, os grupos têm o mesmo tamanho (n = 50). Portanto, a análise pode ser continuada.\nO que fazer se os pressupostos são violados?\nSe a homogeneidade da variância é o problema, um teste possível de ser implementado no R é o F de Welch, aplicando a funçãowelch.test(), incluída no pacote onewaytests (5). Existem também testes não paramétricos, como o Teste de Kruskal-Wallis, que será visto mais adiante (Seção 17.6).\n\n\n13.3.4.2 Execução do teste estatístico\nPara realizar um teste de hipótese ANOVA unidirecional, aplica-se a função aov() do R base. Esta função espera a chamada notação de fórmula, portanto, os dados são incluídos separando as duas variáveis de interesse separadas por ~ (til) e os dados, no qual as variáveis especificadas na fórmula, são encontradas. Além da fórmula e dos dados, a função aov() pode necessitar outros argumentos:\n\neffect.size \\(\\to\\) tamanho do efeito a ser calculado e mostrado nos resultados da ANOVA. Os valores permitidos podem ser “ges” (eta ao quadrado) ou “pes” (eta parcial ao quadrado) ou ambos. O padrão é “ges”;\ncontrasts \\(\\to\\) uma lista de contrastes a ser usada para alguns dos fatores da fórmula\n\n\nmodelo.aov &lt;- aov(pesoRN ~ fumo, dados)\n\nsummary(modelo.aov)\n\n             Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \nfumo          3  5030606 1676869   8.482 2.52e-05 ***\nResiduals   196 38748837  197698                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA saída, liberada pela função summary(), é bem reduzida, relatando as informações específicas da Tabela da ANOVA, a estatística F junto com o valor P e os graus de liberdade, soma dos quadrados (Sum Sq) e quadrados médios (Mean Sq), que com frequência se necessita para o relatório do modelo.\nA variância entre os grupos também é chamada de quadrado médio entre os grupos e é igual à soma dos quadrados entre ou do fator dividida pelos graus de liberdade entre. A variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual e é igual à soma dos quadrados dentro dividida pelos graus de liberdade dentro.\nA ANOVA detectou um efeito significativo do fator, que neste caso é o fumo, o valor \\(F_{calculado} = 8,48 &gt; F_{crítico} = 2,65\\) e o valor P &lt; 0,0001.\nPode-se simplesmente relatar isso e encerrar, mas é provável que se queira saber quais grupos diferem uns dos outros. Lembre-se de que não se pode apenas inferir isso a partir de uma visão dos dados, existem testes estatísticos para ajudar a entender as diferenças dos grupos.\n\n\n\n13.3.5 Testes post-hoc\nOs testes de comparações múltiplas constituem-se em uma análise após a realização da ANOVA. Se houve uma diferença, indicada pela ANOVA, os testes de comparações múltiplas ou também conhecidos como teste post hoc, ajudam a quantificar as diferenças entre os grupos para determinar quais grupos diferem significativamente uns dos outros.\nAqui será usado o HSD de Tukey, que é conservador. HSD vem da expressão em inglês - Honest Significant Difference. Este teste requer um objeto aov no qual executa seu procedimento, que chamaremos de pwc. O procedimento de Tukey HSD executará uma comparação de pares de todas as combinações possíveis dos grupos e testará esses pares para diferenças significativas entre suas médias, tudo enquanto ajusta o valor P a um limite superior de significância para compensar o fato de que muitos testes estatísticos estão sendo realizados e a probabilidade de um falso positivo aumenta com o aumento do número de testes. A função a ser usada é a tukey_hsd(), do pacote rstatix.\n\npwc &lt;- rstatix::tukey_hsd (modelo.aov)\npwc\n\n# A tibble: 6 × 9\n  term  group1   group2   null.value estimate conf.low conf.high      p.adj\n* &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 fumo  não      leve              0   -292.     -523.     -61.8 0.00654   \n2 fumo  não      moderado          0   -243.     -474.     -12.8 0.0341    \n3 fumo  não      pesado            0   -441.     -671.    -210.  0.00000915\n4 fumo  leve     moderado          0     49.0    -181.     279.  0.946     \n5 fumo  leve     pesado            0   -149.     -379.      81.9 0.342     \n6 fumo  moderado pesado            0   -198.     -428.      32.8 0.121     \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nCom base nos valores P &lt; 0,05 tem-se três combinações de grupos que diferem: leve-não, moderado-não e pesado-não. Isto mostra que o grupo que difere é o das mães não fumantes.\nPode-se visualizar isso na Figura 13.9 obtida com a função plot(), usando os resultados da função TukeyHSD() disponível no R base. Esta função gera o teste de Tukey com as diferença entre os pares e os intervalos de confiança que permitem a construção do gráfico. A função par()é empregada para adaptar as margens da figura ao tamanho da mesma e depois é usada novamente para retornar ao padrão par(mar=c(5.1, 4.1, 4.1, 2.1)). O argumento mar é um vetor numérico que define os tamanhos das margens na seguinte ordem: inferior, esquerda, superior e direita.\n\npar(mar=c(3,8,3,3)) # Adaptar o tamanho das margens\nplot(TukeyHSD(modelo.aov, conf.level = 0.95), las = 1)\npar(mar=c(5.1, 4.1, 4.1, 2.1)) # Retorna as margens ao padrão\n\n\n\n\n\n\n\nFigura 13.9: Gráficos do Teste de Tukey\n\n\n\n\n\n\n\n13.3.6 Tamanho do efeito\nUma das medidas de tamanho de efeito mais comumente relatadas para a ANOVA é o eta ao quadrado (\\(\\eta^2\\)), que é um índice da força da associação entre um fator e uma variável dependente. Eta ao quadrado é a proporção da variação total atribuível ao fator. É calculado como a razão da variância do fator para a variância total e os valores variam de 0 a 1.\nEsta medida pode ser obtida com o pacote effectsize (6), usando a função eta_squared()com um objeto da classe tipo modelo.aov.\n\neffectsize::eta_squared (modelo.aov, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\nfumo      | 0.11 | [0.05, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nO eta quadrado é uma estimativa tendenciosa da força da associação, na medida em que superestima os efeitos, especialmente para amostras pequenas. Uma outra medida do tamanho do efeito menos tendenciosa é o ômega ao quadrado (\\(\\omega^2\\)). O ômega ao quadrado é uma medida corrigida, menos enviesada e menos inflacionada. Ela pode ser calculada com a função omega_squared(), também do pacote effectsize:\n\neffectsize::omega_squared (modelo.aov, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Omega2 |       95% CI\n---------------------------------\nfumo      |   0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nApesar de ser controverso, pode-se seguir a orientação da Tabela 13.1), para a interpretação (7):\n\n\n\n\nTabela 13.1: Interpretação do Tamanho do Efeito\n\n\n\nResultadoTamanho do Efeito0.01pequeno0,06médio0,14grande\n\n\n\n\n\n\n\n13.3.7 Conclusão\nO peso dos recém-nascidos foi estatisticamente diferente entre os diferentes grupos, F(3, 196) = 8,48, P = 0.0000252, \\(\\eta^2\\) = 0,11.\nAs análises post-hoc de Tukey revelaram que o peso dos recém-nascidos a termo no grupo das gestantes não fumantes apresentou uma diferença estatisticamente significativa do grupo de tabagismo leve (-292 g, IC95%: -523 a -62 g; P = 0,0065); do grupo de tabagismo moderado (-243 g, IC95%: -474 a -13 g; P = 0,0341) e do grupo de tabagismo pesado (-441 g, IC95%: -671 a -210 g; P &lt; 0,0001), mas entre os grupos de fumantes não houve diferença estatisticamente significativa.\n\n13.3.7.1 Apresentação dos resultados\nSerão apresentados boxplots (Figura 13.10)), com ggboxplot(), do pacote ggpubr, utilizando, para cores, a pallete = \"jama\", do pacote ggsci. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix.\n\ntab.aov &lt;- anova_test(dados, \n                       pesoRN ~ fumo, \n                       type = 2)\n \npwc &lt;- tukey_hsd(dados,pesoRN~fumo)\npwc &lt;- pwc %&gt;% add_xy_position (x = \"fumo\")\np &lt;- ggplot2::ggplot(dados, aes(x=fumo, y=pesoRN)) +\n  stat_boxplot(geom = \"errorbar\", \n               width = 0.1) + \n  geom_boxplot(aes(color = fumo), size = 0.8) +\n  scale_color_nejm() +\n  labs(x = \"Tabagismo\", \n       y = \"Peso ao nascer (g)\",\n       subtitle = get_test_label (tab.aov, detailed = TRUE),\n       caption = get_pwc_label(pwc)) +\n  stat_pvalue_manual (pwc,\n                      label = \"p.adj.signif\",\n                      label.size = 3.5,\n                      hide.ns = TRUE) + \n  theme (text = element_text (size = 12)) +\n  theme_classic()\np +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\nFigura 13.10: Efeito do tabagismo na gestação sobre o peso do recém-nascido.([*]: P entre 0,01 e 0,05; [**]: P entre 0,001 e 0,01; [****]: P &lt; 0,0001).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#anova-de-dois-fatores",
    "href": "13-anova.html#anova-de-dois-fatores",
    "title": "13  Análise de Variância",
    "section": "13.4 ANOVA de dois fatores",
    "text": "13.4 ANOVA de dois fatores\nA ANOVA de dois fatores é uma extensão da ANOVA de um fator. Neste tipo de ANOVA, ao invés de observar o efeito de um fator sobre a variável desfecho contínua, é analisado simultaneamente o efeito de duas variáveis de agrupamento. Outros sinônimos para a ANOVA de dois fatores são: ANOVA fatorial ou ANOVA de duas vias.\nQuando se tem dois ou mais fatores, além de observar o efeito desses fatores sobre a variável desfecho, há necessidade de verificar se eles não interagem entre si. Portanto, é um objetivo importante da ANOVA fatorial avaliar se há um efeito de interação estatisticamente significativo entre os fatores.\n\n13.4.1 Dados usados nesta seção\nO conjunto de dados dadosMemoria.xlsx que contém informações de um teste de memória realizado em homens e mulheres, após o consumo de álcool, categorizado em três grupos (nenhum, 3 latas e 6 latas de cerveja tipo pilsen com 4,5% de álcool). O grupo sem consumo de álcool (cerveja sem álcool) serve como controle. Após o consumo de álcool, foi avaliada a memória para a realização de uma tarefa cognitiva.\nNeste exemplo, modificado de Andy Field (8), o efeito do álcool sobre a memória do indivíduo é a variável focal, a principal preocupação. Acredita-se que o efeito de álcool depende de outro fator, sexo, que são chamados de variáveis moderadoras.\nPara baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho.\n\n13.4.1.1 Leitura dos dados\nA leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função str().\n\ndados &lt;- readxl::read_excel(\"dados/dadosMemoria.xlsx\")\n\nstr(dados)\n\ntibble [48 × 4] (S3: tbl_df/tbl/data.frame)\n $ sexo    : chr [1:48] \"Feminino\" \"Feminino\" \"Feminino\" \"Feminino\" ...\n $ alcool  : chr [1:48] \"nenhum\" \"nenhum\" \"nenhum\" \"nenhum\" ...\n $ escore  : num [1:48] 65 70 60 60 60 55 60 55 70 65 ...\n $ latencia: num [1:48] 2.5 3 1.4 1.5 1.8 2.2 2.3 1.6 3.9 4 ...\n\n\n\n\n13.4.1.2 Exploração e sumarização dos dados\nNa saída da função str(), verifica-se que as variáveis alcool e sexo estão como &lt;chr&gt;e o ideal é que estejam como fatores. Portanto, serão colocadas as categorias do consumo de álcool como fator e em uma ordem lógica (nenhum consumo, três latas e 6 latas). A variável sexo será apenas colocada como fator porque não tem uma ordem lógica. As demais variáveis, id (identificação) e escore(escore de memória) podem permanecer com dbl (numérica).\n\ndados$alcool &lt;- factor(dados$alcool,\n                       levels = c(\"nenhum\",\n                                  \"3 latas\",\n                                  \"6 latas\")) \ndados$sexo &lt;- as.factor(dados$sexo)\n\nA sumarização dos dados será feita com as funções group_by() e summarise() do pacote dplyr para a variável escore por grupos, sexo e alcool.\n\nalpha &lt;- 0.05\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(sexo, alcool) %&gt;% \n  dplyr::summarise(n = n(),\n            media = mean(escore, na.rm=TRUE),\n            dp = sd(escore, na.rm=TRUE),\n            ep = dp/sqrt(n),\n            me = qt((1 - alpha/2),n-1)*ep,\n            linf = media - me,\n            lsup = media + me)\nresumo\n\n# A tibble: 6 × 9\n# Groups:   sexo [2]\n  sexo      alcool      n media    dp    ep    me  linf  lsup\n  &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Feminino  nenhum      8  60.6  4.96  1.75  4.14  56.5  64.8\n2 Feminino  3 latas     8  62.5  6.55  2.31  5.47  57.0  68.0\n3 Feminino  6 latas     8  57.5  7.07  2.5   5.91  51.6  63.4\n4 Masculino nenhum      8  66.9 10.3   3.65  8.64  58.2  75.5\n5 Masculino 3 latas     8  66.9 12.5   4.43 10.5   56.4  77.3\n6 Masculino 6 latas     8  35.6 10.8   3.83  9.06  26.6  44.7\n\n\nOs dados estão estruturados com um desenho onde as células tem um formato 2 x 3 (Tabela 13.2) com os fatores sexo e alcool e 8 indivíduos em cada célula. O fator sexo tem dois níveis (feminino e masculino) e o fator alcool tem três níveis (nenhum, 3 latas e 6 latas). Observe que o desenho é balanceado, pois todas as células têm o mesmo número de indivíduos. Esta estrutura é o caso mais simples; desenhos não balanceados são mais complexos.\n\n\n\n\nTabela 13.2: Número de indivíduos por sexo\n\n\n\nSexoNenhumTrês latas*Seis latas*Feminino888Masculino888* Tamanho médio da lata = 350 ml\n\n\n\n\n\n\n\n13.4.1.3 Visualização gráfica dos dados\nPara visualizar os dados, será construido um gráfico com boxplots (Figura 13.11), usando o pacote ggpubr(9), com a função ggboxplot(), que fornece algumas funções fáceis de usar para criar e personalizar gráficos prontos para publicação baseados em ‘ggplot2’. O boxplot irá plotar os dados agrupados pelas combinações dos níveis dos dois fatores.\n\nggpubr::ggboxplot (dados,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.2,\n                   x = \"alcool\", \n                   y = \"escore\", \n                   color = \"black\",\n                   fill = \"sexo\",\n                   palette = \"bmj\",\n                   ylab = \"Escore da Memória\",\n                   xlab = \"\",\n                   legend.title = \"Sexo\",\n                   legend = \"top\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 13.11: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nAlém dos boxplot, é interessante desenhar um gráfico de linhas (Figura 13.12)) que plota a média (ou outro resumo) da variável escore (resposta) para combinações bidirecionais de fatores, ilustrando assim possíveis interações. Aqui, pode-se usar a função ggline(), também pertencente ao interessante pacote ggpubr.\n\nggpubr::ggline(dados, \n               x = \"alcool\", \n               y = \"escore\", \n               color = \"sexo\",\n               size = 0.7,\n               linetype = \"dashed\",\n               position = position_dodge(width = 0.2),\n               add = \"mean_ci\",\n               palette = c(\"red\", \"dodgerblue4\"))\n\n\n\n\n\n\n\nFigura 13.12: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nO gráfico sugere um possível efeito do álcool sobre a memória, bem como uma interação entre os sexos.\n\n\n\n13.4.2 Hipóteses estatísticas\nComo mencionado acima, uma ANOVA de duas vias é usada para avaliar simultaneamente o efeito de duas variáveis categóricas em uma variável quantitativa contínua. Ela é chamada de ANOVA de duas vias porque compara grupos formados por duas variáveis categóricas independentes.\nNo exemplo, o objetivo é saber se a memória depende da álcool e/ou do sexo. Em particular, estamos interessados em:\n\nmedir e testar a relação entre a alcool e a memória,\nmedir e testar a relação entre sexo e memória, e\npossivelmente verificar se a relação entre álcool e memória é diferente para mulheres e homens (o que é equivalente a verificar se a relação entre sexo e memória depende da álcool)\n\nAs duas primeiras relações são chamadas de efeitos principais, enquanto o item 3 é conhecido como efeito de interação.\nOs efeitos principais testam se pelo menos um grupo é diferente de outro (durante o controle da outra variável independente). Por outro lado, o efeito de interação tem como objetivo testar se a relação entre duas variáveis difere dependendo do nível de uma terceira variável. Em outras palavras, se a variação entre a resposta e a primeira variável categórica não depender das modalidades da segunda variável categórica, então não há interação entre as duas variáveis. Se, ao contrário, houver uma modificação dessa variação, seja por um aumento no efeito da primeira variável, seja por uma diminuição, então há uma interação.\nVoltando ao exemplo, tem-se os seguintes testes de hipótese:\nEfeito principal do sexo no escore de memória:\n\\(H_{0}\\): o escore de memória médio é igual entre mulheres e homens.\n\\(H_{1}\\): o escore de memória médio é diferente entre mulheres e homens.\nEfeito principal do álcool no escore de memória:\n\\(H_{0}\\): o escore de memória médio é igual entre as categorias de ingesta de álcool.\n\\(H_{1}\\): o escore de memória médio é diferente entre as categorias de ingesta de. álcool\nInteração entre sexo e álcool:\n\\(H_{0}\\): não há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é a mesma para mulheres e homens (da mesma forma, a relação entre sexo e memória é a mesma para todas as três categorias de ingesta de álcool).\n\\(H_{1}\\): há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é diferente para mulheres e homens (da mesma forma, a relação entre sexo e memória depende da ingesta de álcool).\n\n\n13.4.3 Pressupostos do modelo\nPara usar uma ANOVA de duas vias, os dados devem atender a certos pressupostos. A ANOVA de duas vias faz todas as suposições usuais de um teste paramétrico de diferença:\n\nIndependência de observações\n\nAs variáveis respostas não devem ser dependentes umas das outras (ou seja, uma não deve causar a outra). Isso é impossível de testar com variáveis categóricas - só pode ser garantido por um bom projeto experimental.\nAlém disso, a variável dependente deve representar observações únicas - não devem ser agrupadas em locais ou indivíduos. Se esta premissa for violada, você pode incluir uma variável de bloqueio e/ou usar uma ANOVA de medidas repetidas.\n\nNormalidade\n\nVariável desfecho normalmente distribuída em todos os grupos.\n\nAusência de valores atípicos (outliers)\n\nUm valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais da série, \\(\\pm 1,5\\) o intervalo interquartil (IIQ) e extremo se estiver \\(\\pm 3\\) IIQ. A existência de outliers implica, tipicamente, em prejuízos à interpretação dos resultados.\n\nHomogeneidade de variância (homocedasticidade)\n\nA variação em torno da média para cada grupo sendo comparado deve ser semelhante entre todos os grupos. Se os dados não atenderem a essa suposição, é possível usar uma alternativa não paramétrica, como o teste de Kruskal-Wallis.\n\n\n13.4.4 Verificação dos pressupostos nos dados brutos\nExiste uma discussão se os pressupostos devem ser avaliados nos dados brutos ou apenas nos resíduos. Aqui serão realizadas as duas abordagens que frequentemente resultam no mesmo resultado.\n\n13.4.4.1 Normalidade\nA variável dependente (escore) deve apresentar distribuição aproximadamente normal dentro de cada grupo. Os grupos aqui serão formados pela combinação das duas variáveis independentes (sexo e alcool). A normalidade será avaliada pelo teste de Shapiro-Wilk, com a função shapiro_test() do pacote rstatix (10), separando os grupos com a função group_by() do pacote dplyr, encadeadas com o operador pipe (%&gt;%):\n\ndados %&gt;% \n     dplyr::group_by (sexo, alcool) %&gt;% \n     rstatix::shapiro_test (escore)\n\n# A tibble: 6 × 5\n  sexo      alcool  variable statistic     p\n  &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Feminino  nenhum  escore       0.872 0.156\n2 Feminino  3 latas escore       0.899 0.283\n3 Feminino  6 latas escore       0.897 0.273\n4 Masculino nenhum  escore       0.941 0.622\n5 Masculino 3 latas escore       0.967 0.870\n6 Masculino 6 latas escore       0.951 0.720\n\n\nOs resultados suportam a conclusão de não rejeição da hipótese nula de que os dados se ajustam a distribuição normal.\n\n\n13.4.4.2 Pesquisa de valores atípicos\nA forma mais simples de verificar a presença de um valor atípico é observar o boxplot, mostrado anteriormente. Se observa a presença de valores atípicos entre as mulheres que não ingeriram álcool e nas que ingeriram 3 latas de cerveja. Agora, para confirmar esse achado, será usado a função identify_outliers (), do pacote rstatix:\n\ndados %&gt;% \n      dplyr::group_by (sexo, alcool) %&gt;% \n      rstatix::identify_outliers(escore)\n\n# A tibble: 2 × 6\n  sexo     alcool  escore latencia is.outlier is.extreme\n  &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 Feminino nenhum      70      3   TRUE       TRUE      \n2 Feminino 3 latas     50      3.2 TRUE       FALSE     \n\n\nA saída do teste confirma a existência dos dois valores atípicos, sendo um deles extremo, entretanto como estes valores são possíveis e, relativamente, próximos da média do sexo feminino, portanto, causam pouca preocupação, principalmente porque o teste de ANOVA é bastante robusto.\n\n\n13.4.4.3 Verificação da homogeneidade das variâncias\nPara verificar a homocedasticidade, como os dados têm distribuição normal, é possível usar o teste de Levene, o leveneTest() do pacote car (11).\n\ncar::leveneTest (escore ~ sexo*alcool, \n                 data = dados, \n                 center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  5  1.5268 0.2021\n      42               \n\n\n\n\n\n13.4.5 Verificação dos pressupostos nos resíduos\nO modelo da ANOVA pode ser considerado como um modelo de regressão. Desta forma, este modelo de regressão vai usar os dados brutos para criar um modelo de previsão para esses dados. Este modelo de regressão não é perfeito, existe uma diferença entre os valores previstos e os valores observados, são os resíduos. Faz sentido, então, preocupar-se com os resíduos quando se analisa fatores tentando explicar uma variável dependente contínua, como na ANOVA, pensando em uma regressão linear simples.\nA ANOVA prevê que todos os valores do grupo sejam iguais a média do grupo. Ou seja, um homem que ingere 3 latas de cerveja tem um valor de seu escore de memória igual ao deste grupo. Por este motivo, fazer a análise dos resíduos é praticamente o mesmo que a análise dos valores brutos.\nPara analisar os resíduos (diferença entre os valores observados e o previsto pelo modelo), em primeiro lugar se constrói o modelo da ANOVA com efeito da interação, usando a função lm() do pacote stats, incluído no R base:\n\nmod.int.lm &lt;- lm(formula = escore ~ alcool * sexo,\n              data = dados)\n\nAo se executar o comando, tem-se a impressão que nada ocorreu, entretanto foi criado o modelo da ANOVA com uma série de variáveis, entre elas os resíduos (residuals). Para observar os resíduos, basta digitar:\n\nmod.int.lm$residuals\n\n      1       2       3       4       5       6       7       8       9      10 \n  4.375   9.375  -0.625  -0.625  -0.625  -5.625  -0.625  -5.625   7.500   2.500 \n     11      12      13      14      15      16      17      18      19      20 \n -2.500   7.500   2.500  -2.500  -2.500 -12.500  -2.500   7.500  12.500  -2.500 \n     21      22      23      24      25      26      27      28      29      30 \n -2.500   2.500  -7.500  -7.500 -16.875 -11.875  13.125  -1.875   3.125   8.125 \n     31      32      33      34      35      36      37      38      39      40 \n  8.125  -1.875 -21.875  -6.875  18.125  -1.875   3.125   3.125  13.125  -6.875 \n     41      42      43      44      45      46      47      48 \n -5.625  -5.625  -5.625  19.375  -0.625 -15.625   9.375   4.375 \n\n\nFunção summary() fornece um resumo estatístico dos resíduos:\n\nsummary(mod.int.lm$residuals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-21.875  -5.625  -0.625   0.000   5.156  19.375 \n\n\n\n13.4.5.1 Avaliação da normalidade dos resíduos\nUma das suposições de uma ANOVA é que os resíduos são normalmente distribuídos. A normalidade dos resíduos, inicialmente, será verificada, usando o teste de Shapiro-Wilk com a função shapiro.test(), também pertencente ao pacote stats.\n\nshapiro_test (mod.int.lm$residuals)\n\n# A tibble: 1 × 3\n  variable             statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;\n1 mod.int.lm$residuals     0.982   0.664\n\n\nO teste entrega um valor P &gt; 0.05, indicando que não é possível rejeitar \\(H_{0}\\) de normalidade dos resíduos.\nUma outra maneira comum de verificar essa suposição é criando um gráfico Q-Q. Se os resíduos forem normalmente distribuídos, os pontos em um gráfico Q-Q ficarão em uma linha diagonal reta. Este gráfico (Figura 13.13) pode ser contruído com a função ggqqplot() do pacote ggpubr.\n\nggpubr::ggqqplot(mod.int.lm$residuals,\n                 conf.int = TRUE,\n                 shape = 19,\n                 xlab = \"Quantis teóricos\",\n                 ylab = \"Resíduos\",\n                 color = \"dodgerblue4\")\n\n\n\n\n\n\n\nFigura 13.13: Normalidade dos resíduos - QQ plot.\n\n\n\n\n\nO gráfico QQ de normalidade, mostra que os resíduos seguem aproximadamente uma linha reta, permitindo assumir a normalidade dos mesmos.\n\n\n13.4.5.2 Pesquisa de valores atípicos nos resíduos\nPara a verificação da presença de valores atípicos entre os resíduos, cria-se uma variável que será denominada de residuos (observe o banco de dados com a função str() para ver o acréscimo dessa variável):\n\ndados$residuos &lt;- mod.int.lm$residuals\nstr (dados)\n\ntibble [48 × 5] (S3: tbl_df/tbl/data.frame)\n $ sexo    : Factor w/ 2 levels \"Feminino\",\"Masculino\": 1 1 1 1 1 1 1 1 1 1 ...\n $ alcool  : Factor w/ 3 levels \"nenhum\",\"3 latas\",..: 1 1 1 1 1 1 1 1 2 2 ...\n $ escore  : num [1:48] 65 70 60 60 60 55 60 55 70 65 ...\n $ latencia: num [1:48] 2.5 3 1.4 1.5 1.8 2.2 2.3 1.6 3.9 4 ...\n $ residuos: Named num [1:48] 4.375 9.375 -0.625 -0.625 -0.625 ...\n  ..- attr(*, \"names\")= chr [1:48] \"1\" \"2\" \"3\" \"4\" ...\n\n\nPara identificar os outliers, usa-se função identify_outliers() do pacote rstatix:\n\ndados %&gt;% \n  dplyr::group_by(sexo, alcool) %&gt;% \n  rstatix::identify_outliers(residuos)\n\n# A tibble: 2 × 7\n  sexo     alcool  escore latencia residuos is.outlier is.extreme\n  &lt;fct&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 Feminino nenhum      70      3       9.38 TRUE       TRUE      \n2 Feminino 3 latas     50      3.2   -12.5  TRUE       FALSE     \n\n\nObservando os resultados com os dados brutos, verifica-se que eles são iguais aos atuais, confirmando, que neste caso, tanto faz avaliar os dados brutos como os resíduos.\n\n\n13.4.5.3 Verificação da homogeneidade da variância nos resíduos\nA verificação da homogeneidade da variância entre os resíduos pode ser feita com o teste de Levene, como feito com os dados brutos.\n\ncar::leveneTest (residuos ~ sexo*alcool, \n                 data = dados, \n                 center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  5  1.5268 0.2021\n      42               \n\n\nUma outra maneira de avaliar a homogeneidade da variância, é construir um gráfico diagnóstico4 (Figura 13.14)) do modelo com a função plot(), tipo 1, resíduos versus ajustes (Residuals vs Fitted).\n\nplot(mod.int.lm, 1)\n\n\n\n\n\n\n\nFigura 13.14: Resíduos versus ajuste\n\n\n\n\n\nNão há correlações óbvias entre resíduos e valores ajustados (a média de cada grupo) no gráfico abaixo,onde a linha vermelha tracejada (@fig-residuals) segue praticamente uma linha horizontal em torno de 0, o que é bom. Como resultado, pode-se, assim como no teste de Levene, assumir que as variâncias são homogêneas.\nVerica-se o mesmo ocorrido com a normalidade, os resultados nos resíduos não diferem daqueles realizados com os dados brutos.\n\n\n\n13.4.6 Realização do teste de ANOVA de dois fatores\nMostrou-se que praticamente todos os pressupostos foram atendidos, portanto, agora pode-se prosseguir com a implementação da ANOVA de duas vias.\nA inclusão de um efeito de interação em uma ANOVA de duas vias não é obrigatória. Entretanto, para evitar conclusões errôneas, recomenda-se verificar primeiro se a interação é significativa ou não e, dependendo dos resultados, incluí-la ou não.\nSe a interação não for significativa, é seguro removê-la do modelo final. Por outro lado, se a interação for significativa, ela deverá ser incluída no modelo final que será usado para interpretar os resultados.\nPortanto, deve-se começar com um modelo que inclui os dois efeitos principais (ou seja, sexo e alcool) e a interação:\n\nmod.aov &lt;- aov(formula = escore ~ alcool * sexo,\n                        data = dados)\nsummary (mod.aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nalcool       2   3332  1666.1  20.065 7.65e-07 ***\nsexo         1    169   168.7   2.032    0.161    \nalcool:sexo  2   1978   989.1  11.911 7.99e-05 ***\nResiduals   42   3488    83.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSemelhante a uma ANOVA de uma via, o princípio de uma ANOVA de duas vias baseia-se na dispersão total dos dados e em sua decomposição em quatro componentes:\n\na parcela atribuível ao primeiro fator\na parcela atribuível ao segundo fator\na parcela atribuível à interação dos dois fatores\na parte não explicada ou residual.\n\nA soma dos quadrados (coluna Sum Sq) mostra esses quatro componentes. A ANOVA de duas vias consiste em usar um teste estatístico para determinar se cada componente de dispersão (atribuível aos dois fatores estudados e à interação deles) é significativamente maior do que o componente residual. Se esse for o caso, concluímos que o efeito considerado (fator A, fator B ou a interação) é significativo.\nVê-se que a variável alcool explica uma grande parte da variabilidade da memória. Ela é o fator mais importante para explicar essa variabilidade. Os valore P são exibidos na última coluna do resultado acima (Pr(&gt;F)). A partir desses valores conclui-se que, no nível de significância de 5%:\n\ncontrolando para o alcool, o escore de memória não é significativamente diferente entre os dois sexos (P = 0,161),\ncontrolando para o sexo, o escore memória é significativamente diferente (P &lt; 0,0001) para pelo menos uma categoria de ingesta de álcool, e\na interação entre sexo e álcool (exibida na linha alcool:sexo no resultado) é significativa (P &lt; 0,0001).\n\nPortanto, com base no efeito de interação significativo, se observa que relação entre os escores de memória e álcool é diferente entre os sexos. Como ela é significativa, deve-se mantê-la no modelo e interpretar os resultados desse modelo.\nSe, ao contrário, a interação não for significativa (ou seja, se o valor P &gt; 0,05), esse efeito de interação do modelo seria removido. Abaixo, segue o código de uma ANOVA de dois fatores sem interação, chamada de modelo aditivo:\n\n mod.aov2 &lt;- aov(formula = escore ~ alcool * sexo, \n                 data = dados)\n\nNa Seção 13.4.5, foi construído um modelo para analisar os resíduos. Este modelo está baseado na semelhança dos modelos de regressão linear (veja Seção 15.3) com o modelo da ANOVA. Observa-se que o código é bem semelhante, usando a fórmula variável dependente ~ variável independentes, o sinal + é usado para incluir variáveis independentes sem interação e o sinal * quando há interação. Ou seja, a ANOVA, como todas as ANOVAs, é na verdade um modelo linear. Observe que o código a seguir, usando a função lm() e após Anova() do pacote car, também funciona e retorna os mesmos resultados 5 :\n\n mod.int.lm &lt;- lm(formula = escore ~ sexo * alcool,\n                   data = dados)\n Anova(mod.int.lm)\n\nAnova Table (Type II tests)\n\nResponse: escore\n            Sum Sq Df F value    Pr(&gt;F)    \nsexo         168.7  1  2.0323    0.1614    \nalcool      3332.3  2 20.0654 7.649e-07 ***\nsexo:alcool 1978.1  2 11.9113 7.987e-05 ***\nResiduals   3487.5 42                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nObserve que a função aov() pressupõe um projeto balanceado, o que significa tamanhos de amostra iguais dentro dos níveis das variáveis de agrupamento independentes. Para verificar se os dados estão balanceados, proceda como mostrado na Tabela 13.2. Os resultados mostram o mesmo número de indivíduos em todas as células, portanto, não importa qual o tipo de ANOVA a ser usado. Os resultados serão iguais. Além disso, aov() usa as somas de quadrados do tipo I.\nPara delineamentos não balanceados, ou seja, números desiguais de indivíduos em cada subgrupo, os métodos recomendados são:\n\na ANOVA do tipo II, quando não há interação significativa, que pode ser feita no R com Anova(mod, type = “II”) ou Anova(mod, type = 2), em que mod é o nome do seu modelo salvo, e\na ANOVA do tipo III, quando há uma interação significativa, que pode ser feita no R com Anova(mod, type = “III”) ou Anova(mod, type = 3).\n\nFundamentalmente, a diferença entre um método e outro é como o R calcula a soma dos quadrados ao calcular a ANOVA. Quando os dados são balanceados, os três tipos dão o mesmo resultado 6.\n\n\n13.4.7 Testes post hoc\nNeste estágio, chegou-se ao ponto em que se constatou que o efeito principal do sexo não é significativo e que o efeito principal do álcool é significativo. Além disso, mais importante, existe uma interação entre o álcool e o sexo, o efeito do álcool depende do sexo.\nNão é possível saber exatamente qual categoria da variável alcool é diferente da outra em termos de escore de memória. Para saber isso, há que comparar cada categoria duas a duas graças aos testes post-hoc, também conhecidos como comparações entre pares (13) (14) (15). Há vários testes post-hoc, sendo os mais comuns o Tukey HSD, que testa todos os pares possíveis de grupos. Será utilizada a função TukeyHSD(), usando como argumento o modelo com interação, mod.aov:\n\nTukeyHSD(mod.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = escore ~ alcool * sexo, data = dados)\n\n$alcool\n                    diff        lwr        upr     p adj\n3 latas-nenhum    0.9375  -6.889643   8.764643 0.9544456\n6 latas-nenhum  -17.1875 -25.014643  -9.360357 0.0000105\n6 latas-3 latas -18.1250 -25.952143 -10.297857 0.0000040\n\n$sexo\n                    diff       lwr      upr     p adj\nMasculino-Feminino -3.75 -9.058607 1.558607 0.1613818\n\n$`alcool:sexo`\n                                       diff        lwr        upr     p adj\n3 latas:Feminino-nenhum:Feminino      1.875 -11.726381  15.476381 0.9983764\n6 latas:Feminino-nenhum:Feminino     -3.125 -16.726381  10.476381 0.9825753\nnenhum:Masculino-nenhum:Feminino      6.250  -7.351381  19.851381 0.7432243\n3 latas:Masculino-nenhum:Feminino     6.250  -7.351381  19.851381 0.7432243\n6 latas:Masculino-nenhum:Feminino   -25.000 -38.601381 -11.398619 0.0000306\n6 latas:Feminino-3 latas:Feminino    -5.000 -18.601381   8.601381 0.8796489\nnenhum:Masculino-3 latas:Feminino     4.375  -9.226381  17.976381 0.9277939\n3 latas:Masculino-3 latas:Feminino    4.375  -9.226381  17.976381 0.9277939\n6 latas:Masculino-3 latas:Feminino  -26.875 -40.476381 -13.273619 0.0000080\nnenhum:Masculino-6 latas:Feminino     9.375  -4.226381  22.976381 0.3286654\n3 latas:Masculino-6 latas:Feminino    9.375  -4.226381  22.976381 0.3286654\n6 latas:Masculino-6 latas:Feminino  -21.875 -35.476381  -8.273619 0.0002776\n3 latas:Masculino-nenhum:Masculino    0.000 -13.601381  13.601381 1.0000000\n6 latas:Masculino-nenhum:Masculino  -31.250 -44.851381 -17.648619 0.0000003\n6 latas:Masculino-3 latas:Masculino -31.250 -44.851381 -17.648619 0.0000003\n\n\nQuando se tem muitos grupos para fazer a comparação, fica mais fácil interpretar, usando gráficos (Figura 13.15):\n\n# Definir as margens do eixo para que os rótulos não sejam cortados\npar(mar = c(4.1, 15, 4.1, 2.1))\n# Criar intervalo de confiança para cada comparação\nplot(TukeyHSD(mod.aov, which = \"alcool:sexo\"), las = 1)\n# Retorna as margens ao padrão\npar(mar=c(5.1, 4.1, 4.1, 2.1)) \n\n\n\n\n\n\n\nFigura 13.15: Resíduos versus ajuste\n\n\n\n\n\nAs Saída exibe resultados onde aparece que o consumo de álcool não afetou a memória das mulheres, mas o consumo de 6 latas de cerveja diminuiu o escore de memória dos homens quando comparados com homens que consumiram cerveja sem álcool ou que ingeriram apenas 3 latas de cerveja. Em outras palavras, os termos de interação dizem como o efeito dos álcool muda quando quem o ingere é do sexo masculino ou feminino. O efeito do álcool na memória das mulheres é pequeno, ficando praticamente estável nas três condições. Por outro lado, os homens permanecem estáveis no seu escore de memória quando quantidades pequenas de álcool são ingeridas, declina rapidamente quando ingerem 6 latas de cerveja.\n\n\n13.4.8 Relatando os resultados de uma ANOVA de dois fatores\nPode-se relatar os resultados da ANOVA de dois fatores da seguinte maneira:\n\nUma ANOVA de dois fatores foi realizada para avaliar se a memória de homens e mulheres era afetada pelo consumo do álcool avliado em três níveis:\n\nNão consumiram álcool\nConsumiram 3 latas de cerveja (~ 1L)\nConsumiram 6 latas de cerveja (~ 2L)\n\nOs dados são apresentados como média e desvio padrão, na Tabela 13.3).7\n\n\n\n\n\nTabela 13.3: Efeito do Álcool* sobre a Memória de acordo com o sexo**\n\n\n\nSexoNenhumUm litroDois litrosFeminino60,6 (5,0)62,5 (6,6)57,5 (7,1)Masculino66,9 (10,3)66,9 (12,5)35,6 (10,8)Valor P0,1450,3960,0003* Um litro de cerveja (4,5%) = 5 unidades de alcool** Escore médio (desvio padrão)\n\n\n\n\n\n\nO efeito principal do sexo na memória foi não significativo (F(1,42) = 2,03, P = 0,1614).\nHouve um efeito principal significativo de acordo com a quantidade de álcool consumida na memória dos participantes (F(2,42) = 20,07, P &lt;0,0001).\nAs análises posteriores (teste de Tukey, Figura 13.15) revelaram que a memória não foi afetada nas mulheres pelo consumo de álcool, mas o consumo de 6 latas de cerveja afetou a memória dos homens quando comparados os homens que não consumiram álcool ou que consumiram até 3 latas de cerveja.\nVisualização dos resultados:\n\nSerão apresentados gráficos de barra de erro (Figura 13.16), com ggbarplot(), do pacote ggpubr, utilizando, para cores tonalidades de cinza. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix.\n\n# Construção de um gráfico de barra de erro\nbe &lt;- ggpubr::ggbarplot(dados, \n                        x = \"alcool\", y = \"escore\", \n                        add = \"mean_ci\",\n                        error.plot = \"upper_errorbar\",\n                        fill = \"sexo\", \n                        palette = c(\"gray60\", \"gray40\"),\n                        position = position_dodge(0.8)) +\n  theme(legend.key.size = unit(0.3, 'cm')) +\n  theme(legend.position = \"right\")\n\n# Comparações por pares (pairwise comparisons)\npwc &lt;- dados %&gt;%\n   dplyr::group_by(alcool) %&gt;%\n   rstatix::tukey_hsd(formula = escore ~ sexo)\n\n# Calcular e adicionar as posições x e y.\npwc &lt;- pwc %&gt;%\n  add_xy_position(fun = \"mean_ci\", \n                  x = \"alcool\", \n                  dodge = 0.8) \n\n# Cálculo do teste estatístico com pacote rstatix\nanova &lt;-  anova_test(mod.aov)\n\n# Acrescentar o teste e o valor P ajustado ao gráfico\nbe + stat_pvalue_manual(pwc,  \n                        label = \"p.adj\", \n                        tip.length = 0.01,\n                        y.position = 85) +\n  labs (x = \"Ingestão de álcool\",\n        y = \"Média escore de memória\",\n        subtitle = rstatix::get_test_label (anova, detailed = TRUE),\n        caption = rstatix::get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 13.16: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\nUma opção, é apresentar os resultados como um gráfico de linhas (Figura 13.17), já mostrado anteriormente , usando a função ggline() do pacote ggpubr, com as cores do periódico Lancet:\n\n# Construção de um gráfico linha\ngl &lt;- ggpubr::ggline(dados, \n                     x = \"alcool\", y = \"escore\", \n                     add = \"mean_ci\",\n                     color = \"sexo\",\n                     size = 0.7,\n                     linetype = \"dashed\", \n                     palette = \"lancet\",\n                     position = position_dodge(0.2)) +\n  theme(legend.key.size = unit(0.3, 'cm')) +\n  theme(legend.position = \"right\")\n\n# Comparações por pares (pairwise comparisons)\npwc &lt;- dados %&gt;%\n  dplyr::group_by(alcool) %&gt;%\n  rstatix::tukey_hsd(formula = escore ~ sexo)\n\n# Calcular e adicionar as posições x e y.\npwc &lt;- pwc %&gt;%\n  add_xy_position(fun = \"mean_ci\", \n                  x = \"alcool\", \n                  dodge = 0.8)\n\n# Cálculo do teste estatístico com pacote rstatix\nanova &lt;-  anova_test(mod.aov)\n\n# Acrescentar o teste e o valor P ajustado ao gráfico\ngl + stat_pvalue_manual(pwc,  \n                        label = \"p.adj\", \n                        tip.length = 0.01,\n                        y.position = 85) +\n  labs (x = \"Ingestão de álcool\",\n        y = \"Média escore de memória\",\n        subtitle = rstatix::get_test_label (anova, \n                                            detailed = TRUE),\n        caption = rstatix::get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 13.17: Efeito do álcool na memória de acordo com o sexo.\n\n\n\n\n\n\n\n\n\n1. Field A, Miles J, Field Z. Comparing several means: ANOVA (GML 1). Em: Discovering Statistics Using R. Sage Publications, Ltd; 2012. p. 399–400. \n\n\n2. Menezes RX de. Análise de Variância. Em: Massad E, Menezes RX de, Silveira PSP, Ortega NRS, editores. Métodos Quantitativos em Medicina. Barueri, São Paulo: Editora Manole Ltda.; 2004. p. 297–300. \n\n\n3. Garren ST. Package fastgraph [Internet]. CRAN. Comprehensive R Archive Network (CRAN); 1919. Disponível em: https://CRAN.R-project.org/package=fastGraph\n\n\n4. Peat J, Barton B. Continuous variables: analysis of variance. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 114. \n\n\n5. Dag O, Dolgun A, Konar NM. Onewaytests: An R Package for One-Way Tests in Independent Groups Designs. R Journal. 2018;10(1):175–99. \n\n\n6. Ben-Shachar MS, Lüdecke D, Makowski D. effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software. 2020;5(56):2815. \n\n\n7. Watson P. Rules of thumb on magnitudes of effect sizes [Internet]. MRC Cognition and Brain Sciences Unit. Cambridge University; 2021. Disponível em: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n8. Field A, Miles J, Field Z. Factorial ANOVA (GLM3). Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 513–4. \n\n\n9. Kassambara A. ggpubr:’ggplot2’ based publication ready plots [R package ggpubr version 0.5.0] [Internet]. The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN); 2022. Disponível em: https://cloud.r-project.org/web/packages/ggpubr/index.html\n\n\n10. Kassambara A. rstatix: Pipe-Friendly Framework for Basic Statistical Tests [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=rstatix\n\n\n11. Fox J, Weisberg S. An R Companion to Applied Regression [Internet]. Third. Thousand Oaks CA: Sage; 2019. Disponível em: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n12. Patterson R, Coffman J, Goldstein-Greenwood J, Others. Understanding Diagnostic Plots for Linear Regression Analysis [Internet]. Research Data Services + Sciences. University of Virginia Library; 2015. Disponível em: https://data.library.virginia.edu/diagnostic-plots/\n\n\n13. Wickens TD, Keppel G. Two-way factorial experiments. Em: Design and analysis: A researcher’s handbook. Pearson Prentice-Hall; 2004. p. 193–286. \n\n\n14. Maxwell SE, Delaney HD, Kelley K. Two-way Between-Subject Factorial Designs. Em: Designing experiments and analyzing data: A model comparison perspective. Third Edition. Routledge; 2017. p. 312–82. \n\n\n15. Lenth R, Singmann H, Love J, Buerkner P, Herve M. Emmeans: Estimated marginal means, aka least-squares means. R package version. 2018;1(1):3.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "13-anova.html#footnotes",
    "href": "13-anova.html#footnotes",
    "title": "13  Análise de Variância",
    "section": "",
    "text": "Pode ser usada também para comparar a média de duas populações e o resultado será o mesmo de um teste t para amostras independentes.↩︎\nNo texto, df1 e df2 (em inglês, df = degree of freedom) foram traduzidos para o português como gl1 e gl2 (gl = graus de liberdade).↩︎\nVolte à Seção 6.6 para mais informações sobre o como fazer gráficos no ggplot2.↩︎\nOutros gráficos diagnósticos podem ser obtidos para analisar resíduos em um modelo de regressão (12)↩︎\nA função Anova() do pacote car, usada para testar efeitos principais e de interação em modelos lineares gerais, não deve ser confundida com a função anova(), da base do R, porque esta fornece resultados sequenciais que dependem da ordem em que as variáveis aparecem no modelo.↩︎\nPara os interessados, pode-se obter maiores informações sobre os diferentes tipos de ANOVA em https://www.r-bloggers.com/2011/03/anova-%E2%80%93-type-iiiiii-ss-explained/↩︎\nlembrar que um litro de cerveja (4,5%) = 5 unidades de alcool↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Análise de Variância</span>"
    ]
  },
  {
    "objectID": "14-anovaRep.html",
    "href": "14-anovaRep.html",
    "title": "14  ANOVA de medidas repetidas",
    "section": "",
    "text": "14.1 Pacotes necessários neste capítulo\nInstalar e carregar os seguintes pacotes:\npacman::p_load(dplyr,\n               flextable,\n               ggplot2, \n               ggpubr, \n               ggsci,\n               knitr,\n               kableExtra,\n               readxl, \n               rstatix, \n               tidyr)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANOVA de medidas repetidas</span>"
    ]
  },
  {
    "objectID": "14-anovaRep.html#anova-de-medidas-repetidas-de-um-fator",
    "href": "14-anovaRep.html#anova-de-medidas-repetidas-de-um-fator",
    "title": "14  ANOVA de medidas repetidas",
    "section": "14.2 ANOVA de medidas repetidas de um fator",
    "text": "14.2 ANOVA de medidas repetidas de um fator\n\n14.2.1 Dados usados nesta seção\nOs dados usados como exemplo contém os escores de autoestima de 10 indivíduos em três pontos do tempo durante uma dieta específica para determinar se esta interfere na sua autoestima. A autoestima foi determinada por uma escala (1), (2), cujos resultados variam de 0 a 30 pontos. Valores entre 15 e 25 caracterizam uma autoestima muito boa; abaixo de 15 é considerada baixa autoestima.\nOs dados podem ser obtidos aqui. Baixe no seu diretório de trabalho e carregue com a função read_excel() do pacote readxl:\n\ndados &lt;- read_excel(\"dados/dadosAutoestima.xlsx\")\n\n\n14.2.1.1 Exploração e transformação dos dados\nIncialmente, observa-se como os dados foram registrados, usando a função head():\n\nhead(dados)\n\n# A tibble: 6 × 4\n     id    t1    t2    t3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 12.0   15.5  21.3\n2     2  7.67  20.7  18.9\n3     3  9.73  13.3  29.3\n4     4 10.3   14.1  25.0\n5     5  8.61  11.7  19.4\n6     6  6.14  16.0  20.0\n\n\nOs dados se encontram no formato amplo (a variável tempo está colocada em três colunas) e para realizar a ANOVA de medidas repetidas, o R necessita que os dados estejam no formato longo. Para fazer esta transformação será usada a função pivot_longer()1 do pacote tidyr (3). Nesta função, no argumento data, coloca-se o nome do conjunto de dados; em cols, há necessidade informar as colunas do formato amplo que serão reunidas. No argumento names_to, nomear a coluna que conterá as colunas unificadas e em values_to, especificar o nome da variável no formato longo que conterá os valores. A variável id e a nova variável tempo devem ser convertida para fatores e o novo conjunto de dados será atribuído a um objeto nomeado dadosL:\n\ndadosL &lt;- dados %&gt;%\n  tidyr::pivot_longer(cols = c(t1, t2, t3),\n                       names_to = \"tempo\",\n                       values_to = \"escores\") %&gt;% \nconvert_as_factor(id, tempo)\n\nhead(dadosL)\n\n# A tibble: 6 × 3\n  id    tempo escores\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;\n1 1     t1      12.0 \n2 1     t2      15.5 \n3 1     t3      21.3 \n4 2     t1       7.67\n5 2     t2      20.7 \n6 2     t3      18.9 \n\n\n\n\n14.2.1.2 Sumarização dos dados\nCalcular algumas estatísticas resumidas dos escores de autoestima por grupos (tempo): média e desvio padrão, usando a funções group_by() e summarise() do pacote dplyr:\n\n dadosL %&gt;%\n   dplyr::group_by(tempo) %&gt;% \n   dplyr::summarise(n = n(),\n                    média = mean(escores, na.rm =TRUE),\n                    dp = sd(escores, na.rm = TRUE))\n\n# A tibble: 3 × 4\n  tempo     n média    dp\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 t1       10  9.42  1.66\n2 t2       10 14.8   2.59\n3 t3       10 22.9   3.43\n\n\n\n\n\n14.2.2 Visualização dos dados\nA visualização pode ser obtida com um conjunto de boxplots (Figura 14.1), acrescido de barras de erro. Estes gráficos permitem visualizar a variação dos escores com o tempo.\n\nggpubr::ggboxplot (dadosL,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1, \n                   x = \"tempo\", \n                   y = \"escores\", \n                   color = \"black\",\n                   fill = \"lightblue\",\n                   ylab = \"Escore de Autoestima\",\n                   xlab = \"Tempo\",\n                   ggtheme = theme_bw(),\n                   legend = \"none\") +\n  scale_fill_grey(start=0.95, end=0.6) +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 14.1: Boxplots mostrando o impacto de uma dieta nos escores de autoestima.\n\n\n\n\n\nOutra maneira de visualizar os dados é através de um gráfico de linha (Figura 14.2) que mostra bem o comportamento dos escores com o tempo:\n\nggpubr::ggline(dadosL, \n                x = \"tempo\",\n                y = \"escores\",\n                color = \"gray60\",\n                add.params = list(color = \"red\"),\n                size = 1,\n                linetype = \"dashed\",\n                add = c(\"mean_ci\"),\n                point.size = 2,\n                point.color = \"red\",\n                ggtheme = theme_bw()) +\n   ylab(\"Escore de Autoestima\") +\n   xlab(\"Tempo\")\n\n\n\n\n\n\n\nFigura 14.2: Gráfico de linha mostrando o impacto de uma dieta nos escores de autoestima.\n\n\n\n\n\nOs dados mostram que existe uma modificação da autoestima (melhora nitidamente!) à medida que o tempo passa, utilizando a dieta.\n\n\n14.2.3 Avaliação dos pressupostos\nA ANOVA de medidas repetidas faz as seguintes suposições sobre os dados:\n\nA amostra foi selecionada aleatoriamente da população;\nA variável dependente é normalmente distribuída na população para cada nível do fator dentro dos sujeitos;\nNão deve e existir outliers extremos;\nExistência de esfericidade\n\n\n14.2.3.1 Identificação de valores atípicos\nNão deve haver valores atípicos em nenhuma célula do delineamento Isso pode ser verificado visualizando os dados nos boxplots, mostrados anteriormente, onde se observa a presença de dois outliers, um no t1 e outro em t2. Além disso, pode-se verificar a presença de valores atípicos, usando a função identify_outliers() do pacote rstatix.\n\ndadosL %&gt;%\n  dplyr::group_by(tempo) %&gt;%\n  rstatix::identify_outliers(escores)\n\n# A tibble: 2 × 5\n  tempo id    escores is.outlier is.extreme\n  &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 t1    6        6.14 TRUE       FALSE     \n2 t2    2       20.7  TRUE       FALSE     \n\n\nA saída confirma a presença de dois valores atípicos, em t1 e em t2. Entretanto, eles não são extremos, não estão afastados acima de 3 intervalos interquatis e , provavelmente, não trarão problemas, apesar da amostra ser pequena.\n\n\n14.2.3.2 Avaliação da normalidade\nPara testar a hipótese de normalidade dos dados, será utilizado o teste de Shapiro-Wilk através da função shapiro_test (), do pacote rstatix e a função group_by (), incluída no pacote dplyr ou rstatix, junto com o operador pipe (%&gt;%):\n\ndadosL %&gt;% \n  dplyr::group_by (tempo) %&gt;% \n  rstatix::shapiro_test(escores)\n\n# A tibble: 3 × 4\n  tempo variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 t1    escores      0.967 0.859\n2 t2    escores      0.876 0.117\n3 t3    escores      0.923 0.380\n\n\nA saída exibe valores P acima de 0,05, significando que os escores de autoestima estão normalmente distribuídos em cada momento do tempo.\nA normalidade pode também ser avaliada com um gráfico QQ (Figura 14.3) para cada um dos momentos, usando a função ggqqplot () do pacote ggpubr, consulte a vinheta do pacote para maiores detalhes. Foi utilizado também o argumento faced.by, que divide em painéis, organizando-os como uma grade, de acordo com o momento ( t1, t2 e t3).\n\nggpubr::ggqqplot(dadosL, \n                 x = \"escores\",\n                 facet.by = \"tempo\",\n                 color = \"tempo\", \n                 palette = get_palette(\"Dark2\", 3),\n                 legend = \"none\")\n\n\n\n\n\n\n\nFigura 14.3: Gráfico QQ para verificar a normalidade\n\n\n\n\n\nObservando o gráfico, como quase todos os pontos caem aproximadamente ao longo da linha de referência, pode-se assumir a normalidade dos escores em todos os momentos do tempo.\n\n\n14.2.3.3 Esfericidade\nA violação da suposição de esfericidade pode distorcer os cálculos de variância resultantes de um teste ANOVA de medidas repetidas mais liberal (ou seja, um aumento na taxa de erro Tipo I). Nesse caso, a ANOVA de medidas repetidas deve ser corrigida apropriadamente dependendo do grau em que a esfericidade foi violada. Na relação entre os escores, há necessidade de pressupor que exista esfericidade (\\(\\epsilon\\) - épsilon), também chamada de circularidade, grosseiramente semelhante à homocedasticidade da ANOVA de uma via. A ANOVA de medidas repetidas pressupõe que as variâncias das diferenças entre todas as combinações de condições relacionadas (ou níveis de grupo) são iguais. A melhor maneira de verificá-la é calcular as diferenças entre os pares de escores em todas as combinações dos níveis de tratamento.\nO teste de esfericidade de Mauchly é usado para avaliar se a suposição de esfericidade é atendida ou não. Isso é relatado automaticamente ao usar a função anova_test () do pacote rstatix. Se o teste resulta em um valor P menor do que 0,05, pode-se concluir de que há uma diferença significativa entre as variâncias das diferenças.\nO principal problema da violação da condição de esfericidade é a ocorrência de testes F não exatos e liberais, com consequente perda do poder do teste. Existem várias correções que podem ser aplicadas para produzir uma razão F válida, através do ajuste dos graus de liberdade.\nAs correções mais frequentemente preconizadas são o \\(\\epsilon\\) de Greenhouse-Geisser (GGe) e o \\(\\epsilon\\) de Huynh-Feldt (HFe). Huynh e Feldt (4) relataram que quando a correção \\(\\epsilon\\) de Greenhouse-Geisser é &gt; 0,75 muitas hipóteses nulas falsas deixam de ser rejeitadas, isto é, o teste é muito conservador, propondo outra correção dos graus de liberdade. É recomendado o uso da correção de Greenhouse-Geisser para o ajuste dos graus de liberdade quando \\(\\epsilon\\) &lt; 0,75 ou nada se sabe a respeito da esfericidade (5). Avaliando o poder destes testes, Muller (6) verificou que a correção de Greenhouse-Geisser fornece um controle adicional do erro Tipo I, enquanto o poder é maximizado.\n\n\n\n14.2.4 Cálculo da estatística do teste\nComo mencionado, será usada a função anova_test(), do pacote rstatix, para o cálculo da ANOVA de medidas repetidas, criando um modelo que será atribuído ao objeto mod.anova:\n\nmod.anova &lt;- rstatix::anova_test(data = dadosL, \n                                 dv = escores, \n                                 wid = id, \n                                 within = tempo)\nmod.anova\n\nANOVA Table (type III tests)\n\n$ANOVA\n  Effect DFn DFd      F        p p&lt;.05   ges\n1  tempo   2  18 55.463 2.02e-08     * 0.829\n\n$`Mauchly's Test for Sphericity`\n  Effect     W     p p&lt;.05\n1  tempo 0.551 0.092      \n\n$`Sphericity Corrections`\n  Effect  GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n1  tempo 0.69 1.38, 12.42 2.16e-06         * 0.774 1.55, 13.94 6.04e-07\n  p[HF]&lt;.05\n1         *\n\n\nEm primeiro lugar, observar o Teste de Mauchly para a esfericidade. Verifica-se que o efeito do tempo tem um valor P = 0,092, ou seja, &gt; 0,05 e, portanto, não houve violação da esfericidade e não há necessidade de observar as correções do \\(\\epsilon\\) de Greenhouse-Geisser (GGe) ou o \\(\\epsilon\\) de Huynh-Feldt (HFe).\nDesta forma, pode-se dizer que houve uma modificação significativa no escore de de autoestima, à medida que o tempo passou (F (2,18) = 55,5, P &lt; 0.0001, \\(\\eta^2 = 0,83\\)).\nUsando a função get_anova_table() do pacote rstatix para extrair a tabela ANOVA, a correção de esfericidade Greenhouse-Geisser é aplicada automaticamente aos fatores que violam a suposição de esfericidade.\n\nrstatix::get_anova_table(mod.anova)\n\nANOVA Table (type III tests)\n\n  Effect DFn DFd      F        p p&lt;.05   ges\n1  tempo   2  18 55.463 2.02e-08     * 0.829\n\n\nOnde,\n\nF indica que se está comparando com uma distribuição F (teste F); (2, 18) indica os graus de liberdade no numerador (DFn) e no denominador (DFd), respectivamente; 55,5 indica o valor da estatística F.\np especifica o valor P.\nges é o tamanho do efeito generalizado (quantidade de variabilidade devido ao fator dentro dos assuntos), eta ao quadrado \\(\\eta^2\\).\n\n\n\n14.2.5 Testes post hoc\nÉ possível fazer comparações por pares. realizando vários testes t pareados entre os níveis do dentro do fator (tempo), usando a função pairwise_t_test(), incluída no R base. Os valores P são ajustados usando o método de correção de testes múltiplos de Bonferroni.\n\npwc &lt;- dadosL %&gt;%\n  pairwise_t_test(\n    escores ~ tempo, paired = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc\n\n# A tibble: 3 × 10\n  .y.     group1 group2    n1    n2 statistic    df         p p.adj p.adj.signif\n* &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 escores t1     t2        10    10     -4.97     9   7.73e-4  2e-3 **          \n2 escores t1     t3        10    10    -13.2      9   3.34e-7  1e-6 ****        \n3 escores t2     t3        10    10     -4.87     9   8.87e-4  3e-3 **          \n\n\nTodas as diferenças pareadas são estatisticamente significativas.\n\n\n14.2.6 Relatando os resultados da ANOVA de medidas repetidas unifatorial\nPode-se relatar de forma simples:\n\nOs escores de autoestima se modificaram de forma significativa de acordo com a passagem do tempo, F(2, 18) = 55,5, p &lt; 0,0001, \\(\\eta^2\\) = 0,82.\nAnálises post hoc, com um ajuste de Bonferroni, revelaram que todas as diferenças pareadas, entre os pontos de tempo, foram estatisticamente diferentes (P &lt; 0,05).\n\nUma opção de apresentação gráfica, é o gráfico de linhas (Figura 14.4), usando a função ggline() do pacote ggpubr, junto com os teste estatísticos:\n\ngl &lt;- ggpubr::ggline(dadosL,\n                     x = \"tempo\",\n                     y = \"escores\",\n                     color = \"gray60\",\n                     add.params = list(color = \"black\"),\n                     size = 0.7,\n                     linetype = \"dashed\",\n                     add = \"mean_ci\",\n                     point.size = 1,\n                     point.color = \"black\")\n\ngl + \n  ggpubr::stat_pvalue_manual(pwc, \n                             label = \"p.adj\", \n                             tip.length = 0.00,\n                             y.position = c(23, 29, 26)) +\n  labs (x = \"Tempo\",\n        y = \"Escore de autoestima\",\n        subtitle = get_test_label (mod.anova, detailed = TRUE),\n        caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 14.4: Impacto de uma dieta específica na autoestima.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANOVA de medidas repetidas</span>"
    ]
  },
  {
    "objectID": "14-anovaRep.html#anova-de-medidas-repetidas-de-dois-fatores",
    "href": "14-anovaRep.html#anova-de-medidas-repetidas-de-dois-fatores",
    "title": "14  ANOVA de medidas repetidas",
    "section": "14.3 ANOVA de medidas repetidas de dois fatores",
    "text": "14.3 ANOVA de medidas repetidas de dois fatores\n\n14.3.1 Dados usados nesta seção\nO conjunto de dados de dadosAutoestima2.xlsx contém as medidas dos escores de autoestima de 12 indivíduos inscritos em 2 ensaios clínicos sucessivos de curto prazo (4 semanas): placebo e dieta especial.\nOs dados podem ser obtidos aqui. Baixe no seu diretório de trabalho.\nCada participante participou dos dois ensaios. A ordem das tentativas foi equilibrada e foi permitido tempo suficiente entre os ensaios para permitir que quaisquer efeitos dos ensaios anteriores se dissipassem (washout).O escore de autoestima foi registrado em três momentos: no início (t1), no meio (t2) e no final (t3) dos ensaios.\nA questão é investigar se esse tratamento dietético de curto prazo pode induzir um aumento significativo do escore de autoestima ao longo do tempo. Em outras palavras, se quer saber se há interação significativa entre dieta e tempo no escore de autoestima.\n\n14.3.1.1 Leitura dos dados\nA leitura dos dados será feita com a função read_excel() do pacote readxl. Após a leitura, será exibida 3 linhas aleatórias por grupo de tratamento, usando a função sample_n_by() do pacote rstatix::\n\nautoestima &lt;- readxl::read_excel(\"dados/dadosAutoestima2.xlsx\")\nset.seed(123)\nautoestima %&gt;% sample_n_by(tratamento, size = 3)\n\n# A tibble: 6 × 5\n     id tratamento    t1    t2    t3\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     3 dieta       27.3  27.3  27.6\n2    12 dieta       24    24    23.4\n3    10 dieta       27    27.9  28.5\n4     2 placebo     29.1  28.5  26.4\n5     6 placebo     21.6  19.5  18.9\n6     5 placebo     23.1  21.9  20.4\n\n\n\n\n14.3.1.2 Transformação dos dados\nOs dados autoestima estão no formato amplo e as colunas t1, t2 e t3 devem ser reunidas, em uma única variável denominada tempo, transformando o formato amplo em longo, usando a função pivot_longer() do pacote tidyr. A seguir, converter em fator esta nova variável tempo e a variável identificadora id:\n\n autoestimaL &lt;- autoestima %&gt;% \n   tidyr::pivot_longer(cols = c(t1, t2, t3),\n                       names_to = \"tempo\",\n                       values_to = \"escores\") %&gt;% \n   convert_as_factor(id, tratamento, tempo)\n\nExplorar o novo conjunto de dados no formato longo:\n\nautoestimaL %&gt;% sample_n_by(tratamento, tempo, size = 1)\n\n# A tibble: 6 × 4\n  id    tratamento tempo escores\n  &lt;fct&gt; &lt;fct&gt;      &lt;fct&gt;   &lt;dbl&gt;\n1 4     dieta      t1       27.3\n2 6     dieta      t2       22.5\n3 9     dieta      t3       27.9\n4 10    placebo    t1       27.6\n5 11    placebo    t2       27.6\n6 5     placebo    t3       20.4\n\n\nNesse exemplo, o efeito do “tempo” no escore de autoestima é nossa variável focal, nossa principal preocupação.\nNo entanto, pensa-se que o efeito “tempo” será diferente se o tratamento for realizado ou não. Nesse cenário, a variável “tratamento” é considerada como variável moderadora.\n\n\n14.3.1.3 Sumarização dos dados\nOs dados serão por tratamento e tempo e, em seguida, serão calculadas algumas estatísticas resumidas da variável de escore: média e sd (desvio padrão).\n\nautoestimaL %&gt;%\n  dplyr::group_by(tratamento, tempo) %&gt;%\n  rstatix::get_summary_stats(escores, type = \"mean_sd\")\n\n# A tibble: 6 × 6\n  tratamento tempo variable     n  mean    sd\n  &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 dieta      t1    escores     12  26.3  2.29\n2 dieta      t2    escores     12  26.4  2.23\n3 dieta      t3    escores     12  26.3  2.44\n4 placebo    t1    escores     12  26.4  2.42\n5 placebo    t2    escores     12  25.2  3.07\n6 placebo    t3    escores     12  23.6  3.16\n\n\n\n\n14.3.1.4 Visualização dos dados\nSerão criados boxplots (Figura 14.5) do escore coloridos pelos grupos de tratamento, com cores da paleta do NEJM, usando o pacote ggsci e a função scale_fill_nejm():\n\nggpubr::ggboxplot (autoestimaL,\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1, \n                   x = \"tempo\", \n                   y = \"escores\", \n                   color = \"black\",\n                   fill = \"tratamento\",\n                   ylab = \"Escore de Autoestima\",\n                   xlab = \"Tempo\",\n                   ggtheme = theme_bw())+\n  scale_fill_nejm() +\n  theme (text = element_text (size = 12)) \n\n\n\n\n\n\n\nFigura 14.5: Impacto de uma dieta específica na autoestima.\n\n\n\n\n\n\n\n\n14.3.2 Avaliação dos pressupostos\nOs pressupostos da ANOVA de medidas repetidas de dois fatores são os mesmos da de um fator (Seção 14.2.3).\n\n14.3.2.1 Identificação dos outliers\nA observação dos boxplots mostra que não existem valores atípicos. Estes outliers serão analisados, usando função identify_outliers() do pacote rstatix.\n\nautoestimaL %&gt;%\n  dplyr::group_by(tratamento, tempo) %&gt;%\n  rstatix::identify_outliers(escores)\n\n[1] tratamento tempo      id         escores    is.outlier is.extreme\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n\nA saída mostra a ausência de valores atípicos, com já se havia observado nos boxplots (Figura 14.5).\n\n\n14.3.2.2 Avaliação da normalidade\nPara testar a hipótese de normalidade dos dados, será utilizado o teste de Shapiro-Wilk através da função shapiro_test (), do pacote rstatix e a função group_by (), incluída no pacote dplyr ou rstatix, junto com o operador pipe (%&gt;%):\n\nautoestimaL %&gt;% \n  dplyr::group_by (tratamento, tempo) %&gt;% \n  rstatix::shapiro_test(escores)\n\n# A tibble: 6 × 5\n  tratamento tempo variable statistic      p\n  &lt;fct&gt;      &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 dieta      t1    escores      0.919 0.279 \n2 dieta      t2    escores      0.923 0.316 \n3 dieta      t3    escores      0.886 0.104 \n4 placebo    t1    escores      0.828 0.0200\n5 placebo    t2    escores      0.868 0.0618\n6 placebo    t3    escores      0.887 0.107 \n\n\nA saída do teste mostra que os escores de autoestima estão normalmente distribuídos em cada momento do tempo, havendo uma exceção: o grupo placebo, no momento t1. Fato que pode ser constatado pela assimetria do boxplot nesse momento do tempo (Figura 14.5).\nÉ possível construir um gráfico QQ (Figura 14.6) para cada um dos momentos, usando a função ggqqplot () do pacote ggpubr, consulte a vinheta do pacote para maiores detalhes. Foi utilizado também a função facet_grid(), do ggplot2, que divide em painéis, organizando-os como uma grade, de acordo com o momento ( t1, t2 e t3).\n\nggpubr::ggqqplot(data = autoestimaL,\n         x = \"escores\", \n         color = \"tempo\", \n         palette = get_palette(\"Dark2\", 3),\n         legend = \"none\",\n         ggtheme = theme_bw()\n         ) + \n  facet_grid(tempo~tratamento)\n\n\n\n\n\n\n\nFigura 14.6: Gráfico QQ para verificar a normalidade\n\n\n\n\n\nObservando o gráfico, como quase todos os pontos caem aproximadamente ao longo da linha de referência, pode-se seguir a análise, pois não há muito problema.\n\n\n14.3.2.3 Esferecidade\nA esferecidade será avaliada junto com a construção do modelo.\n\n\n\n14.3.3 Cálculo da estatística do teste\nÉ realizado da mesma maneira do que a ANOVA de medidas repetidas de uma via com a função anova_test() do pacote rstatix:\n\nmod.anova2 &lt;- rstatix::anova_test(data = autoestimaL, \n                                   dv = escores,\n                                   wid = id,\n                                   within = tempo, \n                                   between = tratamento,\n                                   type = 3)\nmod.anova2\n\nANOVA Table (type III tests)\n\n$ANOVA\n            Effect DFn DFd      F        p p&lt;.05   ges\n1       tratamento   1  22  1.432 2.44e-01       0.059\n2            tempo   2  44 28.417 1.19e-08     * 0.049\n3 tratamento:tempo   2  44 29.259 8.29e-09     * 0.050\n\n$`Mauchly's Test for Sphericity`\n            Effect     W     p p&lt;.05\n1            tempo 0.582 0.003     *\n2 tratamento:tempo 0.582 0.003     *\n\n$`Sphericity Corrections`\n            Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]\n1            tempo 0.705 1.41, 31.03 1.04e-06         * 0.739 1.48, 32.52\n2 tratamento:tempo 0.705 1.41, 31.03 8.00e-07         * 0.739 1.48, 32.52\n     p[HF] p[HF]&lt;.05\n1 6.19e-07         *\n2 4.72e-07         *\n\n\nAnalisando o teste de Mauchly para a esfericidade, verifica-se que houve violação da esfericidade, tanto para o efeito do tempo (W = 0,582, P = 0,003) como para a interação tratamento:tempo (W = 0,582, P = 0,003). Esses resultados indicam a necessidade de correção para ajustar os graus de liberdade. A correção de Greenhouse-Geisser é utilizada, pois é o recomendado quando W &lt; 0,75 (veja Seção 14.2.3.3). O modelo ANOVA, gerado pela função anova_test(), entregou a tabela da ANOVA sem correção, o teste de Mauchly e a correção dos graus de liberdade da esfericidade (Greenhouse-Geisser (GGe) e Huynh-Feldt (HFe)). A tabela da ANOVA corrigida (Tabela 14.1) pode, então, ser obtida, usando a função get_anova_table (), do pacote rstatix, com o mod.anova2 e correction = “GG” como argumentos:\n\n\n\n\nTabela 14.1: Tabela da Anova Corrigida (GGe)\n\n\n\nEffectDFnDFdFpp&lt;.05gestratamento1.0022.001.4320.244000000.059tempo1.4131.0328.4170.00000104*0.049tratamento:tempo1.4131.0329.2590.00000080*0.050\n\n\n\n\n\nComo existe uma interação estatisticamente significativa (F(1,41, 31,03) = 29,26, P &lt; 0,0001) entre tratamento e o tempo, deve-se prestar atenção a este fato. Os efeitos principais isolados perdem a importância. Para esta análise são necessários testes posteriores.\n\n\n14.3.4 Teste post hoc\nUma interação significativa entre os dois fatores indica que o impacto que um fator (tratamento) tem na variável desfecho (escore de autoestima) depende do nível do outro fator (tempo), e vice-versa. Assim, é possível decompor a interação entre os dois fatores significativos em:\n\nEfeito principal simples: executar o modelo unifatorial da primeira variável (tratamento) em cada nível da segunda variável (tempo),\nComparações simples pareadas: se o efeito principal simples for significativo, executar várias comparações pareadas para determinar quais grupos são diferentes.\n\nPara uma interação não significativa entre os dois fatores, há necessidade de determinar se tem algum efeito principal estatisticamente significativo da saída ANOVA.\n\n14.3.4.1 Procedimento para uma interação significativa entre os dois fatores\nEfeito do tratamento\nNo exemplo, será analisado o efeito do tratamento no escore de auto-estima em cada momento no tempo. Note que como o tratamento tem apenas dois níveis (dieta e placebo), o teste de ANOVA e teste t pareado fornecem os mesmos resultados.\nEfeito do tratamento em cada ponto de tempo\n\ntratamento &lt;- autoestimaL %&gt;%\n  dplyr::group_by(tempo) %&gt;%\n  rstatix::anova_test(dv = escores, wid = id, within = tratamento) %&gt;%\n  rstatix::get_anova_table() %&gt;%\n  rstatix::adjust_pvalue(method = \"bonferroni\")\ntratamento\n\n# A tibble: 3 × 9\n  tempo Effect       DFn   DFd      F       p `p&lt;.05`      ges   p.adj\n  &lt;fct&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 t1    tratamento     1    11  0.376 0.552   \"\"      0.000767 1      \n2 t2    tratamento     1    11  9.03  0.012   \"*\"     0.052    0.036  \n3 t3    tratamento     1    11 30.9   0.00017 \"*\"     0.199    0.00051\n\n\nComparações pareadas entre os grupos de tratamentos\n\npwc &lt;- autoestimaL %&gt;%\n  dplyr::group_by(tempo) %&gt;%\n  pairwise_t_test(escores ~ tratamento, \n                  paired = TRUE,\n                  p.adjust.method = \"bonferroni\")\npwc\n\n# A tibble: 3 × 11\n  tempo .y.     group1 group2     n1    n2 statistic    df       p   p.adj\n* &lt;fct&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 t1    escores dieta  placebo    12    12    -0.613    11 0.552   0.552  \n2 t2    escores dieta  placebo    12    12     3.00     11 0.012   0.012  \n3 t3    escores dieta  placebo    12    12     5.56     11 0.00017 0.00017\n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nConsiderando o valor P ajustado de Bonferroni (p.adj), pode-se observar que o efeito principal simples do tratamento não foi significativo no ponto de tempo t1 (P = 1). Torna-se significativo em t2 (p = 0,036) e t3 (p = 0,00051).\nComparações pareadas mostram que o escore médio de autoestima foi significativamente diferente entre o grupo placebo e dieta em t2 (P = 0,12) e t3 (P = 0,00017), mas não em t1 (P = 0,55).\nEfeito do tempo\nObserve que também é possível realizar a mesma análise para a variável tempo em cada nível de tratamento. Esta análise, necessariamente, não precisa ser feita!\nEfeito do tempo em cada nível de tratamento\n\ntempo &lt;- autoestimaL %&gt;%\n  dplyr::group_by(tratamento) %&gt;%\n  rstatix::anova_test(dv = escores, wid = id, within = tempo) %&gt;%\n  rstatix::get_anova_table() %&gt;%\n  adjust_pvalue(method = \"bonferroni\")\ntempo\n\n# A tibble: 2 × 9\n  tratamento Effect   DFn   DFd      F          p `p&lt;.05`      ges     p.adj\n  &lt;fct&gt;      &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 dieta      tempo      2    22  0.078 0.925      \"\"      0.000197 1        \n2 placebo    tempo      2    22 39.7   0.00000005 \"*\"     0.145    0.0000001\n\n\nComparações pareadas entre pontos no tempo\n\npwc2 &lt;- autoestimaL %&gt;%\n  dplyr::group_by(tratamento) %&gt;%\n  pairwise_t_test(escores ~ tempo, paired = TRUE,\n                  p.adjust.method = \"bonferroni\")\npwc2\n\n# A tibble: 6 × 11\n  tratamento .y.     group1 group2    n1    n2 statistic    df         p   p.adj\n* &lt;fct&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 dieta      escores t1     t2        12    12    -0.522    11 0.612     1   e+0\n2 dieta      escores t1     t3        12    12    -0.102    11 0.921     1   e+0\n3 dieta      escores t2     t3        12    12     0.283    11 0.782     1   e+0\n4 placebo    escores t1     t2        12    12     4.53     11 0.000858  3   e-3\n5 placebo    escores t1     t3        12    12     6.91     11 0.0000255 7.65e-5\n6 placebo    escores t2     t3        12    12     6.49     11 0.0000449 1.35e-4\n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nApós a execução do código, verifica-se que o efeito do tempo é significativo apenas para o placebo, F(2, 22) = 39,7, p &lt; 0,0001. As comparações pareadas mostram que todas as comparações entre os pontos de tempo foram estatisticamente significativas para o placebo.\n\n\n14.3.4.2 Procedimento para uma interação não significativa entre os dois fatores\nSe a interação não for significativa, é preciso interpretar os efeitos principais para cada uma das duas variáveis: tratamento e tempo. Um efeito principal significativo pode ser acompanhado com comparações pareadas\nNo exemplo (consulte a Tabela 14.1), não houve efeito principal estatisticamente significativo para o tratamento (F(1, 22) = 1,43, P = 0,244) no escore de autoestima. Entretanto, os escores foram afetados significativamente pelo tempo (F(1,41, 31,03) = 28,41, P &lt; 0,0001).\n\n\n\n14.3.5 Relatando os resultados da ANOVA de medidas repetidas de dois fatores\nO resultado pode ser relatado da seguinte forma:\n\nUma ANOVA de medidas repetidas de dois fatores foi realizada para avaliar o efeito de diferentes tratamentos dietéticos ao longo do tempo no escore de autoestima.\nHouve uma interação estatisticamente significativa entre tratamento e tempo no escore de autoestima, F(1,41, 31,03) = 29,26, P &lt; 0,0001 (Tabela 14.1). Por isso, o efeito da variável tratamento foi analisado em cada ponto de tempo. Os valores de P foram ajustados usando o método de correção de testes múltiplos de Bonferroni. O efeito da variável tratamento foi significativo em t2 (P = 0,036) e t3 (P = 0,00051), mas não no ponto de tempo t1 (P = 1).\nComparações pareadas, usando o teste t pareado, mostram que o escore médio de autoestima foi significativamente diferente entre os ensaios placebo e dieta nos pontos de tempo t2 (P = 0,012) e t3 (P = 0,00017), mas não em t1 (P = 0,55).\n\n\n14.3.5.1 Visualização dos resultados\nOs resultados podem ser visualizados através de gráficos tipo boxplots (Figura 14.7) ou por um gráfico de linha (Figura 14.8). Os gráficos foram construídos , usando as funções ggboxplot() e ggline() do pacote ggpubr. Os boxplots receberam um preenchimento em tons de cinza, bem interessante em publicações científicas; o gráfico de linha, usando o pacote ggsci, recebeu as cores da paleta do periódico Lancet.\nBoxplots\n\nbxp &lt;- ggpubr::ggboxplot (autoestimaL,\n                          bxp.errorbar = TRUE,\n                          bxp.errorbar.width = 0.1, \n                          x = \"tempo\", \n                          y = \"escores\", \n                          color = \"black\",\n                          fill = \"tratamento\",\n                          ylab = \"Escore de Autoestima\",\n                          xlab = \"Tempo\")+\n  scale_fill_grey(start=0.95, end=0.6) +\n  theme(legend.position=\"right\") +\n  theme (text = element_text (size = 12)) \n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"tempo\")\nbxp + \n  stat_pvalue_manual(pwc, \n                     label = \"p.adj\",\n                     tip.length = 0.01, \n                     hide.ns = FALSE,\n                     y.position = c(32, 32, 32)) +\n  labs(subtitle = get_test_label(mod.anova2, detailed = TRUE),\n       caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 14.7: Avaliação da autoestima no decorrer do tempo\n\n\n\n\n\nGráfico de linha\n\nggl &lt;- ggline(\n  autoestimaL,\n  x = \"tempo\",\n  y = \"escores\",\n  color = \"tratamento\",\n  palette = \"lancet\",\n  linetype = \"dashed\",\n  size = 0.7,\n  shape = 19,\n  add = \"mean_ci\",\n  error.plot = \"errorbar\",\n  position = position_dodge(width = 0.2),\n  ggtheme = theme_pubr()) +\n  theme(legend.position=\"right\")\n\nggl + ggpubr::stat_pvalue_manual(pwc,  \n                                 label = \"p.adj\", \n                                 tip.length = 0.00,\n                                 y.position = c(28.5, 28.5, 28.5)) +\n  labs (x = \"Tempo\",\n        y = \"Escore de autoestima\",\n        subtitle = get_test_label (mod.anova2, detailed = TRUE),\n        caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 14.8: Avaliação da autoestima no decorrer do tempo\n\n\n\n\n\n\n\n\n\n1. Rosenberg M. Society and the adolescent self-image. Princeton university press; 2015. \n\n\n2. Dini G, Quaresma M, Ferreira L, et al. Adaptação cultural e validação da versão brasileira da escala de autoestima de Rosenberg. Revista Brasileira de Cirurgia Plástica. 2001;19(1):41–52. \n\n\n3. Wickham H, Girlich M. tidyr: Tidy Messy Data [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=tidyr\n\n\n4. Huynh H, Feldt LS. Estimation of the Box correction for degrees of freedom from sample data in randomized block and split-plot designs. Journal of Educational Statistics. 1976;1(1):69–82. \n\n\n5. Girden ER. Two-Factor Studies with Repeated Measures on Both Factors. Em: ANOVA: repeated measures. Sage; 1992. p. 31–40. (QASS Series; vol. 84). \n\n\n6. Muller KE, Barton CN. Approximate power for repeated-measures ANOVA lacking sphericity. Journal of the American Statistical Association. 1989;84(406):549–55.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANOVA de medidas repetidas</span>"
    ]
  },
  {
    "objectID": "14-anovaRep.html#footnotes",
    "href": "14-anovaRep.html#footnotes",
    "title": "14  ANOVA de medidas repetidas",
    "section": "",
    "text": "Veja a Seção 12.3.1.1 para mais detalhes da função.↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANOVA de medidas repetidas</span>"
    ]
  },
  {
    "objectID": "15-correlacao-regressao.html",
    "href": "15-correlacao-regressao.html",
    "title": "15  Correlação e Regressão",
    "section": "",
    "text": "15.1 Pacotes necessários\npacman::p_load(car,\n               dplyr,\n               flextable,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               knitr,\n               lmtest,\n               readxl,\n               rstatix)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlação e Regressão</span>"
    ]
  },
  {
    "objectID": "15-correlacao-regressao.html#correlação",
    "href": "15-correlacao-regressao.html#correlação",
    "title": "15  Correlação e Regressão",
    "section": "15.2 Correlação",
    "text": "15.2 Correlação\nA correlação é usada para avaliar a força e a direção da relação entre duas variáveis numéricas contínuas, normalmente distribuídas. A maneira mais comum de mostrar a relação entre duas variáveis quantitativas é através de um diagrama ou gráfico de dispersão (scatterplot). A Figura 15.1 exibe um exemplo de um gráfico de dispersão, onde se observa um padrão geral que sugere uma relação entre o estriol urinário (mg/24h) e o peso fetal em uma gravidez normal (1).\n\n\n\n\n\n\n\n\nFigura 15.1: Correlação da excreção de estriol urinário e peso fetal\n\n\n\n\n\nO gráfico de dispersão mostra que os valores de uma variável aparecem no eixo horizontal x e os valores da outra variável aparecem no eixo vertical y. Cada indivíduo nos dados aparece como o ponto no gráfico fixado pelos valores de ambas as variáveis para aquele indivíduo. Normalmente, eixo x é a variável explicativa (ou variável explanatória ou independente) e y a variável desfecho (variável resposta ou dependente).\nEm um diagrama de dispersão deve-se procurar o padrão geral e desvios marcantes desse padrão. Verifica-se o padrão geral, observando a direção, a forma e força do relacionamento. Um tipo importante de desvio é um valor atípico, um valor individual que está fora do padrão geral do relacionamento.\nA Figura 15.1 mostra uma clara direção do padrão geral que se move da esquerda inferior para a direita superior. Este comportamento é denominado de correlação positiva entre as variáveis. A forma do relacionamento é aproximadamente uma linha reta com uma ligeira curva para a direita à medida que se move para cima. A força de uma correlação em um gráfico de dispersão é determinada pela proximidade dos pontos em uma forma clara. No caso, quanto mais se aproxima de uma reta, mais forte é a associação, no caso de uma correlação linear. Duas variáveis estão negativamente associadas quando se comportam de forma oposta ao da Figura 15.1.\nObviamente, nem todos os diagramas de dispersão mostram uma direção clara que permita descrever como correlação positiva ou negativa e não tem uma forma linear, sugerindo que não há correlação, como a Figura 15.2.\n\n\n\n\n\n\n\n\nFigura 15.2: Gráfico de dispersão sugerindo ausência de correlação.\n\n\n\n\n\n\n15.2.1 Coeficiente de correlação de Pearson\nA correlação é quantificada pelo Coeficiente de Correlação Linear de Pearson. Este coeficiente paramétrico, denotado por r, é um número adimensional, independente das unidades usadas para medir as variáveis x e y.\nSuponha que se tenha dados sobre as variáveis x e y para n indivíduos. Os valores para o primeiro indivíduo são \\({x}_{1}\\) e \\({y}_{1}\\), os valores para o segundo indivíduo são \\({x}_{2}\\) e \\({y}_{2}\\) e assim por diante. As médias e desvios padrão das duas variáveis são \\(\\bar{x}\\) e \\({s}_{x}\\) para os valores de x e \\(\\bar{y}\\) e \\({s}_{y}\\) para os valores de y. A correlação r entre x e y é dada pela equação:\n\\[\nr = \\frac{\\sum{(x_{1} - \\bar{x})(y_{1} - \\bar{y})}}{{\\sqrt{\\sum (x_{1} - \\bar{x})^2\\times\\sum (y_{1} - \\bar{y})^2}}}\n\\]\nO Coeficiente de Correlação, r, apresenta as seguintes características:\n\nÉ um valor numérico que varia de -1 a +1 (Figura 15.3):\n\nQuando r = -1, há uma correlação linear negativa ou inversa perfeita;\nQuando r = +1, há uma correlação linear positiva ou direta perfeita;\nQuando r = 0, não há correlação entre as variáveis.\n\n\n\n\n\n\n\n\n\n\nFigura 15.3: Coeficiente de Correlação.\n\n\n\n\n\n\nQuanto mais os pontos se aproximam de uma linha reta, maior a magnitude de r.\nO coeficiente de correlação r é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (leia-se rô).\nA correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação.\nComo r usa os valores padronizados das observações, r não muda se as unidades de medida de x, y ou ambos são modificados. A correlação r em si não tem unidade de medida; é apenas um número.\n\n\n\n15.2.2 Dados usados nesta seção\nEstá claro que existe uma relação entre a idade de crianças e a sua altura (comprimento). Vamos usar os dados coletados em um ambulatório pediátrico de 40 crianças entre 18 e 36 meses (20 meninos e 20 meninas).\nOs dados estão no banco de dadosdadosReg.xlsx. Para baixar o banco de dados, clique aqui. Salve o arquivo no seu diretório de trabalho.\n\n15.2.2.1 Leitura e exploração dos dados\nUsar a função read_excel do pacote readxl para carregar o arquivo. Observar os dados com a função str().\n\ndados &lt;- read_excel(\"dados/dadosReg.xlsx\")\nstr(dados)\n\ntibble [40 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:40] 1 2 3 4 5 6 7 8 9 10 ...\n $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ...\n $ comp  : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ...\n $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ...\n $ sexo  : chr [1:40] \"masc\" \"fem\" \"masc\" \"fem\" ...\n\n\nDe acordo com uma das exigências da correlação, as variáveis idade e comp pertencem a classe das variáveis numéricas. A variável sexo foi lida como um variável numérica e será transformada em fator:\n\ndados$sexo &lt;- as.factor(dados$sexo)\n\n\n\n15.2.2.2 Medidas resumidoras\nAs medidas resumidoras serão calculadas, usando a função get_summary_stats () do pacote rstatix que necessita dos seguintes argumentos:\n\ndados %&gt;%\n  rstatix::get_summary_stats(idade,\n                             comp,\n                             type = \"mean_sd\")\n\n# A tibble: 2 × 4\n  variable     n  mean    sd\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 idade       40  27.0  5.41\n2 comp        40  90.2  6.00\n\n\n\n\n15.2.2.3 Visualização dos dados\nAqui, será usado o gráfico de dispersão (Figura 15.4), usando a função geom_point() do pacote ggplot2:\n\nggplot2::ggplot(dados, \n                aes(x=idade, \n                    y=comp, \n                    color = sexo)) + \n  geom_point(size = 3, \n             shape = 19) +\n  xlab(\"Idade (meses)\") +\n  ylab (\"Comprimento(cm)\") +\n  theme_bw() +\n  theme(text = element_text(size = 12, \n                            color = NULL, \n                            face = \"bold\"))\n\n\n\n\n\n\n\nFigura 15.4: Correlação entre a idade e a altura de uma criança.\n\n\n\n\n\nA separação dos pontos por sexo, usando cores diferentes, não muda a análise e foi realizada apenas para treinamento (e curiosidade!), pois não há motivo para diferença da correlação entre os sexos.\n\n\n\n15.2.3 Pressupostos da Correlação\nA primeira e mais importante etapa antes de analisar os dados, usando a correlação de Pearson é verificar se é apropriado usar este teste estatístico.\nSerão discutidos sete pressupostos, três estão relacionados com o projeto do estudo e como as variáveis foram medidas (pressupostos 1, 2 e 3) e quatro que se relacionam com as características dos dados (pressupostos 4, 5, 6 e 7) (2).\n\nVariáveis numéricas contínuas\n\nAs duas variáveis devem ser medidas em uma escala contínua (são medidas no nível intervalar ou de razão). No exemplo, tanto a variável idade como o comprimento (comp) são variáveis contínuas, como verificado acima.\n\nVariáveis devem estar como pares\n\nAs duas variáveis contínuas devem ser emparelhadas, o que significa que cada caso (por exemplo, cada participante) tem dois valores: um para cada variável.\n\nIndependência das observações\n\nDeve haver independência de casos, o que significa que as duas observações para um caso (por exemplo, a idade e o comprimento) devem ser independentes das duas observações para qualquer outro caso.\nSe estes pressupostos forem atendidos, avalia-se os outros pressupostos:\n\nRelação linear entre as variáveis\n\nO coeficiente de correlação de Pearson é uma medida da força de uma associação linear entre duas variáveis. Dito de outra forma, ele determina se há um componente linear de associação entre duas variáveis contínuas. Por esse motivo, verifica-se a relação entre duas variáveis, em um gráfico de dispersão, para ver se a execução de uma correlação de Pearson é a melhor escolha como medida de associação.\nA variável idade é colocada como variável preditora (eixo x) e comp como desfecho (eixo y). O gráfico de dispersão anterior, mostra uma nítida correlação linear.\n\nNormalidade das variáveis\n\nPara verificar se as variáveis têm distribuição normal, é possível usar o teste de Shapiro-Wilk, usando a função shapiro_test(), incluída no pacote rstatix:\n\ndados %&gt;% shapiro_test(idade, comp)\n\n# A tibble: 2 × 3\n  variable statistic     p\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 comp         0.958 0.141\n2 idade        0.958 0.145\n\n\nO teste de Shapiro-Wilk de ambas as variáveis retorna um valor P &gt; 0,05, indicando que não é possível rejeitar a \\(H_{0}\\); os dados seguem a distribuição normal, portanto o pressuposto foi atendido.\n\nPesquisa de valores atípicos\n\nA identificação dos valores atípicos pode ser feita usando a função identify_outliers() do pacote rstatix.\n\n dados %&gt;% identify_outliers(idade)\n\n[1] id         idade      comp       irmaos     sexo       is.outlier is.extreme\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n dados %&gt;% identify_outliers(comp)\n\n[1] id         idade      comp       irmaos     sexo       is.outlier is.extreme\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n\n\nHomoscedasticidade\n\nA homocedasticidade assume que os dados são igualmente distribuídos sobre a linha de regressão. Descreve uma situação na qual o resíduo é o mesmo em todos os valores das variáveis independentes. A heterocedasticidade (a violação da homocedasticidade) está presente quando o tamanho dos resíduos difere entre os valores de uma variável independente.\nO impacto de violar o pressuposto da homocedasticidade é uma questão de grau, aumentando à medida que a heterocedasticidade aumenta. Dessa forma, avalia-se a homocedasticidade, observando os resíduos.\nUma correlação linear pode ser descrita por uma reta. Em uma correlação linear perfeita, a reta passa por todos os pontos. Normalmente, não é possível traçar uma reta que passe por todos os pontos. A melhor reta é aquela que promove o melhor ajuste,ou seja, é aquela cuja distância dos pontos até a reta é a menor possível. Os resíduos são a diferença entre o valor observado e o valor previsto pelo melhor ajuste, estabelecido pelo modelo de regressao linear.\n\nConstrução do modelo: Para ajustar a um modelo linear, usa-se a função lm que deve conter um objeto da classe formula tipo (x ~ y) como argumento. Demais características da função podem ser obtidas com ?lm ou direto na ajuda do RStudio. O modelo será atribuído a um objeto denominado mod_reg.\n\n\nmod_reg &lt;- lm(comp ~ idade, dados)\n\n\nAnálise gráfica da homoscedasticidade: Pode ser feita através dos gráficos diagnósticos para a regressão linear, utilizados para verificar se o modelo funciona bem para representar os dados. Uma forma de avaliar é verificar como as variâncias se comportam. Os gráficos diagnósticos são apresentados de quatro maneiras diferentes.1 Neste estágio, serão avaliados o primeiro e o terceiro tipo (Figura 15.5).\n\n\npar(mfrow=c(1,2))  # muda o layout do painel para 1 linha e 2 colunas\nplot(mod_reg, which=c(1,3))\npar(mfrow=c(1,1))  # retorna o layaout do painel para o padrão de 1 linha e 1 coluna\n\n\n\n\n\n\n\nFigura 15.5: Gráficos diagnósticos 1 e 3\n\n\n\n\n\nNo gráfico 1, tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero.\nNo gráfico 3 (valores ajustados x resíduos), tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero. Permite verificar se há outliers - valores de resíduos padronizados acima de 3 ou abaixo de -3. Embora o gráfico possa dar uma ideia sobre homocedasticidade, às vezes, um teste mais formal é preferido. Existem vários testes para isso, mas aqui será utilizado o Teste de Breusch-Pagan. A \\(H_{0}\\) e a \\(H_{1}\\) podem ser consideradas como:\n\n\\(H_{0}\\): Homocedasticidade. Os resíduos têm variância constante sobre o modelo verdadeiro.\n\\(H_{1}\\): Heterocedasticidade. Os resíduos não têm variância constante sobre o modelo verdadeiro.\n\nSe o valor P &gt; 0,05 não se rejeita a \\(H_{0}\\) de homocedasticidade. O teste de Breusch-Pagan é encontrado na função bptest(), incluída no pacote lmtest:\n\nlmtest::bptest(mod_reg)\n\n\n    studentized Breusch-Pagan test\n\ndata:  mod_reg\nBP = 0.049988, df = 1, p-value = 0.8231\n\n\nOs resultados não indicam heteroscedasticidade e isso é bom. Desta forma, pode-se aplicar a equação final de predição.\n\n\n15.2.4 Execução do teste de correlação\n\n15.2.4.1 Coeficiente de correlação de Pearson (r)\nO coeficiente de correlação, r, é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (rô).\nComo visto, no início deste capítulo, a correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação.\nComo o r usa os valores padronizados das observações, não muda nada se as unidades de medida de x, y ou ambas são modificadas. A correlação r em si não tem unidade de medida; é apenas um número.\nO cálculo pode ser realizado com a função cor_test() do pacote rstatix que usa os seguintes argumentos:\n\ndata \\(\\to\\) dataframe contendo as variáveis;\n… \\(\\to\\) Uma ou mais expressões (ou nomes de variáveis) sem aspas separadas por vírgulas. Usado para selecionar uma variável de interesse. Alternativa ao argumento vars. Ignorado quando vars é especificado;\nvars2 \\(\\to\\) • vetor de caracteres opcional. Se especificado, cada elemento em vars será testado em relação a todos os elementos em vars2. Aceita nomes de variáveis sem aspas: c(var1, var2);\nalternative \\(\\to\\) hipótese alternativa “two.sided” (bilateral) ou “greater” ou “less” (unilateral a direita ou a esquerda, respectivamente);\nmethod \\(\\to\\) ⟶ qual coeficiente de correlação deve ser usado para o teste. Um dos termos “pearson”, “kendall” ou “spearman” pode ser abreviado;\nconf.level \\(\\to\\) nivel de confiança. Padrão 0.95.\n\n\nr &lt;- dados %&gt;% cor_test(idade, \n                         comp, \n                         method = \"pearson\")\nr\n\n# A tibble: 1 × 8\n  var1  var2    cor statistic        p conf.low conf.high method \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 idade comp   0.96      21.4 7.87e-23    0.927     0.979 Pearson\n\n\nA saída do Coeficiente de Correlação de Pearson (r) é igual 0.96 (IC95%: 0.93, 0.98) o que corresponde a uma correlação linear muito forte (Tabela 15.1)) entre a idade e o comprimento de crianças (3).\nO coeficiente refere a existência de correlação linear, mas não especifica se a relação é de causa e efeito. O valor P especifica se a correlação é igual a zero (\\(H_{0}\\)) ou diferente de zero (\\(H_{1}\\)). No caso, ela é diferente de zero.\nO importante é a magnitude do r, entretanto, o coeficiente r e o valor P devem ser interpretados em conjunto. Se o valor P &gt; 0,05, mesmo que r seja diferente de zero, a correlação não deveria ser interpretada.\n\n\n\n\nTabela 15.1: Interpretação do Coeficiente de Correlação\n\n\n\nCoeficiente de Correlação (r)Interpretação0,0&lt;0,3desprezável0,3&lt;0,5fraca0,5,0,7moderada0,7&lt;0,9forte0,9&lt;1,0muito forte1,0perfeita\n\n\n\n\n\nTalvez a melhor maneira de interpretar a correlação linear é elevar o valor do r ao quadrado para obter o Coeficiente de Determinação (\\(R^{2}\\)). No exemplo usado, tem-se que o \\(R^{2}\\) é igual a \\(0,96^{2} = 0,922\\), então, 92,2% da variação do comprimento da criança (y) podem ser explicados, nesses dados, pela variação da sua idade (x), fato mais ou menos óbvio!.\n\n\n15.2.4.2 Coeficiente de correlação de Spearman (\\(\\rho\\))\nSe os pressupostos são violados é recomendado o uso de correlação não paramétrica (veja a Seção 17.2), incluindo testes de correlação baseados em postos (veja a Seção 17.3) de Spearman e Kendall (4).\nPara calcular o coeficente, Usar a mesma função da correlação de Pearson, mudando o argumento method:\n\nrho &lt;- dados %&gt;% cor_test(idade, \n                           comp, \n                           method = \"spearman\")\nrho\n\n# A tibble: 1 × 6\n  var1  var2    cor statistic        p method  \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 idade comp   0.96      448. 3.30e-22 Spearman\n\n\n\n\n15.2.4.3 Coeficiente de correlação de Kendall (\\(\\tau\\))\nO coeficiente de correlação de postos de Kendall ou estatística tau de Kendall é usado para estimar uma medida de associação baseada em postos. Pode ser usado com variáveis ordinais ou quando não existe relação linear entre as variáveis. Uma vantagem sobre o coeficiente de Spearman é a possibilidade de ser generalizado para um coeficiente de correlação parcial. Deve ser usada ao invés do coeficiente de Spearman quando se tem um conjunto pequeno de dados com um grande número de postos empatados (veja a Seção 17.3). Para o cálculo desse coeficiente, continua-se com a mesma função anterior, mudando o method = “kendall”.\n\ntau &lt;- dados %&gt;% cor_test(idade, \n                           comp, \n                           method = \"kendall\")\ntau\n\n# A tibble: 1 × 6\n  var1  var2    cor statistic        p method \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1 idade comp   0.85      7.57 3.64e-14 Kendall\n\n\nNo caso normal, a correlação de Kendall é mais robusta e eficiente que a correlação de Spearman. Isso significa que a correlação de Kendall é preferida quando há amostras pequenas ou alguns valores atípicos. O rho de Spearman geralmente é maior que o tau de Kendall.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlação e Regressão</span>"
    ]
  },
  {
    "objectID": "15-correlacao-regressao.html#sec-rls",
    "href": "15-correlacao-regressao.html#sec-rls",
    "title": "15  Correlação e Regressão",
    "section": "15.3 Regressão Linear Simples",
    "text": "15.3 Regressão Linear Simples\nA regressão linear simples, assim como a correlação, é uma técnica usada para explorar a natureza da relação entre duas variáveis aleatórias contínuas. A principal diferença entre esses dois métodos analíticos é que a regressão permite investigar a alteração em uma variável, chamada resposta, correspondente a uma determinada alteração em outra, conhecida como variável explicativa. A regressão é um modelo matemático que permite a predição de uma variável resposta a partir de uma outra variável explicativa. A análise de correlação quantifica a força da relação entre as variáveis, tratando-as simetricamente (5).\nA regressão linear simples é chamada assim, porque se tem apenas uma variável independente. Se houver mais de uma variável independente, é chamada de regressão múltipla.\nA representação matemática do modelo de regressão linear populacional é descrita pela equação da reta de melhor ajuste em um conjunto de pares de dados (x, y) em um gráfico de dispersão de pontos.\n\\[\ny = \\beta_{0} + \\beta_{1}x\n\\]\n\n\n\n\n\n\n\n\nFigura 15.6: Reta de regressão, coeficiente angular e coeficiente linear.\n\n\n\n\n\nA inclinação da reta de regressão (\\(\\beta_{1}\\)) determina a variação de y para cada unidade de variação de x e recebe o nome de coeficiente angular ou de regressão. O ponto de interceptação da reta com y quando x é igual a zero é \\(\\beta_{0}\\) e é denominado de coeficiente linear (Figura 15.6). A equação da reta de regressão amostral que estima a reta de regressão populacional é igual a:\n\\[\n\\hat {y} = b_{0} + b_{1}x\n\\]\nA reta do diagrama de dispersão da Figura 15.6 é a melhor reta de ajuste aos dados.\n\n15.3.1 Resíduos\nNo exemplo usado no início (Seção 15.2.2) , verificou-se que existe uma correlação linear entre o a idade e o comprimento de crianças, usando uma amostra de 40 crianças entre 18 e 36 meses. A correlação de Pearson foi muito forte (r = 0,96, P &lt; 0,00001). Esta relação linear pode ser descrita pela reta, mostrada na Figura 15.7.\n\nggplot2::ggplot(dados, \n                aes(x = idade, \n                    y = comp,\n                    color = \"tomato\")) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"steelblue\") +\n  geom_point(size = 3.5) +\n  theme_classic() + \n  xlab(\"Idade (meses\") +\n  ylab(\"Comprimento (cm)\") +\n  theme(text = element_text(size = 12)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigura 15.7: Reta de regressão\n\n\n\n\n\nNão é possível traçar uma reta que passe por todos os pontos. Esta reta ideal descreveria uma correlação perfeita, que não é o caso. Pode haver várias retas, a reta calculada pela regressão linear é aquela que promove o melhor ajuste, ou seja, é aquela cuja distância dos pontos até a reta é a menor possível.\nOs resíduos são a diferença entre o valor observado e o valor previsto pelo modelo de regressão linear, construído anteriormente (mod_reg). A técnica estatística para achar a melhor reta que ajusta um conjunto de dados é denominada de método dos mínimos quadrados (Ordinary Least Square). A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada.\nPara se obter os resíduos, graficamente, pode ser usar os seguintes comandos que resultam na Figura 15.8.\n\n# Obter e salvar os valores preditos e residuais\ndados$previsto &lt;- predict(mod_reg) \n\ndados$residuos &lt;- residuals(mod_reg)\n\n\n# Construção do gráfico com os resíduos\nggplot2::ggplot(dados, \n                aes(x = idade, \n                    y = comp)) +\n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"steelblue\") +\n  geom_segment(aes(xend = idade, \n                   yend = previsto), \n               linewidth = 0.8,\n               linetype = \"dotted\") +\n  geom_point(aes(y = previsto), \n             shape = 19,\n             size = 3,\n             colour = \"red\") +\n  geom_point(size = 3) +\n  theme_classic() + \n  xlab(\"Idade (meses\") +\n  ylab(\"Comprimento (cm)\") +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\nFigura 15.8: Resíduos\n\n\n\n\n\nUma boa maneira de testar a qualidade do ajuste do modelo é observar os resíduos (6) ou as diferenças entre os valores reais (pontos pretos) e os valores previstos (pontos vermelhos). A reta de regressão, em azul no gráfico, representa os valores previstos. A linha vertical pontilhada da linha reta até o valor dos dados observados é o resíduo.\nA ideia aqui é que a soma dos resíduos seja aproximadamente zero ou o mais baixo possível. Na vida real, a maioria dos casos não seguirá uma linha perfeitamente reta, portanto, resíduos são esperados. Na saída do resumo da função lm() em (mod_reg$residuals), você pode ver estatísticas descritivas sobre os resíduos do modelo (residuals), elas mostram como os resíduos são aproximadamente zero. Pode-se observar isso, usando a função summary () e sum():\n\nsummary(mod_reg$residuals)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.20326 -1.20326  0.08994  0.00000  1.25849  3.49221 \n\nsum(mod_reg$residuals)\n\n[1] -3.538836e-15\n\n\nComo se observa, a soma dos residuos é praticamente iguais a zero (\\(-3,54 \\times 10^-15\\)).\n\n\n15.3.2 Análise dos pressupostos do modelo de regressão\nA análise exploratória do conjunto de dados foi feita quando do estudo da Correlação. Assim como a correlação, a regressão linear faz várias suposições sobre os dados.\n\n15.3.2.1 Gráficos diagnóstico\nOs gráficos de diagnóstico da regressão (Figura 15.9) podem ser criados usando a função plot() do R base, como mostrado para a correlação. O modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função. A função par(mfrow = 2, 2) foi utilizada, como de outras vezes, para colocar os gráficos em duas linhas e duas colunas:\nO modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função:\n\npar(mfrow=c(2,2))\nplot (mod_reg)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\nFigura 15.9: Gráficos diagnósticos\n\n\n\n\n\nOs gráficos de diagnóstico mostram resíduos de quatro maneiras diferentes:\n\nResíduos vs. ajustados (Residuals vs Fitted). Usado para verificar os pressupostos de relação linear. Uma linha horizontal, sem padrões distintos é um indicativo de uma relação linear, o que é bom. Os dados do exemplo (linha azul) afastam-se muito pouco do zero, mas a acompanham e não se observa nenhum padrão distinto, como uma parábola por exemplo.\nQ-Q plot. Usado para examinar se os resíduos são normalmente distribuídos. É bom se os pontos residuais seguirem a linha reta tracejada. É possível dizer que os resíduos seguem a linha diagonal, com pequenos desvios toleráveis.\nLocalização da dispersão (scale-location). Usado para verificar a homogeneidade de variância dos resíduos (homocedasticidade). Uma linha horizontal com pontos igualmente dispersos é uma boa indicação de homocedasticidade. No exemplo usado, os resíduos parecem estar dispersos e a linha azul não está próxima do zero, sugerindo um problema com a homocedasticidade, entretanto, não está acima de 3.\nResíduos vs. alavancagem (leverage). Usado para identificar casos influentes, ou seja, valores extremos que podem influenciar os resultados da regressão quando incluídos ou excluídos da análise. Nem todos os outliers são influentes na análise de regressão linear. Mesmo que os dados tenham valores extremos, eles podem não ser influentes para determinar uma linha de regressão. Isso significa que os resultados não seriam muito diferentes, incluindo ou não esses valores. Por outro lado, alguns casos podem ser muito influentes, mesmo que pareçam estar dentro de uma faixa razoável de valores. Outra forma de colocar, é que eles não se entendem com a tendência na maioria dos casos. Ao contrário dos outros gráficos, desta vez os padrões não são relevantes. Deve-se estar atento aos valores distantes no canto superior direito ou no canto inferior direito. Esses pontos são os lugares onde os casos podem ter influência contra uma linha de regressão. Procurar casos fora de uma linha tracejada, distância de Cook. Quando os casos estão fora da distância de Cook (o que significa que têm pontuações altas de distância de Cook), os casos são influentes para os resultados da regressão. Os resultados da regressão serão alterados se excluirmos esses casos.\n\nA aparência dos gráficos do exemplo mostra que não há nenhum caso influente. Pouco se observa as linhas de distância de Cook (uma linha tracejada) porque todos os casos estão bem dentro das linhas de distância de Cook.\n\n\n15.3.2.2 Avaliação da normalidade dos resíduos\nAo analisar os pressupostos da correlação, foi realizado a avaliação da normalidade nos dados brutos que indicaram não ser possível rejeitar a hipótese nula de que os dados têm distribuição normal. Agora, isto será repetido para avaliar a normalidade dos resíduos, usando o mesmo teste, teste de Shapiro-Wilk.\nAo ser criado o modelo de regressão (mod_reg), ele fornece uma série de variáveis que pode ser listada da seguinte maneira:\n\nls(mod_reg)\n\n [1] \"assign\"        \"call\"          \"coefficients\"  \"df.residual\"  \n [5] \"effects\"       \"fitted.values\" \"model\"         \"qr\"           \n [9] \"rank\"          \"residuals\"     \"terms\"         \"xlevels\"      \n\n\nUsando a variável residuals, confirma-se o observado no QQPlot de que os resíduos apresentam distribuição normal, pois o valor de P &gt; 0,05.\n\nshapiro_test(mod_reg$residuals)\n\n# A tibble: 1 × 3\n  variable          statistic p.value\n  &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n1 mod_reg$residuals     0.979   0.655\n\n\nA saída retorna a estatística do teste de Shapiro-Wilk com um valor P = 0,655, mostrando que os dados se ajustam à distribuição normal. Pode-se também construir um gráfico QQ (Figura 15.10), usando a função ggqqplot() do pacote ggpubr que exibe o mesmo resultado.\n\nggpubr::ggqqplot(mod_reg$residuals,\n                  color = \"steelblue4\",\n                  xlab = \"Quantis normais\", \n                  ylab = \"Residuos\",\n                  ggtheme = theme_bw())\n\n\n\n\n\n\n\nFigura 15.10: Gráfico QQ mostrando a normalidade dos resíduos do modelo de regressão linear\n\n\n\n\n\n\n\n15.3.2.3 Pesquisa de valores atípicos nos resíduos\nExiste uma função pode ser usada para verificar valores atípicos nos resíduos da regressão para modelos lineares como rstandard() do pacote stats, que analisa os resíduos padronizados.\nA função padroniza todos os resíduos e inclui no objeto residuos_p. Para analisá-los, faz-se um sumário, usando a função summary(). Esta função exibirá os a estatística dos 5 números mais a média para os resíduos padronizados:\n\nresiduos_p &lt;- rstandard(mod_reg)\nsummary(residuos_p)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-1.9327846 -0.7271178  0.0548028  0.0006059  0.7779208  2.1154118 \n\n\nEm uma amostra normalmente distribuída, ao redor de 95% dos valores estão entre –1,96 e +1,96, 99% deve estar entre –2,58 e +2,58 e quase todos (99,9%) deve se situar entre –3,09 e +3,09.\nPortanto, resíduos padronizados com um valor absoluto maior que 3 são motivo de preocupação porque em uma amostra média é improvável que aconteça um valor tão alto por acaso (7).\nSe a saída da função rstandard() for comparada com o eixo y do gráfico Residuals vs Leverage, dos gráficos diagnósticos, verifica-se valores semelhantes que variam abaixo de 3 e acima de -3, indicando que não há outliers influenciando e a mediana está próxima de zero.\n\n\n15.3.2.4 Homocedasticidade dos resíduos\nNa Seção 15.2.3, foi analisada a homocedasticidade , onde se viu que o teste de Breusch-Pagan, retornou um resultado de P = 0,8231, indicando que a variância permanece praticamente constante, havendo homocedasticidade nos resíduos.\nO problema mais sério associado à heterocedasticidade é o fato de que os erros padrão são tendenciosos. Como o erro padrão é fundamental para a realização de testes de significância e cálculo de intervalos de confiança, os erros padrão tendenciosos levam a conclusões incorretas sobre a significância dos coeficientes de regressão. No geral, no entanto, a violação da suposição de homocedasticidade deve ser bastante grave para apresentar um grande problema, dada a natureza robusta da regressão pelo método ordinary least-squares. No entanto, é importante que a equação final de predição seja aplicada apenas a populações com as mesmas características da amostra do estudo.\n\n\n15.3.2.5 Independência dos resíduos\nOs resíduos no modelo devem ser independentes, ou seja, não devem ser correlacionados entre si. Para verificar isso, pode-se executar o teste Durbin-Watson (teste dw), utilizando a função durbinWatsonTest() do pacote ´car`. O teste retorna um valor entre 0 e 4. Um valor maior que 2 indica uma correlação negativa entre resíduos adjacentes, enquanto um valor menor que 2 indica uma correlação positiva. Se o valor for dois, é provável que exista independência. Existe uma sugestão de que valores abaixo de 1 ou mais de 3 são um motivo definitivo de preocupação (7). É importante mencionar que o teste tem como pressuposto a normalidade dos dados.\n\ndurbinWatsonTest(mod_reg)\n\n lag Autocorrelation D-W Statistic p-value\n   1      -0.1044054      2.204843    0.66\n Alternative hypothesis: rho != 0\n\n\nComo na saída do teste o valor P &gt; 0,05 e a estatística DW é igual a 2,2, não se rejeita a hipótese nula de independência (rho = 0).\n\n\n\n15.3.3 Tamanho amostral na regressão\nO tamanho da amostra deve ser suficiente para suportar o modelo de regressão. É importante coletar dados suficientes para obter um modelo de regressão confiável. O tamanho da amostra necessário para suportar um modelo depende do valor do coeficiente de correlação do modelo (no caso da correlação linear simples é o r de Pearson) e do número de variáveis incluídas.\nA Tabela 15.2 (8) mostra o número de participantes necessários em modelos com 1 a 4 preditores independentes. Como se observa, o requisito de tamanho da amostra aumenta com o número de variáveis preditoras.\n\n\n\n\nTabela 15.2: Tamanho amostral para regressão de acordo com o r de Pearson e o número de preditores\n\n\n\nNúmero de Variáveis Preditorasr de PearsonUmaDuasTrêsQuatro0.21902302652900.3801001151250.445556570\n\n\n\n\n\nExistem muitas regras práticas, sugerindo o tamanho da amostra. Uma delas, diz que se deve ter 10 a 15 casos por variável preditora no modelo. Entretanto, essas regras podem ser duvidosas e o melhor é calcular o tamanho amostral baseado no tamanho do efeito, usando, por exemplo o site StatToDo\n\n\n15.3.4 Realização da regressão linear\nApós analisar os pressupostos do modelo de regressão do exemplo, verificou-se que as variáveis idade e comprimento da criança têm relação linear, que os resíduos do modelo têm distribuição normal, que existe homoscedasticidade e que não há pontos influentes. E, portanto, o modelo permite que se realize uma análise de regressão linear para avaliar a relação entre as variáveis independentes e dependentes.\nPara realizar uma análise de regressão linear simples e verificar os resultados, há necessidade de executar dois comandos. O primeiro, que cria o modelo linear já foi realizado na análise dos gráficos e será repetido aqui. O segundo, imprime o resumo do modelo com a função summary():\n\nmod_reg &lt;- lm (comp ~ idade, dados)\nsummary (mod_reg)\n\n\nCall:\nlm(formula = comp ~ idade, data = dados)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2033 -1.2033  0.0899  1.2585  3.4922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 61.44408    1.36466   45.02   &lt;2e-16 ***\nidade        1.06515    0.04967   21.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.678 on 38 degrees of freedom\nMultiple R-squared:  0.9237,    Adjusted R-squared:  0.9217 \nF-statistic: 459.9 on 1 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nA saída da função summary() primeiro apresenta como o modelo foi obtido e, em seguida, resume os resíduos do modelo. Por último, tem-se os Coeficientes:\n\nAs estimativas (Estimate) para os parâmetros do modelo - o valor do intercepto y (neste caso, 61,44) e o efeito estimado da idade sobre o comprimento (1,1)- significam que para cada unidade de aumento na idade se espera um aumento de 1,1 cm no comprimento.\nO erro padrão dos valores estimados (Std. Error).\nA estatística de teste (t value)\nO valor P (Pr (&gt;| t |)), também conhecido como a probabilidade de encontrar a estatística t fornecida se a hipótese nula de nenhuma correlação for verdadeira.\n\nAs três linhas finais são os diagnósticos do modelo - o mais importante a observar é o valor P (\\(2,2\\times 10^{-16}\\)), que indica se o modelo se ajusta bem aos dados.\n\nA partir desses resultados, pode-se dizer que existe uma correlação positiva significativa entre idade e comprimento (valor P &lt; 0,001), com um aumento de 1,1 cm no comprimento para cada aumento de 1 mês no na idade , possibilitando a previsão comprimento da criança pela idade.\nEstes dados são empregados para formular a equação do modelo de regressão da seguinte maneira:\n\\[\n\\hat {y} = 61,44 + 1,1 x\n\\]\nO erro padrão das estimativas são fornecidos. Esses dados permitem calcular o IC95%. Ou pode-se usar a função confint() do pacote stats, que será colocada dentro da função round() para arredondar os valores até um digito.\n\nround (confint (mod_reg, level = 0.95), 1)\n\n            2.5 % 97.5 %\n(Intercept)  58.7   64.2\nidade         1.0    1.2\n\n\nDessa forma, é possível prever que uma criança de 30 meses, de acordo com o modelo, terá o seguinte comprimento:\n\ncomp_30m &lt;- 61.4 + 1.1 * 30\ncomp_30m\n\n[1] 94.4\n\n\n\nlim.sup &lt;- 64.2 + 1.2*30\nlim.inf &lt;- 58.7 + 1.0*30\nprint (c(lim.inf, lim.sup))\n\n[1]  88.7 100.2\n\n\nOu seja, espera-se que uma criança tenha, aos 30 meses de idade, um comprimento médio de 94,4 cm (IC95%: 88,7-100,2)\n\n\n15.3.5 Visualização dos resultados\nSerá obtido um gráfico de dispersão com a reta de regressão e seu intervalo de confiança de 95% (Figura 15.11). Além disso, adicionou-se a equação do modelo de regressão (o R arredondou os valores), juntamente com o coeficiente de determinação \\(R^{2}\\).\n\nggplot2:: ggplot (dados, aes (x = idade, y = comp)) +\n  geom_point (size = 3) +\n  geom_smooth (method = \"lm\", se = TRUE, color = \"tomato\") +\n  stat_regline_equation (label.y = 100, aes (label = (..eq.label..))) + \n  stat_regline_equation (label.y = 99, aes (label = (..rr.label..))) +       \n  theme_classic () +\n  xlab (\"Idade (meses)\") +\n  ylab (\"Comprimento(cm)\") +\n  theme (text = element_text (size = 12))\n\n\n\n\n\n\n\nFigura 15.11: Resultado da regressão linear\n\n\n\n\n\nNo gráfico, o intervalo de previsão médio de 95% em torno da reta de regressão é um intervalo de confiança de 95%, ou seja, a área na qual há 95% de certeza de que a reta de regressão verdadeira se encontra (9). Esta banda de intervalo é levemente curvada porque os erros na estimativa do intercepto e da inclinação são incluídos em adição ao erro na previsão da variável desfecho.\nSe for observado, o IC95% da reta de regressão obbtida pelo ggplot2 difere um pouco do IC95% da função confint(). Isto ocorre porque:\n\nquando se usa geom_smooth(method = \"lm\", se = TRUE), o intervalo de confiança gerado é baseado na incerteza da previsão média da regressão. Ou seja, ele mostra a faixa onde se espera que a média da variável dependente (comp) esteja para um determinado valor da variável independente (idade) e\n\nquando se usa a função confint(), ela retorna o intervalo de confiança dos coeficientes do modelo de regressão. Ou seja, ela fornece a incerteza associada aos parâmetros estimados (incluindo o intercepto e os coeficientes das variáveis preditoras).\n\nA principal diferença, portanto, é que o intervalo de confiança do ggplot2 reflete a incerteza da linha de regressão ajustada, enquanto confint() fornece a incerteza dos parâmetros do modelo.\n\n\n\n\n1. Greene Jr JW, Touchstone JC. Urinary estriol as an index of placental function. A study of 279 cases. Obstetrical & Gynecological Survey. 1963;18(3):356–9. \n\n\n2. Kassambara A. Correlation Test between two variables in R [Internet]. STHDA - Statistical tools for high-throughput data analysis. 2021. Disponível em: http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r\n\n\n3. Schober P, Boer C, Schwarte LA. Correlation coefficients: appropriate use and interpretation. Anesthesia & Analgesia. 2018;126(5):1763–8. \n\n\n4. De Winter JC, Gosling SD, Potter J. Comparing the Pearson and Spearman correlation coefficients across distributions and sample sizes: A tutorial using simulations and empirical data. Psychological methods. 2016;21(3):273. \n\n\n5. Sedgwick P. Correlation versus linear regression. BMJ. 2013;346. \n\n\n6. Kim H-Y. Statistical notes for clinical researchers: simple linear regression 3–residual analysis. Restorative dentistry & endodontics. 2019;44(1). \n\n\n7. Field A, Miles J, Field Z. Regression. Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 266–76. \n\n\n8. Peat J, Barton B. Correlation and regression. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 209. \n\n\n9. Altman DG, Gardner MJ. Statistics in Medicine: Calculating confidence intervals for regression and correlation. British Medical Journal (Clinical research ed). 1988;296(6631):1238.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlação e Regressão</span>"
    ]
  },
  {
    "objectID": "15-correlacao-regressao.html#footnotes",
    "href": "15-correlacao-regressao.html#footnotes",
    "title": "15  Correlação e Regressão",
    "section": "",
    "text": "Maiores detalhes sobre os testes diagnósticos podem ser encontrados em: https://data.library.virginia.edu/diagnostic-plots/↩︎",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlação e Regressão</span>"
    ]
  },
  {
    "objectID": "16-dadosCategoricos.html",
    "href": "16-dadosCategoricos.html",
    "title": "16  Análise de Dados Categóricos",
    "section": "",
    "text": "16.1 Pacotes necessários\npacman::p_load(coin,\n                DescTools,\n                dplyr,\n                expss,\n                flextable,\n                ggplot2, \n                gmodels,\n                nhstplot,\n                readxl,\n                rstatix,\n                summarytools)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "16-dadosCategoricos.html#sec-qui",
    "href": "16-dadosCategoricos.html#sec-qui",
    "title": "16  Análise de Dados Categóricos",
    "section": "16.2 Qui-Quadrado",
    "text": "16.2 Qui-Quadrado\nDois testes de hipótese são proeminentes na pesquisa na área da saúde. Um é o teste t de duas amostras, que é usado para testar a igualdade de duas médias populacionais independentes. O segundo é o teste qui-quadrado (denotado por \\(\\chi^{2}\\)). O teste é denominado teste qui-quadrado porque usa a distribuição qui-quadrado ou \\(\\chi^{2}\\).\n\n16.2.1 Distribuição qui-quadrado\nSe uma variável X é normalmente distribuída, então a variável \\(X^{2}\\) tem uma distribuição qui-quadrado (1). A distribuição qui-quadrado com k categorias é a distribuição de uma soma dos quadrados de k variáveis aleatórias independentes com distribuição normal. O número de categorias determina o número de graus de liberdade. O formato da distribuição qui-quadrado depende desses graus de liberdade. Em geral, ela é assimétrica com apenas valores positivos, iniciando em zero. A assimetria diminui à medida que aumentam os graus de liberdade. Para cada grau de liberdade tem-se curvas de distribuição diferentes.\n\n\n\n\n\n\n\n\nFigura 16.1: Distribuição do qui-quadrado.\n\n\n\n\n\nA distribuição \\(\\chi^{2}\\) converge para a distribuição normal à medida que os graus de liberdade aumentam, de acordo com o teorema do limite central, entretanto esta convergência é lenta (Figura 16.1).\nA distribuição qui-quadrado tem duas aplicações comuns: primeiro, como um teste para saber se duas variáveis categóricas são independentes ou não (Teste de independência ou associação); segundo, o teste de qualidade do ajuste do qui-quadrado (Teste de aderência ou ajuste) que é usado para comparar uma determinada distribuição com uma distribuição conhecida.\n\n\n16.2.2 Estatística do qui-quadrado\nO cálculo da estatística \\(\\chi^{2}\\) é baseado nas frequências existentes nas células da tabela de contingência. Em primeiro lugar, calcula-se as frequências que se espera em cada célula caso a hipótese nula seja verdadeira (frequências esperadas). Em segundo lugar, usando a equação geral, o teste mede o grau de discrepância entre o conjunto de frequências observadas (O) e o conjunto de frequências esperadas (E).\n\\[\n\\chi^{2}= \\sum \\left [\\frac{\\left (O_{i} - E_{i} \\right )^2}{E_{i}} \\right]\n\\]\nSe \\(O_{i}\\) é muito semelhante ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é baixo; se \\(O_{i}\\) é muito diferente em relação ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é alto.\nAs frequências observadas são o número de sujeitos ou objetos na amostra que se enquadram nas várias categorias da variável de interesse. As frequências esperadas são o número de sujeitos ou objetos na amostra que seria esperado observar se hipótese nula fosse verdadeira.\n\\[\nE = \\frac{total\\ coluna\\ \\times total\\ linha }{total\\ geral}\n\\]\nPor exemplo, suponha a Tabela 16.1:\n\n\n\n\nTabela 16.1: Distribuição de Acidentes Automobilísticos por Gênero\n\n\n\nSexoCom AcidentesSem AcidentesTotal de PessoasHomens164460Mulheres43640Total2080100\n\n\n\n\n\nUsando esses dados (Tabela 16.1), o número de acidentes esperados para os homens é igual a:\n\ntotal_c &lt;- df$Acidentes[3]\ntotal_l &lt;- df$Total[1]\ntotal_geral &lt;- df$Total[3]\nesperado &lt;- (total_c*total_l)/total_geral\nesperado \n\n[1] 12\n\n\nO número de acidentes esperado para os homens é igual a 12, entretanto ocorreram 16. Houve uma diferença. Esta diferença é calculada para todas as células e será o importante no cálculo do qui-quadrado. Após o cálculo, acrescentando os valores esperados à Tabela 16.1, tem-se a Tabela 16.2 que será usada no cálculo do qui-quadrado.\n\n\n\n\nTabela 16.2: Distribuição de Acidentes Automobilísticos por Gênero\n\n\n\nSexoCom AcidentesSem AcidentesTotal de PessoasHomens124860Mulheres83240Total2080100\n\n\n\n\n\nPara o cálculo do \\(\\chi^2\\), como se está comparando proporções em uma tabela contingência \\(2\\times2\\) (ou seja, duas linhas e duas colunas), deve-se aplicar uma correção na fórmula, mostrada acima, para ajustar o viés de continuidade. Esta correção, denominada de correção de Yates, consiste em subtrair 0,5 do valor absoluto da diferença entre a frequência observada e a esperada em cada célula da tabela, antes de elevar ao quadrado e dividir pela frequência esperada.\n\\[\n\\chi^{2}= \\sum \\left [\\frac{\\left |(O_{i} - E_{i}|-0,5 \\right )^2}{E_{i}} \\right]\n\\]\nSubstituindo os dados na fórmula do qui-quadrado, tem-se:\n\\[\n\\chi^{2}= \\left [\\frac{\\left (|16 - 12 \\right |-0.5)^2}{12} \\right]+ \\left [\\frac{\\left (|44 - 48 \\right |-0.5)^2}{48} \\right]+\\left [\\frac{\\left (|4 - 8 \\right |-0.5)^2}{8} \\right]+\\left [\\frac{\\left (|36 - 32 \\right|-0.5)^2}{32} \\right]\n\\]\n\\[\n\\chi^{2}=1,021 + 0,255 + 1,531  +0,383 = 3,19\n\\]\nA correção de Yates reduz o valor do qui-quadrado 1 e torna o teste mais conservador, reduzindo a probabilidade de rejeitar a hipótese nula quando ela é verdadeira. A correção é recomendada em tabelas de contingência \\(2\\times2\\) ou quando o tamanho da amostra é pequeno (menor que 40) ou quando há pelo menos uma célula com frequência esperada menor que 5.\n\n16.2.2.1 Restrições ao qui-quadrado\n\nRegra Geral\n\nO teste pode ser usado, se a frequência observada em cada célula for maior ou igual a 5 e a frequência esperada for maior ou igual a 5.\n\nTabela 2 \\(\\times\\) 2 (gl = 1)\n\nComo mostrado no exemplo acima, neste caso, é recomendada a Correção de Continuidade de Yates, mesmo quando o n for grande.\n\nTabela l \\(\\times\\) c\n\nO teste pode ser usado se o número de células com frequência esperada inferior a 5 for menor do que 20% do total das células e nenhuma frequência esperada é igual a zero.\n\nn pequeno\n\nNeste caso, é preconizado o Teste Exato de Fisher.\n\n\n16.2.2.2 Valor crítico do qui-quadrado\nA estatística de teste (que em certo sentido é a diferença entre as frequências observadas e esperadas) deve ser comparada a um valor crítico para determinar se a diferença é grande ou pequena. Não se pode dizer se uma estatística de teste é grande ou pequena sem colocá-la em perspectiva com o valor crítico. Se a estatística de teste estiver acima do valor crítico, significa que a probabilidade de observar tal diferença entre as frequências observadas e esperadas é improvável.\nO valor crítico pode ser encontrado na tabela estatística da distribuição Qui-quadrado e depende do nível de significância, denotado \\(\\alpha\\), e dos graus de liberdade, denotado \\(gl\\). O nível de significância geralmente é igual a 5%. Os graus de liberdade para um teste de Qui-quadrado de independência são encontrados da seguinte forma:\n\\[\ngl = (numero \\ de \\ linhas - 1) \\ \\times \\ (numero \\ de \\ colunas - 1)\n\\]\nEm uma tabela de contingência 2 \\(\\times\\) 2, como a Tabela 16.1, tem \\(gl = (2 - 1) \\times (2 - 1) = 1\\). Basta agora obter o valor crítico com a função qchisq():\n\nalpha &lt;-  0.05\ngl = 1\nqchisq (1-alpha, gl)\n\n[1] 3.841459\n\n\nEste valor é comparado com o \\(\\chi^{2}_{calculado}\\) para um nível de significância de 5%. Se ele é maior, rejeita-se se a \\(H_{0}\\); caso contrário, não se rejeita. Para obter o valor P, pode-se usar a função pchisq(), onde, como argumento, coloca-se o valor do \\(\\chi^{2}_{calculado}\\), os graus de liberdade e acrescenta-se lower.tail = FALSE para obter a probabilidade da cauda superior, uma vez que a distribuição do qui-quadrado é positiva.\nO valor crítico é igual a 3,84 e, no exemplo, o \\(\\chi^{2}_{calculado}\\) é igual a 3,19, logo a valor P é igual a:\n\npchisq (3.19, 1, lower.tail = FALSE)\n\n[1] 0.07409001\n\n\nDessa forma, conclui-se ,com uma confiança de 95%, que não se rejeita \\(H_{0}\\), ou seja, a proporção acidentes nos homens é igual a proporção de acidentes nas mulheres (\\(\\chi^{2} (1) = 3,19;P=0,074\\)). Observe na Figura 16.2 que o \\(\\chi^{2}_{calculado}\\) localiza-se a esquerda da linha vertical, da área vermelha de rejeição da \\(H_{0}\\). Atente, que se não fosse feita a correção de Yates, a conclusão seria diferente, pois o \\(\\chi^{2}_{calculado}\\) seria igual a 4,17 e estaria um pouco cima do \\(\\chi^{2}_{crítico}\\), na área de rejeição da \\(H_{0}\\). Com a correção de Yates o valor P é igual a 0,074, sem a correção de continuidade igual a 0,041, isto configura o que se chama de valor P marginal e, nesses casos, deve-se ter muito cuidado na conclusão, pois existe risco de erro tipo II, não rejeitar uma \\(H_{0}\\) quando ela é falsa.\n\nplotchisqtest(chisq = 3.84, \n              df = 1,\n              colorleft = \"aliceblue\",\n              colorright = \"red\",\n              ylabel = \"Densidade de probabilidade sob a hipótese nula\")\n\n\n\n\n\n\n\nFigura 16.2: Distribuição do qui-quadrado, gl = 1, alpha = 0,05.\n\n\n\n\n\nO gráfico foi criado com a função plotchisqteste() do pacote nhstplot, pacote simples e conveniente para representar graficamente os testes de significância de hipótese nula mais comuns, como testes F, testes t e testes z (2).\n\n\n\n16.2.3 Qui-quadrado de independência ou associação\n\n16.2.3.1 Dados usados nesta seção\nO exemplo da Tabela 16.1, usado para mostrar a lógica do qui-quadrado, é um teste de independência ou associação. Ali, foi mostrado que não existe uma associação estatisticamente significativa (P &gt; 0,05) entre acidentes automobilísticos e o sexo. Como exercício, serão usados outros dados que necessitam mais manipulação como treinamento do qui-quadrado de associação e do próprio R. Estes dados servirão para testar a hipótese de associação entre tabagismo e baixo peso ao nascer (&lt; 2500g).\nLeitura e transformação dos dados\nOs dados serão provenientes do banco de dados dadosMater.xlsx. bastante usado neste livro 2 . A partir do diretório, será feita a leitura da seguinte maneira:\n\ndados &lt;- read_excel (\"dados/dadosMater.xlsx\")\n\nAdicione a este arquivo uma variável denominada baixoPeso, usando a função mutate() do pacote dplyr e a função ifelse (), da base do R:\n\ndados &lt;- dados %&gt;% mutate(baixoPeso = ifelse(pesoRN &lt; 2500, \"1\", \"2\"))\n\nOnde 1 = sim e 2 = não, ou seja, com peso de nascimento &lt; 2500g e \\(\\ge\\) 2500g.\nO próximo passo é selecionar, deste arquivo, apenas esta variável criada e a variável fumo, porque o objetivo da análise será verificar se existe associação entre tabagismo na gestação e baixo peso ao nascer (&lt; 2500g). Para isso, usa-se a função select() do pacote dplyr:\n\ndados &lt;- dados %&gt;% select (fumo, baixoPeso)\n\nA seguir, será extraída uma amostra deste banco de dados com n = 300, usando a função slice_sample() do pacote dplyr. A função set.seed() apenas garante que os dados selecionados aleatoriamente se mantenham os mesmos em outros sorteios (veja Seção 7.7.2):\n\nset.seed(123)\ndados &lt;- slice_sample(dados, n=300)\nstr(dados)\n\ntibble [300 × 2] (S3: tbl_df/tbl/data.frame)\n $ fumo     : num [1:300] 2 2 1 2 2 2 1 2 2 2 ...\n $ baixoPeso: chr [1:300] \"2\" \"2\" \"1\" \"2\" ...\n\n\nTodos esses passos poderiam ser feitos em um só comando, precedido pela semente (set.seed(123)), usando o operador pipe %&gt;%:\n\nset.seed(123)\ndados &lt;- read_excel (\"dados/dadosMater.xlsx\") %&gt;% \n  mutate(baixoPeso = ifelse(pesoRN &lt; 2500, \"1\", \"2\")) %&gt;% \n  select (fumo, baixoPeso) %&gt;% \n  slice_sample(n=300)\n\nTem-se, agora, um conjunto de dados com duas colunas: fumo, como uma variável numérica e baixoPeso, como caractere. Ambas devem ser transformadas em fator e, onde os rótulos são 1 e 2, passam para “sim” e “não” e mantendo a ordem “sim” e “não”.\n\ndados$fumo &lt;- factor(dados$fumo,\n                     levels = c(1, 2),\n                     labels = c(\"sim\", \"não\"))\n\ndados$baixoPeso &lt;- factor(dados$baixoPeso,\n                          levels = c(1, 2),\n                          labels = c(\"sim\", \"não\"))\nstr (dados)\n\ntibble [300 × 2] (S3: tbl_df/tbl/data.frame)\n $ fumo     : Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 2 2 1 2 2 2 ...\n $ baixoPeso: Factor w/ 2 levels \"sim\",\"não\": 2 2 1 2 1 2 2 2 2 2 ...\n\n\nTabelas\nPara construir a Tabela 16.3, antes será verificado como está distribuição baixo peso de acordo com o tabagismo materno, usando a função with() e, dentro dela, a função table(), seguida da função addmargins():\n\ntab &lt;- with(data = dados, table(fumo, baixoPeso))\naddmargins(tab)\n\n     baixoPeso\nfumo  sim não Sum\n  sim  18  51  69\n  não  22 209 231\n  Sum  40 260 300\n\n\nAssim, tem-se\n\n\n\n\nTabela 16.3: Baixo peso* ao nascer e tabagismo\n\n\n\nTabagismoBaixo PesoSem Baixo PesoTotalSim185169Não22209260Total40260300* Baixo Peso = peso ao nascer &lt; 2500g\n\n\n\n\n\n\nOBSERVAÇÃO: Uma tabela é constituída por linhas e colunas. Para se extrair valores da tabela, usa-se os colchetes, após o nome da tabela. O primeiro valor dentro dos colchetes é referente ao número da linha; o segundo, separado pela virgula, é referente ao número da coluna. Então, tab[1,1] se refere ao valor que está na primeira linha e primeira coluna:\n\n\ntab[1,1]\n\n[1] 18\n\n\nA proporção de baixo peso por categoria de tabagismo (nº de casos/total da linha):\n\nfumantes &lt;-  tab[1,1]/(tab[1,1] + tab[1,2])\nfumantes\n\n[1] 0.2608696\n\nnão.fumantes &lt;- tab[2,1]/(tab[2,1]+ tab[2,2])\nnão.fumantes\n\n[1] 0.0952381\n\n\nVisualização gráfica\nSerá construído um gráfico de barras empilhadas com o ggplot() do pacote ggplot2 (ver Seção 6.6):\n\nggplot(dados) +\n  aes (x = fumo, fill = baixoPeso) +\n  geom_bar (color = \"black\") +\n  scale_fill_manual(values = c(\"gray\", \"aliceblue\")) +\n  labs (title = NULL,  \n        x = \"Tabagismo\",\n        y = \"Frequência\") +\n  annotate(\"text\", x=\"sim\", y=62, label= \"26,1%\") + \n  annotate(\"text\", x = \"não\", y=223, label = \"9,5%\") +\n  theme_bw () +\n  theme (text = element_text (size = 12)) +\n  labs(fill = \"Peso ao nascer &lt; 2500g\")\n\n\n\n\n\n\n\nFigura 16.3: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer.\n\n\n\n\n\nObserva-se, Figura 16.3, que 24,6% das gestantes fumantes geram bebês com baixo peso, enquanto que entre as não fumantes este percentual cai três vezes, indo para 9,5%. É uma diferença grande! Aqui, quase se tem certeza que ela é significativa, mesmo sem cálculos!\n\n\n16.2.3.2 Hipóteses estatísticas\n\n\\(H_{0}\\): a proporção de baixo peso é igual nos dois grupo (fumantes e não fumantes); não há associação entre as variáveis.\n\n\n\\(H_{1}\\): a proporção de baixo peso é diferente nos dois grupo (fumantes e não fumantes); existe associação entre as variáveis.\n\n\n\n16.2.3.3 Cálculo do Qui-quadrado de Pearson no R\nPara este exemplo, o \\(\\chi^{2}\\) irá verificar se existe uma associação entre as variáveis fumo e baixoPeso, assumindo um \\(\\alpha = 0,05\\) que equivale a um valor crítico de 3,84 com um grau de liberdade, em uma tabela \\(2 \\times 2\\). A função chisq_test() do pacote rstatix libera o qui-quadrado, usando os seguintes argumentos:\n\nx \\(\\to\\) vetor numérico ou matriz. Tanto x como y podem ser fatores;\ny \\(\\to\\) vetor numérico. Ignorado quando se x é uma matriz;\ncorrect \\(\\to\\) TRUE é o padrão. Indica se deve ser aplicada a correção de continuidade ao calcular a estatística de teste para tabelas 2 por 2 3;\n… \\(\\to\\) para outros argumentos consulte a ajuda do RStudio.\n\nPara executar a função chisq_test(), basta colocar como argumento as variáveis fumo e baixoPeso ou construir antes uma tabela de contingência com a função table() e depois colocá-la como argumento. Como tabela tab já existe:\n\nteste &lt;- rstatix::chisq_test(tab)\nteste\n\n# A tibble: 1 × 6\n      n statistic        p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   300      11.2 0.000809     1 Chi-square test ***     \n\n\nA saída do teste exibe tudo que é necessário. O \\(\\chi^{2}\\) = 11.2 com um valor muito maior que o valor crítico de 3,84, mostrando que a diferença é estatisticamente significativa (P = 8.09^{-4}). Essas e outras estatísticas podem ser obtidas, usando o objeto teste seguido do sinal $. Por exemplo, para o valor P:\n\nteste$p\n\n[1] 0.000809\n\n\n\nNOTA: Se um aviso como Chi-squared approximation may be incorrect (Aproximação qui-quadrado pode estar incorreta) aparecer, significa que as menores frequências esperadas são inferiores a 5. Para evitar esse problema, é possível usar uma das seguintes opções:\n\n\nreunir alguns níveis (especialmente aqueles com um pequeno número de observações) para aumentar o número de observações nos subgrupos, ou\nusar o teste exato de Fisher.\n\nOutras maneiras de calcular o qui-quadrado no R\nNo pacote gmodels (3), existe uma função muito interessante, a função CrossTable() que que imprime, além de uma tabela de frequência com as proporções, exibe vários testes, como o teste \\(\\chi^2\\), o teste exato de Fisher e o teste de McNemar com e sem correção de continuidade. Consulte a ajuda para melhor estudar esta elegante função! Neste momento, será explorado apenas o qui-quadrado e os valores esperados com três dígitos:\n\nCrossTable (tab,\n            digits = 3,\n            prop.chisq = FALSE,\n            prop.t = FALSE,\n            chisq = TRUE,\n            expected = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|              Expected N |\n|           N / Row Total |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  300 \n\n \n             | baixoPeso \n        fumo |       sim |       não | Row Total | \n-------------|-----------|-----------|-----------|\n         sim |        18 |        51 |        69 | \n             |     9.200 |    59.800 |           | \n             |     0.261 |     0.739 |     0.230 | \n             |     0.450 |     0.196 |           | \n-------------|-----------|-----------|-----------|\n         não |        22 |       209 |       231 | \n             |    30.800 |   200.200 |           | \n             |     0.095 |     0.905 |     0.770 | \n             |     0.550 |     0.804 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        40 |       260 |       300 | \n             |     0.133 |     0.867 |           | \n-------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  12.61347     d.f. =  1     p =  0.0003829762 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  11.22084     d.f. =  1     p =  0.0008088369 \n\n \n\n\nObserve que a saída mostra em cada célula da tabela, o número de casos, o número esperado, a percentagem por linha (nº de casos/total da linha) e a percentagem por coluna (nº de casos/total da coluna). Por último, exibe o qui-quadrado de Pearson com e sem coreção de continuidade de Yates.\n\n\n16.2.3.4 Conclusão e relato dos resultados\nUsando a correção de continuidade de Yates, pois é uma tabela \\(2\\times2\\), vê-se que o valor P é menor que o nível de significância de 5% e, consequentemente, rejeita-se a hipótese nula e conclui-se que existe uma associação significativa entre tabagismo na gestação e o baixo peso ao nascimento (\\(\\chi^{2}_{com \\ correção \\ de \\ Yates} (1)\\) = 11.2; P = 0,00081).\nAlém disso, no relato dos resultados pode-se apresentar uma tabela ou em um gráfico.\nTabela\nPara a construção da tabela, pode-se usar a função ctable() do pacote summarytools(4) para obter uma tabela com todos os dados a serem exibidos. O argumento prop = \"r\" exibe os percentuais das linhas (“c”, nas colunas). Na realidade, são maneiras diferente de se obter o mesmo resultado.\n\nctable(dados$fumo, dados$baixoPeso,\n       prop = \"r\", \n       chisq = TRUE, \n       headings = FALSE,\n       OR = TRUE)\n\n\n------- ----------- ------------ ------------- --------------\n          baixoPeso          sim           não          Total\n   fumo                                                      \n    sim               18 (26.1%)    51 (73.9%)    69 (100.0%)\n    não               22 ( 9.5%)   209 (90.5%)   231 (100.0%)\n  Total               40 (13.3%)   260 (86.7%)   300 (100.0%)\n------- ----------- ------------ ------------- --------------\n\n----------------------------\n Chi.squared   df   p.value \n------------- ---- ---------\n   11.2208     1     8e-04  \n----------------------------\n\n----------------------------------\n Odds Ratio   Lo - 95%   Hi - 95% \n------------ ---------- ----------\n    3.35        1.67       6.71   \n----------------------------------\n\n\nOu seja, os bebês que se expuseram ao fator de risco (fumo) têm uma chance 3,35 (IC95% : 1,67-6,71) vezes maior de apresentar peso &lt; 2500g ao nascer, nessa amostra.\nAlém desses resultados, a tabela final pode conter (e isto é recomendado!) os intervalos de confiança para cada uma das proporções de fumantes e não fumantes. Para isso a função BinomCI() cumpre um papel suficiente (veja Seção 10.7.3).\nEntão, a proporção de baixo peso entre as fumantes é:\n\nBinomCI(18, 69,\n        conf.level = 0.95,\n        method = \"clopper-pearson\")\n\n           est    lwr.ci    upr.ci\n[1,] 0.2608696 0.1625161 0.3805962\n\n\nE, entre as não fumantes é:\n\nBinomCI(22, 231,\n        conf.level = 0.95,\n        method = \"clopper-pearson\")\n\n           est     lwr.ci    upr.ci\n[1,] 0.0952381 0.06065157 0.1406387\n\n\nFinalmente, esses dados podem ser colocados em uma tabela, como a Tabela 16.4:\n\n\n\n\nTabela 16.4: Efeito do tabagismo materno no peso ao nascer\n\n\n\nPeso ao nascerFumantesNão fumantesValor P*Baixo Peso18/6922/2310.00081IC95%16,3-38,16,1-14,1* Qui-quadrado de Pearson com correção\n\n\n\n\n\nGráfico\nUma boa apresentação gráfica complementa o relatório dos resultados. Pode-se fazer isso com gráfico de barras empilhadas (), acompanhado dos percentuais e do tipo de teste realizado, usando a função get_test_label() que necessita do teste calculado com a função chisq_test() do pacote rstatix, apresentado antes. O gráfico assume o aspecto da Figura 16.4):\n\n ggplot(dados) +\n    aes (x = fumo, fill = baixoPeso) +\n    geom_bar (color = \"black\") +\n    scale_fill_manual(values = c(\"gray\", \"gray95\")) +\n    labs (title = NULL,\n          subtitle = get_test_label (teste, detailed = TRUE),\n          x = \"Tabagismo\",\n          y = \"Frequência\") +\n    annotate(\"text\", x=\"sim\", y=62, label= \"24,6% (15,0-36,5)\") + \n    annotate(\"text\", x = \"não\", y=223, label = \"8,2% (5,0-12,5)\") +\n    theme_bw () +\n    theme (text = element_text (size = 12)) +\n    labs(fill = \"Peso ao nascer &lt; 2500g\")\n\n\n\n\n\n\n\nFigura 16.4: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer.\n\n\n\n\n\n\n\n\n16.2.4 Teste Aderência ou do Melhor Ajuste\nO teste de qualidade de ajuste do qui-quadrado (chi-square goodness of fit) é usado para comparar a distribuição observada com uma distribuição esperada, em uma situação em que se tem duas ou mais categorias em dados discretos. Em outras palavras, ele compara várias proporções observadas com as probabilidades esperadas (5).\n\n16.2.4.1 Dados usados nesta seção\nHá uma dúvida se o número de pacientes que procura uma determinada Unidade de Pronto Atendimento (UPA) é aproximadamente o mesmo em todos os dias da semana. Esta é uma informação importante sob o ponto de vista administrativo. Para se atingir este objetivo registrou-se o número de pacientes que procurou a UPA por dia da semana.\nO número de atendimentos nos sete dias (de segunda-feira à domingo) da semana está representada pela frequência observada, freq_obs:\n\nfreq_obs &lt;- c(20, 17, 22, 21, 26, 33, 36)\nfreq_obs\n\n[1] 20 17 22 21 26 33 36\n\n\nO total de atendimentos durante uma semana é igual a:\n\nsoma &lt;- sum(freq_obs)\nsoma\n\n[1] 175\n\n\nAssim, a frequência esperada diária é igual a soma total dos atendimentos dividido pelo número observações (no caso, dias da semana), representada por k:\n\nk = 7\nfreq_esp &lt;- soma/k\nfreq_esp\n\n[1] 25\n\n\nCom estes valores , pode-se criar um vetor, p, com as proporçõess dos atendimentos diários esperados:\n\np &lt;- rep(freq_esp/soma, 7)\np\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n16.2.4.2 Hipóteses estatísticas\n\n\\(H_{0}\\): a distribuição das frequências observadas (O) é igual a distribuição de frequências esperadas (E).\n\n\n\\(H_{1}\\): a distribuição das frequências observadas (O) não é igual a distribuição de frequências esperadas (E)\n\n\n\n16.2.4.3 Cálculo da estatística do teste\nAssumindo um \\(\\alpha = 0,05\\). Os graus de liberdade são calculados como o número de células (k) menos 1: \\(gl = (k - 1)\\). O \\(\\chi^{2}_{crítico}\\) pode ser encontrado usando:\n\nalpha = 0.05\nk = 7\ngl = k - 1\nqchisq (1 - alpha, gl)\n\n[1] 12.59159\n\n\nEm outras palavras, se o \\(\\chi^{2}_{calculado}\\) &gt; \\(\\chi^{2}_{crítico}\\), rejeita-se a \\(H_{0}\\). Na Figura 16.5), o resultado tem que ficar à direita da linha vertical vermelha para que a hipótese nula seja rejeitada. Se cair fora da área de rejeição, à esquerda da linha vertical vermelha, aceita-se a hipóteses nula.\n\nplotchisqtest(chisq = 12.6, \n              df = 6,\n              colorleft = \"aliceblue\",\n              colorright = \"red\",\n              ylabel = \"Densidade de probabilidade\",\n              colorcut = \"red\",)\n\n\n\n\n\n\n\nFigura 16.5: Distribuição do qui-quadrado, gl = 6, alpha = 0,05.\n\n\n\n\n\nA estatística do teste pode ser encontrada, usando a função chisq.test():\n\nteste1 &lt;- chisq.test (x = freq_obs, p = p)\nteste1\n\n\n    Chi-squared test for given probabilities\n\ndata:  freq_obs\nX-squared = 12, df = 6, p-value = 0.06197\n\n\n\n\n16.2.4.4 Conclusão\nObservando-se a saída do teste do qui-quadrado, verifica-se que o \\(\\chi^{2}_{calculado}\\) &lt; \\(\\chi^{2}_{crítico}\\), portanto, não se rejeita a \\(H_{0}\\) e conclui-se que, nesta amostra, com uma confiança de 95%, que a frequência observada de pacientes à UPA é igual a esperada (P = 0.0619688). Lembrando que, neste caso, como se tem um valor P limitrofe, marginal, existe a possibilidade de se estar aceitando uma \\(H_{0}\\) falsa e cometendo um erro tipo II. Seria recomendado, aumentar o tamanho amostral em uma nova coleta, usando estes dados como um piloto para o cálculo amostral.\n\n\n\n16.2.5 Qui-quadrado de Pearson para tabelas extensas\nEste teste é utilizado quando o número de grupos, k, é superior a 2.\n\n16.2.5.1 Dados usados nesta seção\nComo exemplo, será verificado se existe uma tendência de maior taxa de infecção nos neonatos que permanecem mais tempo hospitalizados. O banco de dados dadosCirurgia.xlsx que pode ser encontrado aqui. Salve o mesmo no seu diretório de trabalho.\nEsse banco de dados contém 144 recém-nascidos submetidos a diferentes procedimentos cirúrgicos. As variáveis disponíveis são:\n\nid \\(\\to\\) identificação do neonato;\nsexo \\(\\to\\) sexo do recém-nascido, fem e masc;\npeso \\(\\to\\) peso do neonato em gramas;\ntempohosp \\(\\to\\) tempo de hospitalização em dias;\ninfec \\(\\to\\) presença de infecção secundária: sim e não;\ncirurgia \\(\\to\\) tipo de cirurgia: abdominal, cardíaca, outra.\n\nA variável tempo de hospitalização (tempohosp) é contínua e assimétrica. Para ser usada aqui, será categorizada por quartis. A variável infec (presença de infecção) é uma variável dicotômica (sim, não).\nLeitura e transformação dos dados\nA leitura dos dados é feita com:\n\ncirurgia &lt;- read_excel (\"dados/dadosCirurgia.xlsx\")\nstr(cirurgia)\n\ntibble [144 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:144] 1 2 3 4 5 6 7 8 9 10 ...\n $ sexo     : chr [1:144] \"masc\" \"masc\" \"masc\" \"masc\" ...\n $ peso     : num [1:144] 2020 1850 2540 1150 2900 ...\n $ ig       : num [1:144] 36 30 38 31 36 37 38 39 38 39 ...\n $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ...\n $ infec    : chr [1:144] \"não\" \"não\" \"sim\" \"sim\" ...\n $ cirurgia : chr [1:144] \"abdominal\" \"abdominal\" \"abdominal\" \"outra\" ...\n\n\nCaracterísticamente, a variável tempohosp (tempo de hospitalização) é, quase sempre, assimétrica:\n\ncirurgia %&gt;% ggplot() +\n    geom_histogram(aes(x = tempohosp,\n                       y = after_stat(density)), \n                   fill = \"tomato\",\n                   bins = 20,\n                   col=alpha(\"gray40\",0.5)) +\n  geom_function(fun=dnorm,\n                args=list(mean=mean(cirurgia$tempohosp, na.rm = T),\n                          sd= sd(cirurgia$tempohosp, na.rm = T)), \n                col='dodgerblue4',\n                lwd=1,\n                lty=2) + \n  labs(x='Tempo de hospitalização (dias)',    \n       y='Densidade de probabilidade') +\n  theme_bw()\n\n\n\n\n\n\n\n\nA variável tempohosp será categorizada em quartis. O resumo da mesma, usando a função summary(), fornece orientação para esse procedimento:\n\nsummary (cirurgia$tempohosp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   20.75   27.50   37.42   42.00  245.00 \n\n\nO sumário da variável fornece orientação para a categorização, que será realizada, usando a função cut(), consulte a Seção 6.4.2 para detalhes de construção de uma tabela de frequência:\n\ncirurgia$tempo &lt;- cut(cirurgia$tempohosp, \n                      breaks = c(1, 20.75, 27.50, 42.00, 245),\n                      labels = c(\"&lt;= 21\",\"22-28\", \"29-42\",\"&gt;42\"),\n                      right = FALSE, \n                      include.lowest = TRUE)\n\ntab1 &lt;- table (cirurgia$tempo)\ntab1\n\n\n&lt;= 21 22-28 29-42   &gt;42 \n   36    36    35    37 \n\n\nPor exemplo, 36 recém-nascidos premaneceram 21 dias ou menos no pós-operatório e 37 recém-nascidos ficarm internados mais do que 42 dias.\nAgora, a variável cirurgia$infec será colocada como um fator:\n\ncirurgia$infec &lt;- factor(cirurgia$infec, levels = c(\"sim\", \"não\"))\ntable(cirurgia$infec)\n\n\nsim não \n 56  88 \n\n\nA nova variável tempo será cruzada com a variável infec em uma tabela:\n\ntab2 &lt;- with(data = cirurgia, table(tempo, infec))\naddmargins(tab2)\n\n       infec\ntempo   sim não Sum\n  &lt;= 21   9  27  36\n  22-28  11  25  36\n  29-42  14  21  35\n  &gt;42    22  15  37\n  Sum    56  88 144\n\n\nOu seja, a proporção de recém-nascidos que se infectaram no pós-operatório foi 56/144 = 0,39 ou 39%. Se for calculada a proporção para cada um dos quartis do tempo de hospitalização, através da função ctable() do pacote summarytools, observa-se que a proporção de neonatos infectados no pós-operatório aumenta com o tempo de hospitalização:\n\n\n16.2.5.2 Hipóteses estatísticas\n\n\\(H_{0}\\): A presença de infecção não altera o tempo de hospitalização.\n\n\n\\(H_{1}\\): A presença de infecção altera o tempo de hospitalização.\n\n\n\n16.2.5.3 Cálculo da estatística do teste\nO teste estatístico pode ser calculado, usando o argumento chisq = TRUE na função ctable():\n\nsummarytools::ctable(cirurgia$tempo, cirurgia$infec,\n                      prop = \"r\",\n                      chisq = TRUE,\n                      headings = FALSE)\n\n\n------- ------- ------------ ------------ --------------\n          infec          sim          não          Total\n  tempo                                                 \n  &lt;= 21            9 (25.0%)   27 (75.0%)    36 (100.0%)\n  22-28           11 (30.6%)   25 (69.4%)    36 (100.0%)\n  29-42           14 (40.0%)   21 (60.0%)    35 (100.0%)\n    &gt;42           22 (59.5%)   15 (40.5%)    37 (100.0%)\n  Total           56 (38.9%)   88 (61.1%)   144 (100.0%)\n------- ------- ------------ ------------ --------------\n\n----------------------------\n Chi.squared   df   p.value \n------------- ---- ---------\n   10.5801     3    0.0142  \n----------------------------\n\n\n\n\n16.2.5.4 Conclusão\nA partir desses resultados, pode-se inferir que a menor taxa de infecção ocorre no grupo do primeiro quartil e é significativamente diferente em relação a taxa de infecção do maior quartil, mas sem indicação para os grupos intermediários. É útil fazer o teste de tendência linear (Linear-by-linear Association). Para isso, pode-se usar a função lbl_test () do pacote coin.\n\ncoin::lbl_test (cirurgia$tempo ~ cirurgia$infec)\n\n\n    Asymptotic Linear-by-Linear Association Test\n\ndata:  cirurgia$tempo (ordered) by cirurgia$infec (sim, não)\nZ = 3.1231, p-value = 0.001789\nalternative hypothesis: two.sided\n\n\nEste teste indica uma tendência significativa para a presença de infecção à medida que aumenta o tempo de hospitalização (P = 0,0018).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "16-dadosCategoricos.html#teste-exato-de-fisher",
    "href": "16-dadosCategoricos.html#teste-exato-de-fisher",
    "title": "16  Análise de Dados Categóricos",
    "section": "16.3 Teste exato de Fisher",
    "text": "16.3 Teste exato de Fisher\nO teste do qui-quadrado não é um método apropriado de análise se a amostra é pequena. Por exemplo, se n for menor que 20 ou se n estiver entre 20 e 40 e uma das frequências esperadas for menor que 5, o teste do qui-quadrado deve ser evitado. Nesta situação, é recomendado o teste exato de Fisher.\n\n16.3.1 Dados usados nesta seção\nUm estudo estabeleceu como objetivo verificar se a asma não controlada é um fator de risco para a procura da emergência. Foram acompanhados 16 escolares asmáticos durante um ano com relação ao número de visitas à emergência de acordo com o controle da sua asma.\n\n16.3.1.1 Entrando com os dados\nEm primeiro lugar, serão criadas duas variáveis:\n\nemerg &lt;- c (1,1,2,2,2,2,2,1,1,1,1,1,1,1,1,2)\ncontrole &lt;- c (1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2)\n\nA seguir, usando essas variáveis, será construído um dataframe que será atribuído ao objeto dadosControle:\n\ndadosAsma &lt;- data.frame(emerg, controle)\n\n\n\n16.3.1.2 Transformação dos dados\nAmbas as variáveis são numéricas e serão transformadas em fatores, considerando 1 = sim e 2 = não e mantendo essa ordem:\n\ndadosAsma$emerg &lt;- factor (dadosAsma$emerg,\n                           levels = c (1,2),\n                           labels = c ('consultou', 'não consultou'))\ndadosAsma$controle &lt;- factor (dadosAsma$controle, \n                          levels = c (1,2),\n                          labels = c ('asma controlada', 'não controlada'))\nstr(dadosAsma)\n\n'data.frame':   16 obs. of  2 variables:\n $ emerg   : Factor w/ 2 levels \"consultou\",\"não consultou\": 1 1 2 2 2 2 2 1 1 1 ...\n $ controle: Factor w/ 2 levels \"asma controlada\",..: 1 1 1 1 1 1 1 2 2 2 ...\n\n\n\n\n16.3.1.3 Construção da tabela\nPara o cálculo da estatística do teste, é necessário uma tabela \\(2\\times2\\) (Tabela 16.5), obtida com os dados acima:\n\ntab3 &lt;- with(data = dadosAsma, table(controle, emerg))\n\n\n\n\n\nTabela 16.5: Consulta à emergência e controle da asma (tab3)\n\n\n\nVisita à EmergênciaControleSimNãoTotalAsma controlada257Não controlada819Total10616\n\n\n\n\n\n\n\n\n16.3.2 Hipóteses estatísticas\n\n\\(H_0\\): as variáveis são independentes, não há relação entre as duas variáveis categóricas.\n\n\n\\(H_1\\): as variáveis são dependentes, existe uma relação entre as duas variáveis categóricas. \n\n\n\n16.3.3 Cálculo da estatística do teste\nO teste exato de Fisher é usado quando há pelo menos uma célula na tabela de contingência das frequências esperadas abaixo de 5. Para recuperar as frequências esperadas, use a função chisq.test(), do R base, junto com $expected:\n\nchisq.test (dadosAsma$controle, dadosAsma$emerg)\n\nWarning in chisq.test(dadosAsma$controle, dadosAsma$emerg): Aproximação do\nqui-quadrado pode estar incorreta\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dadosAsma$controle and dadosAsma$emerg\nX-squared = 3.8095, df = 1, p-value = 0.05096\n\n\nA saída do teste imprime um aviso de que o qui-quadrado pode estar incorreto. Há necessidade, devido a presença de três células com valores abaixo de 5, de se usar o teste de Fisher. este pode ser obtido através da função fisher_test(), do pacote rstatix, colocando como argumento uma tabela de contingência \\(2\\times2\\), como a tab3:\n\nrstatix::fisher_test (tab3, detailed = TRUE)\n\n# A tibble: 1 × 8\n      n estimate     p conf.low conf.high method            alternative p.signif\n* &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;   \n1    16   0.0646 0.035 0.000953     0.991 Fisher's Exact t… two.sided   *       \n\n\n\n\n16.3.4 Conclusão\nO valor \\(P=0,035\\) é menor que o nível de significância de 5%, previamente estabelecido, e, portanto, deve-se rejeitar a hipótese nula. No contexto, rejeitar a hipótese nula para o teste exato de independência de Fisher significa que há uma associação significativa entre as duas variáveis categóricas (controle da asma e visitas à emergência).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "16-dadosCategoricos.html#teste-de-macnemar",
    "href": "16-dadosCategoricos.html#teste-de-macnemar",
    "title": "16  Análise de Dados Categóricos",
    "section": "16.4 Teste de Macnemar",
    "text": "16.4 Teste de Macnemar\nÉ um teste estatístico não paramétrico aplicável nos estudos tipo “antes-e-depois” em que cada indivíduo é utilizado como seu próprio controle e a medida é efetuada em escala nominal. O teste de McNemar é usado para determinar se há uma diferença estatisticamente significativa nas proporções entre os dados emparelhados.\nAs medidas coletadas nesses tipos de projetos de estudo não são independentes e, portanto, os testes do Qui-quadrado não podem ser usados porque os pressupostos serão violados.\nO teste de McNemar é usado para avaliar se há uma mudança significativa nas proporções ao longo do tempo para dados emparelhados ou se há uma diferença significativa nas proporções entre casos e controles. O resultado de interesse é a mudança dentro da pessoa (ou diferenças dentro do par) e não há variáveis explicativas.\nO teste é calculado examinando o número de respostas que são concordantes para positivo (sim em ambas as ocasiões) e negativo (não em ambas as ocasiões) e o número de pares disconcordantes (sim e não, ou não e sim). Os pares concordantes não fornecem informações sobre as diferenças e não são usados na avaliação. Em vez disso, deve-se concentrar nos pares discordantes, que podem ser divididos em dois tipos: um par discordante do tipo sim – não e um par discordante tipo não – sim (6).\n\n16.4.1 Pressupostos do teste de McNemar\nOs pressupostos para o teste de McNemar são:\n\nA variável desfecho é binária, dicotômica;\nCada participante é representado na tabela apenas uma vez;\nA diferença entre as proporções emparelhadas é o resultado de interesse;\nO teste de McNemar pode não ser confiável se houver contagens baixas nas células “discordantes”. Existe recomendação de que a soma dessas células seja \\(\\ge 20\\) (7).\n\n\n\n16.4.2 Dados usados nesta seção\nEm uma universidade, um professor de bioestatística comparou as atitudes de 200 estudantes de Medicina em relação à confiança que eles depositam na análise estatística antes e depois da conclusão da disciplina. A pergunta feita foi: Confiam na análise estatística utilizada nos periódicos médicos? As respostas podem ser resumidas na Tabela 16.6:\n\n\n\n\nTabela 16.6: Confiança na análise estatística após término da disciplina\n\n\n\nPós-testePré-TesteSimNãoTotalSim20 (a)8 (b)28Não22 (c)150 (d)172Total42158200\n\n\n\n\n\n\n\n16.4.3 Hipóteses estatísticas\nConsiderando as caselas a, b, c e d da Tabela 16.6, a hipótese nula de homogeneidade marginal indica que as duas probabilidades marginais para cada resultado são as mesmas, isto é,\n\\[\np_{a} + p_{b} = p_{a} + p_{c}\n\\] e\n\\[\np_{c} + p_{d} = p_{b} + p_{d}\n\\]\nAssim, a hipótese nula e a hipótese alternativa são:\n\n\\(H_{0}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste é a mesma.\n\n\n\\(H_{1}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste não é a mesma. \n\n\n\n16.4.4 Lógica do teste\nO teste estatístico de McNemar, com correção de continuidade, é obtido utilizando a equação:\n\\[\n\\chi^{2} = \\frac {\\left (\\left |b - c  \\right |- 1  \\right )^{2}}{b + c}\n\\]\nSob a hipótese nula, com um número suficientemente grande de discordantes (células b e c), o \\(\\chi^{2}\\) tem uma distribuição qui-quadrado com um grau de liberdade. Se o resultado é significativo, isto é, fornece evidências suficientes para rejeitar a hipótese nula, significa que as proporções marginais são significativamente diferentes umas das outras.\nSubstituindo os dados da Tabela na Equação, tem-se:\n\na &lt;- 20\nb &lt;- 8\nc &lt;- 22\nd &lt;- 150\nchi &lt;- ((abs(b - c) - 1)^2)/(b + c)\nchi\n\n[1] 5.633333\n\n\nAssumindo um \\(\\alpha = 0,05\\), pode-se obter valor crítico para o \\(\\chi^{2}\\) para gl = 1, usando a função qchisq(), do pacote stats:\n\nalpha = 0.05\nqchisq(1 - alpha, 1)\n\n[1] 3.841459\n\n\nDesta maneira, rejeita-se a \\(H_{0}\\), pois o \\(\\chi_{calculado}^{2} &gt; \\chi_{crítico}^{2}\\). O valor P pode ser conseguido com a função pchisq():\n\npchisq (5.633, 1, lower.tail = FALSE)\n\n[1] 0.01762544\n\n\n\n\n16.4.5 Cálculo do teste de McNemar no R\nCarregar o arquivo dadosBioestatistica.xlsx, que pode ser encontrado aqui. Este conjunto de dados contem os dados da tabela acima.\n\ndados &lt;- readxl::read_excel(\"dados/dadosBioestatistica.xlsx\")\n\nConstruir uma tabela de contingência\n\ndados$preteste &lt;- factor(dados$preteste, levels = c(\"sim\", \"não\"))\ndados$posteste &lt;- factor(dados$posteste, levels = c(\"sim\", \"não\"))\ntab4 &lt;- table(dados$preteste, dados$posteste, \n            dnn = c(\"Pré-teste\", \"Pós-teste\"))\ntab4\n\n         Pós-teste\nPré-teste sim não\n      sim  20   8\n      não  22 150\n\n\nAgora, pode-se calcular o teste de McNemar4 com a função mcnemar_test() do pacote rstatix:\n\nrstatix::mcnemar_test (tab4, \n                       correct = TRUE)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1   200      5.63     1 0.0176 *        McNemar test\n\n\nO resultado do teste de McNemar com correção de continuidade é exatamente igual ao calculado manualmente.\n\n\n16.4.6 Conclusão\nHouve uma modificação estatisticamente significativa na opinião dos alunos após o curso de Bioestatística em relação à confiança nas análises estatísticas (86% no pré-teste de respostas não x 79% no pós-teste, \\(\\chi^{2} = 5,63, gl = 1, P = 0,018\\)). Alguns alunos (14) mudaram de opinião em relação a sua confiança nas análises estatísticas dos periódicos médicos.\n\n\n\n\n1. Altman DG. Comparing groups: categorical data. Em: Practical Statistics for Medical Research. London: Chapman & Hall/CRC; 1991. p. 244–7. \n\n\n2. Myszkowski N. nhstplot package [Internet]. RDocumentation. 2020. Disponível em: https://rdocumentation.org/packages/nhstplot/versions/1.1.0\n\n\n3. Warnes GR, Bolker B, et al. Gmodels: Various R programming tools for model fitting [Internet]. CRAN R Project. 2022. Disponível em: https://rdrr.io/cran/gmodels/\n\n\n4. Comtois D. summarytools: Tools to Quickly and Neatly Summarize Data [Internet]. CRAN R Project. 2022. Disponível em: https://github.com/dcomtois/summarytools\n\n\n5. Daniel WW, Cross CL. The chi-square distribution and analysis of frequencies. Em: Practical Statistics for Medical Research. Hoboken, NJ: John Wiley & Sons, Inc; 2013. p. 604–19. \n\n\n6. Eliasziw M, Donner A. Application of the McNemar test to non-independent matched pair data. Statistics in medicine. 1991;10(12):1981–91. \n\n\n7. Rosner B. Hypothesis Testing: Categorical Data. Em: Fundamentals of Biostatistics. Seventh Edition. Boston: Cengage; 2011. p. 377.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "16-dadosCategoricos.html#footnotes",
    "href": "16-dadosCategoricos.html#footnotes",
    "title": "16  Análise de Dados Categóricos",
    "section": "",
    "text": "Se o cálculo fosse feito com a fórmula geral, o valor do qui-quadrado seria 4,17. Teste!↩︎\nVer Seção 5.3.↩︎\nQuando não se está trabalhando com uma tabela \\(2 \\times 2\\) e a regra geral for obedecida e o n for grande, pode-se usar o qui-quadrado de Pearson sem correção.↩︎\nÉ possível também obter o teste de McNemar de outras formas, como, por exemplo, usando a função CrossTable() do pacote gmodels.↩︎",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Análise de Dados Categóricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html",
    "href": "17-testes-naoParametricos.html",
    "title": "17  Métodos não paramétricos",
    "section": "",
    "text": "17.1 Pacotes necessários neste capítulo\npacman::p_load (coin,\n                confintr,\n                flextable,\n                ggpubr,\n                ggsci,\n                kableExtra,\n                knitr,\n                readxl,\n                rstatix,\n                tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#sec-distlivre",
    "href": "17-testes-naoParametricos.html#sec-distlivre",
    "title": "17  Métodos não paramétricos",
    "section": "17.2 Distribuição livre",
    "text": "17.2 Distribuição livre\nA maioria dos testes estatísticos, discutidos neste livro, são testes paramétricos. Nestes, o interesse estava focado em estimar ou testar uma hipótese sobre um ou mais parâmetros populacionais e ,por isso, são denominados de paramétricos. Além disso, o aspecto central desses procedimentos era o conhecimento da forma funcional da população da qual foram retiradas as amostras que forneceram a base para a inferência. Por exemplo, o teste t de Student para amostras independentes e a ANOVA são baseados no pressuposto de que os dados foram amostrados de populações que têm distribuição normal.\nOs testes não paramétricos não fazem suposições em relação à distribuição da população. Não têm, portanto, os pressupostos restritivos, comuns nos testes paramétricos. Têm distribuição livre. São baseados em uma ideia simples de ordenação por postos, do valor mais baixo ao mais alto. Analisam somente os postos, ignorando os valores. Podem ser usados tanto com variáveis ordinais como quantitativas numéricas.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#sec-postos",
    "href": "17-testes-naoParametricos.html#sec-postos",
    "title": "17  Métodos não paramétricos",
    "section": "17.3 Postos",
    "text": "17.3 Postos\nOs métodos estatísticos não paramétricos não lidam diretamente com os valores observados. Em função disso, para poder usar a informação fornecida pelas observações, sem trabalhar diretamente com os valores observados, utiliza-se os postos das observações. Posto (rank) de uma observação é a sua posição em relação aos demais valores.\nA atribuição dos postos de uma variável é realizada da seguinte maneira:\n\nColocam-se as observações em ordem crescente;\nAssociam-se valores, correspondendo às suas posições relativas na amostra. O primeiro elemento recebe o valor 1, o segundo o valor 2 e, assim por diante, até que a maior observação receba o valor n;\nSe todas as observações são distintas, os postos são iguais aos valores associados às observações no passo anterior.\nPara observações iguais (empates), associam-se postos iguais à média das suas posições relativas na amostra.\n\nPor exemplo, suponha uma amostra contendo os escores de Apgar no primeiro minuto de 10 recém-nascidos a termo (Tabela 17.1)). Em primeiro lugar, os valores são colocados em ordem crescente e, após, atribui-se postos aos valores. Observe que os postos atribuídos aos valores das posições 4, 5 e 6 são iguais e correspondentes a média de 4, 5 e 6, que é igual a 5. O mesmo ocorreu com os outros valores onde houve empate. A soma dos postos, no exemplo, é igual a 55. Para verificar a correção do cálculo, haja ou não empates, a soma dos postos será sempre \\(\\frac {n\\ \\times \\ (n+1)}{2}\\). No exemplo, n = 10, logo \\(\\frac {10\\ \\times \\ (10+1)}{2}=55\\).\n\n\n\n\nTabela 17.1: Construção dos postos\n\n\n\napgar1ordemposto411.0522.0733.0845.0855.0865.0978.0988.01099.510109.5",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#teste-de-mann-whitney",
    "href": "17-testes-naoParametricos.html#teste-de-mann-whitney",
    "title": "17  Métodos não paramétricos",
    "section": "17.4 Teste de Mann-Whitney",
    "text": "17.4 Teste de Mann-Whitney\nO teste de Mann-Whitney é usado para analisar a diferença na variável dependente (desfecho) para dois grupos independentes. O teste classifica todos os valores dependentes, ou seja, o valor mais baixo obtém o posto um e, em seguida, usa a soma dos postos de cada grupo no cálculo da estatística de teste.\nÉ o substituto do teste t para amostras independentes quando os pressupostos deste teste são violados. Para a aplicação do teste de Mann-Whitney a variável de interesse deve ser ordinal ou numérica. Este teste é equivalente ao desenvolvido por Frank Wilcoxon (1892 – 1965), assim algumas vezes é denominado de Wilcoxon Rank Sum Test ou teste de Wilcoxon-Mann-Whitney. O R usa esta denominação e é importante não confundir com o teste não paramétrico para amostra pareadas, discutido mais adiante.\n\n17.4.1 Dados usados nesta seção\nO arquivo dadosCirurgia.xlsx, já usado na Seção Seção 16.2.5.1, fornecerá os dados para esta seção. Ele contém 144 recém-nascidos que foram submetidos a diferentes procedimentos cirúrgicos. A questão de pesquisa a ser respondida é:\n\nExiste diferença no tempo de hospitalização (tempohosp) dos recém-nascidos de acordo com a presença ou não de infecção (infec)?\n\nEssa pergunta foi respondida de outra maneira, na Seção Seção 16.2.5. Agora, será usado o teste de Mann-Whitney.\n\n17.4.1.1 Leitura, exploração e visualização dos dados\nOs dados serão lidos com a função read_excel() do pacote readxl:\n\ncirurgia &lt;- readxl::read_excel (\"dados/dadosCirurgia.xlsx\")\nstr(cirurgia)\n\ntibble [144 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : num [1:144] 1 2 3 4 5 6 7 8 9 10 ...\n $ sexo     : chr [1:144] \"masc\" \"masc\" \"masc\" \"masc\" ...\n $ peso     : num [1:144] 2020 1850 2540 1150 2900 ...\n $ ig       : num [1:144] 36 30 38 31 36 37 38 39 38 39 ...\n $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ...\n $ infec    : chr [1:144] \"não\" \"não\" \"sim\" \"sim\" ...\n $ cirurgia : chr [1:144] \"abdominal\" \"abdominal\" \"abdominal\" \"outra\" ...\n\n\nA variável infec aparece como caractere e será transformada como fator:\n\ncirurgia$infec &lt;- factor(cirurgia$infec, levels = c(\"sim\", \"não\"))\n\nOs boxplots (Figura 17.1), construídos com a função ggboxplot() do pacote ggpubr com as cores da paleta do New England Journal of Medicine (NEJM), são uma boa maneira de visualizar os dados:\n\nggpubr::ggboxplot(cirurgia,\n                  x = \"infec\",\n                  y = \"tempohosp\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"infec\",\n                  palette = \"nejm\",\n                  legend = \"none\",\n                  ggtheme = theme_bw(),\n                  xlab = \"Presença de infecção\" ,\n                  ylab = \"Tempo de hospitalização (dias)\")   \n\n\n\n\n\n\n\nFigura 17.1: Impacto da infecção no tempo de hopsitalização.\n\n\n\n\n\nOs boxplots exibem uma série de valores atípicos, indicando que existe uma assimetria em ambos os grupos. Essa assimetria também pode ser verificada usando o teste de Shapiro-Wilk, que mostrando valores P &lt; 0,05, confirma que os dados não seguem a distribuição normal. Este teste não é pré-requisito para o teste. Foi realizado como uma demonstração.\n\ncirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  rstatix::shapiro_test(tempohosp)\n\n# A tibble: 2 × 4\n  infec variable  statistic        p\n  &lt;fct&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 sim   tempohosp     0.692 1.47e- 9\n2 não   tempohosp     0.565 9.87e-15\n\n\n\n\n17.4.1.2 Sumarização dos dados\nComo a variável tempohosp é assimétrica conforme mostrado acima, onde ambos os valores P são menores do que 0,05, será realizado um sumário numérico com a obtenção da mediana e IIQ. Isto será feito através da função group_by() e summarise(), incluídas no pacote dplyr.\n\nresumo &lt;- cirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (tempohosp, na.rm = TRUE),\n                   p25=quantile(tempohosp, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(tempohosp, probs = 0.75, na.rm = TRUE))\nresumo\n\n# A tibble: 2 × 5\n  infec     n mediana   p25   p75\n  &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 sim      56      37  22.8    49\n2 não      88      23  18      37\n\n\nOs dados mostram que a mediana de tempo de internação dos neonatos infectados é bem maior do que a mediana dos não infectados.\n\n\n\n17.4.2 Hipóteses estatísticas\nDa mesma maneira que o teste t, as hipóteses estabelecidas comparam dois grupos independentes. Se não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma,\n\n\\(H_{0}\\): As duas populações são iguais.\n\n\n\\(H_{1}\\): As duas populações não são iguais.\n\nNão foi escrita a hipótese nula como sendo que as médias (ou as medianas) são iguais, pois o teste não usa as medidas de posição tradicionais e sim os postos.\n\n\n17.4.3 Pressupostos do teste de Mann_Whitney\nO teste de Mann-Whitney é baseado nos seguintes pressupostos:\n\nOs dados são aleatórios;\nAs amostras são de dois grupos independentes;\nUm dos grupos é denominado de 1 e o outro de 2;\nA variável a ser comparada nos grupos deve ser ordenável;\nO grupo 1 será o grupo de menor tamanho e, se tiverem o mesmo tamanho, o grupo 1 é aquele cuja soma dos postos é a menor.\n\n\n\n17.4.4 Cálculo da estatística de teste\n\n17.4.4.1 Lógica do teste U de Mann-Whitney\nDe acordo com as hipóteses estabelecidas, o teste é bicaudal. Se as observações nos dois grupos forem provenientes da mesma população, a soma dos postos em cada grupo devem ficar próximas.\nPara calcular o teste, procede-se da seguinte maneira:\n\nDeve haver uma variável que identifique o grupo a que pertence cada uma das observações. No exemplo proposto, a variável desfecho é tempohosp e a variável agrupadora é infec, categorizada como sim e não.\nOrdenar de forma crescente todos os valores da variável tempohosp, sem levar em consideração a que grupo pertence. Para realizar este procedimento, será usada a função rank() do R base com o método para empates igual à média dos valores empatados (ties.method=\"average). Ao executar a função, será criada uma nova variável, denotada postos.\n\n\ncirurgia$postos &lt;- rank(cirurgia$tempohosp, ties.method = \"average\") \nhead(cirurgia)\n\n# A tibble: 6 × 8\n     id sexo   peso    ig tempohosp infec cirurgia  postos\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;      &lt;dbl&gt;\n1     1 masc   2020    36        37 não   abdominal   94.5\n2     2 masc   1850    30        37 não   abdominal   94.5\n3     3 masc   2540    38        37 sim   abdominal   94.5\n4     4 masc   1150    31        46 sim   outra      120  \n5     5 masc   2900    36        37 não   abdominal   94.5\n6     6 fem    2480    37        36 sim   abdominal   91  \n\n\n\nVerificar o tamanho (n) de cada grupo (presença ou não de infecção) e somar os postos em cada um dos grupos, usando a função group_by() junto com a função summarise(),\n\n\nresumo &lt;- cirurgia %&gt;% \n  dplyr::group_by(infec) %&gt;% \n  dplyr::summarise(n = n(),\n                   soma = sum(postos))\nresumo\n\n# A tibble: 2 × 3\n  infec     n  soma\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 sim      56 4876.\n2 não      88 5564.\n\n\n\nDenominar de grupo_1 o grupo com menor soma:\n\n\ngrupo_1 &lt;- min(resumo$soma)\ngrupo_1\n\n[1] 4875.5\n\n\n\nDenotar o grupo_1 como T\n\n\nT &lt;- grupo_1\n\nConsequentemente,\n\nn1 &lt;- resumo$n[1]\nn1\n\n[1] 56\n\nn2 &lt;- resumo$n[2]\nn2\n\n[1] 88\n\n\n\nCalcular a estística do teste, usando a fórmula preconizada por Altman (1):\n\n\\[\nU =n_{1} \\times n_{2} \\ +\\left [\\frac{n_{1} \\times \\left (n_{1} + 1  \\right )}{2}  \\right ] - T\n\\]\n\nU &lt;- (n1*n2 + ((n1*(n1 + 1))/2)) - T\nU\n\n[1] 1648.5\n\n\nObs.: O U de Mann-Whitney aparece no teste de Wilcoxon como W, eles são iguais\n\nSe \\(n_{1}\\), \\(n_{2}\\) \\(\\ge\\) 10, a distribuição da estatística do teste pode ser aproximada por uma distribuição normal com média igual a\n\n\\[\n\\mu_{U} =\\left [\\frac{n_{S} \\times \\left (n_{L} + 1  \\right )}{2}  \\right ]\n\\]\nonde \\(n_{S}\\) e \\(n_{L}\\), são, respectivamente, o grupo de menor e maior tamanho. No exemplo, \\(n_{1}\\) e \\(n_{2}\\).\n\nm_U &lt;- (n1*(n1+n2+1))/2\nm_U\n\n[1] 4060\n\n\nE desvio padrão igual a\n\\[\n\\sigma_{U}= \\sqrt {\\frac{n_{L}\\times \\sigma_{U}}{6}}\n\\]\n\ndp_U &lt;- sqrt((n2*m_U)/6)\ndp_U\n\n[1] 244.0219\n\n\nOs resultados fornecem os dados para calcular a estatística \\(Z_{U}\\) com correção de continuidade e, a partir dela, calcular o valor P.\n\\[\nZ_{U}= \\frac{(T -0,5) - \\mu_{U}}{\\sigma_{U}}\n\\]\n\nZ_U &lt;- ((T - 0.5) - m_U)/dp_U\nround(Z_U, 2)\n\n[1] 3.34\n\n\n\nFinalmente, calcula-se o valor P, usando a função pnorm(), multiplicada por 2, pois o teste é bicaudal.\n\n\nP &lt;- pnorm(Z_U, lower.tail = FALSE) * 2\nround(P, 4)\n\n[1] 8e-04\n\n\nNa prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Os cálculos foram mostrados para melhorar o entendimento de como o teste de Mann-Whitney funciona.\n\n\n17.4.4.2 Cálculo do U de Mann-Whitney no R\nO teste pode ser realizado com a função wilcox_test() 1 do pacote rstatix:\n\nteste &lt;- rstatix::wilcox_test(formula = tempohosp ~ infec, data = cirurgia)\nteste\n\n# A tibble: 1 × 7\n  .y.       group1 group2    n1    n2 statistic       p\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tempohosp sim    não       56    88     3280. 0.00083\n\n\nAssim como no cálculo manual, o teste com a função do rstatix, mostra uma diferença estatisticamente significativa (P &lt; 0,001) entre os tempos de hospitalização dos recém-nascidos que realizaram cirurgia no período neonatal que se infectaram ou não.\n\n\n\n17.4.5 Tamanho do efeito\nÉ interessante calcular o tamanho do efeito, a magnitude do efeito. O tamanho do efeito r é calculado como a estatística \\(Z_{U}\\) dividida pela raiz quadrada do tamanho da amostra (\\(n = n_{1} + n_{2}\\)).\n\\[\nr = \\frac {Z_{U}}{\\sqrt{n}}\n\\]\nO valor de \\(Z_{U}\\) é igual a 3.3398648, logo\n\nr &lt;- Z_U/sqrt(n1+n2)\nround(r,3)\n\n[1] 0.278\n\n\nO R possui a função wilcox_effsize() incluída no pacote rstatix. Necessita também do pacote coin (2) instalado para calcular a estatística r.2 A saída exibirá junto a magnitude o efeito, que no caso é pequena (veja Tabela 17.2).\n\nwilcox_effsize(cirurgia, tempohosp~infec)\n\n# A tibble: 1 × 7\n  .y.       group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 tempohosp sim    não      0.279    56    88 small    \n\n\n\n\n\n\nTabela 17.2: Interpretação do valor r\n\n\n\nrmagnitude0,10 &lt; 0,30pequeno0,30 &lt; 0,50médio&gt;= 0,50grandeSem considerar o sinal\n\n\n\n\n\n\n\n17.4.6 Conclusão\nO valor \\(P&lt;0,0001\\) está bem abaixo do nível de significância estabelecido (\\(\\alpha = 0,05)\\). Pode-se concluir que o tempo de hospitalização nos dois grupos é estatisticamente diferente. Entretanto, a magnitude dessa diferença é pequena.\nIsto pode ser visualizado no gráfico (Figura 17.2):\n\nggpubr::ggboxplot(cirurgia,\n                   x = \"infec\",\n                   y = \"tempohosp\",\n                   bxp.errorbar = TRUE,\n                   bxp.errorbar.width = 0.1,\n                   fill = \"infec\",\n                   palette = \"nejm\",\n                   legend = \"none\",\n                   ggtheme = theme_bw(),\n                   xlab = \"Presença de infecção\" ,\n                   ylab = \"Tempo de hospitalização (dias)\") +\n  labs(subtitle = rstatix::get_test_label(teste, detailed = TRUE))\n\n\n\n\n\n\n\nFigura 17.2: Impacto da infecção no tempo de hopsitalização.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#teste-de-wilcoxon",
    "href": "17-testes-naoParametricos.html#teste-de-wilcoxon",
    "title": "17  Métodos não paramétricos",
    "section": "17.5 Teste de Wilcoxon",
    "text": "17.5 Teste de Wilcoxon\nO teste de Wilcoxon, também conhecido como teste dos postos com sinais de Wilcoxon (Wilcoxon Signed-Rank Test), é um teste não paramétrico utilizado em situações em que existem dois conjuntos de dados emparelhados, ou seja, dados provenientes do mesmo participante. O teste não examina os dois grupos individualmente; em vez disso, ele se concentra na diferença existente entre cada par de observações. É um equivalente não paramétrico do teste t pareado.\n\n17.5.1 Dados usados nesta seção\nPata verificar se a realização de exercícios aeróbicos modifica a função respiratória de 10 escolares asmáticos, foi medido o Pico de Fluxo Expiratório Máximo (Peak Flow Meter) no início e no final do programa, após 120 dias. O Pico de Fluxo Expiratório Máximo (PFE) serve como uma forma simples de avaliar a força e a velocidade de saída do ar de dentro dos pulmões. É medido em L/min. Os resultados do estudo tem apenas três variáveis, id, basal e final.\n\nid &lt;- c(1:10)\nbasal &lt;- c(120, 200, 140, 200, 110, 240, 150, 120, 250, 190)\nfinal &lt;- c(220, 300, 230, 180, 300, 330, 230, 250, 300, 200)\n\ndados &lt;- tibble(id, basal, final)\n\nhead (dados)\n\n# A tibble: 6 × 3\n     id basal final\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   120   220\n2     2   200   300\n3     3   140   230\n4     4   200   180\n5     5   110   300\n6     6   240   330\n\n\nA questão de pesquisa a ser respondida, portanto, é:\n\nExiste diferença entre as medidas iniciais e finais do PFE dos escolares asmáticos que entraram em um programa de exercícios aeróbicos?\n\n\n17.5.1.1 Exploração e transformação dos dados\nOs dados estão no formato amplo com as variáveis basal e final classificadas como númericas. Será transformado para o formato longo, usando a função pivot_longer() do pacote tidyr. Este processo é opcional, mas, como foi feito com o teste t pareado, será repetido aqui como treinamento:\n\ndadosL &lt;- dados %&gt;% \n  tidyr::pivot_longer(c(basal, final), \n                      names_to = \"momento\", \n                      values_to = \"medidas\")\nstr(dadosL)\n\ntibble [20 × 3] (S3: tbl_df/tbl/data.frame)\n $ id     : int [1:20] 1 1 2 2 3 3 4 4 5 5 ...\n $ momento: chr [1:20] \"basal\" \"final\" \"basal\" \"final\" ...\n $ medidas: num [1:20] 120 220 200 300 140 230 200 180 110 300 ...\n\n\n\n\n17.5.1.2 Medidas resumidoras\nComo o número de participantes é de apenas 10, a medida de posição mais adequada para resumir os dados é mediana e a medida de dispersão é o intervalo interquartil (IIQ). Para isso, se fará uso das funções group_by() e summarise() do pacote dplyr:\n\nresumo &lt;- dadosL %&gt;% \n  dplyr::group_by(momento) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (medidas, na.rm = TRUE),\n                   p25=quantile(medidas, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(medidas, probs = 0.75, na.rm = TRUE),\n                   media = mean (medidas, na.rm = TRUE),\n                   dp = sd (medidas, na.rm = TRUE),\n                   ep = dp/sqrt(n),\n                   me = ep * qt(1 - (0.05/2), n - 1))\nresumo\n\n# A tibble: 2 × 9\n  momento     n mediana   p25   p75 media    dp    ep    me\n  &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 basal      10     170  125    200   172  50.9  16.1  36.4\n2 final      10     240  222.   300   254  50.4  15.9  36.0\n\n\n\n\n17.5.1.3 Visualização dos dados\nPode-se fazer visualização gráfica dos dados usando um boxplot (Figura 17.3) ou um gráfico de linha (Figura 17.4).\nBoxplot\n\nggpubr::ggboxplot(dadosL,\n                  x = \"momento\",\n                  y = \"medidas\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"momento\",\n                  palette = c(\"cyan4\", \"cyan3\"),\n                  legend = \"none\",\n                  ggtheme = theme_bw(),\n                  xlab = \"Momento\" ,\n                  ylab = \"PEF (L/min) \")+\n  theme (text = element_text (size = 13),\n         axis.text.x= element_text(size = 12)) \n\n\n\n\n\n\n\nFigura 17.3: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos.\n\n\n\n\n\nGráfico de linha\n\nggpubr::ggline(dadosL,\n               x = \"momento\",\n               y = \"medidas\",\n               color = \"cyan4\",\n               linetype = \"dashed\",\n               size = 0.7,\n               add = \"mean_ci\",\n               point.size = 2,\n               xlab = \"Momento\" ,\n               ylab = \"PEF (L/min) \",\n               ggtheme = theme_bw()) +\n  theme (text = element_text (size = 13),\n         axis.text.x= element_text(size = 12))\n\n\n\n\n\n\n\nFigura 17.4: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos.\n\n\n\n\n\n\n\n17.5.1.4 Criação de uma variável que represente a diferença entre os momentos\nA diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados):\n\ndados$D &lt;- dados$basal - dados$final\nhead (dados)\n\n# A tibble: 6 × 4\n     id basal final     D\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1   120   220  -100\n2     2   200   300  -100\n3     3   140   230   -90\n4     4   200   180    20\n5     5   110   300  -190\n6     6   240   330   -90\n\n\nResumo da variável D\nAo resumo será atribuído ao nome resumo2:\n\nresumo2 &lt;- dados %&gt;% \n  dplyr::summarise(n = n (),\n                   mediana = median (D, na.rm = TRUE),\n                   p25=quantile(D, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(D, probs = 0.75, na.rm = TRUE))\nresumo2\n\n# A tibble: 1 × 4\n      n mediana   p25   p75\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    10     -90  -100 -57.5\n\n\nO sinal negativo demonstra que houve um aumento do PFM do momento basal para o final.\n\n\n\n17.5.2 Definição das hipóteses estatísticas\nDa mesma maneira que o teste t pareado, as hipóteses estabelecidas comparam dois grupos dependentes. O teste de Wilcoxon é usado para avaliar a hipótese nula de que a distribuição das diferenças entre os grupos tem uma diferença mediana igual a 0.\n\n\\(H_{0}: D_{i} = 0\\)\n\n\n\\(H_{A}: D_{i} \\ne 0\\)\n\nNote que a \\(H_{A}\\) estabelece que a diferença pode aumentar ou diminuir. Logo, o teste é bicaudal.\n\n\n17.5.3 Execução do teste estatístico\n\n17.5.3.1 Lógica do teste de Wilcoxon\n\nA ideia do teste é verificar se as diferenças positivas são maiores ou menores, em grandeza absoluta, que as diferenças negativas. Para isso, foi criada, anteriormente, a variável D. Agora, será criada outra variável, iguala a variável D, apenas ignorando o sinal, denominada D_abs, diferença absoluta entre as variáveis final e basal.\n\n\ndados$D_abs &lt;- abs(dados$basal - dados$final)\n\n\nExcluir os casos com diferença igual a 0 (zero). Para isso, uma maneira possível é extrair um subconjunto de dados do conjunto principal (dados), criando um conjunto de dados com a função filter() do pacote dplyr, que receberá o nome de dados1. O argumento D_absbs != 0 significa todas as diferenças absolutas diferentes de 0:\n\n\ndados1 &lt;- dados %&gt;% dplyr::filter(D_abs != 0)\n\nObserve que como não há diferenças zeradas. Ou seja, o novo conjunto de dados continua o mesmo. O que pode ser confirmado, executando a função str():\n\nstr (dados1)\n\ntibble [10 × 5] (S3: tbl_df/tbl/data.frame)\n $ id   : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ basal: num [1:10] 120 200 140 200 110 240 150 120 250 190\n $ final: num [1:10] 220 300 230 180 300 330 230 250 300 200\n $ D    : num [1:10] -100 -100 -90 20 -190 -90 -80 -130 -50 -10\n $ D_abs: num [1:10] 100 100 90 20 190 90 80 130 50 10\n\n\n\nOrdenar de forma crescente todos os valores da variável D_abs do banco de dados dados1, usando a função arrange() do pacote dplyr:\n\n\ndados1 &lt;- dados1 %&gt;% dplyr::arrange(dados1$D_abs)\n\n\nEstabelecer postos para os valores ordenados da variável D_abs, do conjunto de dados dados1, fazendo a média das ordens quando houver empate. A execução deste comando cria uma nova variável, chamada postos:\n\n\ndados1$postos &lt;- rank(dados1$D_abs)\n\n\nEstabelecer sinais para os postos, criando dois subconjuntos de dados do conjuntos dados1, um com os escolares com postos positivos (pos) e outros com postos negativos(neg):\n\n\nneg &lt;- dados1 %&gt;% dplyr::filter(D &lt; 0)\npos &lt;- dados1 %&gt;% dplyr::filter(D &gt; 0)\n\n\nSomar todos os postos (variável posto) em cada um dos subconjuntos criados (neg e pos):\n\n\nsoma_neg &lt;- sum(neg$postos)\nsoma_pos &lt;- sum(pos$postos)\nprint (c(soma_neg, soma_pos))\n\n[1] 53  2\n\n\n\nAtribuir a menor soma à estatística do teste, denotada T:\n\n\nT &lt;- min (soma_neg : soma_pos)\nT\n\n[1] 2\n\n\n\nPara dados com tamanhos grandes (&gt; 20 pares), a significância de T pode ser determinada (3), considerando que a distribuição de T tem aproximadamente distribuição normal com média igual a\n\n\\[\n\\mu_{T} =\\frac{n \\times \\left (n + 1  \\right )}{4}\n\\]\nonde n é o tamanho da amostra.\n\nn &lt;- length(dados$D)\nmu_T &lt;- (n * (n + 1))/4\nmu_T\n\n[1] 27.5\n\n\nE desvio padrão igual a:\n\\[\n\\sigma_{T}= \\sqrt {\\frac{n\\left (n + 1  \\right )\\times \\left (2n + 1  \\right )}{24}}\n\\]\n\ndp_T &lt;- sqrt ((n*(n + 1)) * (2 * n + 1) /24) \ndp_T\n\n[1] 9.810708\n\n\nOs resultados da execução das equações fornecem os dados para calcular a estatística Z_T com correção de continuidade e, a partir dela, calcular o valor P.\n\\[\nZ_{T}= \\frac{\\left |T - \\mu_{T}  \\right | - 0,5}{\\sigma_{T}}\n\\]\n\nZ_T &lt;- (abs(T - mu_T)- 0.5)/dp_T\nZ_T\n\n[1] 2.548236\n\n\n\nConcluindo, o valor da estatística de teste T é superior ao \\(Z_{crítico} = 1,96\\), para um \\(\\alpha = 0,05\\). Dessa forma, a \\(H_{0}\\) é rejeitada. Existe uma diferença significativa entre o PFE basal e o PFE final, neste grupo de escolares asmáticos.\nO valor P pode ser obtido com a função pnorm() e multiplicando o resultado por 2, pois o teste é bilateral.\n\n\nP &lt;- pnorm (Z_T, lower.tail = FALSE) * 2\nP\n\n[1] 0.01082692\n\n\nComo já dito anteriormente, na prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Eles são apenas uma demonstração de como o teste funciona.\n\n\n17.5.3.2 Cálculo do teste de Wilcoxon no R\nUsando o conjunto de dados no formato longo (dadosL), calcula-se o teste com a função wilcox_test() do pacote rstatix. É a mesma função utilizada para o teste de U de Mann-Whitney, mudando apenas o argumento paired=FALSE para paired=TRUE:\n\nteste1 &lt;- dadosL %&gt;% \n  rstatix::wilcox_test(medidas ~ momento, paired = TRUE) %&gt;% \n  rstatix::add_significance()\nteste1\n\n# A tibble: 1 × 8\n  .y.     group1 group2    n1    n2 statistic      p p.signif\n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 medidas basal  final     10    10         2 0.0107 *       \n\n\nObserve que o resultado é o mesmo calculado manualmente.\n\n\n\n17.5.4 Tamanho do efeito\nO tamanho do efeito pode ser calculado da mesma forma que para o teste de Mann-Whitney (Seção 17.4.5), usando a mesma equação e os dados obtidos acima, onde 2.548236 e n = 10 tem-se\n\\[\nr = \\frac {Z_{T}}{\\sqrt{n}}\n\\]\nPode-se usar também a função wilcox_effsize() para calcular a estatística r. A Saída exibe junto a magnitude o efeito, que no caso é grande (&gt; 0,5 como mostra a Tabela 17.2 do teste de Mann-Whitney).\n\ndadosL %&gt;% \n  wilcox_effsize(medidas ~ momento, paired = TRUE)\n\n# A tibble: 1 × 7\n  .y.     group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 medidas basal  final    0.823    10    10 large    \n\n\n\n\n17.5.5 Conclusão\nAssumindo um \\(\\alpha = 0,05\\), se o valor P, obtido pelo teste, for menor do que 0,05, rejeita-se a hipótese nula (V = 2, P = 0,01, n = 10).\nPode-se concluir que existe diferença nas medidas do pico de fluxo expiratório máximo no início e no fim do programa de exercícios aeróbicos realizados pelos escolares asmáticos e a magnitude do efeito foi grande (r = 0,82).\nIsto pode ser visualizado na Figura 17.5:\n\nbxp &lt;- ggpubr::ggboxplot(dadosL,\n                         x = \"momento\",\n                         y = \"medidas\",\n                         bxp.errorbar = TRUE,\n                         bxp.errorbar.width = 0.1,\n                         fill = \"momento\",\n                         palette = c(\"cyan4\", \"cyan3\"),\n                         legend = \"none\",\n                         ggtheme = theme_bw(),\n                         xlab = \"Momento\" ,\n                         ylab = \"PEF (L/min) \") +\n  theme (text = element_text (size = 13),\n         axis.text.x = element_text(size = 11))\n\nteste &lt;- dadosL %&gt;% \n  rstatix::wilcox_test (medidas ~ momento, paired = TRUE) %&gt;%\n  rstatix::add_significance ()\nteste &lt;- teste %&gt;% rstatix::add_xy_position ()\n\nbxp + \n  stat_pvalue_manual (teste, \n                      tip.length = 0) +\n  labs (subtitle = get_test_label (stat.test = teste, \n                                   detailed = TRUE))\n\n\n\n\n\n\n\nFigura 17.5: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#sec-kuskalwallis",
    "href": "17-testes-naoParametricos.html#sec-kuskalwallis",
    "title": "17  Métodos não paramétricos",
    "section": "17.6 Teste de Kruskal-Wallis",
    "text": "17.6 Teste de Kruskal-Wallis\nQuando os pressupostos subjacentes à ANOVA não são atendidos, é possível usar o teste não paramétrico de Kruskal-Wallis (KW) para testar a hipótese de que os parâmetros de localização são iguais. Pode ser considerado uma extensão do teste de Wilcoxon-Mann-Whitney.\nEnquanto a ANOVA depende da hipótese de que todas as populações são independentes e normalmente distribuídas, o teste de Kruskal-Wallis exige apenas amostras aleatórias independentes provenientes de suas respectivas populações. Entretanto, este teste somente deve ser aplicado se a amostra for pequena e/ou os pressupostos para a ANOVA forem seriamente violados.\nO teste não usa diretamente medições de quantidade conhecida, utiliza, como outros testes não paramétricos, os postos dos valores analisados. Em função disso, é também conhecido como análise de variância de um fator em postos.\n\n17.6.1 Dados usados nesta seção\nUm experimento foi realizado para verificar se o álcool ou o café afetam os tempos de reação ao dirigir (4). O estudo tem três grupos diferentes de participantes: 10 bebendo água (controle), 10 bebendo cerveja contendo duas unidades de álcool e 10 bebendo café. O tempo de reação em uma simulação de direção foi medido para cada participante.\nOs dados encontram-se no arquivo dadosResposta.xlsx. Clique aqui para baixar e, após, salve o mesmo no seu diretório de trabalho.\nAs variáveis são:\n\nid \\(\\to\\) identificação do participante;\ntempo \\(\\to\\) tempo de reação na simulação de direção em segundos;\nbebida \\(\\to\\) três grupo: água, álcool e café.\n\nO estudo pretende verificar se existe diferença no tempo de reação dos participantes em um teste de direção com a ingesta de água, café e álcool.\n\n17.6.1.1 Leitura e exploração dos dados\nComo o dados estão contidos em um arquivo Excel (.xlsx), serão lidos com a função read_excel() do pacote readxl e a sua estrutura será observada com função str():\n\ndados &lt;- read_excel (\"dados/dadosResposta.xlsx\")\nstr(dados)\n\ntibble [30 × 3] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ...\n $ bebida: chr [1:30] \"agua\" \"agua\" \"agua\" \"agua\" ...\n\n\nO formato do arquivo é o longo. A variável bebida encontra-se como caracter e deve ser transformada em fator e as categorias na sequência: agua, cafe e alcool.\n\ndados$bebida &lt;- factor(dados$bebida, \n                       levels = c(\"agua\", \"cafe\", \"alcool\"))\n\nOs dados serão observados visualmente através de boxplots (Figura 17.6), usando a função ggplot() do pacote ggplot2, com cores do nejm (New England Journal of Medicine) o pacote ggsci.\n\nggpubr::ggboxplot(dados,\n                   x = \"bebida\",\n                   y = \"tempo\",\n                   bxp.errorbar = T,\n                   bxp.errorbar.width = 0.1,\n                   fill = \"bebida\",\n                   palette = \"nejm\",\n                   legend = \"none\",\n                   ggtheme = theme_bw(),\n                   xlab = \"Tipo de bebida\" ,\n                   ylab = \"Tempo de reação (seg)\") +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\nFigura 17.6: Impacto do tipo de bebida no tempo de reação ao dirigir.\n\n\n\n\n\nOs boxplots exibem dados com medianas visualmente diferentes, bigodes diferentes e grupos com presença de outliers. Para verificar o impacto desses achados, pode-se usar a função identify_outliers(), do pacote rstatix que confirma, na sua Saída, a presença de outliers no grupo agua e cafe, sendo dois extremos.\n\ndados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  rstatix::identify_outliers(tempo)\n\n# A tibble: 4 × 5\n  bebida    id tempo is.outlier is.extreme\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 agua       9  1.63 TRUE       FALSE     \n2 agua      10  1.97 TRUE       TRUE      \n3 cafe      19  2.56 TRUE       FALSE     \n4 cafe      20  3.07 TRUE       TRUE      \n\n\nPara avaliar a normalidade será usado o teste de Shapiro-Wilk, com a função shapiro_test() e a função group_by() do pacote dplyr:\n\ndados %&gt;% \n  dplyr::group_by (bebida) %&gt;% \n  rstatix::shapiro_test (tempo) \n\n# A tibble: 3 × 4\n  bebida variable statistic      p\n  &lt;fct&gt;  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 agua   tempo        0.863 0.0837\n2 cafe   tempo        0.815 0.0220\n3 alcool tempo        0.875 0.114 \n\n\nA variável cafe tem uma distribuição que não se ajusta a distribuição normal.\nPara completar a exploração dos dados, será solicitado, usando as funções group_by () e summarise, do pacote dplyr, medidas de localização e dispersão adquadas para variáveis bem assimétricas.\n\nresumo &lt;- dados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  dplyr::summarise(n = n(),\n                   mediana = median (tempo, na.rm = TRUE),\n                   p25=quantile(tempo, probs = 0.25, na.rm = TRUE),\n                   p75=quantile(tempo, probs = 0.75, na.rm = TRUE))\nresumo\n\n# A tibble: 3 × 5\n  bebida     n mediana   p25   p75\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 agua      10   0.845 0.653 0.937\n2 cafe      10   1.44  1.28  1.68 \n3 alcool    10   2.25  1.77  2.85 \n\n\n\n\n\n17.6.2 Hipóteses estatísticas\nSe não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma,\n\n\\(H_{0}\\): As populações são iguais.\n\n\n\\(H_{1}\\): Pelo menos uma das populações tende a exibir valores diferentes do que as outras populações.\n\n\n\n17.6.3 Pressupostos do teste\nO teste de Kruskal-Wallis pressupõe as seguintes condições para o seu adequado uso:\n\nAs amostras são amostras aleatórias independentes de suas respectivas populações;\nA escala de medição utilizada é pelo menos ordinal e, se houver apenas três grupos, deve haver pelo menos 5 casos em cada grupo;\nAs distribuições dos valores nas populações amostradas são idênticas, exceto pela possibilidade de que uma ou mais das populações sejam compostas por valores que tendem a ser maiores do que os das outras populações.\n\n\n\n17.6.4 Execução do teste estatístico\n\n17.6.4.1 Lógica do teste de Kruskall-Wallis\nA teoria do teste Kruskal-Wallis é semelhante à do teste de Mann-Whitney, ou seja, tem como base a soma dos postos. Em primeiro lugar, os escores são ordenados do menor para o maior, independentemente do grupo que pertençam.\nO menor recebe o posto 1 e assim por diante. Após a atribuição dos postos, soma-se os postos por grupo. A soma dos postos de cada grupo é representada por \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\), …, \\(R_{i}\\). A estatística do teste, H, é calculada com a equação (5):\n\\[\nH =\\frac {12}{N \\times \\left (N + 1  \\right )} \\sum_{i=1}^{k} \\frac {R_{i}^{2}}{n_{{i}}}-3 \\times\\left (N + 1\\right)\n\\]\nonde \\(n_{i}\\) é o número de observações no grupo i, \\(N = \\sum_{i=1}^{k}\\times n_{i}\\) (o número total de observações em todos os k grupos) e \\(R_{i}\\) é a soma dos postos das \\(n_{i}\\) observações no grupo i.\nUma boa verificação (mas não uma garantia) de que os postos foram atribuídos corretamente é ver se a soma de todos os postos é igual a \\(\\frac {N \\times \\left (N + 1\\right )}{2}\\).\n\nCriar a variável posto com os postos ordenados de forma crescente, independente do grupo, como realizado no teste de Mann-Whitney:\n\n\ndados$posto &lt;- rank(dados$tempo, ties.method = \"average\")\nstr(dados)\n\ntibble [30 × 4] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ...\n $ bebida: Factor w/ 3 levels \"agua\",\"cafe\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ posto : num [1:30] 1 2 3 4 5 6 7 8 16 22.5 ...\n\n\n\nSomar os postos de cada grupo separadamente:\n\n\nresumo1 &lt;- dados %&gt;% \n  dplyr::group_by(bebida) %&gt;% \n  dplyr::summarise(n = n(),\n                   soma = sum(posto))\nresumo1\n\n# A tibble: 3 × 3\n  bebida     n  soma\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 agua      10  74.5\n2 cafe      10 157  \n3 alcool    10 234. \n\n\n\nCálculo da estatística do teste H\n\n\nN &lt;- 30\nn &lt;- 10\nR_agua &lt;- resumo1[1,3]\nR_alcool &lt;- resumo1[2,3]\nR_cafe &lt;- resumo1[3,3]\n\nH &lt;- (12/(N*(N+1))) * ((R_agua^2/n) + (R_alcool^2/n) + (R_cafe^2/n)) - (3*(N+1))\nH\n\n      soma\n1 16.31806\n\n\n\nCálculo do Valor P\n\nSe existir três grupos, com cinco ou menos participantes em cada grupo, há necessidade de usar a tabela especial para tamanhos de amostra pequenos (6). Se você tiver mais de cinco participantes por grupo, trate H como qui-quadrado. A estatística H é estatisticamente significativo se for igual ou maior que o valor crítico qui-quadrado para o grau de liberdade específico, igual a \\(k - 1\\). Aqui, tem-se 10 participantes por grupo e, assumindo um \\(\\alpha = 0,05\\), o \\(H_{crítico}\\) é igual a:\n\nalpha &lt;- 0.05\nk &lt;- 3\ngl = k - 1\nH_critico &lt;- qchisq(1 - alpha, gl)\nH_critico\n\n[1] 5.991465\n\n\nUma vez que o \\(H_{calculado} = 16,3\\) é maior que \\(H_{crítico} = 6,0\\) , rejeita-se a \\(H_{0}\\). O valor P é obtido através da função pchisq():\n\nH &lt;- 16.32\npchisq(H, 2, lower.tail = FALSE)\n\n[1] 0.0002858624\n\n\nO R tem funções que fazem facilmente esses cálculos enfadonhos. Eles são colocados aqui apenas para ilustrar o raciocínio de como o teste de Kruskal-Wallis funciona. Sempre existem curiosos lendo o livro!\n\n\n17.6.4.2 Teste de Kruskal-Wallis no R\nNo R, pode-se calcular o teste, usando a função kruskal_test() do pacote rstatix, cujos argumentos podem ser consultados na ajuda do RStudio.\n\nteste &lt;- rstatix::kruskal_test (data = dados, formula = tempo ~ bebida)\nteste\n\n# A tibble: 1 × 6\n  .y.       n statistic    df        p method        \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 tempo    30      16.3     2 0.000286 Kruskal-Wallis\n\n\n\n\n\n17.6.5 Tamanho do efeito\nO eta quadrado (\\(\\eta^{2}\\)), com base na estatística H, pode ser usado como a medida do tamanho do efeito do teste de Kruskal-Wallis. É calculado pela equação:\n\\[\n\\eta_{H}^{2} = \\frac {\\left (H - k + 1 \\right)}{\\left (N - k\\right)}\n\\]\nonde H é a estatística obtida no teste de Kruskal-Wallis; k é o número de grupos; N é o número total de observações (7).\nA estimativa eta ao quadrado assume valores de 0 a 1 e, multiplicada por 100, indica a porcentagem de variância na variável dependente explicada pela variável independente. Pode ser obtido no R com a função kruskal_effsize() do pacote rstatix:\n\ndados %&gt;% kruskal_effsize (tempo~bebida)\n\n# A tibble: 1 × 5\n  .y.       n effsize method  magnitude\n* &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;ord&gt;    \n1 tempo    30   0.530 eta2[H] large    \n\n\nUm efeito \\(\\ge 0,14\\) é considerado grande e \\(&lt;0,06\\) é pequeno (8).\n\n\n17.6.6 Testes post hoc\nA partir do resultado do teste de Kruskal-Wallis, sabe-se que há uma diferença significativa entre os grupos, mas não se sabe quais pares de grupos são diferentes.\nUm teste de Kruskal-Wallis significativo é geralmente seguido pelo teste de Dunn (9) para identificar quais grupos são diferentes.\nPara realizar as múltiplas comparações, no R, pode ser usada a função dunn_test(), incluído no pacote rstatix. O ajuste de P é feito pelo método de Bonferroni:\n\npwc &lt;- dados %&gt;% \n  dunn_test (tempo ~ bebida, p.adjust.method = \"bonferroni\") \npwc\n\n# A tibble: 3 × 9\n  .y.   group1 group2    n1    n2 statistic         p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 tempo agua   cafe      10    10      2.10 0.0361    0.108    ns          \n2 tempo agua   alcool    10    10      4.04 0.0000537 0.000161 ***         \n3 tempo cafe   alcool    10    10      1.94 0.0520    0.156    ns          \n\n\nA saída do teste de Dunn, mostra que existe uma diferença estatisticamente significativa apenas entre a água e o álcool, valor P ajustado igual a 1.61^{-4}.\n\n\n17.6.7 Conclusão\nUm teste de Kruskal-Wallis foi realizado para comparar os tempos de reação em uma simulação de direção após beber água, café ou álcool. Houve evidência de uma diferença (P = 0,00029) de pelo menos um par de grupos (Figura 17.7).\nO teste de comparações de pares, usando o teste de Dunn, foi realizado para os três pares de grupos. Houve evidencia de diferença entre o grupo que consumiu duas unidades de álcool e o grupo que ingeriu água (P ajustado (Bonferroni) = 0,00016). Entre os demais pares não houve diferença significativa. O tempo mediano de reação para o grupo que recebeu água foi de 0,84 (0,65 – 0,94) segundos, em comparação com 2,25(1,77 – 2,85) segundos no grupo que bebeu cerveja equivalente a duas unidades de álcool, enquanto para o café foi de 1,45(1,28 – 1,69) segundos.\n\npwc &lt;- pwc %&gt;% rstatix::add_xy_position(x= \"bebida\")\n\nggpubr::ggboxplot(dados,\n                  x = \"bebida\",\n                  y = \"tempo\",\n                  bxp.errorbar = TRUE,\n                  bxp.errorbar.width = 0.1,\n                  fill = \"bebida\",\n                  palette = \"nejm\",\n                  legend = \"none\",\n                  ggtheme = theme_bw())+\n  ggpubr::stat_pvalue_manual (pwc,\n                              label = \"p = {scales::pvalue(p.adj)}\",\n                              label.size = 3.2,\n                              hide.ns = FALSE) +\n  ggplot2::labs(x = \"Tipo de bebida\", \n                y = \"Tempo de reação (seg)\",\n                subtitle = get_test_label (teste, detailed = TRUE),\n                caption = get_pwc_label(pwc))\n\n\n\n\n\n\n\nFigura 17.7: Impacto do tipo de bebida no tempo de reação ao dirigir.\n\n\n\n\n\n\n\n\n\n1. Altman DG. Comparing groups: continuos data. Em: Practical Statistics for Medical Research. London: Chapman & Hall/CRC; 1991. p. 194–7. \n\n\n2. Hothorn T, Hornik K, Van De Wiel MA, Zeileis A. A lego system for conditional inference. The American Statistician. 2006;60(3):257–63. \n\n\n3. Zar JH. Paired-Sample Hypotheses. Em: Biostatistical Analysis. Edinburgh: Pearson; 2014. p. 189–98. \n\n\n4. Karadimitriou SM, Marshall E. Kruskal-Wallis in R [Internet]. Statistics Support for Students. Loughborough; Coventry Universities; 2020. Disponível em: https://www.statstutor.ac.uk/\n\n\n5. Zar JH. Nonparametric Analysis of Variance. Em: Biostatistical Analysis. Edinburgh: Pearson; 2014. p. 226–30. \n\n\n6. Kanji GK. The Kruskal–Wallis test. Em: 100 Statiscal Tests. 3rd Edition. London: Sage publications; 2006. p. 220. \n\n\n7. Tomczak M, Tomczak E. The need to report effect size estimates revisited. An overview of some recommended measures of effect size. Trends in sport sciences. 2014;1(21):19–25. \n\n\n8. Watson P. Rules of thumb on magnitudes of effect sizes [Internet]. MRC Cognition and Brain Sciences Unit. Cambridge University; 2021. Disponível em: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n9. Dunn OJ. Multiple comparisons using rank sums. Technometrics. 1964;6(3):241–52.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "17-testes-naoParametricos.html#footnotes",
    "href": "17-testes-naoParametricos.html#footnotes",
    "title": "17  Métodos não paramétricos",
    "section": "",
    "text": "O cálculo pode também ser realizado, usando a função wilcox.test() do pacote stats, incluído no R base.↩︎\nPara maiores detalhes consulte a ajuda da função.↩︎",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Métodos não paramétricos</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html",
    "href": "18-epidemiologia.html",
    "title": "18  Estatística em Epidemiologia",
    "section": "",
    "text": "18.1 Pacotes necessários\npacman::p_load(BiocManager,\n               car,\n               caret,\n               DescTools,\n               dplyr,\n               epiR,\n               epitools,\n               flextable,\n               ggeffects,\n               ggplot2,\n               ggpubr,\n               ggsci,\n               kableExtra,\n               knitr,\n               limma,\n               mice,\n               MKmisc,\n               performance,\n               pROC,\n               readxl,\n               sjPlot,\n               survival,\n               survminer,\n               vcd)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#sec-diagbayes",
    "href": "18-epidemiologia.html#sec-diagbayes",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.2 Raciocínio bayesiano no diagnóstico médico",
    "text": "18.2 Raciocínio bayesiano no diagnóstico médico\nO processo diagnóstico é o centro da atenção da atividade médica na busca de reduzir as incertezas e reconhecer a que classe pertence determinado paciente. Portanto, é extremamente importante saber quão bem os testes diagnósticos podem prever que um indivíduo é portador de certa condição ou doença. Entende-se aqui como teste diagnóstico todo o processo diagnótico, desde o exame clínico até o mais sofisticado exame de imagem ou laboratorial. A ideia é saber como o teste diagnóstico se comporta para separar um “doente” e um “não doente”; qual a sua validade neste processo?\nDeve-se sempre ter em mente que o estabelecimento do diagnóstico é um processo imperfeito que resulta em uma probabilidade ao invés de uma certeza de estar correto. Ou seja, cada vez mais os médicos têm que aplicar as leis da probabilidade na avaliação de testes diagnósticos e sinais clínicos.\nA abordagem bayesiana denomina de probabilidade a priori a probabilidade estabelecida inicialmente, baseada apenas na experiência do médico, em seu conhecimento em relação a doença suspeitada. Diante de uma evidência de doença, pode ser solictado um teste diagnóstico. Quando ele recebe um teste positivo para uma doença, a probabilidade muda, passa a ser uma probabilidade condicional, probabilidade da doença dado que o teste é positivo, denominada probabilidade a posteriori.\nUm teste que define corretamente quem é doente e quem não é doente é denominado de padrão-ouro ou padrão de referência. Algumas vezes, o teste padrão de referência é simples e barato. Outras vezes, é caro, difícil de obter, tecnicamente complexo, arriscado ou pouco prático. Inclusive, pode não hver padrão-ouro. Em função dessas limitações, outros testes são usados e, como consequência, podem ocorrer erros. Em outras palavras, no processo diagnóstico podem ocorrer falsos positivos e falsos negativos.\nEsta incerteza, na utilização de testes diagnósticos, gera a necessidade de o médico conferir a probabilidade de falsos positivos e falsos negativos na elaboração de um diagnóstico ao receber o resultado positivo ou negativo de um exame. Uma maneira simples de mostrar as relações de um teste diagnóstico e o verdadeiro diagnóstico, é mostrada na tabela de contingência \\(2\\times2\\) (Tabela 18.1).\n\n\n\n\nTabela 18.1: Falsos positivos e falsos negativos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.2.1 Sensibilidade e Especificidade\nAs estatísticas mais utilizadas para descrever a validade dos testes de diagnóstico em contextos clínicos são sensibilidade e a especificidade.\nSensibilidade é a habilidade do teste em identificar corretamente quem tem a doença. É a taxa de verdadeiros positivos (VP) de um teste e corresponde a probabilidade de um indivíduo com a doença ter um teste positivo.\nUm teste sensível raramente deixará passar pessoas que tenham a doença. Testes com sensibilidade alta são úteis para excluir a presença de uma doença. Isto é, um teste negativo exclui virtualmente a possibilidade de o paciente ter a doença de interesse, pois tem pouca probabilidade de produzir resultados falsos negativos. Isto pode ser lembrado pelo mnemônico SnNout, do inglês: High Sensivity, a Negative result rules out the diagnosis (1).\nEspecificidade é a habilidade do teste em identificar corretamente quem não tem a doença. É a taxa de verdadeiros negativos (VN) de um teste e corresponde a probabilidade de um indivíduo sem a doença ter um teste negativo. Um teste específico raramente classificará de forma errônea indivíduos sendo portadores da doença quando eles não são. Os testes muito específicos são usados para confirmar a presença da doença. Se o teste é altamente específico, um teste positivo sugere fortemente a presença da doença de interesse.\nDe forma similar que a sensibilidade pode-se usar o mnemônico SpPin, do inglês: High Specificity, a Positive result rules in the diagnosis (1).\nEstas estatísticas de diagnóstico podem ser calculadas a partir das equações, cujas letras representam as caselas da tabela \\(2 \\times 2\\) da Tabela 18.1;\n\\[\nSensibilidade = \\frac {a}{\\left (a + c\\right )} \\quad \\quad Especificidade = \\frac {d}{\\left (b + d\\right )}\n\\]\nA taxa de falsos negativos (TFN) é a proporção de indivíduos que têm a doença e que têm um resultado de teste negativo e a taxa de falsos positivos (TFP) é a proporção de pacientes que não possuem a doença e que apresentam resultados positivos. Podem ser expressas pelas equações:\n\\[\nTFN= \\frac {c}{\\left (a + c\\right )} \\quad ou \\quad \\left (1 - sensibilidade\\right)\n\\]\n\\[\nTFP= \\frac {b}{\\left (b + d\\right )} \\quad ou \\quad \\left (1 - especifcidade\\right)\n\\]\nIdealmente, um teste de diagnóstico deveria ter altos níveis de sensibilidade e especificidade. No entanto, isso não é possível, pois existe um balanço entre sensibilidade e especificidade. À medida que a especificidade aumenta, a sensibilidade diminui e vice-versa. As curvas ROC podem ser usadas para identificar um ponto de corte em uma medição contínua que maximize a sensibilidade e a especificidade (veja a Seção 18.2.4).\nQuando um clínico tem um paciente cujo teste apresentou resultado positivo, a pergunta mais importante é a seguinte: dado que o teste é positivo, qual é a probabilidade de o paciente ter a doença? A sensibilidade do teste não responde a este questionamento, mas sim a probabilidade de um resultado positivo, dado que o paciente tem a doença (2).\n\n18.2.1.1 Exemplo\nO conjunto de dados dadosApendicite.xlsx contém informações de 156 pacientes que realizaram ultrassonografia abdominal para o diagnóstico de apendicite aguda.Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho.\nForam avaliados pacientes com diagnóstico clínico de apendicite aguda, submetidos à ultrassonografia abdominal e apendicectomia laparoscópica, acompanhado de estudo anatomopatológico dos apêndices extirpados (3). Será avaliado o teste diagnóstico usado.\nLeitura e observação do conjunto de dados\nSerá usado a função read_excel()do pacote readxl para ler os dados e a função str() para observar a sua estrutura:\n\ndados &lt;- readxl::read_excel (\"dados/dadosApendicite.xlsx\")\nstr(dados)\n\ntibble [156 × 3] (S3: tbl_df/tbl/data.frame)\n $ id        : num [1:156] 1 2 3 4 5 6 7 8 9 10 ...\n $ apendicite: num [1:156] 1 1 1 1 1 1 1 1 1 1 ...\n $ eco       : num [1:156] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAs variáveis apendicite e eco foram exibidas como variáveis numéricas e serão transformadas em fatores:\n\ndados$apendicite &lt;- factor(dados$apendicite,\n                           levels = c(1,2),\n                           labels = c(\"Presente\", \n                                      \"Ausente\"))\n\ndados$eco &lt;- factor(dados$eco,\n                    levels = c(1,2),\n                    labels = c(\"Positivo\", \n                               \"Negativo\"))\n\nConstrução de uma tabela de contingência 2$$2\n\ntab_ap &lt;- with(dados, table(eco, apendicite, dnn = c (\"Eco\", \"Apendicite\")))\naddmargins(tab_ap, FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: Eco\n2: Apendicite\n\n\n          Apendicite\nEco        Presente Ausente sum\n  Positivo       85       7  92\n  Negativo       46      18  64\n  sum           131      25 156\n\n\nCálculo da sensibilidade e da especificidade\nPode-se usar a função epi.tests() do pacote epiR (4) que calcula, junto com os intervalos de confiança exatos, a prevalência aparente e verdadeira, sensibilidade, especificidade, valores preditivos positivos e negativos e razões de probabilidade positivas e negativas a partir de dados de contagem fornecidos em uma tabela \\(2\\times2\\). Utiliza os argumentos\n\ndat   dados sob a forma de vetor ou matriz\n\nconf.level   magnitude do intervalode confiança, entre 0 e 1.\n\nOs resultados serão atribuídos a um objeto de nome diag:\n\ndiag &lt;- epiR::epi.tests(tab_ap, \n                        conf.level = 0.95)\nprint(diag)\n\n          Outcome +    Outcome -      Total\nTest +           85            7         92\nTest -           46           18         64\nTotal           131           25        156\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.59 (0.51, 0.67)\nTrue prevalence *                      0.84 (0.77, 0.89)\nSensitivity *                          0.65 (0.56, 0.73)\nSpecificity *                          0.72 (0.51, 0.88)\nPositive predictive value *            0.92 (0.85, 0.97)\nNegative predictive value *            0.28 (0.18, 0.41)\nPositive likelihood ratio              2.32 (1.22, 4.40)\nNegative likelihood ratio              0.49 (0.35, 0.68)\nFalse T+ proportion for true D- *      0.28 (0.12, 0.49)\nFalse T- proportion for true D+ *      0.35 (0.27, 0.44)\nFalse T+ proportion for T+ *           0.08 (0.03, 0.15)\nFalse T- proportion for T- *           0.72 (0.59, 0.82)\nCorrectly classified proportion *      0.66 (0.58, 0.73)\n--------------------------------------------------------------\n* Exact CIs\n\n\nAssim, a sensibilidade é igual a 65% (IC95%: 56 – 73%) e a especificidade é igual a 72% (IC95%: 51 – 88%). Isto significa que um indivíduo com apendicite aguda tem 65% de probabilidade de ter uma ecografia alterada; um indivíduo sem apendicite aguda tem 72% de probabilidade de ter uma ecografia normal. O objetivo do teste de diagnóstico é usá-lo para fazer um diagnóstico, então há necessidade de saber a probabilidade que o teste fornece para um diagnóstico correto. A sensibilidade e a especificidade não fornecem esta informação. Para atingir esse objetivo, usa-se o valor preditivo (5).\n\n\n\n18.2.2 Valor Preditivo\nO propósito de um teste diagnóstico é usar seus resultados para fazer um diagnóstico, portanto, é necessário conhecer a probabilidade de que o resultado do teste forneça o diagnóstico correto (5).\nOs valores preditivos positivo e negativo descrevem a probabilidade de um paciente ter doença, uma vez que os resultados de seus testes são conhecidos.\nO valor preditivo positivo (VPP) de um teste é definido como a proporção de pessoas com um resultado de teste positivo que realmente têm a doença.\nO valor preditivo negativo (VPN) é a proporção de pacientes com resultados de teste negativos que não têm doença.\nComo a sensibilidade e a especificidade, estas estatísticas de diagnóstico também podem ser calculadas a partir da tabela \\(2\\times2\\), mostrada no início:\n\\[\nVPP = \\frac {a}{\\left (a + b\\right )} \\quad \\quad VPN = \\frac {d}{\\left (c + d\\right )}\n\\]\nObservando os resultados anteriores da função epi.tests(), verifica-se que 92% (85/92) dos indivíduos que tiveram teste positivo (ultrassonografia alterada) tinham doença (apendicite aguda). Isso significa que seu VPP é igual a 92% (IC95%: 18 – 41%), ou dito de outra forma, uma pessoa com ultrassonografia positiva tem 92% de probabilidade de ter a apendicite aguda. O VPP é também conhecido como probabilidade pós-teste de doença dado um teste positivo.\nDos 64 pacientes que tiveram ultrassonografia sem alterações, 18 não apresentaram apendicite aguda, portanto, um VPN de 28% (IC95%: 56 – 73%). Isso significa que uma pessoa quem tem um teste negativo tem 28,1% de probabilidade de não ter apendicite aguda.\nEntretanto, essas proporções são de validade limitada. Os valores preditivos de um teste, na prática clínica, dependem criticamente da prevalência da anormalidade nos pacientes testados. No estudo, a prevalência de apendicite aguda é igual a\n\\[\n\\frac {total\\ de\\ casos\\ de \\ apendicite \\ aguda}{total\\ de\\ casos\\ no\\ estudo} = \\frac {131}{156} = 0,84\\ ou\\ 84\\% \\left(IC_{95\\%}:77\\ a\\ 89\\%\\right)\n\\]\nLevando-se em consideração que a prevalência de apendicite aguda na população é de 7% (6), mantendo a sensibilidade (64%) e a especificidade (72%) da ultrassonografia, entre 156 pacientes, selecionados aleatoriamente, se esperaria encontrar aproximadamente 11 casos (7% de 156) de apendicite aguda. Para facilitar a compreensão, observe a a tabela \\(2\\times2\\) (Tabela 18.2):\n\n\n\n\nTabela 18.2: Prevalencia e valor preditivo\n\n\n\n\n\n\n\n\n\n\n\n\n\nO VPP e o VPN são iguais a:\n\na &lt;- 7\nb &lt;- 41\nc &lt;- 4\nd &lt;- 104\nvpp = a/(a + b)\nround(vpp, 3)*100\n\n[1] 14.6\n\nvpn = d/(c + d)\nround(vpn, 3)*100\n\n[1] 96.3\n\n\nAo se comparar o VPP obtido, agora, com o VPP do estudo, observa-se que o mesmo diminuiu bastante, de 92% para 14,6%. O contrário ocorre com a VPN que aumenta substancialmente de 28% para 96,3%, mostrando claramente a influência da prevalência.\nSe a prevalência diminui, o VPP diminui e o VPN aumenta. Portanto, será errado aplicar diretamente os valores preditivos publicados de um teste ao seu pacciente, quando a prevalência da doença em sua população for diferente da prevalência da doença na população em que o estudo publicado foi realizado. Um teste pode ser útil em um lugar e não ter validade em outro onde a prevalência é muito baixa.\nPode-se chegar aos mesmos resultados, usando as equações:\n\\[\nVPP =\\frac{sens \\times prev}{\\left(sens \\times prev\\right) + \\left [\\left (1- espec\\right) \\times \\left (1- prev\\right)\\right ]}\n\\]\n\\[\nVPN =\\frac{espec\\times \\left (1- prev\\right)}{\\left[\\left (1 - sens \\right)\\times prev\\right]+\\left[espec\\times \\left (1 - prev\\right)\\right]}\n\\]\nA prevalência pode ser interpretada como a probabilidade antes da realização do teste, conhecida como probabilidade pré-teste. A diferença entre as probabilidades pré e pós-teste é uma forma de avaliar a utilidade do teste. Esta diferença pode ser mensurada pela razão de probabilidade (likelihood ratio).\n\n\n18.2.3 Razão de Probabilidade\nA Razão de Probabilidades (likelihood ratio) é uma forma alternativa de descrever o desempenho de um teste diagnóstico. Alguns autores a denominam de razão de verossimilhança 1.\nA razão de probabilidades para um resultado de teste é definida como a razão entre a probabilidade de observar aquele resultado em indivíduos com a doença em questão e a probabilidade desse resultado em indivíduos sem a doença (7).\nRazões de probabilidade são, clinicamente, mais úteis do que sensibilidade e especificidade. Fornecem um resumo de quantas vezes mais (ou menos) a probabilidade de os indivíduos com a doença apresentarem aquele resultado específico do que os indivíduos sem a doença, e também podem ser usados para calcular a probabilidade de doença para pacientes individuais (8). Cada vez mais as razões de probabilidade estão se tornando populares para relatar a utilidade dos testes de diagnóstico.\nQuando os resultados do teste são relatados como sendo positivos ou negativos, dois tipos de razões de probabilidades podem ser descritos, a razão de probabilidades para um teste positivo (denotada LR +) e a razão de probabilidades para um teste negativo (denotada LR−).\nA razão de probabilidades para um teste positivo é definida como a probabilidade de um indivíduo com doença ter um teste positivo dividida pela probabilidade de um indivíduo sem doença ter um teste positivo. A fórmula para calcular LR + é\nOu seja,\n\\[\nLR(+)=\\frac{sensibilidade}{1 - especificidade}\n\\]\nRazão de probabilidades positiva maior que 1 significa que um teste positivo tem mais probabilidade de ocorrer em pessoas com a doença do que em pessoas sem a doença. De um modo geral, para os indivíduos que apresentam um resultado positivo, LR (+) &gt; 10 aumenta significativamente a probabilidade de doença (“confirma” a doença), enquanto LR (+) &lt; 0,1, virtualmente, exclui a probabilidade de uma pessoa ter a doença (9).\nUsando os dados incluídos no objeto diag, obtido com a função epi.tests() do pacote epiR, tem-se que a LR (+) da ultrassonografia para o diagnóstico de apendicite aguda é igual 2.32 (IC95%: 1,22 – 4,40). Significa que uma pessoa com apendicite aguda tem cerca de 2,32 vezes mais probabilidade de ter um teste positivo do que uma pessoa que não tem a doença.\nA razão de probabilidade negativa é definida como a probabilidade de um indivíduo com doença ter um teste negativo dividido pela probabilidade de um indivíduo sem doença ter um teste negativo. A fórmula para calcular a LR− é:\nOu seja,\n\\[\nLR(-)=\\frac{sensibilidade}{1-especificidade}\n\\]\nRazão de probabilidade negativa menor que 1 significa que um teste negativo é menos provável de ocorrer em pessoas com a doença do que em pessoas sem a doença. Um LR muito baixo (abaixo de 0,1) praticamente exclui a chance de que uma pessoa tenha a doença (9).\nVoltando aos dados anteriores, a LR (-) para a ultrassonografia é igual a 0.49 (IC95%: 0.35 - 0.68). Significa que a probabilidade de ter um teste negativo para indivíduos com doença é 0,49 vezes ou cerca de metade daqueles sem a doença. Dito de outra forma, os indivíduos sem a doença têm cerca o dobro probabilidade de ter um teste negativo do que os indivíduos com a doença.\n\n18.2.3.1 Estimando a probabilidade de doença\nUma grande vantagem das razões de probabilidade é que elas podem ser usadas para ajudar o médico a adaptar a sensibilidade e a especificidade dos testes aos pacientes individuais. Ao se atender um paciente em uma clínica, pode-se decidir realizar um teste específico, após uma anamnese e um exame físico. A decisão de fazer o teste baseia-se nos sintomas e sinais do paciente e na experiência pessoal. Existe suspeita de um determinado diagnóstico e o objetivo é excluir ou confirmar esse diagnóstico. Antes de solicitar o teste, geralmente existe uma estimativa aproximada da probabilidade do paciente de ter essa doença, conhecida como probabilidade pré-teste ou a priori, que geralmente é estimada com base na experiência pessoal do médico, dados de prevalência local e publicações científicas.\nA razão mais importante pela qual um teste é realizado é tentar modificar a probabilidade de doença. Um teste positivo pode aumentar a probabilidade pós-teste e um teste negativo pode reduzir essa probabilidade. A probabilidade pós-teste de doença é o que mais interessa aos médicos e pacientes, pois isso pode ajudar a decidir se devem confirmar, descartar um diagnóstico ou realizar outros testes.\nOs resultados dos testes clínicos são geralmente usados não para fazer ou excluir categoricamente um diagnóstico, mas para modificar a probabilidade do pré-teste a fim de gerar a probabilidade do pós-teste. O teorema de Bayes é uma relação matemática que permite estimar a probabilidade pós-teste.\nPara se compreender este conceito, é importante entender a diferença entre probabilidade e odds (10).\nProbabilidade é a proporção de pessoas que apresentam uma determinada característica (teste positivo, sinal clínico).\nOdds (chance) representa a razão entre duas características complementares, ou seja, a probabilidade de um evento dividido pela probabilidade do não evento (1 – evento). Ambos contêm as mesmas informações de maneiras diferentes. Por exemplo, usando os dados da tabela tab_ap (veja Seção 18.2.1.1), verifica-se que a probabilidade (p) de uma ultrassonografia positiva para apendicite aguda é igual\n\na &lt;- tab_ap[1,1]\nb &lt;- tab_ap[1,2]\nc &lt;- tab_ap[2,1]\nd &lt;- tab_ap[2,2]\np &lt;- (a + b)/(a + b + c + d)\np\n\n[1] 0.5897436\n\n\ne que o odds da ultrassonografia positiva2 é\n\n odds &lt;- (a + b)/(c + d) \n odds\n\n[1] 1.4375\n\n\nPara transformar a odds em probabilidades e vice-versa, procede-se da seguinte maneira:\n\\[\np=\\frac{odds}{1+odds}\n\\]\nVoltando ao exemplo (Seção 18.2.1.1):\n\np = odds/(1 + odds)\np\n\n[1] 0.5897436\n\n\ne\n\\[\nodds=\\frac{p}{1-p}\n\\]\n\nodds = p/(1-p)\nodds\n\n[1] 1.4375\n\n\nPelo teorema de Bayes, sabendo-se a probabilidade a priori ou probabilidade pré-teste, é possível obter a probabilidade pós-teste ou a posteriori, usando a razão de probabilidades.\nPara atingir este objetivo, basta, inicialmente, multiplicar o odds pré-teste pela razão de probabilidades:\n\\[\nodds_{pos} = odds_{pre \\quad \\times \\quad LR}\n\\]\nApós, para encontrar a probabilidade pós-teste, basta converter o odds pós-teste em probabilidade:\n\\[\np_{pos} = \\frac{odds_{pos}}{1-odds_{pos}}\n\\]\nNo exemplo (Seção 18.2.1.1), foi verificado que o LR (+) é igual a 2,32 e a prevalência de apendicite aguda é em torno de 7% pode-se prever a probabilidade de haver apendicite aguda, diante de uma ultrassonografia alterada:\n\nprev &lt;-  0.07\nLR &lt;-  2.32\n\nodds_pre &lt;-  0.07/(1 -0.07)\n\nodds_pos &lt;- odds_pre * LR\n\np_pos &lt;- odds_pos/(odds_pos +1)\n\nround(p_pos, 3)\n\n[1] 0.149\n\n\nOu, em outras palavras, diante de um teste positivo, a probabilidade de o paciente ter apendicite aguda passa de 7% antes do teste para praticamente 15%!\nEstes cálculos podem ser simplificados, utilizando o nomograma de Fagan (11), extremamente fácil de se usar (12), pois basta unir a probabilidade pré-teste ao LR que a reta apontará para a probabilidade pós-teste (Figura 18.1)).\n\n\n\n\n\n\n\n\nFigura 18.1: Nomograma de Fagan\n\n\n\n\n\n\n\n\n18.2.4 Curva ROC\nNem sempre o resultado de um teste é dicotômico (positivo/negativo). Com frequência, trabalha-se com variáveis contínuas (pressão arterial, glicemia, dosagem do sódio, dosagens hormonais, etc.). Neste caso, não há um resultado “positivo” ou “negativo”. Um “ponto de corte” precisa ser criado, para definir quem será considerado positivo ou negativo.\nA escolha do ponto de corte depende das consequências de um resultado falso positivo ou de um falso negativo. Falsos positivos estão associados com custos (emocional ou financeiro) e com a dificuldade de “desrotular” alguém que recebeu o rótulo de “positivo”. Resultados falsos negativos podem “tranquilizar” pessoas doentes que não são seguidas ou tratadas precocemente.\nA distribuição dos níveis glicêmicos em diabéticos e não diabéticos não tem um ponto de corte bem nítido. As duas populações se sobrepõem (Figura 18.2)), gerando falso positivos ou falso negativos, dependendo do ponto de corte escolhido (10).\n\n\n\n\n\n\n\n\nFigura 18.2: Populações de indivíduos normais (curva em azul) e diabéticos (curva em vermelho)\n\n\n\n\n\nSuponha que ao se examinar uma população fosse escolhido o ponto de corte de 80mg/dL, haveria um aumento no número de indivíduos com teste positivo com uma taxa de falsos positivos elevada, diminuindo a especificidade do teste. Se, por outro lado, o ponto de corte fosse elevado para 200mg/dL, o número de falsos negativos teria um grande aumento, reduzindo a sensibilidade. Esta oscilação entre a sensibilidade e a especificidade ocorre pelo fato de a localização do ponto de corte ser uma decisão arbitrária num contínuo entre o normal e anormal.\nAo se escolher um ponto de corte deve-se fazer um balanço entre a sensibilidade e a especificidade, levando em conta as consequências da escolha. Por exemplo, a triagem para fenilcetonúria em recém-nascidos valoriza a sensibilidade em vez de especificidade; o custo da perda de um caso é alto, pois existe tratamento eficaz. Uma desvantagem é que ocorre um grande número de testes falso positivos que causam angústia e a realização de mais testes.\nEm contraste, a triagem para o câncer de mama deve favorecer a especificidade sobre a sensibilidade, uma vez que uma avaliação mais aprofundada daquelas com teste positivo, implica em biopsias dispendiosas e invasivas.\nAs curvas ROC (Receiver Operating Characteristic) são uma ferramenta inestimável para encontrar o ponto de corte em uma medida com distribuição contínua que melhor prediz se uma condição está presente, por exemplo, se pacientes são positivos ou negativos para a presença de uma doença (13). As curvas ROC são usadas para encontrar um ponto de corte que separa um resultado de teste “normal” de um “anormal” quando o resultado do teste é uma medida contínua. As curvas ROC são traçadas calculando a sensibilidade e a especificidade do teste na predição do diagnóstico para cada valor da medida. A curva permite determinar um ponto de corte para a medição que maximiza a taxa de verdadeiros positivos (sensibilidade) e minimiza a taxa de falsos positivos (1 – especificidade) e, portanto, maximiza a razão de probabilidades (likelihood ratio).\n\n18.2.4.1 Exemplo\nO conjunto de dados dadosTestes.xlsx contém informações para os resultados hipotéticos de três testes bioquímicos diferentes e uma variável (doença) que indica se foi confirmada a doença (padrão-ouro). Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho.\nLeitura e observação dos dados\nComo é um arquivo em Excel, a leitura será realizada pela função read_excel() do pacote readxl:\n\ntestes &lt;- readxl::read_excel(\"dados/dadosTestes.xlsx\")\nstr(testes)\n\ntibble [145 × 5] (S3: tbl_df/tbl/data.frame)\n $ id    : num [1:145] 1 2 3 4 5 6 7 8 9 10 ...\n $ teste1: num [1:145] 25 2.2 46.2 9.9 46.5 36.1 34.8 44.9 36.9 7.1 ...\n $ teste2: num [1:145] 25 2.2 15.6 20.4 15.7 35.7 34.8 55.4 36.9 7.1 ...\n $ teste3: num [1:145] 15 2.2 25 20.4 15.7 36.1 24 55.4 36.9 7.1 ...\n $ doenca: num [1:145] 2 2 1 1 2 2 2 1 2 2 ...\n\n\nA variável doença será transformada em fator:\n\ntestes$doenca &lt;- as.factor(testes$doenca)\n\nAs curvas ROC são usadas para avaliar qual teste é mais útil para prever quais pacientes serão positivos para a doença. A hipótese nula é que a área sob a curva ROC é igual a 0,5, ou seja, a habilidade do teste para identificar casos positivos e negativos é a esperada por acaso.\nA Figura 18.3 mostra a quantidade de sobreposição na distribuição da medição dos testes bioquímicos contínuos em ambos os grupos doença positiva e doença negativa. No Teste 1, a sobreposição é completa e não haverá um ponto de corte que separe efetivamente os dois grupos. Nos Testes 2 e 3, há uma maior separação das medidas de teste entre os grupos, particularmente para Teste 3.\n\n\n\n\n\n\n\n\nFigura 18.3: Resultado do teste vs doença.\n\n\n\n\n\n\n\n18.2.4.2 Construção da curva ROC\nA validade dos testes, na distinção entre os grupos doença-positivo e doença-negativo, pode ser quantificada pelas curvas ROC, usando a função roc() do pacote pROC (14). Este pacote tem várias funções:\n\nauc: calcula a área da curva ROC;\nci: calcula o intervalo de confiança da curva ROC;\nci.auc: calcula o intervalo de confiança da AUC;\nci.se: calcula o intervalo de confiança de sensibilidades em determinadas especificidades;\nci.sp: calcula o intervalo de confiança de especificidades em determinadas sensibilidades;\nci.thresholds: calcula o intervalo de confiança dos limites;\ncoords: Retorna as coordenadas (sensibilidades, especificidades, pontos de corte) de uma curva ROC;\n\nroc: Constroi uma curva ROC;\n\nroc.test: Compara a AUC de duas curvas ROC correlacionadas;\n\nsmooth: suaviza a curva ROC\n\nUsar a função com os argumentos variável resposta (doenca), variável preditora (teste3, teste2 e teste1), indicação de que o gráfico deve ser desenhado (plot = TRUE). Como por padrão o gráfico é plotado com a sensibilidade no eixo x e a especificidade no eixo y; deve-se acrescentar o argumento legacy.axes = TRUE para aparecer o seu complemento, os falsos positivos (\\(1 – especificidade\\)).\nAlém desses, pode-se usar vários outros argumentos como: print.auc = TRUE, que imprime no gráfico a AUC e ci que é o intervalo de confiança da AUC. Para que a sensibilidade e especificidade apareçam como uma percentagem, deve-se usar o argumento percent = TRUE, pois o padrão é FALSE. Os demais argumentos são os rótulos dos eixos, cor da curva, largura da curva (lwd).\n\nroc3 &lt;- roc (testes$doenca,\n             testes$teste3, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             print.auc.y = 0.2,\n             ci = TRUE,\n             ylab=\"Sensibilidade\",\n             xlab=\"1 - Especificdade\",\n             col=\"steelblue\",\n             smooth = TRUE,\n             lwd=2) \n\nroc2 &lt;- roc (testes$doenca,\n             testes$teste2, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             ci = TRUE,\n             print.auc.y=0.13,\n             col=\"chartreuse4\",\n             lwd=2,\n             smooth = TRUE,\n             add=TRUE)\n\nroc1 &lt;- roc (testes$doenca,\n             testes$teste1, \n             plot=TRUE,\n             quiet = TRUE,\n             legacy.axes=TRUE, \n             print.auc=TRUE,\n             ci = TRUE,\n             print.auc.y=0.06,\n             col=\"tomato\",\n             lwd=2,\n             smooth = TRUE,\n             add=TRUE)\n\n# Legendas das curvas ROC\ntext (0.73,0.80,\"Teste 3\", col=\"steelblue\", cex = 1)\ntext (0.53,0.73,\"Teste 2\", col=\"chartreuse4\", cex = 1)\ntext (0.35,0.65,\"Teste 1\", col=\"tomato\", cex = 1) \n\n\n\n\n\n\n\nFigura 18.4: Curvas ROC para os Testes 1, 2 e 3.\n\n\n\n\n\nInterpretação do resultado\nEm uma curva ROC, a sensibilidade é calculada usando cada valor do teste no conjunto de dados como um ponto de corte e é plotada em relação à (1 – especificidade) correspondente nesse ponto, como mostrado na Figura 18.4.\nAssim, a curva são os Verdadeiros Positivos (VP) plotados em relação aos Falsos Positivos (FP), calculados usando cada valor do teste como ponto de corte. A reta diagonal indica onde o teste cairia se os resultados não fossem melhores do que o acaso para predizer a presença de uma doença. O Teste 1 está próximo desta reta, confirmando que ele tem pouca capacidade de discriminar os pacientes doentes e não doentes.\nA área abaixo da reta diagonal é equivalente a 0,5 da área total. Quanto maior a área sob a curva ROC, mais útil é o teste para predizer os pacientes que têm a doença. Uma curva que cai substancialmente abaixo da linha diagonal indica que o teste tem pouca capacidade de diagnosticar a doença. Quando há uma separação perfeita dos valores dos dois grupos, isto é, sem sobreposição das distribuições, a área sob a curva ROC é igual a 1 (a curva ROC alcançará o canto superior esquerdo do gráfico).\nA área sob a curva (Area Under the Curve – AUC) e seu intervalo de confiança de 95% podem ser obtidos com os comandos usados na construção da Figura 18.4 ou separadamente usando as funções auc() e ci.auc() do pacote pROC.\n\nauc (roc1) \n\nArea under the curve: 0.5891\n\nci.auc (roc1)\n\n95% CI: 0.4861-0.6814 (2000 stratified bootstrap replicates)\n\nauc(roc2) \n\nArea under the curve: 0.7616\n\nci.auc(roc2)\n\n95% CI: 0.6761-0.8398 (2000 stratified bootstrap replicates)\n\nauc (roc3) \n\nArea under the curve: 0.898\n\nci.auc(roc3)\n\n95% CI: 0.8358-0.939 (2000 stratified bootstrap replicates)\n\n\nA acurácia geral de um teste pode ser descrita como a área sob a curva; quanto maior for a área, melhor será o teste. Na Figura 18.4, o Teste 3 tem uma AUC maior que os outros dois testes.\nUsa-se a seguinte estimativa (Tabela 18.3) para avaliar a acurácia de um teste ou da capacidade de identificar corretamente uma condição usando curva ROC (15):\n\n\n\n\nTabela 18.3: Acurácia do teste diagnóstico\n\n\n\nAUCQualidade do Teste&gt;0,90excelente0,80 a 0,90muito bom0,70 a 0,80bom0,60 a 0,70suficiente0,50 a 0,60ruim&lt;0,50ignorar teste\n\n\n\n\n\nDesta forma, o Teste 3 pode ser considerado um bom teste e o Teste 1 é um teste ruim.\nComparando duas curvas\nPode-se comparar duas curvas ROC com a função roc.test(), por exemplo, comparando as curvas dos Teste 3 e 2 (16):\n\nroc.test(roc3, roc2)\n\n\n    Bootstrap test for two correlated ROC curves\n\ndata:  roc3 and roc2\nD = 4.6305, boot.n = 2000, boot.stratified = 1, p-value = 3.648e-06\nalternative hypothesis: true difference in AUC is not equal to 0\nsample estimates:\nSmoothed AUC of roc1 Smoothed AUC of roc2 \n           0.8980454            0.7616201 \n\n\nO Teste 3 tem uma AUC que o caracteriza como um bom teste e o teste de DeLong, entregue na saída do roc.test(), resultou que a diferença entre ele o Teste 2 é estatisticamente significativa (P &lt; 0,0001).\n\n\n18.2.4.3 Melhor ponto de corte\nO melhor ponto de corte (Best Critical Value), que às vezes é chamado de ponto de diagnóstico ótimo ou de Youden, é o ponto da curva mais próximo da parte superior do eixo y (Figura 18.4, Teste 3). Este é o ponto em que a taxa de verdadeiros positivos é otimizada e a taxa de falsos positivos é minimizada. O melhor ponto de corte para o Teste 3 é mostrado na Figura 18.5. Este melhor ponto de corte pode ser identificado a partir dos pontos de coordenadas da curva, usando a função roc() com os seguintes argumentos:\n\nbest &lt;- roc (testes$doenca, \n     testes$teste3,\n     plot = TRUE,\n     ci=TRUE,\n     thresholds=\"best\", \n     print.thres=\"best\",\n     legacy.axes=TRUE,\n     main=\"\",\n     ylab=\"Sensibilidade\",\n     xlab=\"1 - Especificidade\",\n     col=\"steelblue\",\n     lwd=2)\nbest\n\n\nCall:\nroc.default(response = testes$doenca, predictor = testes$teste3,     ci = TRUE, plot = TRUE, thresholds = \"best\", print.thres = \"best\",     legacy.axes = TRUE, main = \"\", ylab = \"Sensibilidade\", xlab = \"1 - Especificidade\",     col = \"steelblue\", lwd = 2)\n\nData: testes$teste3 in 48 controls (testes$doenca 1) &gt; 97 cases (testes$doenca 2).\nArea under the curve: 0.8973\n95% CI: 0.8444-0.9502 (DeLong)\n\n\n\n\n\n\n\n\nFigura 18.5: Curvas ROC para os Testes 1, 2 e 3.\n\n\n\n\n\nAssim, para o Teste 3, o ponto de corte ideal é 24,8, onde a especificidade é igual a 0,854 e a sensibilidade é igual 0,845. Estes dados, fornecem um LR para um resultado positivo igual a:\n\\[\nLR \\left(+\\right) = \\frac{0.845}{\\left (1-0.854\\right)} = 5,79\n\\]\nAs coordenadas da curva ROC podem ser obtida com a seguinte programação, a partir de uma sensibilidade e especificidade acima de 0 (zero):\n\ncoordenadas &lt;- testes %&gt;% roc(doenca, teste3) %&gt;% coords (transpose = F)\nhead(coordenadas, 10)\n\n   threshold specificity sensitivity\n1        Inf  0.00000000           1\n2      57.50  0.02083333           1\n3      54.65  0.04166667           1\n4      53.45  0.06250000           1\n5      52.80  0.08333333           1\n6      51.30  0.10416667           1\n7      49.65  0.12500000           1\n8      48.65  0.16666667           1\n9      47.50  0.18750000           1\n10     46.50  0.35416667           1\n\n\nA estatística J de Youden (17) é calculada deduzindo 1 a partir da soma de sensibilidade e especificidade do teste e não é expressa como porcentagem, mas como parte de um número inteiro: \\(\\left (sensibilidade + especificidade\\right) - 1\\). A estatística J de Youden no melhor ponto de corte do Teste 3 é igual a \\(\\left (0,845+ 0,854\\right) - 1 = 0,699\\).\nEste é o maior valor de todos os valores das coordenadas (91 valores) usadas.\n\nyouden &lt;- max(coordenadas$sensitivity + coordenadas$specificity) - 1\nyouden\n\n[1] 0.6995275\n\n\nA Figura 18.5 mostra o ponto de corte ideal. Ele também pode ser obtido com a função coords() do pacote pRoc:\n\nroc3 &lt;- testes %&gt;% roc(doenca, teste3)\ncoords(roc3, x = \"best\", ret=\"threshold\", transpose = FALSE, \n       best.method=\"youden\")\n\n  threshold\n1      24.8\n\n\nO método para obter o melhor ponto de corte (best.method) pode ser pelo método de youden ou closest.topleft. No exemplo, o resultado é o mesmo. Para maiores detalhes consulte a ajuda da função coord(), do pacote pROC.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#estatística-kappa",
    "href": "18-epidemiologia.html#estatística-kappa",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.3 Estatística kappa",
    "text": "18.3 Estatística kappa\nA estatística de concordância kappa (k) de Cohen é utilizada para descrever a concordância entre dois ou mais avaliadores quando realizam uma avaliação nominal ou ordinal de uma mesma amostra (18). A estatística kappa corrige a chance do acaso nas avaliações e é obtida pela fórmula igual a:\n\\[\nk= \\frac{p_{o} - p_{e}}{1 - p_{e}}\n\\]\nOnde \\(p_{o}\\) = proporção observada de concordância e \\(p_{e}\\) = proporção esperada de concordância apenas pelo acaso.\nPor exemplo, dois radiologistas podem revisar independentemente uma série de radiografias do tórax de pacientes para determinar a presença ou ausência de pneumonia. Para avaliar o grau de concordância entre as classificações dos dois médicos, pode ser relatado o percentual de concordância entre os avaliadores (por exemplo, 50% dos avaliadores responderam “sim” nas duas ocasiões). No entanto, esse percentual pode ser enganoso, pois não leva em conta o nível de concordância entre os dois avaliadores que pode ocorrer por acaso. A estatística kappa pode ser usada para avaliar a concordância das respostas para dois ou mais avaliadores após considerar a concordância casual. Portanto, a estatística kappa é uma estimativa da proporção de concordância entre avaliadores que excede a concordância que ocorreria por acaso.\nA interpretação dos valores de kappa é mostrada na Tabela 18.4 (19). Quando a proporção observada de concordância é menor que a esperada por acaso, o kappa terá um valor negativo indicando não concordância. Um valor de kappa igual a 0 indica que a concordância observada é igual à concordância casual.\nO teste de hipóteses testa a hipótese de que a concordância entre os dois avaliadores seja puramente aleatória. Quando o valor P é menor que 0,05, rejeitamos a hipótese de que a concordância foi puramente aleatória. As premissas para o kappa de Cohen são que os participantes ou itens a serem classificados são independentes e também que os avaliadores e categorias são independentes.\n\n\n\n\nTabela 18.4: Valor Kappa e nível de concordância\n\n\n\nValor kappaConcordância&lt;0,00pobre0,00 - 0,20leve0,21 - 0,40razoável0,41 - 0,60moderada0,61 - 0,80substancial0,81 - 1,00quase perfeita\n\n\n\n\n\nExistem diferentes tipos de estatísticas kappa. Para dados com três ou mais categorias possíveis (por exemplo, concordo, concordo parcialmente, discordo) ou para dados categóricos ordenados, o kappa ponderado deve ser usado para que as respostas que estão mais distantes da concordância tenham maior peso do que aquelas próximas à concordância. No exemplo usado, as categorias possíveis são dicotômicas (sim e não), portanto, o kappa não ponderado (unweighted) e o ponderado (weighted) retornam o mesmo resultado.\n\n18.3.1 Exemplo\nO arquivo dadosPneumonia.xlsx contém os dados de 54 crianças com suspeita de pneumonia, cujas radiografias foram avaliadas por dois radiologistas. O objetivo foi medir a concordância diagnóstica dos dois profissionais. Para o cálculo do coeficiente kappa, será usada a função Kappa() do pacote vcd (20). Essa função tem os seguintes argumentos:\n\nx \\(\\longrightarrow\\) matriz ou tabela\nweights \\(\\longrightarrow\\) matriz especificada pelo usuário com as mesmas dimensões de x, desnecessário para kappa não ponderado.\n\nNa impressão do kappa pode-se usar print (k, digits = 3, CI = TRUE, level = 0.95). Onde k é o coeficiente de kappa, calculado pela função Kappa(), CI é o intervalo de confiança e o nível de confiança padrão é 95%.\n\n18.3.1.1 Leitura e exploração dos dados\nO conjunto de dados dadosPneumonia.xlsx pode ser obtido aqui. Após salvar o arquivo em seu diretório, ele pode ser carregado com a função read_excel() do pacote readxl:\n\ndados &lt;- readxl::read_excel(\"dados/dadosPneumonia.xlsx\")\n\n\n\n18.3.1.2 Construção da tabela\nO cálculo do kappa com a função Kappa() exige uma tabela, onde os dados dos dois radiologistas são cruzados. As variáveis a serem cruzadas são rx1 e rx2:\n\ndados$rx1 &lt;- factor(dados$rx1,\n                        ordered=TRUE,\n                        levels = c(\"sim\", \"não\"))                           \ndados$rx2 &lt;- factor(dados$rx2,\n                        ordered=TRUE,\n                        levels = c(\"sim\", \"não\"))\n\ntabk &lt;- with(dados, table(rx1, rx2, dnn = c (\"Radiologista 1\", \"Radiologista 2\")))\naddmargins(tabk, FUN = sum)\n\nMargins computed over dimensions\nin the following order:\n1: Radiologista 1\n2: Radiologista 2\n\n\n              Radiologista 2\nRadiologista 1 sim não sum\n           sim  32   5  37\n           não   3  14  17\n           sum  35  19  54\n\n\n\n\n18.3.1.3 Cálculo do kappa\nO kappa é dado pela execução da função:\n\nk &lt;- vcd::Kappa(tabk)\nprint (k, \n       digits= 3, \n       CI=TRUE, \n       level=0.95)\n\n           value   ASE    z Pr(&gt;|z|) lower upper\nUnweighted 0.667 0.107 6.21 5.42e-10 0.456 0.878\nWeighted   0.667 0.107 6.21 5.42e-10 0.456 0.878\n\n\nA saída exibe o kappa pontual e os intervalos de confiança de 95%, podendo-se concluir, desses resultados, que existe uma boa confiabilidade nos diagnósticos dos radiologistas (k = 0,67, concordância substancial,de acordo com a Tabela 18.4.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#medidas-de-frequência",
    "href": "18-epidemiologia.html#medidas-de-frequência",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.4 Medidas de frequência",
    "text": "18.4 Medidas de frequência\n\n18.4.1 Prevalência\nA prevalência, ou mais adequadamente, a prevalência pontual de uma doença é a proporção da população portadora da doença em um determinado ponto do tempo. É uma medida instantânea por excelência e fornece uma medida estática da frequência da doença. É também conhecida como taxa de prevalência e é expressa em percentagem ou por \\(10^{n}\\) habitantes. As medidas de prevalência geram informações úteis para o planejamento e administração de serviços de saúde.\nA prevalência por período descreve os casos que estavam presentes em qualquer momento durante um determinado período de tempo. Diz o número total de casos de uma doença que se sabe haver existido durante um período de tempo.\nUm tipo especial de prevalência de período é a prevalência ao longo da vida, que mede a frequência cumulativa ao longo da vida de um resultado até o momento presente (ou seja, a proporção de pessoas que tiveram o evento em qualquer momento no passado).\nAs doenças, quanto a sua duração, podem ser agudas e de longa duração ou crônicas. A prevalência é proporcional ao tempo de duração da doença. Hipoteticamente, se o surgimento de novos casos de doença ocorre em ritmo constante e igual para doenças agudas e crônicas, estas últimas acumularão casos, aumentando a prevalência. As doenças agudas tenderão a manter uma prevalência constante. A terapêutica, diminuindo o tempo de duração das doenças, também reduz a prevalência. A prevalência é dada pela razão:\n\\[\nprevalência = \\frac{número \\ de \\ casos \\ conhecidos \\ da \\ doença}{total \\ da \\ População} \\times 10^{n}\n\\]\n\n18.4.1.1 Exemplo\nComo exemplo, será verificada a frequência de tabagismo entre as puérperas da maternidade do HGCS. O banco de dados dadosMater.xlsx contém informação de 1368 nascimentos e pode ser consultado na Seção 5.3. Clique aqui para baixar e depois de salvar em seu diretório de trabalho, carregue-o com a função read_excel() do pacote readxl.\n\ndados &lt;- readxl::read_excel (\"dados/dadosMater.xlsx\")\n\nInicialmente, será verificado quantas fumantes existem. O conjunto de dados contém uma variável fumo, onde 1 = fumante e 2 = não fumante. Portanto, há necessidade de transformar a variável numérica em um fator:\n\ndados$fumo &lt;- factor (dados$fumo,\n                      ordered = TRUE, \n                      levels = c(1,2),\n                      labels = c(\"fumante\", \"não fumante\"))\n \ntabFumo &lt;- with(data = dados, table(fumo))\naddmargins(tabFumo, FUN = sum)\n\nfumo\n    fumante não fumante         sum \n        301        1067        1368 \n\n\nAlém de relatar a estimativa pontual da frequência da doença, é importante fornecer uma indicação da incerteza em torno dessa estimativa pontual. A função epi.conf(), do pacote epiR (4), permite calcular intervalos de confiança para prevalência, motivo da escolha dessa função.\nA função epi.conf() usa os seguintes argumentos:\n\ndat \\(\\longrightarrow\\) matriz ou tabela;\nctype \\(\\longrightarrow\\) tipo de intervalo de confiança a ser calculado. Opções: mean.single, mean.unpair, mean.pair, prop.single, prop.unpaired, prevalence, inc.risk, inc.rate, odds e smr (standardized mortality rate);\n\nmethod \\(\\longrightarrow\\) método a ser usado. Quando ctype = \"inc.risk\" ou ctype = \"prevalence\", as opções são exact, wilson e fleiss Quando ctype = \"inc.rate\" as opções são exact e byar;\nN \\(\\longrightarrow\\) tamanho da população;\nconf.level \\(\\longrightarrow\\) magnitude do intervalo de confiança retornado. Deve ser um único número entre 0 e 1.\n\nConstrução da matriz\nCom os dados da tabFumo, constrói-se uma matriz de duas colunas:\n\nn1 &lt;- tabFumo[1]\nN1 &lt;- tabFumo[1] + tabFumo[2]\nmat1 &lt;- as.matrix(cbind (n1, N1))\nmat1\n\n         n1   N1\nfumante 301 1368\n\n\nCálculo da prevalência\nUsando a função epiR(), tem-se:\n\nepiR::epi.conf(mat1, \n               ctype = \"prevalence\", \n               method = \"exact\", \n               conf.level = 0.95) \n\n        est     lower     upper\n1 0.2200292 0.1983313 0.2429365\n\n\nA saída mostra que a prevalência de fumantes entre as puérperas do HGCS é igual a 22,0% (IC95%: 19,8 – 24,3%).\n\n\n\n18.4.2 Incidência\nA incidência fornece uma medida da frequência com que os indivíduos suscetíveis se tornam casos de doenças, à medida que são observados ao longo do tempo.\nUm caso incidente ocorre quando um indivíduo deixa de ser suscetível e passa a ser doente. A contagem de casos incidentes é o número de tais eventos que ocorrem em uma população durante um período de acompanhamento definido. Existem duas maneiras de expressar a incidência:\nA incidência cumulativa (risco) é a proporção de indivíduos inicialmente suscetíveis em uma população que se tornam novos casos durante um período de acompanhamento definido.\nPara calcular a incidência cumulativa, é necessário primeiro identificar os doentes e após acompanhar por um determinado tempo os não doentes (Figura 18.6).\n\n\n\n\n\n\n\n\nFigura 18.6: Incidência\n\n\n\n\n\nA taxa de incidência (densidade de incidência ou taxa de incidência) é o número de novos casos da doença que ocorrem por unidade de tempo em risco durante um período de acompanhamento definido. Este período é expresso como pessoas-tempo (pessoas-ano, por exemplo).\nO conceito de pessoas-tempo pode ser ilustrado com o seguinte exemplo: a Figura 18.7 representa um estudo epidemiológico hipotético com duração de cinco anos, onde D é o desfecho e C representa os sujeitos que deixaram o estudo por migração ou morte (censurados) por causa não relacionada ao desfecho\n\n\n\n\n\n\n\n\nFigura 18.7: Pessoas-tempo (estudo epidemiológico hipotético).\n\n\n\n\n\nNesse estudo hipotético, o indivíduo 1 permaneceu no estudo 3,5 anos; o indivíduo 2, ficou 5 anos; o indivíduo 3, 4,5 anos e, assim por diante, totalizando 32,5 pessoas-anos. Em outras palavras, ocorreram 4 desfechos durante os 5 anos do estudo, consequentemente, a taxa de incidência (TI) foi de\n\\[\nTI = \\frac{4}{32,5} \\times 1000 = \\frac{123}{1000\\ pessoas-ano}\n\\]\nIsto significa que se fossem acompanhadas 1000 pessoas por um ano, 123 delas apresentariam o desfecho D.\n\n18.4.2.1 Exemplo\nAparentemente, pessoas cegas tem uma menor incidência de câncer e esse efeito parece ser mais pronunciado em pessoas totalmente cegas do que em pessoas com deficiência visual grave.\nPara testar essa hipótese, foi identificada uma coorte de 1.567 pessoas totalmente cegas e 13.292 sujeitos com deficiência visual grave. As informações sobre a incidência de câncer foram obtidas do Registro Sueco de Câncer (21). Foram diagnosticados de 136 casos de câncer em 22050 pessoas-ano em risco totalmente cegas e 1709 casos de câncer em 127650 pessoas-anos em risco com deficiência visual grave.\nA taxa de incidência pode ser calculada, usando-se a mesma função epi.conf(), usada para o cálculo da prevalência, mudando o argumento ctype = “prevalence” para ctype = “inc.rate”, conforme recomendado:\nPessoas totalmente cegas\nInicialmente, contrói-se a matriz:\n\nn2 &lt;- 136\nN2 &lt;- 22050\nmat2 &lt;- as.matrix(cbind (n2, N2))\nmat2\n\n      n2    N2\n[1,] 136 22050\n\n\nLogo, a incidência de câncer nos totalmente cegos é:\n\nepiR::epi.conf(mat2, \n               ctype = \"inc.rate\", \n               method = \"exact\", \n               conf.level = 0.95)*1000\n\n      est    lower    upper\nn2 6.1678 5.174806 7.295817\n\n\nPessoas com grave deficiência visual\nInicialmente, contrói-se a matriz:\n\nn3 &lt;- 1709\nN3 &lt;- 127650\nmat3 &lt;- as.matrix(cbind (n3, N3))\nmat3\n\n       n3     N3\n[1,] 1709 127650\n\n\nLogo, a incidência de câncer nos com grave deficiência visual é:\n\nepiR::epi.conf(mat3, \n               ctype = \"inc.rate\", \n               method = \"exact\", \n               conf.level = 0.95)*1000\n\n        est    lower    upper\nn3 13.38817 12.76088 14.03832\n\n\nAs saídas mostram que para cada 1000 pessoas cegas (a função foi multiplicada por 1000) acompanhadas por um ano, ocorreu 6,2 ((IC95%: 5,2 – 7,3) casos de câncer. Uma taxa de incidência, praticamente, metade da taxa de incidências das pessoas com deficiência visual grave. Os IC95% não são coincidentes, o que significa que essa diferença é significativa. Houve, na amostra, uma incidência menor de câncer entre os indivíduos totalmente cegos, sugerindo que a melatonina possa ser um fator protetor contra o câncer.\n\n\n\n18.4.3 Relação entre prevalência e incidência\nA incidência é uma medida de risco. A prevalência, por não levar em consideração o tempo de duração da doença (t), não tem esta capacidade. Em uma população onde a situação da doença encontra-se em estado estacionário (ou seja, sem grandes migrações ou mudanças ao longo do tempo na incidência/prevalência), a relação entre prevalência e incidência e duração da doença pode ser expressa pela seguinte fórmula (22):\n\\[\nprevalência \\ pontual = incidência \\times duração \\ da \\ doença \\ (t)\n\\]\nPor exemplo, se a incidência da doença for de 0,8% ao ano e sua duração média (sobrevida após o diagnóstico) for de 10 anos, a prevalência pontual será de aproximadamente 8%.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#medidas-de-associação",
    "href": "18-epidemiologia.html#medidas-de-associação",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.5 Medidas de associação",
    "text": "18.5 Medidas de associação\n\n18.5.1 Odds Ratio\nOdds Ratio (OR) é a razão entre dois odds. A Odds Ratio, traduzida como Razão de Chances, está associada, usualmente, com estudos retrospectivos tipo caso-controle com desfechos dicotômicos.\nA odds ratio (OR) expressa a odds de exposição entre os que têm o desfecho (casos) pela odds de exposição nos livres de desfecho (controles).\n\n\n\n\nTabela 18.5: Tabela de contingência 2 x 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsando a Tabela 18.5, a fórmula \\(odds =\\frac{p}{1 -p}\\) e que\n\\[\np_{exp \\ doentes} = \\frac{a}{a+c}\n\\] \\[\np_{exp \\ não \\ doentes} = \\frac{b}{b+d}\n\\] tem-se:\n\\[\nodds_{exp} \\ {casos} = \\frac{\\frac{a}{a+c}}{1- \\frac{a}{a+c}}=\\frac{a}{c}\n\\]\n\\[\nodds_{exp} \\ {controles} = \\frac{\\frac{b}{b+d}}{1- \\frac{b}{b+d}}=\\frac{b}{d}\n\\]\nPortanto, a OR é igual a:\n\\[\nOR = \\frac{odds_{exp}\\ {casos}}{odds_{exp}\\ {controles}}=\\frac{\\frac{a}{c}}{\\frac{b}{d}}=\\frac{a \\times d}{c \\times b}\n\\]\nEm decorrência da última fórmula, a OR é definida como a razão dos produtos cruzados em uma tabela de contingência 2×2.\n\n18.5.1.1 Exemplo\nEm um estudo de caso-controle hipotético, a distribuição das exposições entre os casos e um grupo de pessoas saudáveis (“controles”) é comparada entre si. Os casos correspondem a um tipo raro de câncer, onde se suspeita que exista uma associação à exposição a um determinado fator de risco.\nOs dados desse estudo hipotético estão no arquivo dadosCasoControle.xlsx. O conjunto de dados pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl.\n\ncc &lt;- readxl::read_excel (\"dados/dadosCasoControle.xlsx\")\n\nAs variáveis cc$exposto e cc$desfecho devem ser transformadas em fatores e na ordem sim, não, uma vez que o R coloca em ordem alfabética (não, sim):\n\ncc$exposto &lt;- factor (cc$exposto,\n                      levels = c(\"sim\", \"não\"))\n\ncc$desfecho &lt;- factor (cc$desfecho,\n                       levels = c(\"sim\", \"não\"))\n\nApós essa etapa, construir uma tabela \\(2 \\times 2\\):\n\ntab_cc &lt;- table (cc$exposto, \n                 cc$desfecho, \n                 dnn = c(\"Exposição\", \"Desfecho\"))\naddmargins(tab_cc)         \n\n         Desfecho\nExposição sim não Sum\n      sim  48  20  68\n      não  12  40  52\n      Sum  60  60 120\n\n\nA OR será obtida utilizando a função epi.2by2() do pacote epiR (4). Esta função tem os seguintes argumentos:\n\ndat \\(\\longrightarrow\\) tabela de contingência \\(2 \\times 2\\);\nmethod \\(\\longrightarrow\\) as opções são “cohort.count”, “cohort.time”, “case.control” ou “cross.sectional”.;\nconf.level \\(\\longrightarrow\\) padrão = 0.95;\nunits \\(\\longrightarrow\\) multiplicador para incidência e prevalência;\noutcome \\(\\longrightarrow\\) indicação de como a variável desfecho é representada na tabela de contingência (“as.columns” ou “as.rows”).\n\n\nepiR::epi.2by2(tab_cc, \n               method = \"case.control\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n             Outcome +    Outcome -      Total                       Odds\nExposed +           48           20         68        2.40 (1.43 to 4.23)\nExposed -           12           40         52        0.30 (0.13 to 0.53)\nTotal               60           60        120        1.00 (0.69 to 1.45)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nExposure odds ratio                            8.00 (3.49, 18.34)\nAttrib fraction (est) in the exposed (%)      87.24 (69.26, 95.03)\nAttrib fraction (est) in the population (%)   70.00 (48.69, 82.46)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 26.606 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n\n\nA saída exibe os dados em uma tabela \\(2 \\times 2\\), mostrando as odds e os IC95% e outras estatísticas epidemiológicas relacionadas.\nA OR varia de zero ao infinito. Quando o valor da OR se aproxima de 1, a doença e o fator de risco não estão associados. Acima de 1 significa que existe associação e valores menores de 1 indicam uma associação negativa (efeito protetor).\nNo exemplo hipotético, os indivíduos que se expuseram ao fator de risco têm uma chance 8 vezes maior de apresentar este tipo de câncer. O valor P do qui-quadrado é altamente significativo (P &lt; 0,001).\n\n\n\n18.5.2 Risco Relativo\nO Risco relativo (RR) é a razão entre a incidência de desfecho em indivíduos expostos e a incidência de desfecho em indivíduos não expostos. O RR estima a magnitude da associação entre a exposição e o desfecho (doença). Em outras palavras, compara a probabilidade de ocorrência do desfecho entre os indivíduos expostos com a probabilidade de ocorrência do desfecho nos indivíduos não expostos.\nA partir da tabela de contingência \\(2 \\times 2\\) (Tabela 18.5), tem-se que o estimador do RR é dado por:\n\\[\nRR = \\frac{incidência_{exp}}{incidência_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}}\n\\]\n\n18.5.2.1 Exemplo\nEm 1940, ocorreu um surto de gastroenterite, após um jantar, em uma igreja, na cidade de Lycoming, Condado de Oswego, Nova York. Das 80 pessoas presentes, 75 foram entrevistadas. Quarenta e seis relataram doença gastrointestinal, atendendo à definição de caso.\nAs taxas de ataque (incidência) foram calculadas para aqueles que comeram e não comeram cada um dos 14 itens alimentares consumidos na ceia (23). O pacote epitools (24) contém os dados desta investigação no arquivo oswego.\n\ndata(oswego)\nstr(oswego)\n\n'data.frame':   75 obs. of  21 variables:\n $ id                 : int  2 3 4 6 7 8 9 10 14 16 ...\n $ age                : int  52 65 59 63 70 40 15 33 10 32 ...\n $ sex                : chr  \"F\" \"M\" \"F\" \"F\" ...\n $ meal.time          : chr  \"8:00 PM\" \"6:30 PM\" \"6:30 PM\" \"7:30 PM\" ...\n $ ill                : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ onset.date         : chr  \"4/19\" \"4/19\" \"4/19\" \"4/18\" ...\n $ onset.time         : chr  \"12:30 AM\" \"12:30 AM\" \"12:30 AM\" \"10:30 PM\" ...\n $ baked.ham          : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ spinach            : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ mashed.potato      : chr  \"Y\" \"Y\" \"N\" \"N\" ...\n $ cabbage.salad      : chr  \"N\" \"Y\" \"N\" \"Y\" ...\n $ jello              : chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ rolls              : chr  \"Y\" \"N\" \"N\" \"N\" ...\n $ brown.bread        : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ milk               : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ coffee             : chr  \"Y\" \"Y\" \"Y\" \"N\" ...\n $ water              : chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ cakes              : chr  \"N\" \"N\" \"Y\" \"N\" ...\n $ vanilla.ice.cream  : chr  \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ chocolate.ice.cream: chr  \"N\" \"Y\" \"Y\" \"N\" ...\n $ fruit.salad        : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n\nExistem 75 observações de 21 variáveis, algumas características dos indivíduos como idade, sexo, etc. Importante para a análise é a variável ill (Y – sim, doente; N – não doente) e a variáveis relacionadas aos alimentos ingeridos durante o jantar na igreja. O sorvete de baunilha foi considerado o principal responsável pelo surto.\nA seguir, as variáveis oswego$vanilla.ice.cream e oswego$ill 3 serão transformadas em fator e os níveis colocados na ordem Y, N, uma vez que o R coloca em ordem alfabética (N, Y) :\n\noswego$ill &lt;- factor (oswego$ill,\n                      levels = c (\"Y\", \"N\"))\noswego$vanilla.ice.cream &lt;- factor (oswego$vanilla.ice.cream,\n                                    levels = c (\"Y\", \"N\"))\n\nRealizada essa etapa, será construída uma tabela para o cálculo do RR:\n\ntab_vanilla &lt;- table (oswego$vanilla.ice.cream, \n                      oswego$ill, \n                      dnn = c (\"Vanilla\", \"Ill\"))\ntab_vanilla            \n\n       Ill\nVanilla  Y  N\n      Y 43 11\n      N  3 18\n\n\nO RR será obtido, utilizando a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR, mudando a tabela para tab_vanilla e method = “cohort.count”:\n\nepiR::epi.2by2(tab_vanilla, \n               method = \"cohort.count\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           43           11         54     79.63 (66.47 to 89.37)\nExposed -            3           18         21      14.29 (3.05 to 36.34)\nTotal               46           29         75     61.33 (49.38 to 72.36)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 5.57 (1.94, 16.03)\nInc odds ratio                                 23.45 (5.84, 94.18)\nAttrib risk in the exposed *                   65.34 (46.92, 83.77)\nAttrib fraction in the exposed (%)            82.06 (48.41, 93.76)\nAttrib risk in the population *                47.05 (28.46, 65.63)\nAttrib fraction in the population (%)         76.71 (37.11, 91.37)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 27.223 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nOs resultados da saída indicam que os indivíduos que ingeriram sorvete de baunilha (n = 54) tiveram um risco maior de desenvolver gastrenterite aguda quando comparado aos que não ingeriram (n = 21). Dividindo o risco dos indivíduos expostos (incidência = 79,6) pelo risco dos não expostos (incidência = 14,3), encontra-se o RR = 5,57. Isso confirma que o sorvete de baunilha foi o principal responsável.\nQuanto maior o RR mais forte é a associação entre a doença em questão e a exposição ao fator de risco. Um RR = 1 indica que a doença e a exposição ao fator de risco não estão associadas. Valores &lt; 1 indicam uma associação negativa entre o fator de risco e a doença (efeito protetor).\n\n\n\n18.5.3 Odds Ratio vs Risco Relativo\nA OR não deve ser entendida como uma medida aproximada do RR, exceto para doenças raras (doenças, em geral com prevalência menor do que 10%). Caso contrário, a OR tenderá a superestimar a magnitude da associação e o OR afasta-se da hipótese nula da não associação (OR =1), independentemente de ser um fator de risco ou de proteção. A discrepância (d)4 entre as estimativas do RR e OR pode ser definido como a razão entre o OR e o RR estimados (25). Em outras palavras, a discrepância corresponde a uma proporção do RR (26).\n\\[\nd = \\frac {1- p_{não \\ exp}}{1- p_{exp}}= \\frac{\\frac{c}{c + d}}{\\frac{a}{a + b}}\n\\]\nLogo,\n\\[\nOR = RR \\times d\n\\]\nPara finalizar, uma comparação entre OR e RR é mostrada na Tabela 18.6 (27).\n\n\n\n\nTabela 18.6: Força de associação do RR comparado com a OR.\n\n\n\nORRRMagnitude1,01,0insignificante1,51,2pequena3,51,9moderada9,03,0grande325,7muito grande36019quase perfeitainfinitoinfinitoperfeita\n\n\n\n\n\n\n\n18.5.4 Razão de Prevalência\nQuando dados transversais estão disponíveis, muitas vezes as associações são avaliadas, usando a razão de prevalência pontual (RPP).\nTendo o mesmo princípio das duas medidas anteriores, a razão de prevalência (RPP) compara a prevalência do desfecho entre os expostos com a prevalência do desfecho entre os não expostos.\nMatematicamente, a RPP é calculada de maneira semelhante ao RR. Apenas, deve-se ter em mente que o desfecho e a exposição foram medidos no mesmo momento, enquanto para o cálculo do RR há necessidade de calcular a incidência.\nTomando como base a estrutura da tabela de contingência 2 x 2 , Tabela 18.5, tem-se:\n\\[\nRPP = \\frac{prevalência \\ de \\ doença_{exp}}{prevalência \\ de \\ doença_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}}\n\\]\nTambém é possível verificar a prevalência de exposição entre doentes e não doentes:\n\\[\nRPP = \\frac{prevalência \\ de \\ exposição_{doentes}}{prevalência \\ de \\ exposição_{não \\ doentes}}=\\frac{\\frac{a}{a + c}}{\\frac{b}{b + d}}\n\\]\n\n18.5.4.1 Exemplo\nEm um estudo transversal (28), foi verificada a prevalência de infecções congênitas entre as puérperas com idade igual ou acima de 20 anos comparadas às mulheres com menos de 20 anos (adolescentes). A hipótese foi de que as adolescentes tinham uma prevalência maior de infecções.\nParte dos dados estão no arquivo dadosMater.xlsx, que contém, como já mencionado, informações de 1368 nascimentos. Entre essas, tem-se a idade das mães (idadeMae) e se foi diagnosticada infecção congênita (infCong).\nO arquivo pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl.\n\ndados &lt;- readxl::read_excel (\"dados/dadosMater.xlsx\")\n\nA partir da variável idadeMae, criar a variável faixaEtaria, dividindo as parturientes em menores de 20 anos (adolescentes) e ≥ 20 anos. Para isso, usou-se a função cut() do pacote base. Revise os argumentos desta função.\n\ndados$faixaEtaria &lt;- cut (dados$idadeMae,\n                          breaks=c(13,20,46),\n                          labels = c(\"&lt;20a\",\"=&gt;20a\"),\n                          right = FALSE,\n                          include.lowest = TRUE)\n\nA variável ìnfCong encontra-se como uma variável numérica e deve ser transformada em fator:\n\ndados$infCong &lt;- factor (dados$infCong,\n                         ordered = TRUE, \n                         levels = c (1,2),\n                         labels = c (\"sim\", \"não\"))\n\nApós estes procedimentos, constroi-se uma tabela \\(2 \\times 2\\):\n\ntab_infCong &lt;- table(dados$faixaEtaria,\n                     dados$infCong,\n                     dnn = c(\"Faixa Etária\", \"Inf. Cong.\"))\naddmargins(tab_infCong)            \n\n            Inf. Cong.\nFaixa Etária  sim  não  Sum\n       &lt;20a     7  212  219\n       =&gt;20a  119 1030 1149\n       Sum    126 1242 1368\n\n\nCálculo da RPP\nUsando a tabela tab_infCong com a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR e RR, e mudando a tabela para tab_infCong e method = “cross.sectional”, obtem-se:\n\nepiR::epi.2by2(tab_infCong, \n               method = \"cross.sectional\", \n               conf.level = 0.95, \n               units = 100, \n               outcome = \"as.columns\")\n\n             Outcome +    Outcome -      Total               Prev risk *\nExposed +            7          212        219       3.20 (1.29 to 6.47)\nExposed -          119         1030       1149     10.36 (8.65 to 12.26)\nTotal              126         1242       1368      9.21 (7.73 to 10.87)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nPrev risk ratio                                0.31 (0.15, 0.65)\nPrev odds ratio                                0.29 (0.13, 0.62)\nAttrib prev in the exposed *                   -7.16 (-10.08, -4.24)\nAttrib fraction in the exposed (%)            -224.02 (-584.89, -53.29)\nAttrib prev in the population *                -1.15 (-3.48, 1.19)\nAttrib fraction in the population (%)         -12.45 (-17.53, -7.58)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 11.278 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nA saída exibe várias informações. Foi feita a hipótese de uma maior prevalência entre as mulheres com menos de 20 anos. Por este motivo, elas aparecem como as expostas (Exposed +) e tem uma prevalência de 3,20/100, enquanto as mulheres com mais de 20 anos tiveram uma prevalência de 10,36/100. Isto mostra que a razão de prevalência é igual a 0,31 (IC95%: 0,15-0,65)5, ou seja, abaixo de 1, sugerindo que ao contrário da hipótese inicial, as adolescentes têm, neste estudo, uma menor prevalência de infecções congênitas.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#medidas-de-impacto",
    "href": "18-epidemiologia.html#medidas-de-impacto",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.6 Medidas de impacto",
    "text": "18.6 Medidas de impacto\n\n18.6.1 Risco Atribuível\nO Risco Atribuível (RA) possui características de medida de impacto. O RA, ao invés de concentrar-se na associação em si, refere-se mais às consequências e às repercussões da exposição sobre a ocorrência do desfecho.\nO RA é a medida do excesso ou acréscimo absoluto de risco que pode ser atribuído à exposição (29). Com o RA é possível estimar o número de casos que podem ser prevenidos se a exposição for eliminada e assim estimar a magnitude do impacto, em termos de saúde pública, imposto por esta exposição.\nO risco de desenvolver o desfecho (incidência) está aumentado em RA nos indivíduos expostos em comparação com os que não estão expostos. Nos estudos de coorte, costuma-se usar mais a expressão Risco Atribuível ou Diferença de Risco. Nos ensaios clínicos, usa-se mais a expressão Redução Absoluta do Risco (RAR), pois se espera que a intervenção reduza o risco.\nCalcula-se o RA ou a RAR pela diferença absoluta entre as incidências dos expostos e não expostos:\n\\[\nRA = \\left|I_{expostos} - I_{não \\ expostos}\\right|\n\\]\nUtilizando a tabela de contingência \\(2 \\times 2\\) (Tabela 18.5), o RA fica expresso da seguinte maneira:\n\\[\nRA = \\left|\\frac{a}{a + b} -\\frac{c}{c + d}\\right|\n\\]\nNo exemplo do Risco Relativo, o RA pode ser calculado usando a mesma tabela de contingência, repetida aqui para facilitar a leitura (Tabela 18.7):\n\n\n\n\nTabela 18.7: Taxa de ataque de gastrenterite com sorvete de baunilha - Oswego\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogo,\n\\[\nRA = \\left|\\frac{43}{43 + 11} -\\frac{3}{3 + 18}\\right| = \\left|0,796 - 0,143\\right| = 0,653\n\\]\nO risco atribuível na exposição mede o excesso de risco associado a uma determinada categoria de exposição. Por exemplo, com base no exemplo, a incidência cumulativa de gastrenterite aguda entre os indivíduos que comeram o sorvete de baunilha é de 79,6% e para os que não ingeriram o sorvete (categoria de referência ou não exposta) foi de 14,3%. Desta forma, o risco excessivo associado à exposição 79,6 – 14,3 = 65,3%. Ou seja, assumindo uma associação causal (sem confusão ou viés), a não ocorrência da festa diminuiria o risco no grupo exposto de 79,6% para 14,3%.\nO RA expresso em relação à incidência nos expostos e apresentado em percentual é denominado de Risco Atribuível Proporcional (RAP) ou Fração Atribuível nos Expostos.\nO RAP informa qual a proporção de desfecho, expresso em percentagem, entre os expostos que poderia ter sido prevenida se a exposição fosse eliminada. É dado pela fórmula:\n\\[\nRAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100\n\\]\nNo exemplo do surto de gastrenterite aguda no jantar da igreja de Oswego (Seção 18.5.2), tem-se:\n\\[\nRAP = \\left(\\frac{0,796 - 0,143}{0,796}\\right) \\times 100 = 82,06 \\%\n\\]\nSe a causalidade foi estabelecida, essa medida pode ser interpretada como a porcentagem do risco total de gastrenterite aguda que é atribuível à ingesta de sorvete de baunilha.\nOutra maneira de se chegar a este mesmo resultado é através do RR, usando a seguinte fórmula\n\\[\nRAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(\\frac{I_{expostos}}{I_{expostos}} - \\frac{I_{não \\ expostos }}{I_{expostos}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(1 - \\frac{1}{\\frac{I_{expostos }}{I_{não \\ expostos}}}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(1 - \\frac{1}{RR}\\right) \\times 100\n\\]\n\\[\nRAP = \\left(\\frac{RR - 1}{RR}\\right) \\times 100\n\\]\nNo exemplo, o RR é igual a 5,57, logo:\n\\[\nRAP = \\left(\\frac{5,57 - 1}{5,57}\\right) \\times 100 = 82,05\\%\n\\]\n\n\n18.6.2 Redução Relativa do Risco\nQuando se avalia um tratamento ou alguma intervenção em que se suponha haver uma redução do risco — por exemplo, o uso da aspirina para reduzir a ocorrência de infarto agudo do miocárdio —, o termo Risco Atribuível é substituído por Redução do Risco Atribuível e é calculado da mesma forma apresentada na equação do Risco Atribuível.\nNeste caso, ao invés de usar o Risco Atribuível Proporcional (RAP), onde se pressupõe que a exposição é um fator de risco para a doença e o RR \\(&lt;\\) 1, usa-se a Redução Relativa do Risco, pois a exposição é supostamente um fator protetor, como se espera que ocorra nos ensaios clínicos.\nEsta medida, análoga ao RAP, é também chamada de Eficácia, definida como a proporção da incidência nos indivíduos não tratados (por exemplo, o grupo controle) que é reduzida pela intervenção (30).\nO cálculo da Redução Relativa do Risco (RRR) é semelhante ao Risco Atribuível Proporcional (RAP), onde a incidência nos expostos é a incidência no grupo que recebeu a intervenção (ou taxa de eventos no grupo tratamento) e a incidência nos não expostos é incidência nos controles (ou taxa de eventos nos controles – TEC). Como se supõe que a incidência nos controles seja maior que a incidência no grupo de tratamento, a equação fica:\n\\[\nRRR = \\left(\\frac{I_{controle} - I_{tratamento}}{I_{controle}}\\right) \\times 100\n\\]\nAlternativamente, a RRR pode ser estimada pela equação:\n\\[\nRRR = \\left(1 - RR\\right) \\times 100\n\\]\nO Physicians’ Health Study (31) é um ensaio clinico randomizado controlado, duplo cego, desenhado com o objetivo de determinar se uma dose baixa de aspirina (325 mg a cada 48 horas) diminui a mortalidade cardiovascular e se o betacaroteno reduz a incidência de câncer. Participaram deste estudo 22071 indivíduos por uma média de 60,2 meses.\nO estudo do componente aspirina mostrou os seguintes resultados (Tabela 18.8):\n\n\n\n\nTabela 18.8: Physicians’ Health Study, componente aspirina e IAM\n\n\n\n\n\n\n\n\n\n\n\n\n\nA incidência cumulativa de Infarto Agudo de Miocárdio (IAM) em ambos os grupos foi:\n\\[\nIncidencia_{aspirina} = \\frac{139}{11037} = 0,0126\n\\]\n\\[\nIncidencia_{placebo} = \\frac{239}{11034} = 0,0217\n\\]\n\\[\nRR = \\frac{0,0126}{0,0217} = 0,58\n\\]\nLogo, a RRR é igual a:\n\\[\nRRR = \\left(1 - 0,58\\right) \\times 100 = 42\\%\n\\]\nOu seja, houve uma redução de 42% no risco de IAM no grupo que usou aspirina e a conclusão dos autores foi que este ensaio clínico demonstrou, em relação à prevenção primária de doença cardiovascular, uma diminuição no risco de IAM.\nEstes cálculos podem ser realizados com a função risks() do pacote MKmisc (32). Esta função calcula o risco relativo (RR), odds ratio (OR), redução relativa do risco (RRR) e outras estatísticas epidemiológicas, como RAR, NNT.\nA função risks() usa como argumento:\n\np0 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo não exposto;\np1 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo exposto.\n\nAlém disso, para o seu funcionamento, deve-se ter instalado o pacote BiocManager para poder instalar o pacote limma, necessário para a execução do pacote MKmisc. Veja início do capítulo em pacotes usados neste capítulo.\nA função risks() será usada dentro da função round() para reduzir o número de dígitos decimais:\n\np0 &lt;- 0.0217\np1 &lt;- 0.0126\nround(MKmisc::risks(p0,p1), 4)\n\n      p0       p1       RR       OR      RRR      ARR      NNT \n  0.0217   0.0126   0.5806   0.5753   0.4194   0.0091 109.8901 \n\n\n\n\n18.6.3 Número Necessário para Tratar\nOs resultados da função risks() entrega junto o Número Necessário para Tratar (NNT) que deve ser arredondado para o número inteiro mais próximo (no caso, 110) e significa a estimativa do número de indivíduos que devem receber uma intervenção terapêutica, durante um período específico de tempo, para evitar um efeito adverso ou produzir um desfecho positivo.\nO NNT equivale à recíproca do RAR (Redução Absoluta do Risco ou Diferença de Risco):\n\\[\nNNT = \\frac{1}{RAR} = \\frac{1}{I_{não \\ expostos} - I_{expostos}}\n\\]\nNo exemplo do Physicians’ Health Study (31), o RAR igual a:\n\\[\nRA = \\left|I_{expostos} - I_{não \\ expostos}\\right| = \\left|0,0126 - 0,0217\\right| = 0,0091\n\\]\n\\[\nNNT = \\frac{1}{0,0091} = 109,89 \\simeq 110\n\\]\nPode-se calcular os IC95%, calculando o NNT para os limites do RAR usando a seguinte equação (33):\n\\[\nIC_{95\\%} \\longrightarrow RAR \\pm z_{\\left({1 - \\frac{\\alpha}{2}}\\right)} \\times EP_{RAR}\n\\] Onde,\n\\[\nEP_{RAR} = \\sqrt{\\frac{p0\\left(1 - p0\\right)}{n_{1}}+\\frac{p1\\left(1 - p1\\right)}{n_{2}}}\n\\]\nUsando esses dados, pode-se criar um script no RStudio para os cálculos:\nVetor dos dados\n\na &lt;- 139\nb &lt;- 10898\nc &lt;- 239\nd &lt;- 10795\ndados &lt;- c (a, b, c, d)\n\nMatriz dos dados6\n\nmat_iam &lt;- matrix (dados, byrow = TRUE, nrow = 2)\ntratamento &lt;- c (\"aspirina\", \"placebo\")\ndesfecho &lt;- c (\"IAM\", \"s/IAM\")\nrownames (mat_iam) &lt;- tratamento\ncolnames (mat_iam) &lt;- desfecho\nmat_iam\n\n         IAM s/IAM\naspirina 139 10898\nplacebo  239 10795\n\n\nCálculo das incidências no grupo tratamento e no grupo placebo\nNa matriz o que está entre colchetes [1,1] significa: linha 1 e coluna 1, ou seja, o valor 139.\n\nn1 &lt;-mat_iam [1,1] + mat_iam [1,2]\nn1\n\n[1] 11037\n\np1 &lt;- mat_iam [1,1] / n1\nround (p1, 4)\n\n[1] 0.0126\n\nn0 &lt;- mat_iam [2,1] + mat_iam [2,2]\nn0\n\n[1] 11034\n\np0 &lt;- mat_iam [2,1] / n0\nround (p0, 4)\n\n[1] 0.0217\n\n\nOs resultados da matriz de dados e o cálculo das incidências p0 (incidência no grupo placebo) e p1 (incidência no grupo de tratamento) já eram conhecidos e foram repetidos apenas para entrar na programação do cálculo do IC95%.\nCálculo do erro padrão da RAR\n\nRAR &lt;- abs(p0 - p1)\nNNT &lt;- 1/RAR\n\nalpha &lt;- 0.05\nz &lt;- qnorm (1 - (alpha/2))\nround (z, 3)\n\n[1] 1.96\n\nEP_RAR &lt;- sqrt((((p0*(1-p0)) / n0)) + (((p1*(1-p1)) / n1)))\n\n# Limite inferior\nli_RAR &lt;- RAR - (z * EP_RAR)\nround (li_RAR, 4)\n\n[1] 0.0056\n\n# Limite superior\nls_RAR &lt;- RAR + (z * EP_RAR)\nround (ls_RAR, 4)\n\n[1] 0.0125\n\nround(print(c(li_RAR, RAR, ls_RAR), 4))\n\n[1] 0.005645 0.009066 0.012488\n\n\n[1] 0 0 0\n\n\nPortando, ao Redução Absoluta do Risco foi igual a 0,0091 (IC95%: 0,0056-0,0125). A partir destes resultados, pode-se calcular o intervalo de confiança para o NNT:\n\nli_NNT &lt;- 1/ls_RAR\nls_NNT &lt;- 1/li_RAR\n\nli_NNT \n\n[1] 80.07881\n\nls_NNT\n\n[1] 177.1497\n\n\nConcluindo, o uso da aspirina no Physicians’ Health Study reduziu o risco de infarto agudo do miocárdio em 42% (RRR), ou seja, foi eficaz. Por outro lado, para ter este impacto será necessário tratar 110 (IC95%: 80-177) pacientes para que um tenha benefício. Este NNT é grande; o ideal é um NNT &lt; 10. Apesar disso, como a aspirina tem baixo custo e seus benefícios suplantam os efeitos adversos, seu uso pode estar justificado.\n\n\n18.6.4 Número Necessário para Causar Dano\nDeve-se comparar o NNT com o Número Necessário para causar Dano (NND), em inglês, Number Needed to Harm (NNH). Deve ser interpretado como o número de pacientes tratados para que um deles apresente um efeito adverso.\nO NND é calculado pela recíproca do aumento absoluto do risco (ARA), equivalente a diferença de risco ou redução absoluta do risco:\n\\[\nNND = \\frac{1}{ARA} = \\frac{1}{I_{expostos} - I_{não \\ expostos}}\n\\]\n\n18.6.4.1 Exemplo\nNo Physicians’ Health Study (31) sobre o uso de aspirina na prevenção de IAM, foi verificado também os efeitos colaterais da aspirina, como acidentes vasculares cerebrais (AVC), Tabela 18.9.\n\n\n\n\nTabela 18.9: Physicians’ Health Study, componente aspirina e AVC.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCálculo das incidências\n\np0 &lt;- 98/11034\nround(p0, 4)\n\n[1] 0.0089\n\np1 &lt;- 119/11037\nround(p1, 4)\n\n[1] 0.0108\n\n\nPara o cálculo do NND, usa-se a função risk(), como mencionado antes:\n\np0 &lt;- 0.0089\np1 &lt;- 0.0108\nround (MKmisc::risks (p0, p1), 4)\n\n      p0       p1       RR       OR      RRI      ARI      NNH \n  0.0089   0.0108   1.2135   1.2158   0.2135   0.0019 526.3158 \n\n\nOs resultados mostram que o NND7 é igual a 526. Ou seja, para evitar um IAM há necessidade de tratar 110 pacientes e a cada 526 tratados espera-se um caso de AVC, havendo um benefício bem maior quando comparado ao risco de AVC.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#análise-de-sobrevida",
    "href": "18-epidemiologia.html#análise-de-sobrevida",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.7 Análise de sobrevida",
    "text": "18.7 Análise de sobrevida\nA análise de sobrevida é utilizada quando se pretende investigar o tempo entre o início de um estudo e a ocorrência subsequente de um evento que modifica o estado de saúde do indivíduo. É bastante usada em estudos sobre câncer, por exemplo, analisando o tempo desde a cirurgia até a morte, o tempo desde o início do tratamento até a progressão da doença, o tempo desde a resposta até a recorrência da doença. Ela também é usada para medir a ocorrência de outros eventos como o tempo desde a infecção pelo vírus da imunodeficiência humana (HIV) até o desenvolvimento da Síndrome de Imunodeficiência Adquirida (SIDA), o tempo de hospitalização, tempo de amamentação, etc.\nO interesse está centrado na verificação do efeito dos fatores de risco ou de prognóstico sobre o tempo de sobrevida de um indivíduo ou de um grupo, bem como definir as probabilidades de sobrevida em diversos momentos no seguimento do grupo. Considera-se tempo de sobrevida, ou simplesmente sobrevida, o tempo a entre a entrada do indivíduo no estudo e a ocorrência do evento de interesse. Com relação aos dados relacionados ao tempo, podem ocorrer problemas. O tempo para um evento geralmente não tem distribuição normal. Além disso, nem sempre se pode esperar até que o evento ocorra em todos os pacientes e alguns pacientes abandonam o estudo mais cedo. Todos devem ser considerados e as análises de sobrevida contornam esses problemas.\nEm estudos de sobrevida, os indivíduos são observados até a ocorrência de um evento final que, geralmente, corresponde à morte, ou à variação de um parâmetro biológico ou outro evento que indique a modificação do estado inicial (cura, recorrência, retorno ao trabalho, etc.) O evento final é denominado de falha, por referir-se, em geral, a algo indesejável.\n\n18.7.1 Dados Censurados\nQuando, em um estudo de sobrevida, os pacientes que saem do estudo ou que não vivenciam o evento são chamados de observações censuradas.\nEsses tempos de sobrevida censurados subestimam o verdadeiro (mas desconhecido) tempo para o evento. Quando o evento (supondo que ocorreria) está além do final do período de acompanhamento, a censura costuma ser chamada de censura à direita.\nA censura também pode ocorrer quando se observa a presença de um evento, mas não se sabe onde começou. Por exemplo, considere um estudo que investigue o tempo para a recorrência de um câncer após a remoção cirúrgica do tumor primário. Se os pacientes forem examinados 3 meses após a cirurgia e já tinham recorrência, então o tempo de sobrevida será censurado a esquerda, porque o tempo real (desconhecido) de recorrência ocorreu menos de 3 meses após a cirurgia.\nOs dados de tempo do evento também podem ser censurados em intervalos, o que significa que os indivíduos entram e saem da observação. Se considerarmos o exemplo anterior e os pacientes também forem examinados aos 6 meses, aqueles que estão livres da doença aos 3 meses e perdem o acompanhamento entre 3 e 6 meses são considerados censurados no intervalo. A maioria dos dados de sobrevivência incluem observações censuradas à direita (34).\n\n\n18.7.2 Método de Kaplan-Meier\nO método de Kaplan-Meier (KM) é um método não paramétrico usado para estimar a probabilidade de sobrevivência a partir dos tempos de sobrevivência observados (35).\nA função de sobrevida é a probabilidade de sobreviver a pelo menos um determinado ponto no tempo e o gráfico desta probabilidade é a curva de sobrevida. O método de sobrevida de Kaplan-Meier pode ser usado para comparar as curvas de sobrevida de dois ou mais grupos, como comparar um grupo tratado a um grupo não tratado (placebo), ou homens comparados a mulheres.\nA curva de sobrevida KM, um gráfico da probabilidade de sobrevida de Kaplan-Meier em relação ao tempo, fornece um resumo útil dos dados que podem ser usados para estimar medidas como a mediana de sobrevida.\n\n18.7.2.1 Pressupostos do método de Kaplan-Meier\nOs pressupostos para o uso da análise de sobrevida são as seguintes (36):\n\nos participantes devem ser independentes, ou seja, cada participante aparece apenas uma vez no grupo;\nos grupos devem ser independentes, ou seja, cada participante está apenas em um grupo;\ntodos os participantes são livres de eventos quando se inscrevem no estudo;\na medição do tempo até o evento deve ser precisa;\no ponto inicial e o evento são claramente definidos;\nas perspectivas de sobrevida dos participantes permanecem constantes, ou seja, os participantes inscritos no início ou no final do estudo devem ter as mesmas perspectivas de sobrevida;\na probabilidade de censura não está relacionada à probabilidade do evento.\n\nComo em todas as análises, se o número total de pacientes em qualquer grupo for pequeno, digamos menos de 30 participantes em cada grupo, os erros padrão em torno das estatísticas resumidas serão grandes e, portanto, as estimativas de sobrevida serão imprecisas. Para estudos de sobrevida, recomenda-se fazer o cálculo do tamanho amostral previamente. O R dispõe de um pacote que possibilita este cálculo, o powerSurvEpi (37).\n\n\n18.7.2.2 Exemplo\nO arquivo dadosSobrevida.xlsx contém as informações de 60 pacientes selecionados para um ensaio clínico randomizado hipotético de dois tratamentos nos quais 32 pacientes receberam o novo tratamento e 28 pacientes receberam o tratamento padrão. Para obter o arquivo, clique aqui e salve o mesmo em seu diretório de trabalho.\nDestes pacientes, 33 eram mulheres e 27 homens. Durante o estudo (65 meses), um total de 21 pacientes morreram (7 mulheres e 14 homens).\nCarregar o conjunto de dados\nA partir do diretório de trabalho, carregue para um objeto que será denominado de sobrevida, usando a função read_excel() do pacote readxl e observe os dados com a função head().\n\nsobrevida &lt;- readxl::read_excel(\"dados/dadosSobrevida.xlsx\")\nhead (sobrevida)\n\n# A tibble: 6 × 5\n     id evento tempo sexo  grupo\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1    22      0     5 fem   novo \n2    21      0     7 masc  novo \n3    19      0     8 fem   novo \n4    13      0     9 fem   novo \n5    50      1     9 masc  novo \n6    20      1    12 masc  novo \n\n\nA Saída exibe um banco de dados com cinco variáveis:\n\nid \\(\\longrightarrow\\) Identificação do indivíduo\nevento \\(\\longrightarrow\\) Desfecho. 0 = censurado; 1 = morte\ntempo \\(\\longrightarrow\\) Sobrevida em meses\nsexo \\(\\longrightarrow\\) 1 = masculino; 2 = feminino\ngrupo \\(\\longrightarrow\\) Grupo de tratamento: 1 = nova droga; 2 = padrão\n\nConstruir uma tabela tratamento vs evento\n\ntable (sobrevida$grupo, \n       sobrevida$evento, \n       dnn = c(\"Tratamento\", \"Evento\"))\n\n          Evento\nTratamento  0  1\n    novo   24  8\n    padrão 15 13\n\n\nA saída mostra o número em cada grupo, o número de eventos e o número censurados. Houve menos eventos, mas mais pacientes censurados no grupo do tratamento novo.\nCalcular as estimativas de sobrevida de Kaplan-Meier para a construção da Curva de Sobrevida de cada tratamento\nPara isso, usa-se a função survfit() do pacote survival(38). Seus principais argumentos incluem:\n\nobjeto de sobrevida, criado usando a função Surv(), aninhada na função survfit()\ne o conjunto de dados contendo as variáveis.\n\nPara a construção da tabela e da curva de sobrevida, digite e execute o seguinte:\n\ntabsurv &lt;- survfit (Surv (tempo, evento) ~ grupo, data = sobrevida)\n\nsummary(tabsurv) \n\nCall: survfit(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n                grupo=novo \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    9     29       1    0.966  0.0339       0.9013        1.000\n   12     27       1    0.930  0.0479       0.8404        1.000\n   15     26       1    0.894  0.0579       0.7874        1.000\n   16     25       1    0.858  0.0657       0.7387        0.997\n   32     15       1    0.801  0.0826       0.6545        0.980\n   36     13       1    0.739  0.0965       0.5725        0.955\n   40     11       1    0.672  0.1086       0.4897        0.923\n   58      2       1    0.336  0.2438       0.0811        1.000\n\n                grupo=padrão \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     28       3    0.893  0.0585       0.7853        1.000\n    2     25       1    0.857  0.0661       0.7369        0.997\n    3     24       1    0.821  0.0724       0.6911        0.976\n    4     23       2    0.750  0.0818       0.6056        0.929\n    7     20       1    0.712  0.0859       0.5625        0.902\n   17     19       1    0.675  0.0892       0.5210        0.875\n   21     17       2    0.596  0.0947       0.4361        0.813\n   38      9       1    0.529  0.1048       0.3592        0.780\n   52      2       1    0.265  0.1944       0.0628        1.000\n\n\nA Tabela de sobrevida é uma tabela descritiva com a coluna time, indicando o dia em que o evento ocorreu. A coluna n.risk indica o número de pacientes sob risco naquele momento. A coluna denominada n.event indica o número total de pacientes que sofreram o evento desde o início do estudo até o momento avaliado. A coluna survival indica a proporção de pacientes que sobreviveram desde o início do estudo até aquele momento. Por exemplo, a sobrevida cumulativa é de 0,801 aos 32 meses no grupo tratamento novo e de 0,529 aos 38 meses no grupo tratamento padrão.\nO método Kaplan-Meier produz uma única estatística resumida do tempo de sobrevida, isto é, a média ou mediana. O tempo médio de sobrevida é estimado a partir dos tempos observados e é mostrado para cada grupo na tabela de médias e medianas para o tempo de sobrevida.\nA sobrevida média é calculada como a soma do tempo dividido pelo número de pacientes que permanecem sem censura. Essa estatística pode ser usada para indicar o período de tempo em que um paciente pode sobreviver. O tempo mediano de sobrevida é o ponto em que metade dos pacientes experimentou o evento. Se a curva de sobrevida não cair para 0,5 (ou seja, probabilidade de sobrevida de 50%), o tempo mediano de sobrevida não poderá ser calculado.\nEstes dados podem ser visualizados na Saída, obtida com o comando:\n\nsummary(tabsurv)$table\n\n             records n.max n.start events    rmean se(rmean) median 0.95LCL\ngrupo=novo        32    32      32      8 49.92533  4.078218     58      40\ngrupo=padrão      28    28      28     13 36.62437  5.403382     52      21\n             0.95UCL\ngrupo=novo        NA\ngrupo=padrão      NA\n\n\nVisualização da curva de sobrevida\nPode-se visualizar a curva (Figura 18.8) de uma maneira simples, utilizando a função plot() do pacote básico do R:\n\nplot (tabsurv, col = c (\"steelblue\", \"rosybrown\"), lwd = 2)\nlegend (legend = c (\"Tratamento novo\", \"Tratamento padrão\"), \n        fill = c (\"steelblue\", \"rosybrown\"), \n        bty=\"n\", \n        cex = 1, \n        y = 0.3,\n        x = 5)\n\n\n\n\n\n\n\nFigura 18.8: Curva de sobrevida comparando dois grupos de tratamento.\n\n\n\n\n\nOutra maneira, mais sofisticada, de produzir a curva de KM é usando a função ggsurvplot(), incluída no pacote survminer (39) que utiliza o pacote ggplot2 (Figura 18.9)\nCom essa função é possível mostrar:\n\nos intervalos de confiança de 95% da função de sobrevida, usando o argumento conf.int = TRUE;\no número e/ou a porcentagem de indivíduos em risco por tempo, utilizando a opção risk.table. Os valores permitidos para a risk.table incluem:\nTRUE ou FALSE especificando se deve mostrar ou não a tabela de risco. O padrão é FALSE.\nabsolute ou percentage: para mostrar o número absoluto e o percentual de sujeitos em risco por tempo, respectivamente. Use abs_pct para mostrar o número absoluto e a porcentagem.\no nrisk_cumcensor e nrisk_cumevents . Mostra o número em risco e o número acumulado de censura e eventos, respectivamente.\no valor P do teste Log-Rank comparando os grupos usando pval = TRUE.\nlinha horizontal/vertical na sobrevida mediana usando o argumento surv.median.line. Os valores permitidos incluem um de c(“nenhum”, “hv”, “h”, “v”). Onde v = vertical, h = horizontal.\n\n\nggsurvplot (tabsurv,\n            pval = TRUE, \n            conf.int = FALSE,\n            risk.table = \"abs_pct\",\n            risk.table.col = \"strata\", \n            surv.median.line = \"hv\", \n            ggtheme = theme_bw (), \n            legend.labs = c (\"Tratamento Novo\", \n                             \"Tratamento padrão\"),\n            palette = c (\"steelblue\", \"tomato\"))\n\n\n\n\n\n\n\nFigura 18.9: Curva de sobrevida comparando dois grupos de tratamento, usando ggsurvplot().\n\n\n\n\n\nO teste Log Rank pondera todos os pontos de tempo igualmente e é a estatística de sobrevida mais usada (40). O teste de log rank é um teste não paramétrico, que não faz suposições sobre as distribuições de sobrevivência. Os pressupostos deste teste são os mesmos do método de Kaplan-Meier. No exemplo, o valor P do teste é fornecido na Figura 18.9 e é igual a 0,083, ou seja, acima de 0,05, indicando não rejeição da \\(H_{0}\\). A hipótese nula diz que não há diferença na sobrevivência entre os dois grupos.\nEssencialmente, o teste de log rank compara o número observado de eventos em cada grupo com o que seria esperado se a hipótese nula fosse verdadeira (ou seja, se as curvas de sobrevivência fossem idênticas). A estatística de log rank é aproximadamente distribuída como uma estatística de teste qui-quadrado.\nA função survdiff(), também do pacote survival, pode ser usada para calcular o teste de log-rank comparando duas ou mais curvas de sobrevida e pode ser usado da seguinte forma:\n\ndif_sobrevida &lt;- survdiff (Surv (tempo, evento) ~ grupo, data = sobrevida)\ndif_sobrevida\n\nCall:\nsurvdiff(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n              N Observed Expected (O-E)^2/E (O-E)^2/V\ngrupo=novo   32        8    11.91      1.28      3.01\ngrupo=padrão 28       13     9.09      1.68      3.01\n\n Chisq= 3  on 1 degrees of freedom, p= 0.08 \n\n\nA suposição de que o risco de um evento em um grupo em comparação com o outro grupo não muda ao longo do tempo é chamado de risco proporcional. Se as curvas de sobrevida se cruzam, isso sugere que os riscos não são proporcionais. Nessa situação, o teste log rank será menos poderoso e um teste alternativo deve ser considerado, como a Regressão de Cox ou Modelo de Riscos Proporcionais.\n\n\n\n18.7.3 Regressão de Cox ou Modelo de Riscos Proporcionais\nO modelo tem como objetivo a examinar simultaneamente como os fatores especificados influenciam a taxa de ocorrência de um determinado evento (por exemplo, infecção, morte) em um determinado ponto no tempo. Essa taxa é referida como hazard ratio.\nGeralmente, as variáveis preditoras (ou fatores) são denominadas covariáveis. O modelo de Cox é expresso pela função de risco denotada por h(t). Pode ser interpretada como o risco de morrer no tempo t e estimada da seguinte forma:\n\\[\nh\\left(t\\right) = h_{0} \\left(t\\right) \\times e^{\\left( {b_{1}x_{1}+b_{2}x_{2}+...+b_{n}x_{n}} \\right)}\n\\]\nOnde,\n\nt é o tempo de sobrevida, indica que o risco varia com o tempo;\nh(t) é a função de risco (hazard) determinada por um conjunto de n covariáveis (\\(x_{1}, x_{2}, ..., x_{n}\\));\nOs coeficientes (\\(b_{1}, b_{2}, ..., b_{n}\\)) medem o tamanho do efeito das covariáveis;\nh(0) é o risco basal, o valor do risco se todos os \\(x_{i}\\) fossem iguais a zero (\\(exp(0) = 1\\)).\n\nAs quantidades exp(\\(b_{i}\\)) são chamadas de hazard ratio (HR). Uma hazard ratio acima de 1 indica uma covariável que está positivamente associada à probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida.\nResumindo,\n\nHR = 1: Sem efeito\nHR &lt;1: Redução do risco\nHR&gt; 1: Aumento do risco\n\nPara calcular o modelo de Cox no R serão utilizados os mesmos dados da do arquivo dadosSobrevida.xlsx.\nO pacote survival tem uma função para calcular o modelo de Cox, coxph(), que usa os argumentos:\n\nformula \\(\\longrightarrow\\) é o modelo linear com um objeto de sobrevivida como variável desfecho. O objeto de sobrevida é criado usando a função Surv() como segue: Surv(tempo, evento).\ndata \\(\\longrightarrow\\) um banco de dados contendo as variáveis.\n\n\nmod.cox &lt;- coxph (Surv (tempo, evento) ~ grupo, data = sobrevida)\nmod.cox\n\nCall:\ncoxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n              coef exp(coef) se(coef)     z      p\ngrupopadrão 0.7698    2.1593   0.4505 1.709 0.0875\n\nLikelihood ratio test=3.03  on 1 df, p=0.08171\nn= 60, number of events= 21 \n\n\nA função summary() fornece um relatório mais completo:\n\nsummary(mod.cox)\n\nCall:\ncoxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida)\n\n  n= 60, number of events= 21 \n\n              coef exp(coef) se(coef)     z Pr(&gt;|z|)  \ngrupopadrão 0.7698    2.1593   0.4505 1.709   0.0875 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ngrupopadrão     2.159     0.4631     0.893     5.221\n\nConcordance= 0.637  (se = 0.054 )\nLikelihood ratio test= 3.03  on 1 df,   p=0.08\nWald test            = 2.92  on 1 df,   p=0.09\nScore (logrank) test = 3.06  on 1 df,   p=0.08\n\n\nOs resultados da regressão de Cox, podem ser interpretados da seguinte forma:\n\nSignificância estatística. A coluna marcada com z fornece o valor da estatística Wald. Corresponde à razão de cada coeficiente de regressão para seu erro padrão (\\(z = \\frac{coef}{EP_{coef}}\\)). A estatística Wald avalia se o coeficiente beta (\\(\\beta\\)) de uma determinada variável é estatisticamente diferente de 0. A partir da saída, pode-se concluir que não há diferença estatisticamente significativa entre os grupos (P = 0,0875).\nCoeficientes de regressão. A seguir deve-se observar, no modelo de Cox, o sinal dos coeficientes de regressão (coef). Um sinal positivo significa que o hazard (risco) é maior e, portanto, pior o prognóstico, para sujeitos com valores mais elevados dessa variável. No exemplo, a variável grupo é codificada como 1=novo, 2=padrão. O resumo do modelo de Cox fornece a hazard ratio (HR) para o segundo grupo em relação ao primeiro grupo, ou seja, tratamento padrão versus tratamento novo. O coeficiente beta para grupo = 0,7698 indica que os indivíduos do tratamento padrão têm maior risco de morte (taxas de sobrevivência mais baixas) do que os do grupo tratamento novo, nesses dados. Entretanto, esta diferença não é estatisticamente significativa.\nHazard ratios. Os coeficientes exponenciados (exp(coef) = exp(0,7698) = 2,1593), também conhecidos como hazard ratio, fornecem o tamanho do efeito das covariáveis. Por exemplo, ser do grupo padrão aumenta o risco por um fator de 2,1593. Se esta diferença fosse significativa (P &lt; 0,05), pertencer ao grupo padrão estaria associado a um mau prognóstico.\nIntervalos de confiança das taxas de risco. O resultado do resumo também fornece intervalos de confiança de 95% para a razão de risco (exp(coef)), limite inferior de 95% = 0,893, limite superior de 95% = 5,221, mostrando a não significância estatística, pois cruza o 1.\nSignificância estatística global do modelo. Finalmente, a saída fornece valores de P para três testes alternativos para significância geral do modelo: O teste de razão de verossimilhança (Likelihood ratio test), teste de Wald e a estatística logrank. Esses três métodos são equivalentes. Para um tamanho amostral grande, eles darão resultados semelhantes. Para n pequeno, eles podem diferir um pouco. O teste de razão de verossimilhança tem melhor comportamento para tamanhos de amostra pequenos, por isso é geralmente preferido.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#regressão-logística-binária",
    "href": "18-epidemiologia.html#regressão-logística-binária",
    "title": "18  Estatística em Epidemiologia",
    "section": "18.8 Regressão logística binária",
    "text": "18.8 Regressão logística binária\nA regressão logística (também conhecida como regressão logit ou modelo logit) foi desenvolvida como um a extensão do modelo linear pelo estatístico David Cox em 1958 (41). Pertence a uma família, denominada Modelo Linear Generalizado (GLM) e é um modelo de regressão em que o desfecho Y é categórico.\nA regressão logística permite estimar a probabilidade de uma resposta categórica com base em uma ou mais variáveis preditoras (X). Possibilita informar se a presença de um preditor aumenta (ou diminui) a probabilidade de um determinado desfecho em uma porcentagem específica. No caso em que Y é binário - ou seja, assume apenas dois valores, 0 e 1, que representam desfechos como aprovação/reprovação, sim/não, vivo/morto ou saudável/doente, tem-se a regressão logística binária.\nNa regressão logística binária, as variáveis que afetam a probabilidade do resultado são medidas como Odds Ratio, que são chamadas de Odds Ratios ajustadas (42).\nNa regressão linear, os valores das variáveis desfecho são preditos a partir de uma ou mais variáveis explicativas. Na regressão logística, uma vez que o desfecho é binário, a probabilidade de o desfecho ocorrer é calculada com base nos valores das variáveis explicativas. A regressão logística é semelhante à regressão linear na medida em que uma equação de regressão pode ser usada para prever a probabilidade de ocorrência de um desfecho. No entanto, a equação de regressão logística é expressa em termos logarítmicos (ou logits) e, portanto, os coeficientes de regressão devem ser convertidos para serem interpretados.\nEmbora as variáveis explicativas ou preditores no modelo possam ser variáveis contínuas ou categóricas, a regressão logística é mais adequada para medir os efeitos das exposições ou variáveis explicativas que são variáveis binárias. Variáveis contínuas podem ser incluídas, mas a regressão logística produzirá uma estimativa de risco para cada unidade de medida. Assim, a suposição de que o efeito de risco é linear sobre cada unidade da variável deve ser atendida e a relação não deve ser curva ou ter um ponto de corte sobre o qual o efeito ocorre. Além disso, as interações entre variáveis explicativas podem ser incluídas (42). Os casos em que a variável dependente tem mais de duas categorias de resultados podem ser analisados com regressão logística multinomial, não mostrada neste livro.\n\n18.8.1 Função Logistica\nNa regressão logística, para a estimação dos coeficientes (coeficientes \\(\\beta\\)) das variáveis independentes, utiliza-se o modelo logit (43):\n\\[\nlogit_{i} = ln(\\frac{p}{1-p})=b_{0} + b_{1}x_{1} +...+ b_{n}x_{n}\n\\] ou\n\\[\nlogit_{i} = ln(\\frac{p}{1-p})=e^{b_{0} + b_{1}x_{1} +...+ b_{n}x_{n}}\n\\]\nonde p é a probabilidade de desfecho.\nA função logística transforma valores entre \\(-\\infty\\) a \\(+\\infty\\) em valores entre 0 e 1, de modo que os números que especificam as probabilidades estarão entre 0 e 1. Esse termo logarítmico é chamado de \\(log_{OR}\\) (razão de chances logarítmica) ou logit. As chances (odds) associadas a alguma probabilidade são \\(p/(1-p)\\), o que é evidente se for entendido que um evento com chances a para b significa que em \\(a+b\\) tentativas independentes e identicamente distribuídas, espera-se um sucesso. Portanto, a probabilidade de sucesso deve ser \\(a/(a+b)\\).\nInvertendo, se a probabilidade de sucesso é \\(a/(a+b)\\), então a razão se torna \\([a/(a+b)]/[1-a/(a+b)]\\) ou \\(a/b\\), que é a razão de chances ou odds ratio.\nPara concluir o modelo, é necesário especificar a distribuição do desfecho \\(Y_{i}\\). Ela é binomial com probabilidade de sucesso p, de modo que nenhum parâmetro extra, como um desvio padrão, é necessário (44).\nA regressão logística frequentemente utiliza a curva logística (Figura 18.10) para assim representar a relação entre a variável dependente e as independentes. Os valores previstos, portanto permanecem entre 0 e 1, sendo definidos pelos coeficientes estimados.\n\n\n\n\n\n\n\n\nFigura 18.10: Curva logística geral\n\n\n\n\n\nA estimação dos coeficientes na regressão logística, diferentemente da regressão múltipla que utiliza o método dos mínimos quadrados, é realizada através da máxima verossimilhança. Este método busca encontrar as estimativas mais prováveis dos coeficientes e maximizar a probabilidade de ocorrência de um evento. A qualidade do ajuste do modelo é avaliada pelo “pseudo” \\(R^2\\) e pela análise da precisão preditiva (matriz de confusão).\nO valor da verossimilhança (likelihood) é semelhante ao procedimento das somas dos quadrados na regressão múltipla, estimando o quão bem o método de máxima verossimilhança se ajusta ao modelo. O ajuste da estimação do modelo é dado pelo valor de -2 vezes o logaritmo da verossimilhança (-2LL), sendo que, quanto menor este valor, melhor o modelo (45).\n\n\n18.8.2 Pressupostos da regressão logística\nPara garantir que a regressão seja adequada, para melhorar a precisão do modelo, assumimos quatro pressupostos principais:\n\nIndependência dos dados: o valor de uma observação não influencia ou afeta o valor de outras observações. Este é o pressuposto básico.\nLinearidade dos dados: a relação entre as variáveis independentes e a curva logística da variável dependente é considerada linear (quanto mais/menos de uma, mais/menos de outra). Linearidade dos Dados pode ser verificada graficamente observando a dispersão dos resíduos com os valores previstos pela regressão.\nIndependência dos erros/resíduos: os erros (também chamados de resíduos) não devem possuir correlação. Este pressuposto pode ser testado pelo teste de Durbin-Watson (veja a Seção 15.3.2.5) e observando o gráfico quantil-quantil (Q-Q) dos resíduos padronizados.\nHomogeneidade de Variância dos erros/resíduos: os erros devem ter média zero e desvio padrão constante ao longo das observações. Similar ao teste de Levene, mas aplicado aos resíduos da regressão. Pode ser testado usando o Teste de Breusch-Pagan (veja a Seção 15.3.2.4).\nAusência de Multicolinearidade: multicolinearidade é a ocorrência de alta correlação entre duas ou mais variáveis independentes e pode levar a resultados distorcidos. Em geral, a multicolinearidade pode fazer com que os intervalos de confiança se ampliem, ou até mudar o sinal de influência das variáveis independentes (de positivo para negativo, por exemplo). Portanto, as inferências estatísticas de uma regressão com multicolinearidade não são confiáveis. Pode ser testado usando o Fator de Inflação de Variância (Variance Inflation Factor – VIF, veja a Seção 18.8.5.3).\n\n\n\n18.8.3 Dados para a regressão logística\nO RMS Titanic (Figura 18.11), considerado o navio inafundável de sua época, sofreu um trágico destino em sua viagem inaugural com destino a Nova Iorque. Construído com o intuito de ser o maior e mais luxuoso transatlântico, o Titanic colidiu com um iceberg na noite de 14 de abril de 1912, no Oceano Atlântico Norte, afundando rapidamente e causando a morte de inúmeras pessoas.\n\n\n\n\n\n\n\n\nFigura 18.11: Titanic parado em Queenstown, 11 de abril.de 1912\n\n\n\n\n\nO Titanic partiu em sua primeira e única viagem com 1 316 passageiros a bordo: 325 na primeira classe, 285 na segunda e 706 na terceira. Deles, 922 embarcaram em Southampton, 274 em Cherbourg-Octeville na França e 120 em Queenstown na Irlanda. Além desses passageiros, havia 908 tripulantes, totalizando m 2224 pessoas. O número total de mortos mais aceito é 1514, quase 70% dos que embarcaram na viagem (46). Os dados de 1309 passageiros estão em um arquivo denominado dadosTitanic.xlsx, obtido no pacote titanic, modificado estruturalmente e traduzido, sem alterar os dados, para chegar a este arquivo que pode ser baixado aqui. Possui 1309 observações e 12 variáveis:\n• id \\(\\longrightarrow\\) identificação do passageiro\n• sobreviveu \\(\\longrightarrow\\) 0 = não; 1 = sim\n• classe \\(\\longrightarrow\\) classe do passageiro (categórica): 1 = 1ª classe; 2 = 2ª classe e 3 = 3ª classe (qualitativa)\n• nome \\(\\longrightarrow\\) nome do passageiro (nominal)\n• sexo \\(\\longrightarrow\\) masc = masculino; fem = feminino (binária)\n• idade \\(\\longrightarrow\\) idade em anos (numérica contínua)\n• irco \\(\\longrightarrow\\) número de irmãos/cônjuges a bordo (numérica discreta)\n• pafi \\(\\longrightarrow\\) número de pais/filhos a bordo (numérica discreta)\n• ticket \\(\\longrightarrow\\) número do bilhete de embarque (nominal)\n• tarifa \\(\\longrightarrow\\) valor pago pela passagem em dólares (numérica contínua)\n• cabine \\(\\longrightarrow\\) número de identificação da cabine (nominal)\n• porto_embarque \\(\\longrightarrow\\) porto de embarque: C = Cherbourg, Q = Queenstown, S = Southampton\n\n18.8.3.1 Leitura dos dados\nApós fazer o download do banco de dados em seu diretório 8, carregue-o no RStudio, usando a função read_excel() do pacote readxl:\n\ndadosTitanic &lt;- readxl::read_excel(\"dados/dadosTitanic.xlsx\")\n\n\n\n18.8.3.2 Explorando e preparando os dados\nDê uma olhada nas variáveis do banco de dados. Para essa ação, pode-se usar a função str():\n\nstr(dadosTitanic)\n\ntibble [1,309 × 12] (S3: tbl_df/tbl/data.frame)\n $ id            : num [1:1309] 1 2 3 4 5 6 7 8 9 10 ...\n $ sobreviveu    : num [1:1309] 0 1 1 1 0 0 0 0 1 1 ...\n $ classe        : num [1:1309] 3 1 3 1 3 3 1 3 3 2 ...\n $ nome          : chr [1:1309] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ sexo          : chr [1:1309] \"masc\" \"fem\" \"fem\" \"fem\" ...\n $ idade         : num [1:1309] 22 38 26 35 35 NA 54 2 27 14 ...\n $ irco          : num [1:1309] 1 1 0 1 0 0 0 3 0 1 ...\n $ pafi          : num [1:1309] 0 0 0 0 0 0 0 1 2 0 ...\n $ ticket        : chr [1:1309] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ tarifa        : num [1:1309] 7.25 71.28 7.92 53.1 8.05 ...\n $ cabine        : chr [1:1309] NA \"C85\" NA \"C123\" ...\n $ porto_embarque: chr [1:1309] \"S\" \"C\" \"S\" \"S\" ...\n\n\nObserva-se que se tem um tibble com 1309 linhas (casos = passageiros) e 12 colunas (variáveis). Algumas dessas variáveis não terão utilidade para a análise de regressão logística: por exemplo, a coluna índice (id), o nome do passageiro, o número do ticket de embarque, a tarifa, o número da cabine e o porto de embarque. Elas serão removidas do banco de dados, usando a função select() do pacote dplyr (veja a Seção 5.3):\n\ndadosTitanic &lt;- dplyr::select(dadosTitanic, \n                              -c(id, nome, ticket, tarifa, cabine, porto_embarque))\n\n\n\n18.8.3.3 Verificando e tratando os dados omissos\nPara verificar os NAs no banco de dados, pode-se usar o comando que soma os dados faltantes em cada coluna do banco de dados:\n\ncolSums(is.na(dadosTitanic))\n\nsobreviveu     classe       sexo      idade       irco       pafi \n         0          0          0        263          0          0 \n\n\nA coluna da variável idade contém 263 valores faltantes, ou seja, 20% estão ausentes. A melhor forma de tratar dados faltantes depende de diversos fatores, como (47) :\n\nMecanismo de geração dos dados faltantes: Por que os dados estão faltando? É aleatório, relacionado a outras variáveis ou a alguma característica da população?\n\nQuantidade de dados faltantes: 263 valores faltantes representam uma proporção considerável dos dados (20%).\n\nImpacto na análise: Como a presença de dados faltantes pode afetar os resultados da sua análise?\n\nO que fazer?\n\nExclusão de casos:\n\n\n\nListwise deletion: Remover todas as observações com algum dado faltante. Não recomendado neste caso, pois você perderia uma quantidade significativa de dados.\nPairwise deletion: Utilizar todas as observações disponíveis para cada análise. Pode gerar resultados inconsistentes.\n\n\n\nImputação:\n\n\n\nImputação por média, mediana ou moda: Substituir os valores faltantes pela média, mediana ou moda da variável. Simples, mas pode subestimar a variância.\n\nImputação por regressão: Utilizar um modelo de regressão para prever os valores faltantes com base em outras variáveis. Mais preciso, mas pode ser enviesado se o modelo não for adequado.\n\nImputação múltipla: Criar múltiplos conjuntos de dados, cada um com diferentes valores imputados, e combinar os resultados das análises. Método mais robusto e permite estimar a incerteza.\n\nImputação por K-Nearest Neighbors: Substituir os valores faltantes pela média dos k vizinhos mais próximos. Útil para dados numéricos e pode capturar padrões locais.\n\nConsiderando a quantidade de dados faltantes e a natureza da variável idade, a imputação múltipla é uma excelente opção. Ela permite lidar com a incerteza associada aos valores imputados e fornece uma estimativa mais precisa dos parâmetros do modelo. Para isso, será usada a função mice() do pacote mice (Multivariate Imputation by Chained Equations) (48) para fazer a imputação múltipla. O argumento m = 5 se refere ao número de múltiplas imputações (cinco é o padrão); method = “pmm” é o método de imputação, onde se usa a correspondência da média preditiva (predictive mean matching – dados numéricos) como método de imputação. O método pmm garante que os valores imputados sejam próximos de valores reais do conjunto de dados, mantendo a coerência estatística. A função mice() produz várias cópias completas de dataframe, cada uma com diferentes imputações dos dados ausentes. Essa transformação será atribuída a um objeto de nome dados.\n\ndados &lt;- mice::mice(dadosTitanic, m=5, method=\"pmm\")\n\n\n iter imp variable\n  1   1  idade\n  1   2  idade\n  1   3  idade\n  1   4  idade\n  1   5  idade\n  2   1  idade\n  2   2  idade\n  2   3  idade\n  2   4  idade\n  2   5  idade\n  3   1  idade\n  3   2  idade\n  3   3  idade\n  3   4  idade\n  3   5  idade\n  4   1  idade\n  4   2  idade\n  4   3  idade\n  4   4  idade\n  4   5  idade\n  5   1  idade\n  5   2  idade\n  5   3  idade\n  5   4  idade\n  5   5  idade\n\n\nA seguir, cria-se um novo dataframe com os dados imputados, usando a função complete(), também do pacote mice. Essa função retorna um ou vários desses conjuntos de dados, sendo que o padrão é o primeiro. Este novo conjunto de dados será atribuído ao objeto dados_completos. O comando escolhe a primeira imputação:\n\ndados_completos &lt;- mice::complete(dados, 1) \n\nAnalizando os dados_completos\nNovamente, se usará as funções colSums(is.na()) e str():\n\ncolSums(is.na(dados_completos))\n\nsobreviveu     classe       sexo      idade       irco       pafi \n         0          0          0          0          0          0 \n\nstr(dados_completos)\n\n'data.frame':   1309 obs. of  6 variables:\n $ sobreviveu: num  0 1 1 1 0 0 0 0 1 1 ...\n $ classe    : num  3 1 3 1 3 3 1 3 3 2 ...\n $ sexo      : chr  \"masc\" \"fem\" \"fem\" \"fem\" ...\n $ idade     : num  22 38 26 35 35 48 54 2 27 14 ...\n $ irco      : num  1 1 0 1 0 0 0 3 0 1 ...\n $ pafi      : num  0 0 0 0 0 0 0 1 2 0 ...\n\n\nAgora, existem 1309 observações e 7 variáveis. Os dados faltantes não estão mais presentes, foram substituídos.\n\n\n18.8.3.4 Outras transformações necessárias\nAinda há necessidade de outras transformações, pois verifica-se que existem variáveis classificadas como numéricas (classe, sexo e a variável desfecho sobreviveu), mas que são fatores. Esta modificação será realizada, usando a função mutate() do pacote dplyr(veja a Seção 5.3):\n\ndados_completos &lt;- dados_completos %&gt;%\n  mutate(sexo = factor(sexo)) %&gt;%\n  mutate(classe = factor(classe)) %&gt;% \n  mutate(sobreviveu = factor(sobreviveu))\n\nstr(dados_completos)\n\n'data.frame':   1309 obs. of  6 variables:\n $ sobreviveu: Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ classe    : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ sexo      : Factor w/ 2 levels \"fem\",\"masc\": 2 1 1 1 2 2 2 2 1 1 ...\n $ idade     : num  22 38 26 35 35 48 54 2 27 14 ...\n $ irco      : num  1 1 0 1 0 0 0 3 0 1 ...\n $ pafi      : num  0 0 0 0 0 0 0 1 2 0 ...\n\n\nFinalmente, os dados estão prontos para se iniciar a análise de regressão logística 9.\n\n\n\n18.8.4 Construção do Modelo de Regressão Logística\n\n18.8.4.1 Divisão dos dados em Treino e Teste\nA divisão dos dados em conjuntos de treino e teste é uma prática frequente em aprendizado de regressão logística. Serve para avaliar a performance de um modelo de forma imparcial precisa. O conjunto de Treino é a parte dos dados que o modelo utiliza para aprender as relações entre as variáveis independentes e a variável dependente. O modelo analisa esses dados para encontrar os melhores coeficientes que descrevem a relação entre as variáveis. O conjunto de Teste é a parte dos dados que o modelo não analisou durante o treinamento. Ele é utilizado para avaliar a capacidade do modelo de fazer previsões em novos dados. Ao comparar as previsões do modelo com os valores reais no conjunto de teste, podemos medir a sua precisão e generalização. Essa técnica é utilizada para evitar o excesso de ajuste dos dados (overfitting) (49), estimar a acurácia do modelo e comparação de modelos. A divisão mais comum é 70% para treinamento e 30% para teste, mas essa proporção pode variar dependendo do tamanho do conjunto de dados e da complexidade do problema.\n\n# Definindo a semente para reprodutibilidade\nset.seed(123)\n\n# Embaralhando os dados\ndados_completos &lt;- dados_completos %&gt;% sample_frac(size = 1)\n\n# Definindo o ponto de corte para treino\nsplit_index &lt;- round(0.7 * nrow(dados_completos))\n\n# Dividindo os dados\ndadosTreino &lt;- dados_completos %&gt;% slice(1:split_index)\ndadosTeste &lt;- dados_completos %&gt;% slice((split_index + 1):n())\n\n# Visualizando o tamanho das amostras\ndim(dadosTreino)\n\n[1] 916   6\n\ndim(dadosTeste)\n\n[1] 393   6\n\n# Explorando a estrutura dos dadosTreino\nstr (dadosTreino)\n\n'data.frame':   916 obs. of  6 variables:\n $ sobreviveu: Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 2 2 2 1 ...\n $ classe    : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 2 3 1 1 2 2 3 1 ...\n $ sexo      : Factor w/ 2 levels \"fem\",\"masc\": 2 2 2 2 1 2 1 1 1 2 ...\n $ idade     : num  44 47 30 40.5 44 45 0.92 24 22 36 ...\n $ irco      : num  0 0 0 0 0 0 1 1 2 0 ...\n $ pafi      : num  0 0 0 0 0 0 2 1 0 0 ...\n\nstr(dadosTeste)\n\n'data.frame':   393 obs. of  6 variables:\n $ sobreviveu: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 1 1 2 1 1 ...\n $ classe    : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 3 3 3 1 2 3 1 3 3 ...\n $ sexo      : Factor w/ 2 levels \"fem\",\"masc\": 2 1 2 2 1 2 1 1 2 1 ...\n $ idade     : num  30 18 7 25 60 30 8 22 51 28 ...\n $ irco      : num  1 0 4 0 1 1 3 0 0 0 ...\n $ pafi      : num  2 0 1 0 0 1 1 2 0 0 ...\n\n\nA função sample_frac() do pacote dplyr tem a função de embaralhar os dados para garantir a aleatoriedade; o argumento size =1 significa a fração de linhas selecionadas (0 e 1), sendo 1 (u)m equivalente a 100% das linhas. A função slice(), também do pacote dplyr, primeiro seleciona as primeiras 70% observações para treino e as últimas 30% para teste. Por último, a função str(), visualiza os dadosTreino que serão utilizados na construção do modelo de regressão logística.\n\n\n18.8.4.2 Criação do Modelo de Regressão Logística\nInicialmente, serão usados os dadosTreino para criar o modelo de regressão logística.\nNo modelo, a variável resposta ou desfecho é representada por sobreviveu; todas as outras variáveis são variáveis explicativas.\nA função nativa glm() – generalized linear model - é usada para aplicar uma regressão logística no R. Sua funcionalidade é idêntica à função lm() da regressão linear. Necessita alguns argumentos:\n\nformula \\(\\to\\) objeto da classe formula. Um preditor típico tem o formato resposta \\~ preditor em que resposta, na regressão logística binária, é uma variável dicotômica e o preditor pode ser uma série de variáveis numéricas ou categóricas;\nfamily \\(\\to\\) uma descrição da distribuição de erro e função de link a ser usada no modelo glm, pode ser uma string que nomeia uma função de family. O padrão é family = gaussian(). No caso da regressão logística binária, family = binomial() ou family = binomial (link =”logit”). Para outras informações, use help(glm) ou help(family);\ndata \\(\\to\\) banco de dados.\n… \\(\\to\\) ………….\n\nDentro dos parênteses da função glm(), são fornecidas informações essenciais sobre o modelo. À esquerda do til (~), encontra-se a variável dependente, que deve ser codificada como 0 e 1 para que a função a interprete corretamente como binária. Após o til, são listadas as variáveis preditoras. Quando se utiliza um ponto (~.), isso indica a inclusão de todas as variáveis preditoras disponíveis. Já o uso do asterisco (*) entre duas variáveis preditoras especifica que, além dos efeitos principais, também deve ser considerado um termo de interação entre elas. No exemplo apresentado, essa análise, inicialmente, não será solicitada. Por fim, após a vírgula, define-se que a distribuição utilizada é a binomial. Como a função glm usa logit como link padrão para uma variável de desfecho binomial, não há necessidade de especificá-lo explicitamente no modelo.\nO modelo inicial de regressão logística do tipo entrada forçada (enter), método padrão de conduzir uma regressão, que consiste em simplesmente colocar todos os preditores no modelo de regressão em um bloco e estimar parâmetros para cada um (50). O dataframe dadosTreino.xlsx será usado com todos os preditores dentro da função. O objeto criado será denominado de modelo1.\n\nmodelo1 &lt;- glm(sobreviveu ~., \n               data = dadosTreino, \n               family = binomial(link = \"logit\"))\n\nsummary(modelo1)\n\n\nCall:\nglm(formula = sobreviveu ~ ., family = binomial(link = \"logit\"), \n    data = dadosTreino)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.705414   0.386251   9.593  &lt; 2e-16 ***\nclasse2     -1.040567   0.272522  -3.818 0.000134 ***\nclasse3     -2.035809   0.256852  -7.926 2.26e-15 ***\nsexomasc    -3.567786   0.219514 -16.253  &lt; 2e-16 ***\nidade       -0.022291   0.006916  -3.223 0.001268 ** \nirco        -0.385627   0.112853  -3.417 0.000633 ***\npafi        -0.074808   0.116112  -0.644 0.519396    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1210.4  on 915  degrees of freedom\nResidual deviance:  722.4  on 909  degrees of freedom\nAIC: 736.4\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n18.8.4.3 Interpretação dos Coeficientes\nEm uma regressão logística, a resposta que está sendo modelada é o log(odds) ou logit de que o desfecho é igual a 1. Os coeficientes de regressão fornecem a mudança no log(odds) no desfecho para a mudança de uma unidade na variável preditora, mantendo todas as outras variáveis preditivas constantes (51).\nNa saída da função summary(), são produzidos os coeficientes da regressão na coluna Estimate, associados ao respectivos desvio padrão dos resíduos (Std. Error), o valor z (estatística de Wald) e os valores do P (Pr(&gt;|t|). Importante destacar que a hipótese nula dos coeficientes da regressão é de que “os coeficientes são nulos/zeros”. Então, os valores P devem ser interpretados como a probabilidade de se observar valores de coeficientes tão extremos dado que a hipótese nula é verdadeira. Estes coeficientes estão modelando a probabilidade do evento desfecho (sobreviveu) ocorrer com base nas variáveis explicativas (sexo, idade, classe do passageiro, número de irmãos/conjuges [irco] a bordo e número de pais/filhos [pafi] a bordo). Os coeficientes da regressão representam o impacto de cada variável explicativa na probabilidade de sobrevivência.\nNas variáveis contínuas , para cada aumento de uma unidade na idade, por exemplo, o \\(log(odds)\\) de sobreviver 1 = sim (versus 0 = não) diminui, pois o coeficiente é negativo, em -0.0222909.\nPara variáveis categóricas, o desempenho de cada categoria é avaliado em relação a uma categoria de base. A categoria de base para a variável sexo é o sexo feminino (primeira categoria que aparece quando se observa os níveis) e para a classe do passageiro é a 1ª classe.\n\nlevels(dadosTreino$sexo)\n\n[1] \"fem\"  \"masc\"\n\nlevels(dadosTreino$classe)\n\n[1] \"1\" \"2\" \"3\"\n\n\nResumindo:\n\n(Intercept): Representa o log(odds) de sobreviver para um indivíduo com todas as variáveis explicativas iguais a zero (ou a sua categoria de referência). No contexto do Titanic, seria um indivíduo do sexo feminino, com idade zero e da 1ª classe. Não tem interpretação prática direta, nesse caso, pois não há passageiros com idade zero. Serve como ponto de partida para comparar os outros coeficientes.\n\nclasse2: O coeficiente negativo indica que os passageiros da segunda classe tinham menor probabilidade de sobreviver comparados aos da primeira classe (a categoria de referência).\n\nclasse3: O coeficiente negativo) indica que os passageiros da terceira classe tinham muito menor probabilidade de sobreviver comparados aos da primeira classe.\nsexomasc: O coeficiente negativo indica que ser do sexo masculino diminui significativamente a probabilidade de sobreviver, comparado a ser do sexo feminino (a categoria de referência). Ou seja, os homens tinham menos chances de sobreviver.\n\nidade: O coeficiente negativo indica que cada aumento de um ano na idade diminui ligeiramente a probabilidade de sobreviver. Isso sugere que as crianças tinham mais chances de sobreviver do que os adultos.\n\nirco : Ter mais irmãos ou cônjuges a bordo reduziu a chance de sobrevivência.\npafi: O número de pais ou filhos a bordo não teve um efeito estatisticamente significativo na sobrevivência (valor P alto).\n\nOutras métricas da saída do sumário do modelo\n\nSignificância estatística: o R, para facilitar, informa com asteriscos quais variáveis possuem coeficientes estatisticamente significativos: * para P &lt; 0,05, ** para P* &lt; 0,01, e *** para P &lt; 0,001.\n\nDeviance residual: Indica o ajuste do modelo; quanto menor, melhor.\n\nAIC (Critério de Informação de Akaike) 10: Ajuda a comparar modelos; quanto menor, melhor o ajuste e a simplicidade.\n\n\n\n18.8.4.4 Avaliação do impacto com a função anova()\nAlém de interpretar os coeficientes, mostrados na saída do sumário do modelo gerado pela função glm(), pode-se usar a função anova() para observar o impacto da adição de cada variável no modelo:\n\nanova(modelo1, test='Chisq')\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: sobreviveu\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                     915    1210.44              \nclasse  2    63.38       913    1147.06 1.728e-14 ***\nsexo    1   401.14       912     745.92 &lt; 2.2e-16 ***\nidade   1     6.25       911     739.66   0.01239 *  \nirco    1    16.85       910     722.82 4.049e-05 ***\npafi    1     0.42       909     722.40   0.51835    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPrincipais pontos\n\nNULL Deviance: Representa o desvio do modelo sem preditores (modelo apenas com o intercepto).\nResidual Deviance: Indica o desvio do modelo conforme as variáveis são adicionadas. Um menor valor de deviance indica um melhor ajuste.\nPr(&gt;Chi): Valor P do teste qui-quadrado para cada variável, verificando se sua inclusão melhora significativamente o modelo.\n\nInterpretação das variáveis\n\nclasse: A inclusão da classe do passageiro reduz significativamente o desvio (deviance), indicando que a classe é um preditor importante.\nsexo: O sexo é a variável que mais reduz o desvio, confirmando que ser mulher aumentava a chance de sobrevivência.\nidade: A idade tem um efeito significativo na sobrevivência, mas menor que classe e sexo\nirco: Número de irmãos/cônjuges tem um impacto relevante.\npafi: O número de pais/filhos a bordo não melhora significativamente o modelo (P &gt; 0.05), sugerindo que essa variável não tem grande impacto na sobrevivência.\n\nConclusão\nO modelo sugere que classe, sexo, idade e irco (número de irmãos/cônjuges) são fatores relevantes para prever a sobrevivência. Por outro lado, número de pais/filhos não tem um impacto significativo. Isso pode indicar que ter viajado com crianças não afetou tanto a probabilidade de sobrevivência.\n\n\n18.8.4.5 Uso do Odds Ratio (OR) na interpretação dos coeficientes\nNumericamente, os coeficientes de regressão logística não são facilmente interpretáveis em escala bruta, pois estão representados como log (odds) ou logit . Para tornar mais simples, pode-se inverter a transformação logística exponenciando os coeficientes (\\(e^x\\)). Isso faz com que os coeficientes se transformem em razões de chance (odds ratio), ficando mais intuitivos facilitando a interpretação. Isso é realizado pela função exp() e pela função confint(), que retorna os intervalos de confiança de 95%:\n\nodds_ratio1 &lt;- round (exp(cbind(OR = coef(modelo1), confint(modelo1))), 3)\n\nWaiting for profiling to be done...\n\nprint(odds_ratio1)\n\n                OR  2.5 % 97.5 %\n(Intercept) 40.667 19.427 88.459\nclasse2      0.353  0.206  0.599\nclasse3      0.131  0.078  0.214\nsexomasc     0.028  0.018  0.043\nidade        0.978  0.965  0.991\nirco         0.680  0.538  0.839\npafi         0.928  0.737  1.163\n\n\nObservando, novamente, a variável idade, vê-se que à medida que ela aumenta, reduz a chance de sobrevivência no naufrágio do Titanic. A OR nos informa que a cada aumento de uma unidade na idade, há uma diminuição na chance de sobrevivência (sobreviveu = 1) de \\(1 – OR \\times 100\\)%, ou seja, uma diminuição de 2.2%.\nPara o sexo masculino, há uma diminuição (coeficiente negativo) na chance de sobrevivência. O fato de ser homem reduz em 97.2% a chance de sobreviver.\nPara a 3ª classe, comparada à 1ª classe, há uma diminuição de 86.9% na chance de sobrevivência. Da mesma forma a 2ª classe, comparada à 1ª classe, tem uma diminuição de 64.7%% na chance de sobrevivência 11.\n\n\n18.8.4.6 Visualização da regressão logística\nUma maneira interessante de visualizar a regressão logística, pode ser feita usando a função plot_model() do pacote sjPlot (52). Essa função, como padrão, produz um gráfico de floresta (forest plot), onde é possível visualizar as variáveis no eixo vertical e o tamanho do efeito (OR), os coeficientes, no eixo horizontal (Figura 18.12). Além disso, coeficientes positivos são representados com a cor azul e negativos em vermelho; e os intervalos de confiança 95% como uma linha ao redor do valor médio do coeficiente (ponto). Ao se especificar o tipo como std em plot_model(), o gráfico de floresta produzido utiliza os valores padronizados em desvios padrões.\n\nlibrary(sjPlot)\nforest_raw &lt;- plot_model(modelo1)\nforest_std &lt;- plot_model(modelo1, type = \"std\")\n\nlibrary(patchwork)\nwrap_plots(forest_raw, forest_std, nrow = 2)\n\n\n\n\n\n\n\nFigura 18.12: Gráfico de Floresta dos Coeficientes de uma Regressão Logística em formato Odds Ratio\n\n\n\n\n\n\n\n18.8.4.7 Remoção de variáveis não significativas\nA variável pafi mostrou-se não significativa no ajuste do modelo1 e será removida da análise:\n\nmodelo2 &lt;- glm(sobreviveu ~ classe + sexo + idade + irco, family = binomial(link = 'logit'), data=dadosTreino)\nsummary(modelo2)\n\n\nCall:\nglm(formula = sobreviveu ~ classe + sexo + idade + irco, family = binomial(link = \"logit\"), \n    data = dadosTreino)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.650122   0.375713   9.715  &lt; 2e-16 ***\nclasse2     -1.036232   0.272403  -3.804 0.000142 ***\nclasse3     -2.027471   0.256308  -7.910 2.57e-15 ***\nsexomasc    -3.537566   0.213552 -16.565  &lt; 2e-16 ***\nidade       -0.021943   0.006888  -3.186 0.001445 ** \nirco        -0.405917   0.108946  -3.726 0.000195 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1210.44  on 915  degrees of freedom\nResidual deviance:  722.82  on 910  degrees of freedom\nAIC: 734.82\n\nNumber of Fisher Scoring iterations: 5\n\n\nDa mesma forma que com o modelo1, pode-se calcular, para tornar a interpretação mais intutiva, as OR:\n\nodds.ratio2 = exp(cbind(OddsRatio = coef(modelo2),confint(modelo2)))\n\nWaiting for profiling to be done...\n\nprint(odds.ratio2)\n\n              OddsRatio       2.5 %      97.5 %\n(Intercept) 38.47934893 18.75841672 81.95938113\nclasse2      0.35478917  0.20659099  0.60186653\nclasse3      0.13166810  0.07888462  0.21578590\nsexomasc     0.02908405  0.01889416  0.04369283\nidade        0.97829590  0.96503498  0.99148346\nirco         0.66636523  0.53152001  0.81573981\n\n\nQuando se compara os dois modelos para ver se houve uma melhora no ajuste, é interessante observar os AICs.\n\naic &lt;- AIC(modelo1, modelo2)\naic\n\n        df      AIC\nmodelo1  7 736.3994\nmodelo2  6 734.8166\n\n\nO modelo1 tem uma AIC (736.4) discretamente maior que a do modelo2 (734.82). Isto indica que o ajuste do modelo2 é um pouco melhor (teve menos informação perdida no ajuste), entretanto, esta ligeira melhora não modifica a interpretação.A função anova(), que a diferença entre os modelos não é significativa (P &gt; 0.05):\n\nanova(modelo1, modelo2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sobreviveu ~ classe + sexo + idade + irco + pafi\nModel 2: sobreviveu ~ classe + sexo + idade + irco\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       909     722.40                     \n2       910     722.82 -1 -0.41717   0.5184\n\n\nOs dois modelo têm a mesma interpretação, mas o modelo2 ajusta melhor os dados.\n\n\n18.8.4.8 Construção do modelo e interação\nA interação entre variáveis pode ser crucial em modelos de regressão logística, pois pode revelar padrões que não são evidentes ao analisar cada variável separadamente. Usar interações pode tornar o modelo mais informativo, mas também mais complexo. Por isso, é sempre bom verificar métricas como AIC ou testes estatísticos para ver se a inclusão da interação realmente melhora a previsão.\nSerá construído um gráfico (Figura 18.13)), usando o pacote ggeffects e sua função ggpredict() (53). Este calcula os efeitos marginais a partir de modelos estatísticos e retorna o resultado como estruturas de dados organizadas. Essas estruturas de dados estão prontas para serem usadas com o pacote ggplot2. Os efeitos marginais podem ser calculados para muitos modelos diferentes, incluindo termos de interação. Junto será criado o modelo de interação (modelo3)\n\ntitanic_df &lt;- dadosTreino %&gt;%\n  dplyr::select(- pafi) %&gt;%\n  mutate(classe = factor(classe, levels = c(1, 2, 3), \n                         labels = c(\"1ª Classe\", \"2ª Classe\", \"3ª Classe\"))) \ntitanic_df &lt;- dadosTreino %&gt;%\n  dplyr::select(- pafi) %&gt;%\n  mutate(classe = factor(classe, levels = c(1, 2, 3), \n                         labels = c(\"1ª Classe\", \"2ª Classe\", \"3ª Classe\"))) \n\nmodelo3 &lt;- glm(sobreviveu ~ sexo * classe + idade + irco, data = titanic_df, family = \"binomial\")\nsummary(modelo3)\n\n\nCall:\nglm(formula = sobreviveu ~ sexo * classe + idade + irco, family = \"binomial\", \n    data = titanic_df)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               4.466967   0.661859   6.749 1.49e-11 ***\nsexomasc                 -4.463273   0.623370  -7.160 8.07e-13 ***\nclasse2ª Classe          -0.671057   0.785028  -0.855 0.392651    \nclasse3ª Classe          -3.186429   0.624252  -5.104 3.32e-07 ***\nidade                    -0.022705   0.007084  -3.205 0.001351 ** \nirco                     -0.379739   0.110176  -3.447 0.000568 ***\nsexomasc:classe2ª Classe -0.803877   0.862622  -0.932 0.351389    \nsexomasc:classe3ª Classe  1.691644   0.669402   2.527 0.011501 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1210.44  on 915  degrees of freedom\nResidual deviance:  700.01  on 908  degrees of freedom\nAIC: 716.01\n\nNumber of Fisher Scoring iterations: 6\n\npred_effects &lt;- ggpredict(modelo3, terms = c(\"classe\", \"sexo\"))\n\n\nplot(pred_effects, show_data = FALSE, connect_lines = T) +\n  labs(x = \"Classe do Passageiro\",\n       y = \"Probabilidade Predita de Sobrevivência\",\n       title = \"Interação entre Sexo e Classe na Sobrevivência do Titanic\", \n       color = \"Sexo\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        legend.position = \"right\") +\n  # Ajustar limites do eixo Y para probabilidades (0 a 1)\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1),\n                     expand = expansion(add = c(0,0.05)))\n\n\n\n\n\n\n\nFigura 18.13: Gráfico de linha da interação sexo:classe\n\n\n\n\n\nO gráfico da Figura 18.13 sugere uma interação entre classe e sexo na taxa de sobrevivência.\nInterpretação dos coeficientes gerados no modelo com interação\nInterpretar os coeficientes do modelo de interação em conjunto com o gráfico da Figura 18.13 é fundamental. Os coeficentes são em escala logit e, para melhor interpretá-lo, há necessidade de exponenciá-los:\n\nodds_ratios &lt;- exp(cbind(OddsRatio = coef(modelo3), confint(modelo3)))\n\nWaiting for profiling to be done...\n\n\nO modelo com interação (modelo3) apresentou uma ajuste um pouco melhor, comparado com modelo sem interação (modelo2). Para a comparação, pode-se usar o AIC de ambos os modelos:\n\naic &lt;- AIC(modelo2, modelo3)\naic\n\n        df      AIC\nmodelo2  6 734.8166\nmodelo3  8 716.0065\n\n\nA saída mostra que existe uma diferença de 18.81 pontos. Isso é uma forte evidência que o modelo com a interação melhora o ajuste.\nO coeficiente de interação positivo como 1.6916435 não significa que homens da 3ª classe têm alta probabilidade de sobreviver. Significa que o efeito negativo de ser homem na 3ª classe é menos grave (ou a relação da classe com a sobrevivência é diferente para homens) do que seria previsto apenas pelos efeitos principais sexomasc e 3ª Classe somados linearmente. É uma correção para a relação aditiva simples.\nO gráfico mostra claramente que homens da 3ª classe têm uma probabilidade predita de sobrevivência bem baixa, que é a mais baixa de todas as categorias.\nDesta forma, a interpretação é semelhante a que foi mostrada com o modelo sem interação:\n\nSer mulher é um fator de sobrevivência extremamente forte.\n\nSer da 1ª classe é um fator de sobrevivência positivo.\n\nIdade avançada e ter mais irmãos/cônjuges são fatores negativos para a sobrevivência.\n\nExiste uma interação significativa onde a penalidade de ser homem (em termos de chances de sobrevivência) é diferente entre as classes, sendo muito mais forte nas classes superiores e menos acentuada (mas ainda presente) na 3ª classe (onde as chances já são globalmente baixas para todos, mas a queda de chances para mulheres da 3ª classe é mais drástica que para homens entre 2ª e 3ª).\n\nPortanto, deve-se manter o modelo com a interação (modelo3). Ele é mais preciso e oferece uma compreensão mais detalhada das complexas relações entre sexo, classe e sobrevivência (ajustado por idade e irco).\nPara finalizar, será mostrada, na Figura 18.14, as taxas reais de sobrevivência, para comparar com a Figura 18.13, indicando que os homens da 3ª classe tiveram uma baixa taxa de sobrevivência, mesmo com o modelo de interação tenha mostrado um coeficiente positivo. Isto apenas mostra que atenuou o efeito.\n\n# Converter a variável \"sobreviveu\" para numérica, se necessário\nggplot(dadosTreino, \n       aes(x = classe, \n           fill = factor(sobreviveu))) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ sexo) +\n  labs(title = \"Proporção de sobrevivência por sexo e classe\",\n       x = \"Classe do passageiro\", \n       y = \"Proporção\",\n       fill = \"Sobrevivência\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigura 18.14: Proporção de sobrevivência por sexo e classe\n\n\n\n\n\nPara comparar os modelos, estatisticamente, pode-se usar a função anova() do pacote car:\n\nanova &lt;- anova(modelo2, modelo3, test = \"Chisq\")\nanova\n\nAnalysis of Deviance Table\n\nModel 1: sobreviveu ~ classe + sexo + idade + irco\nModel 2: sobreviveu ~ sexo * classe + idade + irco\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       910     722.82                          \n2       908     700.01  2    22.81 1.114e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA ANOVA mostra um valor P muito baixo, indicando uma diferença estatisricamente signficativa entre os modelos.\n\n\n18.8.4.9 Curva ROC para comparar os modelos\nA curva ROC é um gráfico permite explicar o desempenho do modelo avaliando a taxa de verdadeiros positivos (sensibilidade) versus a taxa de falsos positivos (\\(1 - especificidade\\)). A AUC (Área sob a Curva ROC) indica a qualidade do modelo (veja também a Seção 18.2.4):\n\n\\(AUC \\approx 0.5\\) \\(\\to\\) O modelo não tem poder preditivo (equivalente ao puro acaso).\n\n\\(AUC \\gt 0.7\\) \\(\\to\\) Modelo razoável.\n\n\\(AUC \\gt 0.8\\) \\(\\to\\) Modelo muito bom.\n\\(AUC \\gt 0.9\\) \\(\\to\\) Excelebte desempenho.\n\nPredição da variável desfecho nos modelos\n\nprob_pred &lt;- predict(modelo2, type = \"response\")\nprob_pred_int &lt;- predict(modelo3, type = \"response\")\n\nCrição de um objeto ROC\n\nlibrary(pROC)\nroc2&lt;- roc(response = dadosTreino$sobreviveu, predictor = prob_pred) \nroc3 &lt;- roc(response = dadosTreino$sobreviveu, predictor = prob_pred_int)\n\nPlotar a curva ROC\n\nplot(roc2, \n     main = \"Curva ROC\", \n     print.auc = TRUE, \n     legacy.axes=TRUE,\n     print.auc.y = 0.2,\n     ylab=\"Sensibilidade\",\n     xlab=\"1 - Especificidade\")\n\nplot(roc3, \n     main = \"\",\n     col=\"tomato\",\n     print.auc = TRUE, \n     legacy.axes=TRUE,\n     print.auc.y = 0.1,\n     add =TRUE)\n\n\n\n\n\n\n\nFigura 18.15: Comparação das curvas ROC(modelo sem interação = preto; modelo com interação vermelho)\n\n\n\n\n\nAs curvas ROC são praticamente muito semelhantes, com AUCs muito próximos.\n\n\n\n18.8.5 Avaliação do Modelo de Regressão Logística\n\n18.8.5.1 Matriz de Confusão\nPrimeiro foi feito o treinamento com o conjunto de dados dadosTreino. Agora, comparam-se as previsões (pred_class) com os valores reais (dadosTeste$sobreviveu). Para isso será necessário a função confusionMatrix() do pacote caret.\nFazendo previsões (probabilidades)\n\ndadosTeste &lt;- dadosTeste %&gt;%\n  dplyr::select(- pafi) %&gt;%\n  mutate(classe = factor(classe, levels = c(1, 2, 3), \n                         labels = c(\"1ª Classe\", \"2ª Classe\", \"3ª Classe\"))) \n\nset.seed(234)\npred_prob &lt;- predict(modelo3, newdata = dadosTeste, type = \"response\")\n\nA seguir, converte-se as probabilidades em classes (fatores):\n\npred_class &lt;- ifelse(pred_prob &gt; 0.5, 1, 0)\npred_class &lt;- factor(pred_class, levels = c(0, 1))\n\nCriando a matriz de confusão\n\nlibrary(caret)\nconf_matrix &lt;- confusionMatrix(pred_class, \n                               dadosTeste$sobreviveu, \n                               positive =\"1\")\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 232  26\n         1  20 115\n                                        \n               Accuracy : 0.883         \n                 95% CI : (0.847, 0.913)\n    No Information Rate : 0.6412        \n    P-Value [Acc &gt; NIR] : &lt;2e-16        \n                                        \n                  Kappa : 0.7432        \n                                        \n Mcnemar's Test P-Value : 0.461         \n                                        \n            Sensitivity : 0.8156        \n            Specificity : 0.9206        \n         Pos Pred Value : 0.8519        \n         Neg Pred Value : 0.8992        \n             Prevalence : 0.3588        \n         Detection Rate : 0.2926        \n   Detection Prevalence : 0.3435        \n      Balanced Accuracy : 0.8681        \n                                        \n       'Positive' Class : 1             \n                                        \n\n\nA tabela de contingência do início da matriz de confusão, mostra a comparação dos dados preditos e os dados reais (reference) e o significado de cada célula, onde Classe 0 representa negativo e Classe 1, positivo.\nA Acurácia (88.3%) é a proporção de previsões corretas. O modelo tem uma alta acurácia, isso indica um bom desempenho geral. Um modelo é considerado razoavelmente bom se a precisão do modelo for superior a 70%.\nA Sensibilidade (Recall) é a proporção de verdadeiros positivos (VP) identificados corretamente. O modelo o modelo identifica corretamete 81.6% dos verdadeiros positivos.\nA Especificidade é a proporção de verdadeiros negativos(VN). O modelo reconhece 92.1% dos verdadeiros negativos.\nO Valor P (McNemar’s Test) testa se há viés significativo na classificação dos erros. O valor alto (P = 0.461) sugere que os erros entre classes não são estatisticamente diferentes.\nO Kappa mede concordância ajustada para a distribuição dos dados. Um valor próximo de 1 indica boa concordância entre previsões e valores reais.\nO escore F1 (não listado diretamente) representa o equilíbrio entre precisão e recall. Pode ser obtido pela equação: \\[F1 = 2 \\times \\frac{VPN \\times sensibilidade}{VPN + sensibilidade}\\]\nOu consultando conf_matrix$byClass[7] (83.3%).\nA Precisão (VPP) é a proporção de previsões positivas que estavam corretas, também denomonada de valor preditivo positivo (VPP). A precisão do modelo é igual 85.2, é considerada um valor alto.\n\n\n18.8.5.2 Uso da curva ROC para avaliar o modelo\nAvaliar um modelo de Regressão Logística no R, usando a curva ROC/AUC também é bem direto (veja Seção 18.8.4.9 e Seção 18.2.4). É uma abordagem bastante poderosa que permite visualizar os grupos no mesmo gráfico.\nObtenção das probabilidades de predição\nPara se obter uma curva ROC, há necessidade das probabilidades previstas pelo modelo para a classe positiva (no exemplo, o nível 1 da variável sobreviveu).\n\n# Probabilidades de predição no conjunto de teste\n# O argumento 'type = \"response\"' garante que serão obtidas as probabilidades\nprob_pred_teste &lt;- predict(modelo3, newdata = dadosTeste, type = \"response\")\n\n# Probabilidades de predição no conjunto de treino\nprob_pred_treino &lt;- predict(modelo3,type = \"response\")\n\nCriação de objetos ROC\nA seguir, serão criados os objetos ROC necessários para a construção das curvas ROC e calcular a área sob a curva (AUC):\n\nroc_obj_treino &lt;- roc(response = dadosTreino$sobreviveu, predictor = prob_pred_treino) \nroc_obj_teste &lt;- roc(response = dadosTeste$sobreviveu, predictor = prob_pred_teste) \n\nauc_valor_treino &lt;- auc(roc_obj_treino)\nauc_valor_teste &lt;- auc(roc_obj_teste)\n\nVisualização da curva ROC\nCom as informações obtidas, pode-se visualizar a curva ROC. Para criar um gráfico mais atraente, será usado, agora, o pacote ggplot2.\ninicialmente, serão criados dataframes com os dados para serem usados na função ggplot():\n\n# Uma coluna chamada Model é adicionada para identificar qual curva pertence a qual modelo\n\n# # Dataframe dos dados treino\ndados_roc_treino &lt;- data.frame(\n  TFP = 1 - roc_obj_treino$specificities,\n  TVP = roc_obj_treino$sensitivities,\n  Model = \"Treino\")\n\n# Dataframe dos dados teste\ndados_roc_teste &lt;- data.frame(\n  TFP = 1 - roc_obj_teste$specificities,\n  TVP = roc_obj_teste$sensitivities,\n  Model = \"Teste\")\n\ndados_roc_combinados &lt;- rbind(dados_roc_treino, dados_roc_teste)\n\nUsando estes dois dataframes, obtemos a Figura 18.16\n\nggplot(dados_roc_combinados, \n       aes(x = TFP, y = TVP, color = Model)) +\n  geom_line(size = 1.2) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"navy\", size = 0.5) +\n  labs(title = \"Comparação de Curvas ROC\",\n       x = \"Taxa de Falsos Positivos (TFP)\",\n       y = \"Taxa de Verdadeiros Positivos (TVP)\",\n       color = \"Dados\") + # Legenda para as cores\n  annotate(\"text\", x = 0.6, y = 0.2, label = paste(\"AUC Teste =\", round(auc_valor_teste, 2)),\n           color = \"tomato\", size = 4) +\n  annotate(\"text\", x = 0.6, y = 0.1, label = paste(\"AUC Treino =\", round(auc_valor_treino, 2)),\n           color = \"steelblue\", size = 4) +\n  scale_color_manual(values = c(\"Teste\" = \"tomato\",\n                                \"Treino\" = \"steelblue\")) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nFigura 18.16: Desempenho do modelo treino e do modelo teste)\n\n\n\n\n\nA Curva ROC mostra o quão bem seu modelo distingue entre as classes positiva e negativa em diferentes limites de classificação. Uma curva que se aproxima do canto superior esquerdo indica um modelo com bom poder de discriminação.\nA AUC (Area Under the Curve) é um resumo numérico desse desempenho. Um valor de AUC de 1 significa um modelo perfeito, enquanto 0.5 indica um modelo que se comporta aleatoriamente (como jogar uma moeda para classificar). Quanto maior a AUC, melhor o desempenho geral do seu modelo em separar as classes. Em geral, uma AUC acima de 0.7 é considerada razoável, acima de 0.8 é boa e acima de 0.9 é excelente.\nA AUC Teste = 0.91 é considerada excelente. Isso significa que o modelo tem uma capacidade muito alta de distinguir entre as classes positiva e negativa no conjunto de dados não visto (teste). Uma AUC Treino = 0.89 também é um desempenho muito forte. A diferença entre as duas é mínima (0.02). Isso é um sinal extremamente positivo. Significa que o modelo não está sofrendo de:\n\nOverfitting (sobreajuste): Se houvesse overfitting, a AUC de treino seria significativamente mais alta que a AUC de teste, indicando que o modelo “memorizou” os dados de treino e não generaliza bem para dados novos.\n\nUnderfitting (subajuste): Se houvesse underfitting, ambas as AUCs seriam baixas, indicando que o modelo não aprendeu os padrões nos dados.\n\nConcluindo o modelo de regressão logística apresentou um excelente desempenho e está generalizando muito bem para novos dados. A performance consistente entre treino e teste é um indicativo de um modelo robusto e confiável para a tarefa de classificação.\n\n\n18.8.5.3 Outras avaliações do modelo\nLinearidade e Colinearidade\nEsta suposição pode ser testada de diversas maneiras. O pacote performance (moderno e abrangente), usando a função check_model() faz várias verificações que inclui a linearidade de forma visual (Figura 18.17) e Figura 18.18), para preditores contínuos através de resíduos parciais. Serão avaliados os dois modelos (modelo2 e modelo3)\n\nlibrary(performance)\nplot(check_model(modelo2, residual_type = \"normal\"))\n\n\n\n\n\n\n\n\n\nFigura 18.17: Avaliação do modelo com foco na linearidade\n\n\n\n\n\nO modelo3 (com interação sexo:classe) que em outras testagens ele teve um melhor ajuste, será avaliado:\n\nlibrary(performance)\nplot(check_model(modelo3, residual_type = \"normal\"))\n\n\n\n\n\n\n\n\n\nFigura 18.18: Avaliação do modelo com interação com foco na linearidade\n\n\n\n\n\nOs gráficos gerados incluem diversas verificações fundamentais que devem ser observadas comparativamente nas Figura 18.17 e Figura 18.18.\n\nVerificação preditiva posterior (Posterior Predictive Check) – Compara os dados observados com os preditos pelo modelo. Os intervalos do modelo devem incluir os pontos observados. Comparando os dois modelos, os pontos observados os parecem ter um alinhamento dos intervalos preditivos semelhantes.\nResíduos agrupados (Binned Residuals) – Avalia se há padrões nos resíduos. Pontos vermelhos indicam desvios que podem sugerir problemas de ajuste. Aqui, o modelo de interação tem uma maior quantidade de pontos fora do intervalo de erro.\nObservações Influentes (Influential Observations) – Identifica pontos com alta influência no modelo, o que pode indicar necessidade de ajustes ou remoção de outliers. Ambos os modelos não parecem ter pontos pontos influentes. A inclusão da interação não impactou muito quais observações têm maior peso sobre o modelo.\nColinearidade – Exibe o VIF (Fator de Inflação da Variância) das variáveis preditoras. Valores altos (acima de 5) podem indicar problemas de multicolinearidade. Na comparação dos dois modelos, este é um ponto importante, pois com a introdução da interação, os VIFs ficaram elevados. Pode significar que as variáveis interagem de forma redundante, prejudicando a interpretação dos coeficientes. O VIF pode ser também obtido para cada variável, usando a função vif() do pacote car:\n\n\nvif_valores &lt;- car::vif(modelo2)\nprint(vif_valores)\n\n           GVIF Df GVIF^(1/(2*Df))\nclasse 1.336849  2        1.075278\nsexo   1.232028  1        1.109967\nidade  1.221055  1        1.105013\nirco   1.128565  1        1.062340\n\nvif_valores2 &lt;- car::vif(modelo3)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\nprint(vif_valores2)\n\n                 GVIF Df GVIF^(1/(2*Df))\nsexo         9.826704  1        3.134757\nclasse      28.580915  2        2.312166\nidade        1.222732  1        1.105772\nirco         1.120413  1        1.058496\nsexo:classe 37.637060  2        2.476874\n\n\n\nNormalidade dos Resíduos (Normality of Residuals) – Usa um gráfico Q-Q para verificar se os resíduos seguem uma distribuição normal. Desvios sistemáticos podem sinalizar problemas. Ambos, como era de esperar, parecem não ter uma distribuição normal. Entretanto, ao contrário da regressão linear, a regressão logística não assume a normalidade dos resíduos. Os resíduos de um modelo de regressão logística não se distribuem normalmente, pois a variável resposta é binária (0 ou 1) e o modelo prediz probabilidades.\n\nConcluindo, com esses resultados, parece que a interação sexo:classe não agregou muito ao modelo. Ela aumentou a colinearidade e fez os resíduos se tornarem menos confiáveis, sem impacto positivo claro nas previsões.\nRazão de Verossimilhança\nEm estatística, a verossimilhança (Likelihood) de um modelo é uma medida da probabilidade de observar os dados amostrais que temos, dada uma certa configuração dos parâmetros do modelo. Em outras palavras, ela nos diz quão “provável” é que o modelo, com seus coeficientes estimados, tenha gerado os dados que observamos.\nNo contexto da regressão logística, os parâmetros do modelo (os coeficientes \\(\\beta\\)) são estimados usando o método de máxima verossimilhança (veja também Seção 18.8.1). Esse método busca encontrar os valores dos coeficientes que maximizam a probabilidade de que os eventos observados (por exemplo, “sucesso” ou “fracasso” da variável dependente) tenham ocorrido.\nA razão de verossimilhança (-2LL), ou mais precisamente, -2 vezes o logaritmo da verossimilhança, é uma medida de desajuste do modelo. É uma transformação do valor da verossimilhança que é mais conveniente para cálculos e interpretação.\nMatematicamente, ela é expressa como:\\[-2LL=-2 \\times ln(L)\\] Onde:\n\nL é a verossimilhança do modelo\nln é o logaritmo natural\n\nQuanto menor o valor de -2LL, melhor é o ajuste do modelo aos dados. Isso ocorre porque um modelo com um bom ajuste tem uma verossimilhança (L) maior, e como -2LL envolve o logaritmo negativo da verossimilhança, um L maior resulta em um -2LL menor.\nA importância do -2LL reside principalmente em sua utilização para realizar o Teste da Razão de Verossimilhança (Likelihood Ratio Test). Este teste compara a qualidade do ajuste de dois modelos:\n\nModelo Nulo (ou Modelo Base): Um modelo mais simples, geralmente contendo apenas a constante (intercepto), sem as variáveis preditoras de interesse.\nModelo Proposto: O modelo completo que se está testando, que inclui as variáveis preditoras.\n\nO teste da razão de verossimilhança calcula a diferença entre o -2LL do modelo nulo e o -2LL do modelo proposto:\n\\[G = (-2LL _{Modelo \\ Nulo}) - (-2LL_{Modelo \\ Proposto})\\] Essa estatística G segue uma distribuição qui-quadrado (\\(\\chi^2\\)) com graus de liberdade iguais à diferença no número de parâmetros entre os dois modelos.Um valor G grande e estatisticamente significativo (com um p-valor baixo), indica que o modelo proposto se ajusta significativamente melhor aos dados do que o modelo nulo (sem as variáveis preditoras). Isso sugere que as variáveis independentes no modelo contribuem para explicar a variável dependente.\nO -2LL do modelo nulo (Null deviance) e do modelo proposto (Residual deviance) podem ser verificados através de:\n\nmodelo2$null.deviance\n\n[1] 1210.441\n\nmodelo2$deviance\n\n[1] 722.8166\n\n\nO cálculo da estatística G (Teste da Razão de Verossimilhança) pode ser obtido, como visto pela diferença entre deviance do modelo nulo e do modelo ajustado:\n\nestatistica_G &lt;- modelo2$null.deviance - modelo2$deviance \nprint(estatistica_G)\n\n[1] 487.6243\n\n# Graus de liberdade para o teste G\ndf_G &lt;- modelo2$df.null - modelo2$df.residual\nprint(df_G)\n\n[1] 5\n\n# Teste de significância \nvalor_P &lt;- format(1 - pchisq(estatistica_G, \n                             df = df_G), \n                  sientific = TRUE)\nprint(valor_P)\n\n[1] \"0\"\n\n\nO valor P é extremamente baixo, isso é interpretado como uma diferença altamente significativa entre os modelos. No contexto de teste de verossimilhança, isso sugere que o modelo mais complexo se ajusta significativamente melhor do que o modelo nulo.\nEstatística \\(R^2\\)\nR ao quadrado é uma métrica útil para a avaliação da regressão linear simples e múltipla, mas não tem o mesmo significado na regressão logística.\nOs estatísticos criaram análogos do \\(R^2\\) para regressão logística referidas, coletivamente, como pseudo R ao quadrado. Este não tem a mesma interpretação, na medida em que não é simplesmente a proporção da variância explicada pelo modelo.\nExistem muitas maneiras diferentes de calcular um \\(R^2\\) para regressão logística e nenhum consenso sobre qual é a melhor. Os dois métodos mais frequentemente relatados em softwares estatísticos são um proposto por McFadden (54) e outro por Cox-Snell (55). Também é bastante relatado o de Nagelkerke (56). Os valores mais altos indicam um melhor ajuste do modelo.\nO \\(R^2\\) de McFadden é uma versão, baseada no log-likelihood para o modelo somente com o intercepto e o modelo estimado completo. O \\(R^2\\) de Cox e Snell é baseado no log-likelihood para o modelo em comparação com o log-likelihood para um modelo basal. No entanto, com resultados categóricos, tem um valor máximo teórico inferior a 1, mesmo para um modelo “perfeito”.\nO \\(R^2\\) de Nagelkerke é uma versão ajustada do \\(R^2\\) de Cox e Snell que ajusta a escala da estatística para cobrir todo o intervalo de 0 a 1.\nPode-se obter os vários valores de \\(R^2\\), usando a função PseudoR2() do pacote DescTools:\n\nlibrary(DescTools)\nPseudoR2(modelo2, which =c(\"Nagelkerke\", \"McFadden\", \"CoxSnell\"))\n\nNagelkerke   McFadden   CoxSnell \n 0.5629342  0.4028485  0.4127713 \n\n\nComo se verifica, na Saída do código, todos os valores de \\(R^2\\) diferem ligeiramente, mas podem ser usados como medidas de tamanho de efeito para o modelo. Então, basicamente, o pseudo R quadrado pode ser interpretado como \\(R^2\\), mas não se espera que seja tão grande. Uma regra prática, bastante útil é que o pseudo R quadrado de McFadden variando de 0,2 a 0,4 indica um ajuste muito bom do modelo (57).\nPesquisa de valores atípicos e pontos de alavancagem\nUm outlier é uma observação que não é bem prevista pelo modelo de regressão ajustado (ou seja, tem um grande resíduo positivo ou negativo). Uma observação com um alto valor de alavancagem possui uma combinação incomum de valores preditores. Ou seja, é um outlier no espaço do preditor.\nUma observação influente é uma observação que tem um impacto desproporcional na determinação dos parâmetros do modelo. As observações influentes são identificadas usando uma estatística chamada distância de Cook ou D de Cook.\nA identificação dos outliers é feita essencialmente através dos resíduos padronizados (58), com a função rstandard():\n\nresiduos_p &lt;- rstandard(modelo2)\nsummary(residuos_p)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-2.5513 -0.5440 -0.3651 -0.0538  0.5175  2.7016 \n\n\nEm uma amostra normalmente distribuída, ao redor de 95% dos valores estão entre –1,96 e +1,96, 99% deve estar entre –2,58 e +2,58 e quase todos (99,9%) devem situar-se entre –3,09 e +3,09. Portanto, resíduos padronizados com um valor absoluto maior que 3 são motivo de preocupação porque em uma amostra média é improvável que aconteça um valor tão alto por acaso (59). Na Saída da função rstandard(), observa-se que os valores estão dentro de –3 e +3.\nEssa avaliação pode ser acompanhada de um gráfico diagnóstico, usando a função plot() para o modelo modelo2(para maiores detalhes veja as Seção 15.3.2.1 e Seção 15.3.2.4).\n\nplot (modelo2, which = 5)\n\n\n\n\n\n\n\nFigura 18.19: Gráfico diagnóstico dos resíduos e pontos de alavancagem.\n\n\n\n\n\nSão produzidos vários gráficos com a função plot(), mas o foco é o gráfico 5 (Figura 18.19) que confirma os achados de não existirem valores atípicos que comprometam o ajuste do modelo. A Figura 18.17 exibe um gráfico igual, sem mostrar as distância s de Cook.\nPara a análise dos pontos influentes, pode-se verificar os pontos de alavancagem (leverage) com a função hatvalues() do pacote stats, incluído no R base.\n\nhat &lt;- hatvalues(modelo2)\nsummary (hat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.002009 0.002894 0.006226 0.006550 0.007706 0.078849 \n\n\nOs valores de alavancagem podem estar entre 0 (indicando que o caso não tem qualquer influência) e 1 (indicando que o caso tem grande influência). Se nenhum caso exercer influência indevida sobre o modelo, se espera que todos os valores de alavancagem estivessem próximos do valor médio. Alguns autores (60), recomendam investigar casos com valores superiores a duas vezes a média (0.0131004) como ponto de corte para identificar casos com influência indevida. Alguns valores (\\(\\sim\\) 5%) estão acima de duas vezes a média, o que se pode considerar pouco. Principalmente, porque o maior valor está bem longe do valor igual a 1 (0.0788494). É interessante fazer a análise, observando junto a distância de Cook para ver se um ponto é um outlier significativo.\n\ncook &lt;- cooks.distance(modelo2)\nsummary (cook)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n2.089e-06 3.885e-05 1.684e-04 1.043e-03 5.462e-04 3.074e-02 \n\n\nSe a distância de Cook é \\(\\lt\\) 1, não há necessidade real de excluir esses valores, uma vez que não tem um grande efeito na análise de regressão (61). Na Figura 18.19), verifica-se que os casos mais distantes não alcançam a distância de Cook.\n\n\n\n\n1. Straus SE, Glasziou P, et al. Diagnosis and screening. Em: Evidence-Based Medicine: How to Practice and Teach EBM. Fifth Edition. Edinburgh: Elsevier; 2019. p. 185–218. \n\n\n2. Altman DG, Bland JM. Diagnostic tests. 1: Sensitivity and specificity. BMJ: British Medical Journal. 1994;308(6943):1552. \n\n\n3. Peixoto R de O, Nunes TA, Gomes CA. Indices diagnósticos da ultrassonografia abdominal na apendicite aguda: influência do genero e constituição física, tempo evolutivo da doença e experiencia do radiologista. Revista do Colégio Brasileiro de Cirurgiões. 2011;38:105–11. \n\n\n4. Mark, Sergeant E, et al. epiR: Tools for the Analysis of Epidemiological Data [Internet]. 2022. Disponível em: https://CRAN.R-project.org/package=epiR\n\n\n5. Altman DG, Bland JM. Diagnostic tests 2: predictive values. Bmj. 1994;309(6947):102. \n\n\n6. Pereira Lima A, Vieira FJ, Oliveira GP de M, et al. Perfil clinico-epidemiologico da apendicite aguda: analise retrospectiva de 638 casos. Revista do Colegio Brasileiro de Cirurgiões. 2016;43:248–53. \n\n\n7. Halkin A, Reichman J, Schwaber M, Paltiel O, Brezis M. Likelihood ratios: getting diagnostic testing into perspective. QJM: monthly journal of the Association of Physicians. 1998;91(4):247–58. \n\n\n8. Deeks JJ, Altman DG. Diagnostic tests 4: likelihood ratios. Bmj. 2004;329(7458):168–9. \n\n\n9. Guyatt G, Rennie D, et al. Diagnostic Tests. Em: User’s Guides to Medical Literature: A Manual for Evidence-Based Clinical Practice. 3rd Edition. New York: JAMA; 2015. p. 607–31. \n\n\n10. Oliveira Filho PF de. Testes Diagnósticos. Em: Epidemiologia e Bioestatística: Fundamentos para a leitura crítica. Segunda Edição. Rio de Janeiro: Editora Rubio; 2022. p. 89–105. \n\n\n11. Fagan T. Nomogram for Bayes’s theorem. New England Journal of Medicine. 1975;293:257. \n\n\n12. Caraguel CG, Vanderstichel R. The two-step Fagan’s nomogram: ad hoc interpretation of a diagnostic test result without calculation. BMJ Evidence-Based Medicine. 2013;18(4):125–8. \n\n\n13. Altman DG, Bland JM. Diagnostic tests 3: receiver operating characteristic plots. BMJ: British Medical Journal. 1994;309(6948):188. \n\n\n14. Robin X, Turck N, Hainard A, et al. pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics. 2011;12(1):1–8. \n\n\n15. Borges LSR. Diagnostic accuracy measures in cardiovascular research. Int J Cardiovasc Sci. 2016;29(3):218–22. \n\n\n16. DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics. 1988;44:837–45. \n\n\n17. Youden WJ. Index for rating diagnostic tests. Cancer. 1950;3(1):32–5. \n\n\n18. Cohen J. A coefficient of agreement for nominal scales. Educational and Psychological Measurement. 1960;20(1):37–46. \n\n\n19. Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. 1977;159–74. \n\n\n20. Zeileis A, Meyer D, Hornik K. Residual-based shadings for visualizing (conditional) independence. Journal of Computational and Graphical Statistics. 2007;16(3):507–25. \n\n\n21. Feychting M, Osterlund B, Ahlbom A. Reduced cancer incidence among the blind. Epidemiology. 1998;9(5):490–4. \n\n\n22. Szklo M, Nieto FJ. Measuring Disease Occurrence. Em: Epidemiology: beyond the basics. Fourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p. 80–1. \n\n\n23. Gross M. Oswego County revisited. Public health reports. 1976;91(2):168. \n\n\n24. Aragon TJ. epitools: Epidemiology Tools [Internet]. 2020. Disponível em: https://CRAN.R-project.org/package=epitools\n\n\n25. Szklo M, Nieto FJ. Measuring Associations Between Exposures and Outcomes. Em: Epidemiology: beyond the basics. Fourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p. 88–94. \n\n\n26. Davies HTO, Crombie IK, Tavakoli M. When can odds ratios mislead? BMJ. 1998;316(7136):989–91. \n\n\n27. Hopkins WG. A Scale of Magnitudes for Effect Statistics [Internet]. A New View of Statistics. Sportscience; 2016. Disponível em: http://www.sportsci.org/resource/stats/index.html\n\n\n28. Madi JM, Souza R da S de, Araujo BF de, Oliveira Filho PF, et al. Prevalence of toxoplasmosis, HIV, syphilis and rubella in a population of puerperal women using Whatman 903 filter paper. The Brazilian Journal of Infectious Diseases. 2010;14(1):24–9. \n\n\n29. Szklo M, Nieto FJ. Measuring Associations Between Exposures and Outcomes. Em: Epidemiology: beyond the basics. Fourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p. 84–102. \n\n\n30. Szklo M, Nieto FJ. Measuring Associations Between Exposures and Outcomes. Em: Epidemiology: beyond the basics. Fourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p. 97–8. \n\n\n31. Physicians’ Health Study Research Group* SC of the. Final report on the aspirin component of the ongoing Physicians’ Health Study. New England Journal of Medicine. 1989;321(3):129–35. \n\n\n32. Kohl M. Package «MKmisc» [Internet]. 2019. Disponível em: https://github.com/stamats/MKmisc\n\n\n33. Bender R. Calculating confidence intervals for the number needed to treat. Controlled clinical trials. 2001;22(2):102–10. \n\n\n34. Clark TG, Bradburn MJ, Love SB, Altman DG. Survival analysis Part I: basic concepts and first analyses. British Journal of Cancer. 2003;89(2):232–8. \n\n\n35. Kaplan EL, Meier P. Nonparametric Estimation from Incomplete Observations. Journal of the American Statistical Association. 1958;53(282):457–81. \n\n\n36. Peat J, Barton B. Survival analyses. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 352–3. \n\n\n37. Qiu W, Chavarro J, Lazarus R, Rosner B, Ma J. powerSurvEpi: Power and Sample Size Calculation for Survival Analysis of Epidemiological Studies. R package version 00. 2015;9. \n\n\n38. Therneau T et al. A package for Survival Analysis in R. R package version. 2015;2(7). \n\n\n39. Kassambara A, Kosinski M, Biecek P, Fabian S. Survminer: Drawing Survival Curves Using ggplot2. URL https://CRAN R-project org/package= survminer R package version 04. 2021;9. \n\n\n40. Bland JM, Altman DG. The logrank test. BMJ. 2004;328(7447):1073. \n\n\n41. Cox DR. The regression analysis of binary sequences. Journal of the Royal Statistical Society Series B: Statistical Methodology. 1958;20(2):215–32. \n\n\n42. Peat J, Barton B. Adjusted odds ratios. Em: Medical statistics : a guide to SPSS, data analysis, and critical appraisal. New York, NY: John Wiley & Sons; 2014. p. 298–308. \n\n\n43. Hair JF, Black WC, Babin BJ, Anderson RE, Tatham RL. Regressão logística: Regressão com uma variável dependente binária. Em: Análise Multivariada de Dados. 6a Edição. bookman; 2009. p. 283–302. \n\n\n44. Verzani J. Extensions to linear model. Em: Using R for Introductory Statistics. Second Edition. Chapman & Hall/CRC; 2014. p. 440–8. \n\n\n45. Stoltzfus JC. Logistic regression: a brief primer. Academic emergency medicine. 2011;18(10):1099–104. \n\n\n46. Wikipédia. RMS Titanic — Wikipédia, a enciclopédia livre [Internet]. 2025. Disponível em: https://pt.wikipedia.org/w/index.php?title=RMS_Titanic&oldid=69869298\n\n\n47. Prabhakaran S. Missing Value Treatment [Internet]. datascienceplus. 2016. Disponível em: https://datascienceplus.com/missing-value-treatment/\n\n\n48. Van Buuren S, Groothuis-Oudshoorn K. mice: Multivariate imputation by chained equations in R. Journal of statistical software. 2011;45:1–67. \n\n\n49. Subramanian J, Simon R. Overfitting in prediction models–is it a problem only in high dimensions? Contemporary clinical trials. 2013;36(2):636–41. \n\n\n50. Field A, Miles J, Field Z. Logistic regression. Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 320. \n\n\n51. Hosmer Jr DW, Lemeshow S, Sturdivant RX. Interpretation of Fitted Logistic Regression Model. Em: Applied logistic regression. Third Edition. John Wiley & Sons; 2013. p. 49–64. \n\n\n52. Lüdecke D. sjPlot: Data Visualization for Statistics in Social Science [Internet]. 2024. Disponível em: https://CRAN.R-project.org/package=sjPlot\n\n\n53. Lüdecke D. ggeffects: Tidy Data Frames of Marginal Effects from Regression Models. Journal of Open Source Software. 2018;3(26):772. \n\n\n54. McFadden D. Conditional Logit Analysis of Qualitative Choice Behavior. Em: Zarembka P, editor. Frontiers in econometrics. Academic Press; 1974. p. 105-142-142. \n\n\n55. Cox DR, Snell EJ. Analysis of Binary Data. Second Edition. Chapman; Hall/CRC; 1989. \n\n\n56. Nagelkerke NJ et al. A note on a general definition of the coefficient of determination. Biometrika. 1991;78(3):691–2. \n\n\n57. McFadden D. Quantitative methods for analysing travel behaviour of individuals: some recent developments. Em: Behavioural travel modelling. Routledge; 2021. p. 279–318. \n\n\n58. Bobbitt Z. How to Calculate Standardized Residuals in R [Internet]. Statology. 2020. Disponível em: https://www.statology.org/standardized-residuals-in-r/\n\n\n59. Field A, Miles J, Field Z. Outliers and influential cases. Em: Discovering statistics using R. Sage Publications, Ltd; 2012. p. 288–92. \n\n\n60. Hoaglin DC, Welsch RE. The hat matrix in regression and ANOVA. The American Statistician. 1978;32(1):17–22. \n\n\n61. Stevens JP. Outliers and influential data points in regression analysis. Psychological bulletin. 1984;95(2):334.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "18-epidemiologia.html#footnotes",
    "href": "18-epidemiologia.html#footnotes",
    "title": "18  Estatística em Epidemiologia",
    "section": "",
    "text": "Verossimilhança no sentido de a qualidade de algo que parece verdadeiro ou provável, que não contraria a verdade↩︎\nExistem duas maneiras de descrever uma estimativa de odds: ou como um número isolado, por exemplo, 0,25, subentendendo que expressa uma razão, 0,25:1,0, ou de forma clara como uma razão 1:4. Ou seja, para cada indivíduo com o fator existem quatro sem o fator. Tradicional e comumente usados no mundo das apostas em corridas de cavalos.↩︎\nFoi mantido o nome das variáveis em inglês, pois no banco de dados oswego elas estão nessa língua.↩︎\nem inglês, built-in bias↩︎\nObservem que todo o intervalo de confiança de 95% encontra-se abaixo de 1, indicando que existe significância estatística.↩︎\nAproveite para revisar como construir matriz↩︎\nEm inglês, NNH (number needed to harm).↩︎\nLembre-se que o diretório mostrado no comando é o do autor, você deverá usar o do seu computador↩︎\nTodo esse processo foi realizado com objetivo didático, para revisar a realização de manipulação de dados no R↩︎\nÉ fundamentado na teoria da informação, teoria matemática da informação que estuda a quantificação, armazenamento e comunicação da informação. Quando um modelo estatístico é usado para representar um determinado processo, a representação nunca será exata, ou seja, o modelo nunca será perfeito e certamente algumas informações serão perdidas. O AIC estima a quantidade relativa de informação perdida por um determinado modelo: quanto menos informações um modelo perde, maior a qualidade desse modelo e menor a pontuação AIC.↩︎\nVeja mais detalhes sobre Odds Ratio na Seção 18.5.1.↩︎",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estatística em Epidemiologia</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Armitage P, Berry G, Matthews JNS. Statistical\nmethods in medical research. John Wiley & Sons; 2008. \n\n\n2. Massad E, Silveira PSP, Menezes RX de, Ortega\nNRS. Métodos quantitativos em medicina. Editora Manole Ltda; 2004.\n\n\n\n3. Kendall MG. Studies in the history of\nprobability and statistics. Where shall the history of statistics begin?\nBiometrika. 1960;47(3/4):447–9. \n\n\n4. Breve história dos censos [Internet]. Instituto\nNacional de Estatistica. Statistics Portugal; 2014. Available from: https://censos.ine.pt/xportal/xmain?xpid=CENSOS&amp;xpgid=censos_bhistoria\n\n\n5. Salgado-Neto G, Salgado A. Sir francis galton e\nos extremos superiores da curva normal. Revista de Ciências Humanas.\n2011;45(1):223–39. \n\n\n6. Stolley PD, Lasky T. The beginnings of\nepidemiology. In: Investigating disease patterns. Scientific American\nLibrary; 2000. p. 23–49. \n\n\n7. Stolley PD, Lasky T. Lung cancer: New methods\nof studying disease. In: Investigating disease patterns. Scientific\nAmerican Library; 2000. p. 51–79. \n\n\n8. Editors History com. Florence\nNightingale.\nhttps://www.history.com/topics/womens-history/florence-nightingale-1;\n\n\n\n9. Moore DS. Topics in inferency. In: The basic\npractice of statistics. W.H. Freeman; 2000. p. 417. \n\n\n10. Hald A. Biography of fisher. In: A history of\nparametric statistics inference from bernoulli to fisher,1713-1935. John\nWiley & Sons; 2007. p. 159–63. \n\n\n11. Salsburg D. Uma senhora toma chá... In: Uma\nsenhora toma chá. Zahar; 2009. p. 17–23. \n\n\n12. Kruskal W. The significance of fisher: A review\nof r.a. Fisher: The life of a scientist. Journal of the American\nStatistical Association [Internet]. 1980;75(372):1019–30. Available\nfrom: https://doi.org/10.1080/01621459.1980.10477590\n\n\n13. Matthews R, Chalmers I, Rothwell P. Douglas g\naltman: Statistician, researcher, and driving force behind global\ninitiatives to improve the reliability of health research. British\nMedical Journal Publishing Group; 2018. \n\n\n14. Altman DG. The scandal of poor medical\nresearch. Vol. 308, Bmj. British Medical Journal Publishing Group; 1994.\np. 283–4. \n\n\n15. R\nCore Team. The r project for statistical computing | what is r?\nDisponível em: https://www.r-project.org/about.html; 2022. \n\n\n16. R\nCore Team. The r project for statistical computing | CRAN mirrors.\nDisponível em: https://cran.r-project.org/mirrors.html; 2022. \n\n\n17. Whitney L et al. R programming language\ncontinues to grow in popularity [Internet]. TechRepublic. 2020.\nAvailable from: https://www.techrepublic.com/article/r-programming-language-continues-to-grow-in-popularity\n\n\n18. Oliveira Filho PF de. Natureza dos dados. In:\nEpidemiologia e bioestatística–fundamentos para a leitura crítica. 2ª\nedição. Editora Rubio; 2022. p. 3–6. \n\n\n19. Kirkwood BR, Sterne JA. Defining the data. In:\nEssential medical statistics. Second Edition. Blackwell Science Company;\n2003. p. 9–14. \n\n\n20. Sternbach GL. The glasgow coma scale. The\nJournal of emergency medicine. 2000;19(1):67–71. \n\n\n21. Pediatrics AA of, Obstetricians AC of. The\napgar score. Pediatrics. 2006;117(4):1444–7. \n\n\n22. Bowers D. First things first-the nature of\ndata. In: Medical statistics from scratch. Second Edition. John Wiley;\nSons; 2008. p. 3–13. \n\n\n23. Ribeiro Mendes F. O que é um trabalho\ncientífico. In: Iniciacão cientifica. Autonomia Editora; 2012. p. 17–26.\n\n\n\n24. Hulley SB, Cummings SR, Browner WS, Grady DG,\nNewman TB. Elaborando a questão de pesquisa e desenvolvendo o plano de\nestudo. In: Delineando a pesquisa clinica. Quarta Edição. Artmed\nEditora; 2015. p. 15–24. \n\n\n25. McCombes S. Sampling methods [Internet].\nhttps://www.scribbr.com/methodology/sampling-methods/. scribbr.com Team;\n2019. Available from: https://www.scribbr.com/\n\n\n26. Callegari-Jacques SM. Amostras. In:\nBioestatistica: Principios e aplicações. Artmed Editora; 2003. p. 146–7.\n\n\n\n27. Faul F, Erdfelder E, Lang A-G, Buchner A. G*\npower 3: A flexible statistical power analysis program for the social,\nbehavioral, and biomedical sciences. Behavior research methods.\n2007;39(2):175–91. \n\n\n28. Cohen J. Statistical power analysis for the\nbehavioral sciences. Lawrence Erlbaum Associates; 1988. \n\n\n29. Grimes DA, Schulz KF. An overview of clinical\nresearch: The lay of the land. The lancet. 2002;359(9300):57–61. \n\n\n30. Fletcher RH, Fletcher SW, Fletcher GS.\nPrognóstico. In: Epidemiologia clínica: Elementos essenciais. Artmed\nEditora; 2014. p. 108–9. \n\n\n31. Grimes DA, Schulz KF. Descriptive studies: What\nthey can and cannot do. The Lancet. 2002;359(9301):145–9. \n\n\n32. Fletcher RH, Fletcher SW, Fletcher GS. Risco:\nDa doença à exposição. In: Epidemiologia clínica: Elementos essenciais.\nArtmed Editora; 2014. p. 88. \n\n\n33. Grimes DA, Schulz KF. Compared to what? Finding\ncontrols for case-control studies. The Lancet. 2005;365(9468):1429–33.\n\n\n\n34. Ernster VL. Nested case-control studies.\nPreventive Medicine. 1994;23(5):587–90. \n\n\n35. Newman TB, Browner WS, Cummings SR, Hulley SB.\nDelineando estudos de caso-controle. In: Delineando a pesquisa clinica.\nQuarta Edição. Artmed Editora; 2015. p. 111. \n\n\n36. Grimes DA, Schulz KF. Cohort studies: Marching\ntowards outcomes. The Lancet. 2002;359(9303):341–5. \n\n\n37. Fletcher RH, Fletcher SW, Fletcher GS. Risco:\nDa doença à exposição. In: Epidemiologia clínica: Elementos essenciais.\nArtmed Editora; 2014. p. 68. \n\n\n38. Celentano DD, Szklo M. Cohort studies. In:\nGordis epidemiology. 6th Edition. Elsevier; 2019. p. 179. \n\n\n39. Kannel WB, McGee DL. Diabetes and\ncardiovascular risk factors: The framingham study. Circulation.\n1979;59(1):8–13. \n\n\n40. Coutinho M. Principios de epidemiologia clínica\naplicada a cardiologia. Arquivos Brasileiros de Cardiologia.\n1998;71:109–16. \n\n\n41. McCambridge J, Witton J, Elbourne DR.\nSystematic review of the hawthorne effect: New concepts are needed to\nstudy research participation effects. Journal of Clinical Epidemiology.\n2014;67(3):267–77. \n\n\n42. Bland JM, Altman DG. Statistic notes:\nRegression towards the mean. BMJ. 1994;308(6942):1499. \n\n\n43. Kabisch M, Ruckes C, Seibert-Grafe M, Blettner\nM. Randomized controlled trials: Part 17 of a series on evaluation of\nscientific publications. Deutsches Ärzteblatt\nInternational. 2011;108(39):663. \n\n\n44. Fletcher RH, Fletcher SW, Fletcher GS.\nTratamento. In: Epidemiologia clínica: Elementos essenciais. Artmed\nEditora; 2014. p. 143. \n\n\n45. Elander G, Hermerén G. Placebo effect and\nrandomized clinical trials. Theoretical Medicine. 1995;16(2):171–82.\n\n\n\n46. Schulz KF, Grimes DA. Blinding in randomised\ntrials: Hiding who got what. The Lancet. 2002;359(9307):696–700. \n\n\n47. Montori VM, Guyatt GH. Intention-to-treat\nprinciple. CMAJ. 2001;165(10):1339–41. \n\n\n48. Christensen E. Methodology of superiority vs.\nEquivalence trials and non-inferiority trials. Journal of hepatology.\n2007;46(5):947–54. \n\n\n49. Health Improvement O for, Disparities.\nCrossover randomised controlled trial: Comparative studies [Internet].\nOffice for Health Improvement and Disparities. UK Health improvement;\n2020. Available from: https://www.gov.uk/guidance/crossover-randomised-controlled-trial-comparative-studies\n\n\n50. Hennekens CH, Buring JE, et al. Lack of effect\nof long-term supplementation with beta carotene on the incidence of\nmalignant neoplasms and cardiovascular disease. New England Journal of\nMedicine. 1996;334(18):1145–9. \n\n\n51. Stanley K. Design of randomized controlled\ntrials. Circulation. 2007;115(9):1164–9. \n\n\n52. Chang W. Cookbook for r. Cookbook for R.\nhttp://www.cookbook-r.com; 2021. \n\n\n53. Verzani J. Using r for introductory statistics.\nChapman; Hall/CRC; 2004. \n\n\n54. Damiani A, Milz B, Lente C, al et. Ciência de\ndados em r [Internet]. R6 Consultoria; 2015. Available from: https://livro.curso-r.com/index.html\n\n\n55. Zuur AF, Ieno EN, Meesters EH. Getting data\ninto r. In: A beginner’s guide to r. Springer; 2009. p. 29–56. \n\n\n56. Wickham H, François R, Henry L, Müller K, et\nal. Dplyr: A grammar of data manipulation. R package version 04.\n2015;3:156. \n\n\n57. Wickham H, Grolemund G. 15 factors|r for data\nscience [Internet]. Welcome | R for Data Science. O’Reilly; 2017.\nAvailable from: https://r4ds.had.co.nz/factors.html\n\n\n58. Ooms J. Writexl: Export data frames to excel\n’xlsx’ format [Internet]. 2022. Available from: https://CRAN.R-project.org/package=writexl\n\n\n59. Team RC. Write.table: Data output/CSV files\n[Internet]. DataCamp; 2022. Available from: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/write.table\n\n\n60. Wickham H, Averick M, Bryan J, Chang W, et al.\nWelcome to the tidyverse. Journal of Open Source Software.\n2019;4(43):1686. \n\n\n61. Wickham H. Tidy data. Journal of Statistical\nSoftware. 2014;59(10):11–23. \n\n\n62. Wickham H, Girlich M. Tidyr: Tidy messy data\n[Internet]. 2022. Available from: https://CRAN.R-project.org/package=tidyr\n\n\n63. Fisher RA. The use of multiple measurements in\ntaxonomic problems. Annals of eugenics. 1936;7(2):179–88. \n\n\n64. Grolemund G, Wickham H. Dates and times made\neasy with lubridate. Journal of Statistical Software [Internet].\n2011;40(3):1–25. Available from: https://www.jstatsoft.org/v40/i03/\n\n\n65. Field A, Miles J, Field Z. Everithing you ever\nwanted to know about statistics (well, sort of). In: Discovering\nstatistics using r. Sage Publications, Ltd; 2012. p. 38. \n\n\n66. Oliveira Filho PF de. Tabelas. In:\nEpidemiologia e bioestatística-fundamentos para a leitura crítica. 2ª\nedição. Editora Rubio; 2022. p. 9–12. \n\n\n67. Arango HG. Organização dos dados em tabelas.\nIn: Bioestatística: Teórica e computacional. 3ª edição. Guanabara\nKoogan; 2009. p. 32–57. \n\n\n68. Gohel D, Skintzos P. Flextable: Functions for\ntabular reporting [Internet]. 2024. Available from: https://CRAN.R-project.org/package=flextable\n\n\n69. Arango HG. Números de classes e intervalo de\nclasses. In: Bioestatística teórica e computacional. Terceira edição.\nGuanabara Koogan, RJ; 2009. p. 35–40. \n\n\n70. Rasmussen KM, Yaktine AL, et al. Weight gain\nduring pregnancy: Reexamining the guidelines. 2009; \n\n\n71. Field A, Miles J, Field Z. Exploring data with\ngraphs. In: Discovering statistics using r. Sage Publications, Ltd;\n2012. p. 117. \n\n\n72. Wickham H. Getting started with ggplot2. In:\nggplot2. Second edition. Springer; 2016. p. 11–31. \n\n\n73. Tufte ER. Aesthetics and technique in data\ngraphical design. In: The visual display of quantitative information.\nSecond edition. Graphics Press; 2001. p. 178. \n\n\n74. Lemon J, Bolker B, Oom S, et al. Package\n“plotrix.” Vienna: R Development Core Team. 2015; \n\n\n75. Kabacoff RI. Basic graphs. In: R in action:\nData analysis and graphics with r. Manning Publications Co.; 2011. p.\n120–4. \n\n\n76. Harrell FE, Dupont C. Hmisc: Harrell\nmiscellaneous [Internet]. R package version. 2022. Available from: https://cran.r-project.org/web/packages/Hmisc/index.html\n\n\n77. Wickham H. A layered grammar of graphics.\nJournal of Computational and Graphical Statistics. 2010;19(1):3–28.\n\n\n\n78. Wickham H. ggplot2: Elegant graphics for data\nanalysis [Internet]. Springer-Verlag New York; 2016. Available from: https://ggplot2.tidyverse.org\n\n\n79. Rinker TW, Kurkiewicz D. Pacman: Package\nmanagement for r [Internet]. Buffalo, New York; 2018. Available from: http://github.com/trinker/pacman\n\n\n80. Debnath L, Basu K. A short history of\nprobability theory and its applications. International Journal of\nMathematical Education in Science and Technology. 2015;46(1):13–39.\n\n\n\n81. Menezes RX de. Introdução à probabilidade. In:\nMassad E, Menezes RX de, Silveira PSP, Ortega NRS, editors. Métodos\nquantitativos em medicina. Barueri, São Paulo: Editora Manole Ltda.;\n2004. p. 151–87. \n\n\n82. Pagano M, Kimberly G. Theoretical probability\ndistributions. In: Principles of biostatistics. Second Edition. CRC\nPress; 2000. p. 162. \n\n\n83. Gonzalez JCS. Normal distribution in r\n[Internet]. R CODER. 2021. Available from: https://r-coder.com/\n\n\n84. Robertson E, O’Connor J. Jacob (jacques)\nbernoulli [Internet]. Maths History. School of Mathematics; Statistics,\nUniversity of St Andrews; 2022. Available from: https://mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Jacob/\n\n\n85. Fisher LD, Van Belle G. Poisson random\nvariables. In: Biostatistics: A methodology for the health sciences. New\nYork, NY: John Wiley & Sons; 1993. p. 211–8. \n\n\n86. Peat J, Barton B. Descriptive statistics. In:\nMedical statistics : A guide to SPSS, data analysis, and critical\nappraisal. New York, NY: John Wiley & Sons; 2014. p. 24–51. \n\n\n87. Joanes D, Gill C. Comparing measures of sample\nskewness and kurtosis. Journal of the Royal Statistical Society.\n1998;47(1):183–9. \n\n\n88. George D, Mallery P. Descriptive statistics.\nIn: IBM SPSS statistics 26 step by step: A simple guide and reference.\nNew York, NY: Taylor & Francis Group; 2020. p. 114–20. \n\n\n89. Pagano M, Gavreau K. The central limit theorem.\nIn: Principles of biostatistics. Second Edition. Pacific Grove, CA:\nDuxbury; 2000. p. 197–8. \n\n\n90. Motulsky H. The theory of confidence intervals.\nIn: Intuitive biostatistics: A nonmathematical guide to statistical\nthinking. Second Edition. New York, NY: Oxford University Press; 2010.\np. 96–102. \n\n\n91. Signorell A et al. DescTools: Tools for\ndescriptive statistics [Internet]. 2022. Available from: https://cran.r-project.org/package=DescTools\n\n\n92. Kelen GD, Brown CB, Ashton J. Statistical\nreasoning in clinical trials: Hypothesis testing. Am J Emerg Med.\n1988;1(1):52–61. \n\n\n93. Menezes RX de, Burattini MN. Testes de hipótese\ne intervalos de confiança. In: Massad E, Menezes RX de, Silveira PSP,\nOrtega NRS, editors. Métodos quantitativos em medicina. Barueri, São\nPaulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n94. Guyatt G, Jaeschke R, Heddle N, et al. Basic\nstatistics for clinicians: 1. Hypothesis testing. CMAJ: Canadian Medical\nAssociation Journal. 1995;152(1):27. \n\n\n95. Fletcher RH, Fletcher SW, Fletcher GS. Acaso.\nIn: Epidemiologia clínica: Elementos essenciais. Quinta Edição. Artmed\nEditora; 2014. p. 108–9. \n\n\n96. Menezes RX de, Burattini MN. Testes de hipótese\ne intervalos de confiança. In: Massad E, Menezes RX de, Silveira PSP,\nOrtega NRS, editors. Métodos quantitativos em medicina. Barueri, São\nPaulo: Editora Manole Ltda.; 2004. p. 225–41. \n\n\n97. Pagano M, Kimberly G. Comparison of two means.\nIn: Principles of biostatistics. Second Edition. CRC Press; 2000. p.\n262–72. \n\n\n98. Zimmerman DW. A note on preliminary tests of\nequality of variances. Br J Math Stat Psychol. 2004;57(1):173–81. \n\n\n99. Razali NM, Wah YB, et al. Power comparisons of\nshapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests.\nJournal of statistical modeling and analytics. 2011;2(1):21–33. \n\n\n100. Ghasemi A, Zahediasl S. Normality tests for\nstatistical analysis: A guide for non-statisticians. International\njournal of endocrinology and metabolism. 2012;10(2):486. \n\n\n101. Yap BW, Sim CH. Comparisons of various types of\nnormality tests. Journal of Statistical Computation and Simulation.\n2011;81(12):2141–55. \n\n\n102. Fox J, Weisberg S. An r companion to applied\nregression [Internet]. Third. Thousand Oaks CA: Sage; 2019.\nAvailable from: https://socialsciences.mcmaster.ca/jfox/Books/Companion/\n\n\n103. Kassambara A. Rstatix: Pipe-friendly framework\nfor basic statistical tests [Internet]. 2022. Available from: https://CRAN.R-project.org/package=rstatix\n\n\n104. Cohen J. Statistical power analysis for the\nbehavioral sciences. 2nd Edition. Routledge; 1988. \n\n\n105. Lindenau JD, Guimaraes LSP. Calculating the\neffect size in SPSS. Revista HCPA [Internet]. 2012;32(3):363–81.\nAvailable from: https://seer.ufrgs.br/hcpa\n\n\n106. Field A, Miles J, Field Z. Comparing several\nmeans: ANOVA (GML 1). In: Discovering statistics using r. Sage\nPublications, Ltd; 2012. p. 399–400. \n\n\n107. Menezes RX de. Análise de variância. In: Massad\nE, Menezes RX de, Silveira PSP, Ortega NRS, editors. Métodos\nquantitativos em medicina. Barueri, São Paulo: Editora Manole Ltda.;\n2004. p. 297–300. \n\n\n108. Garren ST. Package fastgraph [Internet]. CRAN.\nComprehensive R Archive Network (CRAN); 1919. Available from: https://CRAN.R-project.org/package=fastGraph\n\n\n109. Peat J, Barton B. Continuous variables:\nAnalysis of variance. In: Medical statistics : A guide to SPSS, data\nanalysis, and critical appraisal. New York, NY: John Wiley & Sons;\n2014. p. 114. \n\n\n110. Dag O, Dolgun A, Konar NM. Onewaytests: An r\npackage for one-way tests in independent groups designs. R Journal.\n2018;10(1):175–99. \n\n\n111. Ben-Shachar MS, Lüdecke D, Makowski D.\nEffectsize: Estimation of effect size indices and standardized\nparameters. Journal of Open Source Software. 2020;5(56):2815. \n\n\n112. Watson P. Rules of thumb on magnitudes of\neffect sizes [Internet]. MRC Cognition and Brain Sciences Unit.\nCambridge University; 2021. Available from: https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize\n\n\n113. Field A, Miles J, Field Z. Factorial ANOVA\n(GLM3). In: Discovering statistics using r. Sage Publications, Ltd;\n2012. p. 513–4. \n\n\n114. Kassambara A. Ggpubr:’ggplot2’ based\npublication ready plots [r package ggpubr version 0.5.0] [Internet]. The\nComprehensive R Archive Network. Comprehensive R Archive Network (CRAN);\n2022. Available from: https://cloud.r-project.org/web/packages/ggpubr/index.html\n\n\n115. Patterson R, Coffman J, Goldstein-Greenwood J,\nOthers. Understanding diagnostic plots for linear regression analysis\n[Internet]. Research Data Services + Sciences. University of Virginia\nLibrary; 2015. Available from: https://data.library.virginia.edu/diagnostic-plots/\n\n\n116. Wickens TD, Keppel G. Two-way factorial\nexperiments. In: Design and analysis: A researcher’s handbook. Pearson\nPrentice-Hall; 2004. p. 193–286. \n\n\n117. Maxwell SE, Delaney HD, Kelley K. Two-way\nbetween-subject factorial designs. In: Designing experiments and\nanalyzing data: A model comparison perspective. Third Edition.\nRoutledge; 2017. p. 312–82. \n\n\n118. Lenth R, Singmann H, Love J, Buerkner P, Herve\nM. Emmeans: Estimated marginal means, aka least-squares means. R package\nversion. 2018;1(1):3. \n\n\n119. Rosenberg M. Society and the adolescent\nself-image. Princeton university press; 2015. \n\n\n120. Dini G, Quaresma M, Ferreira L, et al.\nAdaptação cultural e validação da versão brasileira da escala de\nautoestima de rosenberg. Revista Brasileira de Cirurgia Plástica.\n2001;19(1):41–52. \n\n\n121. Huynh H, Feldt LS. Estimation of the box\ncorrection for degrees of freedom from sample data in randomized block\nand split-plot designs. Journal of Educational Statistics.\n1976;1(1):69–82. \n\n\n122. Girden ER. Two-factor studies with repeated\nmeasures on both factors. In: ANOVA: Repeated measures. Sage; 1992. p.\n31–40. (QASS series; vol. 84). \n\n\n123. Muller KE, Barton CN. Approximate power for\nrepeated-measures ANOVA lacking sphericity. Journal of the American\nStatistical Association. 1989;84(406):549–55. \n\n\n124. Greene Jr JW, Touchstone JC. Urinary estriol as\nan index of placental function. A study of 279 cases. Obstetrical &\nGynecological Survey. 1963;18(3):356–9. \n\n\n125. Kassambara A. Correlation test between two\nvariables in r [Internet]. STHDA - Statistical tools for high-throughput\ndata analysis. 2021. Available from: http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r\n\n\n126. Schober P, Boer C, Schwarte LA. Correlation\ncoefficients: Appropriate use and interpretation. Anesthesia &\nAnalgesia. 2018;126(5):1763–8. \n\n\n127. De\nWinter JC, Gosling SD, Potter J. Comparing the pearson and spearman\ncorrelation coefficients across distributions and sample sizes: A\ntutorial using simulations and empirical data. Psychological methods.\n2016;21(3):273. \n\n\n128. Sedgwick P. Correlation versus linear\nregression. BMJ. 2013;346. \n\n\n129. Kim H-Y. Statistical notes for clinical\nresearchers: Simple linear regression 3–residual analysis. Restorative\ndentistry & endodontics. 2019;44(1). \n\n\n130. Field A, Miles J, Field Z. Regression. In:\nDiscovering statistics using r. Sage Publications, Ltd; 2012. p. 266–76.\n\n\n\n131. Peat J, Barton B. Correlation and regression.\nIn: Medical statistics : A guide to SPSS, data analysis, and critical\nappraisal. New York, NY: John Wiley & Sons; 2014. p. 209. \n\n\n132. Altman DG, Gardner MJ. Statistics in medicine:\nCalculating confidence intervals for regression and correlation. British\nMedical Journal (Clinical research ed). 1988;296(6631):1238. \n\n\n133. Altman DG. Comparing groups: Categorical data.\nIn: Practical statistics for medical research. London: Chapman &\nHall/CRC; 1991. p. 244–7. \n\n\n134. Myszkowski N. Nhstplot package [Internet].\nRDocumentation. 2020. Available from: https://rdocumentation.org/packages/nhstplot/versions/1.1.0\n\n\n135. Warnes GR, Bolker B, et al. Gmodels: Various r\nprogramming tools for model fitting [Internet]. CRAN R Project. 2022.\nAvailable from: https://rdrr.io/cran/gmodels/\n\n\n136. Comtois D. Summarytools: Tools to quickly and\nneatly summarize data [Internet]. CRAN R Project. 2022. Available\nfrom:\nhttps://github.com/dcomtois/summarytools\n\n\n137. Daniel WW, Cross CL. The chi-square\ndistribution and analysis of frequencies. In: Practical statistics for\nmedical research. Hoboken, NJ: John Wiley & Sons, Inc; 2013. p.\n604–19. \n\n\n138. Eliasziw M, Donner A. Application of the\nMcNemar test to non-independent matched pair data. Statistics in\nmedicine. 1991;10(12):1981–91. \n\n\n139. Rosner B. Hypothesis testing: Categorical data.\nIn: Fundamentals of biostatistics. Seventh Edition. Boston: Cengage;\n2011. p. 377. \n\n\n140. Altman DG. Comparing groups: Continuos data.\nIn: Practical statistics for medical research. London: Chapman &\nHall/CRC; 1991. p. 194–7. \n\n\n141. Hothorn T, Hornik K, Van De Wiel MA, Zeileis A.\nA lego system for conditional inference. The American Statistician.\n2006;60(3):257–63. \n\n\n142. Zar JH. Paired-sample hypotheses. In:\nBiostatistical analysis. Edinburgh: Pearson; 2014. p. 189–98. \n\n\n143. Karadimitriou SM, Marshall E. Kruskal-wallis in\nr [Internet]. Statistics Support for Students. Loughborough; Coventry\nUniversities; 2020. Available from: https://www.statstutor.ac.uk/\n\n\n144. Zar JH. Nonparametric analysis of variance. In:\nBiostatistical analysis. Edinburgh: Pearson; 2014. p. 226–30. \n\n\n145. Kanji GK. The kruskal–wallis test. In: 100\nstatiscal tests. 3rd Edition. London: Sage publications; 2006. p. 220.\n\n\n\n146. Tomczak M, Tomczak E. The need to report effect\nsize estimates revisited. An overview of some recommended measures of\neffect size. Trends in sport sciences. 2014;1(21):19–25. \n\n\n147. Dunn OJ. Multiple comparisons using rank sums.\nTechnometrics. 1964;6(3):241–52. \n\n\n148. Straus SE, Glasziou P, et al. Diagnosis and\nscreening. In: Evidence-based medicine: How to practice and teach EBM.\nFifth Edition. Edinburgh: Elsevier; 2019. p. 185–218. \n\n\n149. Altman DG, Bland JM. Diagnostic tests. 1:\nSensitivity and specificity. BMJ: British Medical Journal.\n1994;308(6943):1552. \n\n\n150. Peixoto R de O, Nunes TA, Gomes CA. Indices\ndiagnósticos da ultrassonografia abdominal na apendicite aguda:\nInfluência do genero e constituição física, tempo evolutivo da doença e\nexperiencia do radiologista. Revista do Colégio Brasileiro de\nCirurgiões. 2011;38:105–11. \n\n\n151. Altman DG, Bland JM. Diagnostic tests 2:\nPredictive values. Bmj. 1994;309(6947):102. \n\n\n152. Pereira Lima A, Vieira FJ, Oliveira GP de M, et\nal. Perfil clinico-epidemiologico da apendicite aguda: Analise\nretrospectiva de 638 casos. Revista do Colegio Brasileiro de Cirurgiões.\n2016;43:248–53. \n\n\n153. Halkin A, Reichman J, Schwaber M, Paltiel O,\nBrezis M. Likelihood ratios: Getting diagnostic testing into\nperspective. QJM: monthly journal of the Association of Physicians.\n1998;91(4):247–58. \n\n\n154. Deeks JJ, Altman DG. Diagnostic tests 4:\nLikelihood ratios. Bmj. 2004;329(7458):168–9. \n\n\n155. Guyatt G, Rennie D, et al. Diagnostic tests.\nIn: User’s guides to medical literature: A manual for evidence-based\nclinical practice. 3rd Edition. New York: JAMA; 2015. p. 607–31. \n\n\n156. Oliveira Filho PF de. Testes diagnósticos. In:\nEpidemiologia e bioestatística: Fundamentos para a leitura crítica.\nSegunda Edição. Rio de Janeiro: Editora Rubio; 2022. p. 89–105. \n\n\n157. Caraguel CG, Vanderstichel R. The two-step\nfagan’s nomogram: Ad hoc interpretation of a diagnostic test result\nwithout calculation. BMJ Evidence-Based Medicine. 2013;18(4):125–8.\n\n\n\n158. Fagan T. Nomogram for bayes’s theorem. New\nEngland Journal of Medicine. 1975;293:257. \n\n\n159. Altman DG, Bland JM. Diagnostic tests 3:\nReceiver operating characteristic plots. BMJ: British Medical Journal.\n1994;309(6948):188. \n\n\n160. Robin X, Turck N, Hainard A, et al. pROC: An\nopen-source package for r and s+ to analyze and compare ROC curves. BMC\nBioinformatics. 2011;12(1):1–8. \n\n\n161. Borges LSR. Diagnostic accuracy measures in\ncardiovascular research. Int J Cardiovasc Sci. 2016;29(3):218–22. \n\n\n162. DeLong ER, DeLong DM, Clarke-Pearson DL.\nComparing the areas under two or more correlated receiver operating\ncharacteristic curves: A nonparametric approach. Biometrics.\n1988;44:837–45. \n\n\n163. Youden WJ. Index for rating diagnostic tests.\nCancer. 1950;3(1):32–5. \n\n\n164. Cohen J. A coefficient of agreement for nominal\nscales. Educational and Psychological Measurement. 1960;20(1):37–46.\n\n\n\n165. Landis JR, Koch GG. The measurement of observer\nagreement for categorical data. Biometrics. 1977;159–74. \n\n\n166. Zeileis A, Meyer D, Hornik K. Residual-based\nshadings for visualizing (conditional) independence. Journal of\nComputational and Graphical Statistics. 2007;16(3):507–25. \n\n\n167. Szklo M, Nieto FJ. Measuring disease\noccurrence. In: Epidemiology: Beyond the basics. Fourth Edition.\nBurlington, MA: Jones & Bartlett Learning; 2019. p. 80–1. \n\n\n168. Mark, Sergeant E, et al. epiR: Tools for the\nanalysis of epidemiological data [Internet]. 2022. Available from: https://CRAN.R-project.org/package=epiR\n\n\n169. Feychting M, Osterlund B, Ahlbom A. Reduced\ncancer incidence among the blind. Epidemiology. 1998;9(5):490–4. \n\n\n170. Gross M. Oswego county revisited. Public health\nreports. 1976;91(2):168. \n\n\n171. Aragon TJ. Epitools: Epidemiology tools\n[Internet]. 2020. Available from: https://CRAN.R-project.org/package=epitools\n\n\n172. Szklo M, Nieto FJ. Measuring associations\nbetween exposures and outcomes. In: Epidemiology: Beyond the basics.\nFourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p.\n88–94. \n\n\n173. Davies HTO, Crombie IK, Tavakoli M. When can\nodds ratios mislead? BMJ. 1998;316(7136):989–91. \n\n\n174. Hopkins WG. A scale of magnitudes for effect\nstatistics [Internet]. A New View of Statistics. Sportscience; 2016.\nAvailable from: http://www.sportsci.org/resource/stats/index.html\n\n\n175. Szklo M, Nieto FJ. Measuring associations\nbetween exposures and outcomes. In: Epidemiology: Beyond the basics.\nFourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p.\n84–102. \n\n\n176. Madi JM, Souza R da S de, Araujo BF de,\nOliveira Filho PF, et al. Prevalence of toxoplasmosis, HIV, syphilis and\nrubella in a population of puerperal women using whatman\n903 filter paper. The Brazilian Journal of Infectious\nDiseases. 2010;14(1):24–9. \n\n\n177. Szklo M, Nieto FJ. Measuring associations\nbetween exposures and outcomes. In: Epidemiology: Beyond the basics.\nFourth Edition. Burlington, MA: Jones & Bartlett Learning; 2019. p.\n97–8. \n\n\n178. Physicians’ Health Study Research Group* SC of\nthe. Final report on the aspirin component of the ongoing physicians’\nhealth study. New England Journal of Medicine. 1989;321(3):129–35.\n\n\n\n179. Kohl M. Package “MKmisc”\n[Internet]. 2019. Available from: https://github.com/stamats/MKmisc\n\n\n180. Bender R. Calculating confidence intervals for\nthe number needed to treat. Controlled clinical trials.\n2001;22(2):102–10. \n\n\n181. Clark TG, Bradburn MJ, Love SB, Altman DG.\nSurvival analysis part i: Basic concepts and first analyses. British\nJournal of Cancer. 2003;89(2):232–8. \n\n\n182. Kaplan EL, Meier P. Nonparametric estimation\nfrom incomplete observations. Journal of the American Statistical\nAssociation. 1958;53(282):457–81. \n\n\n183. Peat J, Barton B. Survival analyses. In:\nMedical statistics : A guide to SPSS, data analysis, and critical\nappraisal. New York, NY: John Wiley & Sons; 2014. p. 352–3. \n\n\n184. Qiu W, Chavarro J, Lazarus R, Rosner B, Ma J.\npowerSurvEpi: Power and sample size calculation for survival analysis of\nepidemiological studies. R package version 00. 2015;9. \n\n\n185. Therneau T et al. A package for survival\nanalysis in r. R package version. 2015;2(7). \n\n\n186. Kassambara A, Kosinski M, Biecek P, Fabian S.\nSurvminer: Drawing survival curves using ggplot2. URL https://CRAN\nR-project org/package= survminer R package version 04. 2021;9. \n\n\n187. Bland JM, Altman DG. The logrank test. BMJ.\n2004;328(7447):1073. \n\n\n188. Cox DR. The regression analysis of binary\nsequences. Journal of the Royal Statistical Society Series B:\nStatistical Methodology. 1958;20(2):215–32. \n\n\n189. Peat J, Barton B. Adjusted odds ratios. In:\nMedical statistics : A guide to SPSS, data analysis, and critical\nappraisal. New York, NY: John Wiley & Sons; 2014. p. 298–308. \n\n\n190. Hair JF, Black WC, Babin BJ, Anderson RE,\nTatham RL. Regressão logística: Regressão com uma variável dependente\nbinária. In: Análise multivariada de dados. 6a Edição. bookman; 2009. p.\n283–302. \n\n\n191. Verzani J. Extensions to linear model. In:\nUsing r for introductory statistics. Second Edition. Chapman &\nHall/CRC; 2014. p. 440–8. \n\n\n192. Stoltzfus JC. Logistic regression: A brief\nprimer. Academic emergency medicine. 2011;18(10):1099–104. \n\n\n193. Wikipédia. RMS titanic —\nwikipédia, a enciclopédia livre [Internet]. 2025. Available\nfrom: https://pt.wikipedia.org/w/index.php?title=RMS_Titanic&oldid=69869298\n\n\n194. Prabhakaran S. Missing value treatment\n[Internet]. datascienceplus. 2016. Available from: https://datascienceplus.com/missing-value-treatment/\n\n\n195. Van Buuren S, Groothuis-Oudshoorn K. Mice:\nMultivariate imputation by chained equations in r. Journal of\nstatistical software. 2011;45:1–67. \n\n\n196. Subramanian J, Simon R. Overfitting in\nprediction models–is it a problem only in high dimensions? Contemporary\nclinical trials. 2013;36(2):636–41. \n\n\n197. Field A, Miles J, Field Z. Logistic regression.\nIn: Discovering statistics using r. Sage Publications, Ltd; 2012. p.\n320. \n\n\n198. Lüdecke D. sjPlot: Data visualization for\nstatistics in social science [Internet]. 2024. Available from: https://CRAN.R-project.org/package=sjPlot\n\n\n199. Lüdecke D. Ggeffects: Tidy data frames\nof marginal effects from regression models. Journal of Open Source\nSoftware. 2018;3(26):772. \n\n\n200. Hosmer Jr DW, Lemeshow S, Sturdivant RX.\nInterpretation of fitted logistic regression model. In: Applied logistic\nregression. Third Edition. John Wiley & Sons; 2013. p. 49–64. \n\n\n201. McFadden D. Conditional logit analysis of\nqualitative choice behavior. In: Zarembka P, editor. Frontiers in\neconometrics. Academic Press; 1974. p. 105-142-142. \n\n\n202. Cox DR, Snell EJ. Analysis of binary data.\nSecond Edition. Chapman; Hall/CRC; 1989. \n\n\n203. Nagelkerke NJ et al. A note on a general\ndefinition of the coefficient of determination. Biometrika.\n1991;78(3):691–2. \n\n\n204. McFadden D. Quantitative methods for analysing\ntravel behaviour of individuals: Some recent developments. In:\nBehavioural travel modelling. Routledge; 2021. p. 279–318. \n\n\n205. Bobbitt Z. How to calculate standardized\nresiduals in r [Internet]. Statology. 2020. Available from: https://www.statology.org/standardized-residuals-in-r/\n\n\n206. Field A, Miles J, Field Z. Outliers and\ninfluential cases. In: Discovering statistics using r. Sage\nPublications, Ltd; 2012. p. 288–92. \n\n\n207. Hoaglin DC, Welsch RE. The hat matrix in\nregression and ANOVA. The American Statistician. 1978;32(1):17–22.\n\n\n\n208. Stevens JP. Outliers and influential data\npoints in regression analysis. Psychological bulletin. 1984;95(2):334.\n\n\n\n209. Meyer D, Dimitriadou E, Hornik K, Weingessel A,\nLeisch F, Chang C-C, et al. Package “e1071.” The R Journal.\n2019;1–67.",
    "crumbs": [
      "References"
    ]
  }
]